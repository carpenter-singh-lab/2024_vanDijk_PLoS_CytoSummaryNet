{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0e0135f",
      "metadata": {
        "id": "a0e0135f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Training stuff\n",
        "from pytorch_metric_learning import losses, distances\n",
        "from networks.SimpleMLPs import MLPsumV2\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Evaluation stuff\n",
        "import pandas as pd\n",
        "import utils\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cceeeff9",
      "metadata": {
        "id": "cceeeff9"
      },
      "outputs": [],
      "source": [
        "def get_correlated_dataset(n, dependency, mu, input_dim):\n",
        "    latent = np.random.randn(n, input_dim)\n",
        "    dependent = latent.dot(dependency)\n",
        "    scaled = dependent\n",
        "    scaled_with_offset = scaled + mu\n",
        "    # return x and y of the new, correlated dataset\n",
        "    return scaled_with_offset[:, 0], scaled_with_offset[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2a746885",
      "metadata": {
        "id": "2a746885",
        "outputId": "c74dbfe4-7357-4c84-9027-fa295e0c838e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x600 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "import scipy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def generate_batched_data(num_samples, skewness, nr_replicates):\n",
        "    data = []\n",
        "    for _ in range(nr_replicates):\n",
        "        samples = np.random.standard_normal(num_samples)\n",
        "        samples = samples + skewness * (samples ** 3)\n",
        "        data.append(samples)\n",
        "\n",
        "    # Convert data to a numpy array and reshape for batching\n",
        "    batch = np.array(data)\n",
        "\n",
        "    distribution_means = np.mean(batch, axis=1)\n",
        "    distribution_stds = np.std(batch, axis=1)\n",
        "\n",
        "    norm_batch = (batch - distribution_means[:, np.newaxis]) / distribution_stds[:, np.newaxis]\n",
        "\n",
        "    return norm_batch[:, :, np.newaxis]\n",
        "\n",
        "\n",
        "# Parameters\n",
        "num_samples = 800\n",
        "num_distributions = 10\n",
        "skewness_values = np.linspace(-1, 1, num_distributions)  # Varying skewness values\n",
        "nr_replicates = 4  # Number of samples per class or distribution\n",
        "\n",
        "# Create a 2x5 grid of plots\n",
        "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, skewness in enumerate(skewness_values):\n",
        "    # Generate skewed data with varying skewness values and different points\n",
        "    skewed_data = generate_batched_data(num_samples, skewness, nr_replicates)\n",
        "\n",
        "    # Plot the distribution\n",
        "    axes[i].hist(skewed_data[0], bins=30, density=True, alpha=0.7)\n",
        "    axes[i].set_title(f\"Class label: {i}\\nSkewness: {scipy.stats.skew(skewed_data[0])[0]}\\nMean: {np.mean(skewed_data)}\\nStd: {np.std(skewed_data)}\")\n",
        "    axes[i].set_xlabel(\"Value\")\n",
        "    axes[i].set_ylabel(\"Density\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a 2x5 grid of plots\n",
        "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Create a table to display statistics\n",
        "stat_table_data = []\n",
        "\n",
        "for i, skewness in enumerate(skewness_values):\n",
        "    # Generate skewed data with varying skewness values and different points\n",
        "    skewed_data = generate_batched_data(num_samples, skewness, nr_replicates)\n",
        "\n",
        "    # Plot the distribution\n",
        "    axes[i].hist(skewed_data[0], bins=30, density=True, alpha=0.7, color='blue', edgecolor='black')\n",
        "    axes[i].set_title(f\"Class {i+1}\")\n",
        "    axes[i].grid(False)\n",
        "\n",
        "    # Collect statistics for the table\n",
        "    skewness_value = scipy.stats.skew(skewed_data[0]).round(2)\n",
        "    mean_value = np.mean(skewed_data).round(2)\n",
        "    std_value = np.std(skewed_data).round(2)\n",
        "    stat_table_data.append([skewness_value, mean_value, std_value])\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "qCkhzQG95LzW",
        "outputId": "bf2566c8-d9a6-453b-9428-78acff24ec5a"
      },
      "id": "qCkhzQG95LzW",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x600 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "table_rows = []\n",
        "for i, (skewness, mean, std) in enumerate(stat_table_data):\n",
        "    table_rows.append([i+1, f'{skewness[0]}', f'{mean}', f'{std}'])\n",
        "\n",
        "# Create a table on the side for statistics\n",
        "stat_table = plt.table(cellText=table_rows,\n",
        "                       cellLoc='center',\n",
        "                       colLabels=['Class', 'Skewness', 'Mean', 'Std'],\n",
        "                       )\n",
        "stat_table.auto_set_font_size(False)\n",
        "stat_table.set_fontsize(12)\n",
        "stat_table.scale(1, 2)  # Adjust the scaling as needed\n",
        "\n",
        "plt.gca().add_table(stat_table)\n",
        "plt.axis('off')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        },
        "id": "DITNOJfwAUg3",
        "outputId": "f361b3b0-26db-4e39-eb5a-f8315d728f25"
      },
      "id": "DITNOJfwAUg3",
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 1.0, 0.0, 1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 71
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NvgNOvmX7rCf"
      },
      "id": "NvgNOvmX7rCf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2cfab3e",
      "metadata": {
        "id": "c2cfab3e",
        "outputId": "ace76465-809e-42f1-a361-6949ad12ab9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nr. train samples: 6 nr. val samples: 4 data shape: (4, 800, 1)\n"
          ]
        }
      ],
      "source": [
        "nr_replicates = 4\n",
        "num_samples = 800\n",
        "num_distributions = 10\n",
        "skewness_values = np.linspace(-1, 1, num_distributions)  # Varying skewness values\n",
        "\n",
        "train_samples = []\n",
        "val_samples = []\n",
        "\n",
        "for i, skewness in enumerate(skewness_values):\n",
        "    data = generate_batched_data(num_samples, skewness, nr_replicates)\n",
        "\n",
        "    if i < 6:\n",
        "        train_samples.append({'data':data,\n",
        "         'label': [i]*nr_replicates})\n",
        "    else:\n",
        "        val_samples.append({'data': data,\n",
        "             'label': [i]*nr_replicates})\n",
        "\n",
        "print(\"nr. train samples:\", len(train_samples), \"nr. val samples:\", len(val_samples), \"data shape:\", data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bdf6d16",
      "metadata": {
        "id": "1bdf6d16"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Shuffle the training samples\n",
        "import random\n",
        "random.shuffle(train_samples)\n",
        "\n",
        "# Batch size\n",
        "bs = 24\n",
        "\n",
        "# Lists to store data and labels for each batch\n",
        "batched_data = []\n",
        "batched_labels = []\n",
        "\n",
        "for idx in range(0, len(train_samples), bs):\n",
        "    batch_data = []\n",
        "    batch_label = []\n",
        "\n",
        "    # Collect data and labels for the current batch\n",
        "    for sample in train_samples[idx:idx + bs]:\n",
        "        batch_data.append(torch.tensor(sample['data'], dtype=torch.float32))\n",
        "        batch_label.append(torch.tensor(sample['label'], dtype=torch.int64))\n",
        "\n",
        "    batched_data.append(torch.cat(batch_data))\n",
        "    batched_labels.append(torch.cat(batch_label))\n",
        "\n",
        "# Concatenate batched data and labels\n",
        "batched_data = torch.cat(batched_data)\n",
        "batched_labels = torch.cat(batched_labels)\n",
        "\n",
        "# Create a DataLoader for batching\n",
        "train_dataset = TensorDataset(batched_data, batched_labels)\n",
        "train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2ab5f0d",
      "metadata": {
        "id": "b2ab5f0d",
        "outputId": "377536f0-a58b-437c-85ed-ccd9065309b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0. Training loss: 0.7775054574012756. Validation loss: 2.7070956230163574.\n",
            "Epoch 1. Training loss: 0.777502715587616. Validation loss: 2.707095146179199.\n",
            "Epoch 2. Training loss: 0.7774999141693115. Validation loss: 2.70709490776062.\n",
            "Epoch 3. Training loss: 0.7774972319602966. Validation loss: 2.707094669342041.\n",
            "Epoch 4. Training loss: 0.777494490146637. Validation loss: 2.707094192504883.\n",
            "Epoch 5. Training loss: 0.7774917483329773. Validation loss: 2.7070939540863037.\n",
            "Epoch 6. Training loss: 0.7774888873100281. Validation loss: 2.7070937156677246.\n",
            "Epoch 7. Training loss: 0.7774860858917236. Validation loss: 2.7070932388305664.\n",
            "Epoch 8. Training loss: 0.7774832844734192. Validation loss: 2.7070932388305664.\n",
            "Epoch 9. Training loss: 0.7774805426597595. Validation loss: 2.707092761993408.\n",
            "Epoch 10. Training loss: 0.7774777412414551. Validation loss: 2.707092523574829.\n",
            "Epoch 11. Training loss: 0.7774749398231506. Validation loss: 2.70709228515625.\n",
            "Epoch 12. Training loss: 0.7774722576141357. Validation loss: 2.707092046737671.\n",
            "Epoch 13. Training loss: 0.7774694561958313. Validation loss: 2.707091808319092.\n",
            "Epoch 14. Training loss: 0.7774667739868164. Validation loss: 2.7070913314819336.\n",
            "Epoch 15. Training loss: 0.7774639129638672. Validation loss: 2.7070913314819336.\n",
            "Epoch 16. Training loss: 0.7774611115455627. Validation loss: 2.7070910930633545.\n",
            "Epoch 17. Training loss: 0.7774582505226135. Validation loss: 2.7070908546447754.\n",
            "Epoch 18. Training loss: 0.7774555683135986. Validation loss: 2.707090377807617.\n",
            "Epoch 19. Training loss: 0.7774527668952942. Validation loss: 2.707090377807617.\n",
            "Epoch 20. Training loss: 0.7774500250816345. Validation loss: 2.707089900970459.\n",
            "Epoch 21. Training loss: 0.7774472236633301. Validation loss: 2.707089424133301.\n",
            "Epoch 22. Training loss: 0.7774443626403809. Validation loss: 2.7070889472961426.\n",
            "Epoch 23. Training loss: 0.777441680431366. Validation loss: 2.7070889472961426.\n",
            "Epoch 24. Training loss: 0.7774388790130615. Validation loss: 2.7070884704589844.\n",
            "Epoch 25. Training loss: 0.7774361968040466. Validation loss: 2.7070884704589844.\n",
            "Epoch 26. Training loss: 0.7774333357810974. Validation loss: 2.707087755203247.\n",
            "Epoch 27. Training loss: 0.777430534362793. Validation loss: 2.707087516784668.\n",
            "Epoch 28. Training loss: 0.7774277329444885. Validation loss: 2.707087516784668.\n",
            "Epoch 29. Training loss: 0.7774249911308289. Validation loss: 2.707087278366089.\n",
            "Epoch 30. Training loss: 0.7774222493171692. Validation loss: 2.7070868015289307.\n",
            "Epoch 31. Training loss: 0.77741938829422. Validation loss: 2.7070865631103516.\n",
            "Epoch 32. Training loss: 0.7774167060852051. Validation loss: 2.7070860862731934.\n",
            "Epoch 33. Training loss: 0.7774138450622559. Validation loss: 2.7070860862731934.\n",
            "Epoch 34. Training loss: 0.777411162853241. Validation loss: 2.707085609436035.\n",
            "Epoch 35. Training loss: 0.7774083614349365. Validation loss: 2.707085609436035.\n",
            "Epoch 36. Training loss: 0.7774055004119873. Validation loss: 2.707084894180298.\n",
            "Epoch 37. Training loss: 0.7774026989936829. Validation loss: 2.7070846557617188.\n",
            "Epoch 38. Training loss: 0.7773998379707336. Validation loss: 2.7070841789245605.\n",
            "Epoch 39. Training loss: 0.777397096157074. Validation loss: 2.7070841789245605.\n",
            "Epoch 40. Training loss: 0.7773943543434143. Validation loss: 2.7070841789245605.\n",
            "Epoch 41. Training loss: 0.7773916125297546. Validation loss: 2.7070839405059814.\n",
            "Epoch 42. Training loss: 0.7773888111114502. Validation loss: 2.707082986831665.\n",
            "Epoch 43. Training loss: 0.7773858904838562. Validation loss: 2.707082986831665.\n",
            "Epoch 44. Training loss: 0.7773831486701965. Validation loss: 2.707082748413086.\n",
            "Epoch 45. Training loss: 0.7773804068565369. Validation loss: 2.707082509994507.\n",
            "Epoch 46. Training loss: 0.7773775458335876. Validation loss: 2.7070820331573486.\n",
            "Epoch 47. Training loss: 0.7773747444152832. Validation loss: 2.7070815563201904.\n",
            "Epoch 48. Training loss: 0.7773719429969788. Validation loss: 2.7070813179016113.\n",
            "Epoch 49. Training loss: 0.7773692011833191. Validation loss: 2.7070813179016113.\n",
            "Epoch 50. Training loss: 0.7773664593696594. Validation loss: 2.707080841064453.\n",
            "Epoch 51. Training loss: 0.7773635387420654. Validation loss: 2.707080602645874.\n",
            "Epoch 52. Training loss: 0.777360737323761. Validation loss: 2.707080364227295.\n",
            "Epoch 53. Training loss: 0.7773578763008118. Validation loss: 2.707080125808716.\n",
            "Epoch 54. Training loss: 0.7773551344871521. Validation loss: 2.7070798873901367.\n",
            "Epoch 55. Training loss: 0.7773523330688477. Validation loss: 2.7070794105529785.\n",
            "Epoch 56. Training loss: 0.7773496508598328. Validation loss: 2.7070794105529785.\n",
            "Epoch 57. Training loss: 0.7773467898368835. Validation loss: 2.7070789337158203.\n",
            "Epoch 58. Training loss: 0.7773439288139343. Validation loss: 2.707078218460083.\n",
            "Epoch 59. Training loss: 0.7773411273956299. Validation loss: 2.707078218460083.\n",
            "Epoch 60. Training loss: 0.7773383259773254. Validation loss: 2.707077980041504.\n",
            "Epoch 61. Training loss: 0.7773354649543762. Validation loss: 2.707077741622925.\n",
            "Epoch 62. Training loss: 0.7773327827453613. Validation loss: 2.7070772647857666.\n",
            "Epoch 63. Training loss: 0.7773299217224121. Validation loss: 2.7070772647857666.\n",
            "Epoch 64. Training loss: 0.7773270606994629. Validation loss: 2.7070767879486084.\n",
            "Epoch 65. Training loss: 0.777324378490448. Validation loss: 2.7070765495300293.\n",
            "Epoch 66. Training loss: 0.7773215174674988. Validation loss: 2.707076072692871.\n",
            "Epoch 67. Training loss: 0.7773186564445496. Validation loss: 2.707075834274292.\n",
            "Epoch 68. Training loss: 0.7773159146308899. Validation loss: 2.707075595855713.\n",
            "Epoch 69. Training loss: 0.7773130536079407. Validation loss: 2.707075357437134.\n",
            "Epoch 70. Training loss: 0.7773101925849915. Validation loss: 2.7070751190185547.\n",
            "Epoch 71. Training loss: 0.7773074507713318. Validation loss: 2.7070748805999756.\n",
            "Epoch 72. Training loss: 0.7773045897483826. Validation loss: 2.7070746421813965.\n",
            "Epoch 73. Training loss: 0.7773018479347229. Validation loss: 2.7070741653442383.\n",
            "Epoch 74. Training loss: 0.7772991061210632. Validation loss: 2.7070741653442383.\n",
            "Epoch 75. Training loss: 0.7772960662841797. Validation loss: 2.70707368850708.\n",
            "Epoch 76. Training loss: 0.7772932648658752. Validation loss: 2.7070729732513428.\n",
            "Epoch 77. Training loss: 0.7772905230522156. Validation loss: 2.7070729732513428.\n",
            "Epoch 78. Training loss: 0.7772877216339111. Validation loss: 2.7070724964141846.\n",
            "Epoch 79. Training loss: 0.7772848010063171. Validation loss: 2.7070724964141846.\n",
            "Epoch 80. Training loss: 0.7772820591926575. Validation loss: 2.7070720195770264.\n",
            "Epoch 81. Training loss: 0.7772791981697083. Validation loss: 2.7070717811584473.\n",
            "Epoch 82. Training loss: 0.7772764563560486. Validation loss: 2.707071542739868.\n",
            "Epoch 83. Training loss: 0.7772734761238098. Validation loss: 2.707071304321289.\n",
            "Epoch 84. Training loss: 0.7772707939147949. Validation loss: 2.707070827484131.\n",
            "Epoch 85. Training loss: 0.7772677540779114. Validation loss: 2.7070703506469727.\n",
            "Epoch 86. Training loss: 0.7772651314735413. Validation loss: 2.7070703506469727.\n",
            "Epoch 87. Training loss: 0.7772622108459473. Validation loss: 2.7070701122283936.\n",
            "Epoch 88. Training loss: 0.7772594094276428. Validation loss: 2.7070698738098145.\n",
            "Epoch 89. Training loss: 0.7772565484046936. Validation loss: 2.7070696353912354.\n",
            "Epoch 90. Training loss: 0.7772538661956787. Validation loss: 2.7070693969726562.\n",
            "Epoch 91. Training loss: 0.7772509455680847. Validation loss: 2.707068920135498.\n",
            "Epoch 92. Training loss: 0.7772481441497803. Validation loss: 2.707068920135498.\n",
            "Epoch 93. Training loss: 0.7772452235221863. Validation loss: 2.7070682048797607.\n",
            "Epoch 94. Training loss: 0.7772424817085266. Validation loss: 2.7070682048797607.\n",
            "Epoch 95. Training loss: 0.7772396206855774. Validation loss: 2.7070677280426025.\n",
            "Epoch 96. Training loss: 0.7772367596626282. Validation loss: 2.7070674896240234.\n",
            "Epoch 97. Training loss: 0.7772338390350342. Validation loss: 2.7070672512054443.\n",
            "Epoch 98. Training loss: 0.7772310376167297. Validation loss: 2.707066535949707.\n",
            "Epoch 99. Training loss: 0.7772281765937805. Validation loss: 2.707066535949707.\n",
            "Epoch 100. Training loss: 0.7772253155708313. Validation loss: 2.707066059112549.\n",
            "Epoch 101. Training loss: 0.7772225737571716. Validation loss: 2.7070658206939697.\n",
            "Epoch 102. Training loss: 0.7772195935249329. Validation loss: 2.7070655822753906.\n",
            "Epoch 103. Training loss: 0.777216911315918. Validation loss: 2.7070653438568115.\n",
            "Epoch 104. Training loss: 0.7772140502929688. Validation loss: 2.7070651054382324.\n",
            "Epoch 105. Training loss: 0.7772111892700195. Validation loss: 2.7070648670196533.\n",
            "Epoch 106. Training loss: 0.7772083282470703. Validation loss: 2.707064628601074.\n",
            "Epoch 107. Training loss: 0.7772054672241211. Validation loss: 2.707064390182495.\n",
            "Epoch 108. Training loss: 0.7772026658058167. Validation loss: 2.707064151763916.\n",
            "Epoch 109. Training loss: 0.7771998047828674. Validation loss: 2.707063674926758.\n",
            "Epoch 110. Training loss: 0.7771970629692078. Validation loss: 2.7070631980895996.\n",
            "Epoch 111. Training loss: 0.7771942019462585. Validation loss: 2.7070631980895996.\n",
            "Epoch 112. Training loss: 0.7771912217140198. Validation loss: 2.7070624828338623.\n",
            "Epoch 113. Training loss: 0.7771883606910706. Validation loss: 2.7070627212524414.\n",
            "Epoch 114. Training loss: 0.7771856188774109. Validation loss: 2.707062244415283.\n",
            "Epoch 115. Training loss: 0.7771826386451721. Validation loss: 2.707062005996704.\n",
            "Epoch 116. Training loss: 0.7771798968315125. Validation loss: 2.707061529159546.\n",
            "Epoch 117. Training loss: 0.7771770358085632. Validation loss: 2.707061290740967.\n",
            "Epoch 118. Training loss: 0.7771742343902588. Validation loss: 2.7070608139038086.\n",
            "Epoch 119. Training loss: 0.7771713733673096. Validation loss: 2.7070605754852295.\n",
            "Epoch 120. Training loss: 0.7771685123443604. Validation loss: 2.7070603370666504.\n",
            "Epoch 121. Training loss: 0.7771657109260559. Validation loss: 2.7070600986480713.\n",
            "Epoch 122. Training loss: 0.7771628499031067. Validation loss: 2.707059621810913.\n",
            "Epoch 123. Training loss: 0.7771599888801575. Validation loss: 2.707059383392334.\n",
            "Epoch 124. Training loss: 0.7771570682525635. Validation loss: 2.707059383392334.\n",
            "Epoch 125. Training loss: 0.7771542072296143. Validation loss: 2.707058906555176.\n",
            "Epoch 126. Training loss: 0.7771512866020203. Validation loss: 2.7070586681365967.\n",
            "Epoch 127. Training loss: 0.7771485447883606. Validation loss: 2.7070584297180176.\n",
            "Epoch 128. Training loss: 0.7771456241607666. Validation loss: 2.7070579528808594.\n",
            "Epoch 129. Training loss: 0.7771428227424622. Validation loss: 2.7070579528808594.\n",
            "Epoch 130. Training loss: 0.7771399617195129. Validation loss: 2.707057476043701.\n",
            "Epoch 131. Training loss: 0.7771369814872742. Validation loss: 2.707057237625122.\n",
            "Epoch 132. Training loss: 0.7771341800689697. Validation loss: 2.707056999206543.\n",
            "Epoch 133. Training loss: 0.7771313190460205. Validation loss: 2.707056760787964.\n",
            "Epoch 134. Training loss: 0.7771285176277161. Validation loss: 2.7070560455322266.\n",
            "Epoch 135. Training loss: 0.7771255970001221. Validation loss: 2.7070558071136475.\n",
            "Epoch 136. Training loss: 0.7771227955818176. Validation loss: 2.7070553302764893.\n",
            "Epoch 137. Training loss: 0.7771199345588684. Validation loss: 2.7070553302764893.\n",
            "Epoch 138. Training loss: 0.7771171927452087. Validation loss: 2.70705509185791.\n",
            "Epoch 139. Training loss: 0.77711421251297. Validation loss: 2.707054853439331.\n",
            "Epoch 140. Training loss: 0.777111291885376. Validation loss: 2.7070541381835938.\n",
            "Epoch 141. Training loss: 0.7771084904670715. Validation loss: 2.7070541381835938.\n",
            "Epoch 142. Training loss: 0.7771055698394775. Validation loss: 2.7070536613464355.\n",
            "Epoch 143. Training loss: 0.7771027088165283. Validation loss: 2.7070534229278564.\n",
            "Epoch 144. Training loss: 0.7770998477935791. Validation loss: 2.7070531845092773.\n",
            "Epoch 145. Training loss: 0.7770969867706299. Validation loss: 2.7070529460906982.\n",
            "Epoch 146. Training loss: 0.7770941853523254. Validation loss: 2.7070529460906982.\n",
            "Epoch 147. Training loss: 0.7770912647247314. Validation loss: 2.707052230834961.\n",
            "Epoch 148. Training loss: 0.7770883440971375. Validation loss: 2.707052230834961.\n",
            "Epoch 149. Training loss: 0.777085542678833. Validation loss: 2.7070517539978027.\n",
            "Epoch 150. Training loss: 0.777082622051239. Validation loss: 2.7070517539978027.\n",
            "Epoch 151. Training loss: 0.7770798206329346. Validation loss: 2.7070512771606445.\n",
            "Epoch 152. Training loss: 0.7770769000053406. Validation loss: 2.7070508003234863.\n",
            "Epoch 153. Training loss: 0.7770740985870361. Validation loss: 2.7070510387420654.\n",
            "Epoch 154. Training loss: 0.7770711779594421. Validation loss: 2.707050323486328.\n",
            "Epoch 155. Training loss: 0.7770683169364929. Validation loss: 2.70704984664917.\n",
            "Epoch 156. Training loss: 0.7770654559135437. Validation loss: 2.70704984664917.\n",
            "Epoch 157. Training loss: 0.7770624756813049. Validation loss: 2.7070493698120117.\n",
            "Epoch 158. Training loss: 0.7770597338676453. Validation loss: 2.7070491313934326.\n",
            "Epoch 159. Training loss: 0.7770566940307617. Validation loss: 2.7070486545562744.\n",
            "Epoch 160. Training loss: 0.7770537734031677. Validation loss: 2.7070486545562744.\n",
            "Epoch 161. Training loss: 0.7770511507987976. Validation loss: 2.707048177719116.\n",
            "Epoch 162. Training loss: 0.7770484089851379. Validation loss: 2.707047700881958.\n",
            "Epoch 163. Training loss: 0.7770457863807678. Validation loss: 2.707047462463379.\n",
            "Epoch 164. Training loss: 0.7770431041717529. Validation loss: 2.7070472240448.\n",
            "Epoch 165. Training loss: 0.7770403027534485. Validation loss: 2.7070472240448.\n",
            "Epoch 166. Training loss: 0.7770376801490784. Validation loss: 2.7070469856262207.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 167. Training loss: 0.7770349383354187. Validation loss: 2.7070462703704834.\n",
            "Epoch 168. Training loss: 0.7770323753356934. Validation loss: 2.7070460319519043.\n",
            "Epoch 169. Training loss: 0.7770295143127441. Validation loss: 2.707045555114746.\n",
            "Epoch 170. Training loss: 0.7770268321037292. Validation loss: 2.707045316696167.\n",
            "Epoch 171. Training loss: 0.7770242094993591. Validation loss: 2.707044839859009.\n",
            "Epoch 172. Training loss: 0.7770214080810547. Validation loss: 2.7070446014404297.\n",
            "Epoch 173. Training loss: 0.7770187854766846. Validation loss: 2.7070441246032715.\n",
            "Epoch 174. Training loss: 0.7770159840583801. Validation loss: 2.7070438861846924.\n",
            "Epoch 175. Training loss: 0.7770133018493652. Validation loss: 2.707043409347534.\n",
            "Epoch 176. Training loss: 0.7770106792449951. Validation loss: 2.707043170928955.\n",
            "Epoch 177. Training loss: 0.7770079970359802. Validation loss: 2.707042694091797.\n",
            "Epoch 178. Training loss: 0.7770052552223206. Validation loss: 2.7070422172546387.\n",
            "Epoch 179. Training loss: 0.7770025730133057. Validation loss: 2.7070419788360596.\n",
            "Epoch 180. Training loss: 0.7769998908042908. Validation loss: 2.7070417404174805.\n",
            "Epoch 181. Training loss: 0.7769971489906311. Validation loss: 2.7070415019989014.\n",
            "Epoch 182. Training loss: 0.7769944667816162. Validation loss: 2.707041025161743.\n",
            "Epoch 183. Training loss: 0.7769916653633118. Validation loss: 2.707040786743164.\n",
            "Epoch 184. Training loss: 0.7769891619682312. Validation loss: 2.707040309906006.\n",
            "Epoch 185. Training loss: 0.7769863605499268. Validation loss: 2.7070398330688477.\n",
            "Epoch 186. Training loss: 0.7769835591316223. Validation loss: 2.7070398330688477.\n",
            "Epoch 187. Training loss: 0.7769808769226074. Validation loss: 2.7070391178131104.\n",
            "Epoch 188. Training loss: 0.7769782543182373. Validation loss: 2.7070388793945312.\n",
            "Epoch 189. Training loss: 0.7769754528999329. Validation loss: 2.707038402557373.\n",
            "Epoch 190. Training loss: 0.7769728302955627. Validation loss: 2.707038164138794.\n",
            "Epoch 191. Training loss: 0.7769701480865479. Validation loss: 2.7070374488830566.\n",
            "Epoch 192. Training loss: 0.7769672870635986. Validation loss: 2.7070374488830566.\n",
            "Epoch 193. Training loss: 0.7769646048545837. Validation loss: 2.7070369720458984.\n",
            "Epoch 194. Training loss: 0.7769619822502136. Validation loss: 2.7070367336273193.\n",
            "Epoch 195. Training loss: 0.776959240436554. Validation loss: 2.707036256790161.\n",
            "Epoch 196. Training loss: 0.7769565582275391. Validation loss: 2.707036256790161.\n",
            "Epoch 197. Training loss: 0.7769537568092346. Validation loss: 2.707035541534424.\n",
            "Epoch 198. Training loss: 0.7769511342048645. Validation loss: 2.7070353031158447.\n",
            "Epoch 199. Training loss: 0.7769483923912048. Validation loss: 2.7070350646972656.\n",
            "Epoch 200. Training loss: 0.7769456505775452. Validation loss: 2.7070345878601074.\n",
            "Epoch 201. Training loss: 0.7769429683685303. Validation loss: 2.707034111022949.\n",
            "Epoch 202. Training loss: 0.776940107345581. Validation loss: 2.707033634185791.\n",
            "Epoch 203. Training loss: 0.7769375443458557. Validation loss: 2.707033634185791.\n",
            "Epoch 204. Training loss: 0.7769348621368408. Validation loss: 2.707033157348633.\n",
            "Epoch 205. Training loss: 0.7769321799278259. Validation loss: 2.7070326805114746.\n",
            "Epoch 206. Training loss: 0.7769293785095215. Validation loss: 2.7070322036743164.\n",
            "Epoch 207. Training loss: 0.7769266963005066. Validation loss: 2.7070322036743164.\n",
            "Epoch 208. Training loss: 0.7769238948822021. Validation loss: 2.707031726837158.\n",
            "Epoch 209. Training loss: 0.7769212126731873. Validation loss: 2.70703125.\n",
            "Epoch 210. Training loss: 0.7769185900688171. Validation loss: 2.707030773162842.\n",
            "Epoch 211. Training loss: 0.7769157886505127. Validation loss: 2.7070302963256836.\n",
            "Epoch 212. Training loss: 0.7769131064414978. Validation loss: 2.7070302963256836.\n",
            "Epoch 213. Training loss: 0.7769103646278381. Validation loss: 2.7070295810699463.\n",
            "Epoch 214. Training loss: 0.7769076824188232. Validation loss: 2.7070295810699463.\n",
            "Epoch 215. Training loss: 0.776904821395874. Validation loss: 2.707029104232788.\n",
            "Epoch 216. Training loss: 0.7769021987915039. Validation loss: 2.70702862739563.\n",
            "Epoch 217. Training loss: 0.7768993973731995. Validation loss: 2.707028388977051.\n",
            "Epoch 218. Training loss: 0.7768967151641846. Validation loss: 2.7070281505584717.\n",
            "Epoch 219. Training loss: 0.7768940329551697. Validation loss: 2.7070276737213135.\n",
            "Epoch 220. Training loss: 0.77689129114151. Validation loss: 2.7070274353027344.\n",
            "Epoch 221. Training loss: 0.7768885493278503. Validation loss: 2.707026958465576.\n",
            "Epoch 222. Training loss: 0.7768859267234802. Validation loss: 2.707026958465576.\n",
            "Epoch 223. Training loss: 0.7768831253051758. Validation loss: 2.707026481628418.\n",
            "Epoch 224. Training loss: 0.7768804430961609. Validation loss: 2.7070260047912598.\n",
            "Epoch 225. Training loss: 0.7768776416778564. Validation loss: 2.7070255279541016.\n",
            "Epoch 226. Training loss: 0.7768749594688416. Validation loss: 2.7070255279541016.\n",
            "Epoch 227. Training loss: 0.7768721580505371. Validation loss: 2.7070250511169434.\n",
            "Epoch 228. Training loss: 0.776869535446167. Validation loss: 2.707024574279785.\n",
            "Epoch 229. Training loss: 0.7768667340278625. Validation loss: 2.707024097442627.\n",
            "Epoch 230. Training loss: 0.7768639922142029. Validation loss: 2.707024097442627.\n",
            "Epoch 231. Training loss: 0.7768612504005432. Validation loss: 2.7070233821868896.\n",
            "Epoch 232. Training loss: 0.7768585681915283. Validation loss: 2.7070231437683105.\n",
            "Epoch 233. Training loss: 0.7768557667732239. Validation loss: 2.7070231437683105.\n",
            "Epoch 234. Training loss: 0.7768530249595642. Validation loss: 2.7070226669311523.\n",
            "Epoch 235. Training loss: 0.7768502831459045. Validation loss: 2.707022190093994.\n",
            "Epoch 236. Training loss: 0.7768476009368896. Validation loss: 2.707021713256836.\n",
            "Epoch 237. Training loss: 0.7768449187278748. Validation loss: 2.7070212364196777.\n",
            "Epoch 238. Training loss: 0.7768421173095703. Validation loss: 2.7070212364196777.\n",
            "Epoch 239. Training loss: 0.7768394351005554. Validation loss: 2.7070207595825195.\n",
            "Epoch 240. Training loss: 0.776836633682251. Validation loss: 2.7070207595825195.\n",
            "Epoch 241. Training loss: 0.7768338322639465. Validation loss: 2.707019805908203.\n",
            "Epoch 242. Training loss: 0.7768311500549316. Validation loss: 2.707019567489624.\n",
            "Epoch 243. Training loss: 0.7768284678459167. Validation loss: 2.707019090652466.\n",
            "Epoch 244. Training loss: 0.7768257260322571. Validation loss: 2.7070188522338867.\n",
            "Epoch 245. Training loss: 0.7768229842185974. Validation loss: 2.7070188522338867.\n",
            "Epoch 246. Training loss: 0.7768203616142273. Validation loss: 2.7070186138153076.\n",
            "Epoch 247. Training loss: 0.7768175005912781. Validation loss: 2.7070178985595703.\n",
            "Epoch 248. Training loss: 0.7768146395683289. Validation loss: 2.707017421722412.\n",
            "Epoch 249. Training loss: 0.7768118381500244. Validation loss: 2.707017183303833.\n",
            "Epoch 250. Training loss: 0.77680903673172. Validation loss: 2.707016706466675.\n",
            "Epoch 251. Training loss: 0.7768061757087708. Validation loss: 2.7070164680480957.\n",
            "Epoch 252. Training loss: 0.7768034338951111. Validation loss: 2.7070162296295166.\n",
            "Epoch 253. Training loss: 0.7768006324768066. Validation loss: 2.7070159912109375.\n",
            "Epoch 254. Training loss: 0.7767977714538574. Validation loss: 2.7070157527923584.\n",
            "Epoch 255. Training loss: 0.7767949104309082. Validation loss: 2.707014799118042.\n",
            "Epoch 256. Training loss: 0.7767921090126038. Validation loss: 2.707014799118042.\n",
            "Epoch 257. Training loss: 0.7767893671989441. Validation loss: 2.707014322280884.\n",
            "Epoch 258. Training loss: 0.7767865061759949. Validation loss: 2.7070140838623047.\n",
            "Epoch 259. Training loss: 0.7767836451530457. Validation loss: 2.7070136070251465.\n",
            "Epoch 260. Training loss: 0.7767808437347412. Validation loss: 2.7070133686065674.\n",
            "Epoch 261. Training loss: 0.776777982711792. Validation loss: 2.7070131301879883.\n",
            "Epoch 262. Training loss: 0.7767751216888428. Validation loss: 2.707012891769409.\n",
            "Epoch 263. Training loss: 0.7767723202705383. Validation loss: 2.707012176513672.\n",
            "Epoch 264. Training loss: 0.7767694592475891. Validation loss: 2.7070119380950928.\n",
            "Epoch 265. Training loss: 0.7767665982246399. Validation loss: 2.7070114612579346.\n",
            "Epoch 266. Training loss: 0.7767636775970459. Validation loss: 2.7070114612579346.\n",
            "Epoch 267. Training loss: 0.7767608761787415. Validation loss: 2.7070109844207764.\n",
            "Epoch 268. Training loss: 0.7767581343650818. Validation loss: 2.707010269165039.\n",
            "Epoch 269. Training loss: 0.7767552733421326. Validation loss: 2.707010269165039.\n",
            "Epoch 270. Training loss: 0.7767524123191833. Validation loss: 2.7070095539093018.\n",
            "Epoch 271. Training loss: 0.7767495512962341. Validation loss: 2.7070093154907227.\n",
            "Epoch 272. Training loss: 0.7767466902732849. Validation loss: 2.7070090770721436.\n",
            "Epoch 273. Training loss: 0.7767438888549805. Validation loss: 2.7070086002349854.\n",
            "Epoch 274. Training loss: 0.7767409682273865. Validation loss: 2.707008123397827.\n",
            "Epoch 275. Training loss: 0.776738166809082. Validation loss: 2.70700740814209.\n",
            "Epoch 276. Training loss: 0.7767353057861328. Validation loss: 2.707007646560669.\n",
            "Epoch 277. Training loss: 0.7767324447631836. Validation loss: 2.70700740814209.\n",
            "Epoch 278. Training loss: 0.7767295837402344. Validation loss: 2.7070069313049316.\n",
            "Epoch 279. Training loss: 0.7767267227172852. Validation loss: 2.7070064544677734.\n",
            "Epoch 280. Training loss: 0.7767239212989807. Validation loss: 2.7070062160491943.\n",
            "Epoch 281. Training loss: 0.7767210602760315. Validation loss: 2.707005739212036.\n",
            "Epoch 282. Training loss: 0.7767181396484375. Validation loss: 2.707005500793457.\n",
            "Epoch 283. Training loss: 0.7767153382301331. Validation loss: 2.707005023956299.\n",
            "Epoch 284. Training loss: 0.7767124772071838. Validation loss: 2.7070045471191406.\n",
            "Epoch 285. Training loss: 0.7767096161842346. Validation loss: 2.7070040702819824.\n",
            "Epoch 286. Training loss: 0.7767067551612854. Validation loss: 2.7070038318634033.\n",
            "Epoch 287. Training loss: 0.7767038941383362. Validation loss: 2.707003593444824.\n",
            "Epoch 288. Training loss: 0.776701033115387. Validation loss: 2.707003355026245.\n",
            "Epoch 289. Training loss: 0.7766982913017273. Validation loss: 2.707002878189087.\n",
            "Epoch 290. Training loss: 0.7766953110694885. Validation loss: 2.707002639770508.\n",
            "Epoch 291. Training loss: 0.7766924500465393. Validation loss: 2.7070021629333496.\n",
            "Epoch 292. Training loss: 0.7766897082328796. Validation loss: 2.7070021629333496.\n",
            "Epoch 293. Training loss: 0.7766867280006409. Validation loss: 2.707001209259033.\n",
            "Epoch 294. Training loss: 0.7766839861869812. Validation loss: 2.707000970840454.\n",
            "Epoch 295. Training loss: 0.776681125164032. Validation loss: 2.707000732421875.\n",
            "Epoch 296. Training loss: 0.7766782641410828. Validation loss: 2.707000494003296.\n",
            "Epoch 297. Training loss: 0.7766754031181335. Validation loss: 2.707000255584717.\n",
            "Epoch 298. Training loss: 0.7766724228858948. Validation loss: 2.7069997787475586.\n",
            "Epoch 299. Training loss: 0.7766696810722351. Validation loss: 2.7069993019104004.\n",
            "Epoch 300. Training loss: 0.7766668200492859. Validation loss: 2.7069990634918213.\n",
            "Epoch 301. Training loss: 0.7766639590263367. Validation loss: 2.706998586654663.\n",
            "Epoch 302. Training loss: 0.7766610980033875. Validation loss: 2.706998109817505.\n",
            "Epoch 303. Training loss: 0.7766582369804382. Validation loss: 2.706998109817505.\n",
            "Epoch 304. Training loss: 0.776655375957489. Validation loss: 2.7069976329803467.\n",
            "Epoch 305. Training loss: 0.7766525149345398. Validation loss: 2.7069969177246094.\n",
            "Epoch 306. Training loss: 0.776649534702301. Validation loss: 2.7069969177246094.\n",
            "Epoch 307. Training loss: 0.7766467928886414. Validation loss: 2.706996440887451.\n",
            "Epoch 308. Training loss: 0.7766438126564026. Validation loss: 2.706995964050293.\n",
            "Epoch 309. Training loss: 0.7766409516334534. Validation loss: 2.706995725631714.\n",
            "Epoch 310. Training loss: 0.7766382098197937. Validation loss: 2.7069952487945557.\n",
            "Epoch 311. Training loss: 0.7766352295875549. Validation loss: 2.7069950103759766.\n",
            "Epoch 312. Training loss: 0.7766324877738953. Validation loss: 2.7069945335388184.\n",
            "Epoch 313. Training loss: 0.776629626750946. Validation loss: 2.7069942951202393.\n",
            "Epoch 314. Training loss: 0.7766267657279968. Validation loss: 2.70699405670166.\n",
            "Epoch 315. Training loss: 0.7766239047050476. Validation loss: 2.706993579864502.\n",
            "Epoch 316. Training loss: 0.7766209244728088. Validation loss: 2.7069931030273438.\n",
            "Epoch 317. Training loss: 0.7766180634498596. Validation loss: 2.7069926261901855.\n",
            "Epoch 318. Training loss: 0.7766152024269104. Validation loss: 2.7069926261901855.\n",
            "Epoch 319. Training loss: 0.7766123414039612. Validation loss: 2.7069923877716064.\n",
            "Epoch 320. Training loss: 0.7766094207763672. Validation loss: 2.7069921493530273.\n",
            "Epoch 321. Training loss: 0.776606559753418. Validation loss: 2.706991195678711.\n",
            "Epoch 322. Training loss: 0.7766037583351135. Validation loss: 2.706990957260132.\n",
            "Epoch 323. Training loss: 0.7766008377075195. Validation loss: 2.7069907188415527.\n",
            "Epoch 324. Training loss: 0.7765979766845703. Validation loss: 2.7069904804229736.\n",
            "Epoch 325. Training loss: 0.7765951752662659. Validation loss: 2.7069900035858154.\n",
            "Epoch 326. Training loss: 0.7765922546386719. Validation loss: 2.7069895267486572.\n",
            "Epoch 327. Training loss: 0.7765893340110779. Validation loss: 2.706989288330078.\n",
            "Epoch 328. Training loss: 0.7765865325927734. Validation loss: 2.70698881149292.\n",
            "Epoch 329. Training loss: 0.7765836119651794. Validation loss: 2.7069883346557617.\n",
            "Epoch 330. Training loss: 0.7765806317329407. Validation loss: 2.7069880962371826.\n",
            "Epoch 331. Training loss: 0.776577889919281. Validation loss: 2.7069876194000244.\n",
            "Epoch 332. Training loss: 0.7765749096870422. Validation loss: 2.7069876194000244.\n",
            "Epoch 333. Training loss: 0.776572048664093. Validation loss: 2.706986904144287.\n",
            "Epoch 334. Training loss: 0.7765693068504333. Validation loss: 2.706986904144287.\n",
            "Epoch 335. Training loss: 0.7765663266181946. Validation loss: 2.7069859504699707.\n",
            "Epoch 336. Training loss: 0.7765634655952454. Validation loss: 2.7069859504699707.\n",
            "Epoch 337. Training loss: 0.7765606045722961. Validation loss: 2.7069854736328125.\n",
            "Epoch 338. Training loss: 0.7765577435493469. Validation loss: 2.7069849967956543.\n",
            "Epoch 339. Training loss: 0.7765548229217529. Validation loss: 2.7069849967956543.\n",
            "Epoch 340. Training loss: 0.7765519618988037. Validation loss: 2.706984281539917.\n",
            "Epoch 341. Training loss: 0.7765490412712097. Validation loss: 2.706984281539917.\n",
            "Epoch 342. Training loss: 0.7765462398529053. Validation loss: 2.7069835662841797.\n",
            "Epoch 343. Training loss: 0.7765433192253113. Validation loss: 2.7069835662841797.\n",
            "Epoch 344. Training loss: 0.7765403389930725. Validation loss: 2.7069833278656006.\n",
            "Epoch 345. Training loss: 0.7765374779701233. Validation loss: 2.7069826126098633.\n",
            "Epoch 346. Training loss: 0.7765347361564636. Validation loss: 2.706981897354126.\n",
            "Epoch 347. Training loss: 0.7765317559242249. Validation loss: 2.706981897354126.\n",
            "Epoch 348. Training loss: 0.7765288352966309. Validation loss: 2.706981658935547.\n",
            "Epoch 349. Training loss: 0.7765259742736816. Validation loss: 2.7069811820983887.\n",
            "Epoch 350. Training loss: 0.7765231132507324. Validation loss: 2.7069807052612305.\n",
            "Epoch 351. Training loss: 0.7765201926231384. Validation loss: 2.7069802284240723.\n",
            "Epoch 352. Training loss: 0.776517391204834. Validation loss: 2.706979990005493.\n",
            "Epoch 353. Training loss: 0.7765143513679504. Validation loss: 2.706979751586914.\n",
            "Epoch 354. Training loss: 0.7765114903450012. Validation loss: 2.706979513168335.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 355. Training loss: 0.7765085697174072. Validation loss: 2.706979274749756.\n",
            "Epoch 356. Training loss: 0.7765057682991028. Validation loss: 2.7069787979125977.\n",
            "Epoch 357. Training loss: 0.7765028476715088. Validation loss: 2.7069783210754395.\n",
            "Epoch 358. Training loss: 0.7764999270439148. Validation loss: 2.706977605819702.\n",
            "Epoch 359. Training loss: 0.7764971256256104. Validation loss: 2.706977605819702.\n",
            "Epoch 360. Training loss: 0.7764942646026611. Validation loss: 2.706977367401123.\n",
            "Epoch 361. Training loss: 0.7764912247657776. Validation loss: 2.706976890563965.\n",
            "Epoch 362. Training loss: 0.7764884829521179. Validation loss: 2.7069766521453857.\n",
            "Epoch 363. Training loss: 0.7764855027198792. Validation loss: 2.7069761753082275.\n",
            "Epoch 364. Training loss: 0.7764825820922852. Validation loss: 2.7069759368896484.\n",
            "Epoch 365. Training loss: 0.7764796614646912. Validation loss: 2.7069754600524902.\n",
            "Epoch 366. Training loss: 0.7764768600463867. Validation loss: 2.706974983215332.\n",
            "Epoch 367. Training loss: 0.7764738202095032. Validation loss: 2.706974506378174.\n",
            "Epoch 368. Training loss: 0.7764710783958435. Validation loss: 2.706974506378174.\n",
            "Epoch 369. Training loss: 0.7764680981636047. Validation loss: 2.7069737911224365.\n",
            "Epoch 370. Training loss: 0.7764651775360107. Validation loss: 2.7069735527038574.\n",
            "Epoch 371. Training loss: 0.7764623165130615. Validation loss: 2.7069733142852783.\n",
            "Epoch 372. Training loss: 0.7764594554901123. Validation loss: 2.706973075866699.\n",
            "Epoch 373. Training loss: 0.7764565348625183. Validation loss: 2.706972122192383.\n",
            "Epoch 374. Training loss: 0.7764536738395691. Validation loss: 2.706972122192383.\n",
            "Epoch 375. Training loss: 0.7764506936073303. Validation loss: 2.7069718837738037.\n",
            "Epoch 376. Training loss: 0.7764477729797363. Validation loss: 2.7069711685180664.\n",
            "Epoch 377. Training loss: 0.7764448523521423. Validation loss: 2.7069711685180664.\n",
            "Epoch 378. Training loss: 0.7764420509338379. Validation loss: 2.706970691680908.\n",
            "Epoch 379. Training loss: 0.7764391303062439. Validation loss: 2.706970453262329.\n",
            "Epoch 380. Training loss: 0.7764361500740051. Validation loss: 2.706969738006592.\n",
            "Epoch 381. Training loss: 0.7764332890510559. Validation loss: 2.706969738006592.\n",
            "Epoch 382. Training loss: 0.7764303684234619. Validation loss: 2.7069692611694336.\n",
            "Epoch 383. Training loss: 0.7764274477958679. Validation loss: 2.7069687843322754.\n",
            "Epoch 384. Training loss: 0.7764246463775635. Validation loss: 2.7069685459136963.\n",
            "Epoch 385. Training loss: 0.7764216065406799. Validation loss: 2.706968069076538.\n",
            "Epoch 386. Training loss: 0.7764187455177307. Validation loss: 2.706968069076538.\n",
            "Epoch 387. Training loss: 0.7764158844947815. Validation loss: 2.70696759223938.\n",
            "Epoch 388. Training loss: 0.7764129638671875. Validation loss: 2.7069668769836426.\n",
            "Epoch 389. Training loss: 0.776409924030304. Validation loss: 2.7069668769836426.\n",
            "Epoch 390. Training loss: 0.7764070630073547. Validation loss: 2.7069664001464844.\n",
            "Epoch 391. Training loss: 0.7764042019844055. Validation loss: 2.706965923309326.\n",
            "Epoch 392. Training loss: 0.7764013409614563. Validation loss: 2.706965684890747.\n",
            "Epoch 393. Training loss: 0.7763984203338623. Validation loss: 2.706965208053589.\n",
            "Epoch 394. Training loss: 0.7763954997062683. Validation loss: 2.7069649696350098.\n",
            "Epoch 395. Training loss: 0.7763925194740295. Validation loss: 2.7069644927978516.\n",
            "Epoch 396. Training loss: 0.7763895988464355. Validation loss: 2.7069642543792725.\n",
            "Epoch 397. Training loss: 0.7763867378234863. Validation loss: 2.706963539123535.\n",
            "Epoch 398. Training loss: 0.7763838171958923. Validation loss: 2.706963539123535.\n",
            "Epoch 399. Training loss: 0.7763808369636536. Validation loss: 2.706963062286377.\n",
            "Epoch 400. Training loss: 0.7763779759407043. Validation loss: 2.7069625854492188.\n",
            "Epoch 401. Training loss: 0.7763750553131104. Validation loss: 2.7069625854492188.\n",
            "Epoch 402. Training loss: 0.7763721346855164. Validation loss: 2.7069616317749023.\n",
            "Epoch 403. Training loss: 0.7763692736625671. Validation loss: 2.7069616317749023.\n",
            "Epoch 404. Training loss: 0.7763664126396179. Validation loss: 2.706960916519165.\n",
            "Epoch 405. Training loss: 0.7763635516166687. Validation loss: 2.706960916519165.\n",
            "Epoch 406. Training loss: 0.7763605117797852. Validation loss: 2.706960439682007.\n",
            "Epoch 407. Training loss: 0.7763574719429016. Validation loss: 2.7069599628448486.\n",
            "Epoch 408. Training loss: 0.7763546109199524. Validation loss: 2.7069597244262695.\n",
            "Epoch 409. Training loss: 0.7763516902923584. Validation loss: 2.7069592475891113.\n",
            "Epoch 410. Training loss: 0.7763487696647644. Validation loss: 2.7069590091705322.\n",
            "Epoch 411. Training loss: 0.7763459086418152. Validation loss: 2.706958293914795.\n",
            "Epoch 412. Training loss: 0.776343047618866. Validation loss: 2.706958055496216.\n",
            "Epoch 413. Training loss: 0.7763400673866272. Validation loss: 2.7069578170776367.\n",
            "Epoch 414. Training loss: 0.7763371467590332. Validation loss: 2.7069573402404785.\n",
            "Epoch 415. Training loss: 0.7763342261314392. Validation loss: 2.7069573402404785.\n",
            "Epoch 416. Training loss: 0.7763312458992004. Validation loss: 2.7069568634033203.\n",
            "Epoch 417. Training loss: 0.7763283848762512. Validation loss: 2.706956386566162.\n",
            "Epoch 418. Training loss: 0.7763254046440125. Validation loss: 2.706955909729004.\n",
            "Epoch 419. Training loss: 0.7763225436210632. Validation loss: 2.706955671310425.\n",
            "Epoch 420. Training loss: 0.7763195037841797. Validation loss: 2.7069554328918457.\n",
            "Epoch 421. Training loss: 0.7763166427612305. Validation loss: 2.7069549560546875.\n",
            "Epoch 422. Training loss: 0.7763137221336365. Validation loss: 2.7069549560546875.\n",
            "Epoch 423. Training loss: 0.7763107419013977. Validation loss: 2.706954002380371.\n",
            "Epoch 424. Training loss: 0.7763078808784485. Validation loss: 2.706954002380371.\n",
            "Epoch 425. Training loss: 0.7763049006462097. Validation loss: 2.706953525543213.\n",
            "Epoch 426. Training loss: 0.7763020396232605. Validation loss: 2.7069530487060547.\n",
            "Epoch 427. Training loss: 0.7762991786003113. Validation loss: 2.7069528102874756.\n",
            "Epoch 428. Training loss: 0.7762961387634277. Validation loss: 2.7069520950317383.\n",
            "Epoch 429. Training loss: 0.7762932181358337. Validation loss: 2.7069520950317383.\n",
            "Epoch 430. Training loss: 0.776290237903595. Validation loss: 2.70695161819458.\n",
            "Epoch 431. Training loss: 0.7762873768806458. Validation loss: 2.706951141357422.\n",
            "Epoch 432. Training loss: 0.7762844562530518. Validation loss: 2.706951141357422.\n",
            "Epoch 433. Training loss: 0.7762814164161682. Validation loss: 2.7069504261016846.\n",
            "Epoch 434. Training loss: 0.7762784957885742. Validation loss: 2.7069499492645264.\n",
            "Epoch 435. Training loss: 0.7762755751609802. Validation loss: 2.7069499492645264.\n",
            "Epoch 436. Training loss: 0.776272714138031. Validation loss: 2.706949234008789.\n",
            "Epoch 437. Training loss: 0.7762697339057922. Validation loss: 2.70694899559021.\n",
            "Epoch 438. Training loss: 0.7762668132781982. Validation loss: 2.7069482803344727.\n",
            "Epoch 439. Training loss: 0.7762638926506042. Validation loss: 2.7069480419158936.\n",
            "Epoch 440. Training loss: 0.7762608528137207. Validation loss: 2.7069475650787354.\n",
            "Epoch 441. Training loss: 0.7762579321861267. Validation loss: 2.7069473266601562.\n",
            "Epoch 442. Training loss: 0.7762550711631775. Validation loss: 2.706947088241577.\n",
            "Epoch 443. Training loss: 0.776252031326294. Validation loss: 2.706947088241577.\n",
            "Epoch 444. Training loss: 0.7762491703033447. Validation loss: 2.70694637298584.\n",
            "Epoch 445. Training loss: 0.7762461304664612. Validation loss: 2.7069458961486816.\n",
            "Epoch 446. Training loss: 0.7762432098388672. Validation loss: 2.7069458961486816.\n",
            "Epoch 447. Training loss: 0.7762402892112732. Validation loss: 2.7069451808929443.\n",
            "Epoch 448. Training loss: 0.776237428188324. Validation loss: 2.7069449424743652.\n",
            "Epoch 449. Training loss: 0.7762344479560852. Validation loss: 2.706944465637207.\n",
            "Epoch 450. Training loss: 0.7762314677238464. Validation loss: 2.706944227218628.\n",
            "Epoch 451. Training loss: 0.7762284874916077. Validation loss: 2.706943988800049.\n",
            "Epoch 452. Training loss: 0.7762255668640137. Validation loss: 2.7069435119628906.\n",
            "Epoch 453. Training loss: 0.7762226462364197. Validation loss: 2.7069430351257324.\n",
            "Epoch 454. Training loss: 0.7762197852134705. Validation loss: 2.706942558288574.\n",
            "Epoch 455. Training loss: 0.7762168049812317. Validation loss: 2.706942081451416.\n",
            "Epoch 456. Training loss: 0.7762138247489929. Validation loss: 2.706942081451416.\n",
            "Epoch 457. Training loss: 0.7762107849121094. Validation loss: 2.706941604614258.\n",
            "Epoch 458. Training loss: 0.7762079238891602. Validation loss: 2.7069413661956787.\n",
            "Epoch 459. Training loss: 0.7762048840522766. Validation loss: 2.7069406509399414.\n",
            "Epoch 460. Training loss: 0.7762019634246826. Validation loss: 2.7069406509399414.\n",
            "Epoch 461. Training loss: 0.7761989235877991. Validation loss: 2.706939935684204.\n",
            "Epoch 462. Training loss: 0.7761960625648499. Validation loss: 2.706939458847046.\n",
            "Epoch 463. Training loss: 0.7761931419372559. Validation loss: 2.706939458847046.\n",
            "Epoch 464. Training loss: 0.7761901021003723. Validation loss: 2.7069389820098877.\n",
            "Epoch 465. Training loss: 0.7761872410774231. Validation loss: 2.7069385051727295.\n",
            "Epoch 466. Training loss: 0.7761842608451843. Validation loss: 2.706937789916992.\n",
            "Epoch 467. Training loss: 0.7761813998222351. Validation loss: 2.7069380283355713.\n",
            "Epoch 468. Training loss: 0.7761783599853516. Validation loss: 2.706937313079834.\n",
            "Epoch 469. Training loss: 0.7761754393577576. Validation loss: 2.706937313079834.\n",
            "Epoch 470. Training loss: 0.7761724591255188. Validation loss: 2.7069365978240967.\n",
            "Epoch 471. Training loss: 0.77616947889328. Validation loss: 2.7069363594055176.\n",
            "Epoch 472. Training loss: 0.7761664986610413. Validation loss: 2.7069358825683594.\n",
            "Epoch 473. Training loss: 0.7761635780334473. Validation loss: 2.7069356441497803.\n",
            "Epoch 474. Training loss: 0.7761606574058533. Validation loss: 2.706935405731201.\n",
            "Epoch 475. Training loss: 0.7761576771736145. Validation loss: 2.706934928894043.\n",
            "Epoch 476. Training loss: 0.7761546969413757. Validation loss: 2.706934690475464.\n",
            "Epoch 477. Training loss: 0.7761518359184265. Validation loss: 2.7069342136383057.\n",
            "Epoch 478. Training loss: 0.776148796081543. Validation loss: 2.7069337368011475.\n",
            "Epoch 479. Training loss: 0.7761457562446594. Validation loss: 2.7069334983825684.\n",
            "Epoch 480. Training loss: 0.7761428356170654. Validation loss: 2.70693302154541.\n",
            "Epoch 481. Training loss: 0.7761399149894714. Validation loss: 2.706932544708252.\n",
            "Epoch 482. Training loss: 0.7761368155479431. Validation loss: 2.7069320678710938.\n",
            "Epoch 483. Training loss: 0.7761339545249939. Validation loss: 2.7069318294525146.\n",
            "Epoch 484. Training loss: 0.7761309742927551. Validation loss: 2.7069315910339355.\n",
            "Epoch 485. Training loss: 0.7761280536651611. Validation loss: 2.7069311141967773.\n",
            "Epoch 486. Training loss: 0.7761251330375671. Validation loss: 2.7069308757781982.\n",
            "Epoch 487. Training loss: 0.7761220932006836. Validation loss: 2.70693039894104.\n",
            "Epoch 488. Training loss: 0.7761190533638. Validation loss: 2.706930160522461.\n",
            "Epoch 489. Training loss: 0.7761160731315613. Validation loss: 2.706930160522461.\n",
            "Epoch 490. Training loss: 0.7761130332946777. Validation loss: 2.7069292068481445.\n",
            "Epoch 491. Training loss: 0.7761101722717285. Validation loss: 2.7069289684295654.\n",
            "Epoch 492. Training loss: 0.776107132434845. Validation loss: 2.7069284915924072.\n",
            "Epoch 493. Training loss: 0.7761042714118958. Validation loss: 2.706928014755249.\n",
            "Epoch 494. Training loss: 0.776101291179657. Validation loss: 2.706928014755249.\n",
            "Epoch 495. Training loss: 0.7760982513427734. Validation loss: 2.7069272994995117.\n",
            "Epoch 496. Training loss: 0.7760952115058899. Validation loss: 2.706927537918091.\n",
            "Epoch 497. Training loss: 0.7760922908782959. Validation loss: 2.7069263458251953.\n",
            "Epoch 498. Training loss: 0.7760893702507019. Validation loss: 2.7069263458251953.\n",
            "Epoch 499. Training loss: 0.7760863304138184. Validation loss: 2.706925868988037.\n",
            "Epoch 500. Training loss: 0.7760834097862244. Validation loss: 2.706925392150879.\n",
            "Epoch 501. Training loss: 0.7760803699493408. Validation loss: 2.7069249153137207.\n",
            "Epoch 502. Training loss: 0.7760773301124573. Validation loss: 2.7069249153137207.\n",
            "Epoch 503. Training loss: 0.7760744690895081. Validation loss: 2.7069244384765625.\n",
            "Epoch 504. Training loss: 0.7760714888572693. Validation loss: 2.7069239616394043.\n",
            "Epoch 505. Training loss: 0.7760685086250305. Validation loss: 2.7069239616394043.\n",
            "Epoch 506. Training loss: 0.7760655283927917. Validation loss: 2.706923484802246.\n",
            "Epoch 507. Training loss: 0.7760624885559082. Validation loss: 2.706923007965088.\n",
            "Epoch 508. Training loss: 0.7760595679283142. Validation loss: 2.7069225311279297.\n",
            "Epoch 509. Training loss: 0.7760565280914307. Validation loss: 2.7069222927093506.\n",
            "Epoch 510. Training loss: 0.7760536074638367. Validation loss: 2.7069218158721924.\n",
            "Epoch 511. Training loss: 0.7760505676269531. Validation loss: 2.7069215774536133.\n",
            "Epoch 512. Training loss: 0.7760475277900696. Validation loss: 2.706920623779297.\n",
            "Epoch 513. Training loss: 0.7760446071624756. Validation loss: 2.706920862197876.\n",
            "Epoch 514. Training loss: 0.776041567325592. Validation loss: 2.7069203853607178.\n",
            "Epoch 515. Training loss: 0.7760385870933533. Validation loss: 2.7069201469421387.\n",
            "Epoch 516. Training loss: 0.776035726070404. Validation loss: 2.7069194316864014.\n",
            "Epoch 517. Training loss: 0.7760326862335205. Validation loss: 2.7069191932678223.\n",
            "Epoch 518. Training loss: 0.776029646396637. Validation loss: 2.706918954849243.\n",
            "Epoch 519. Training loss: 0.776026725769043. Validation loss: 2.706918239593506.\n",
            "Epoch 520. Training loss: 0.776023805141449. Validation loss: 2.706918239593506.\n",
            "Epoch 521. Training loss: 0.7760207056999207. Validation loss: 2.7069177627563477.\n",
            "Epoch 522. Training loss: 0.7760177254676819. Validation loss: 2.7069170475006104.\n",
            "Epoch 523. Training loss: 0.7760147452354431. Validation loss: 2.7069168090820312.\n",
            "Epoch 524. Training loss: 0.7760117650032043. Validation loss: 2.706916332244873.\n",
            "Epoch 525. Training loss: 0.7760087847709656. Validation loss: 2.706916093826294.\n",
            "Epoch 526. Training loss: 0.776005744934082. Validation loss: 2.706916093826294.\n",
            "Epoch 527. Training loss: 0.776002824306488. Validation loss: 2.7069149017333984.\n",
            "Epoch 528. Training loss: 0.7759997844696045. Validation loss: 2.7069149017333984.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 529. Training loss: 0.775996744632721. Validation loss: 2.7069144248962402.\n",
            "Epoch 530. Training loss: 0.7759937644004822. Validation loss: 2.7069144248962402.\n",
            "Epoch 531. Training loss: 0.7759907245635986. Validation loss: 2.706913948059082.\n",
            "Epoch 532. Training loss: 0.7759878039360046. Validation loss: 2.706913709640503.\n",
            "Epoch 533. Training loss: 0.7759847640991211. Validation loss: 2.7069129943847656.\n",
            "Epoch 534. Training loss: 0.7759817242622375. Validation loss: 2.7069127559661865.\n",
            "Epoch 535. Training loss: 0.7759787440299988. Validation loss: 2.7069125175476074.\n",
            "Epoch 536. Training loss: 0.7759758830070496. Validation loss: 2.70691180229187.\n",
            "Epoch 537. Training loss: 0.775972843170166. Validation loss: 2.706911563873291.\n",
            "Epoch 538. Training loss: 0.7759697437286377. Validation loss: 2.706911325454712.\n",
            "Epoch 539. Training loss: 0.7759668231010437. Validation loss: 2.706911087036133.\n",
            "Epoch 540. Training loss: 0.7759637832641602. Validation loss: 2.7069101333618164.\n",
            "Epoch 541. Training loss: 0.7759607434272766. Validation loss: 2.7069098949432373.\n",
            "Epoch 542. Training loss: 0.7759578227996826. Validation loss: 2.706909656524658.\n",
            "Epoch 543. Training loss: 0.7759547829627991. Validation loss: 2.706909418106079.\n",
            "Epoch 544. Training loss: 0.7759518623352051. Validation loss: 2.706908941268921.\n",
            "Epoch 545. Training loss: 0.7759487628936768. Validation loss: 2.706908702850342.\n",
            "Epoch 546. Training loss: 0.7759457230567932. Validation loss: 2.7069084644317627.\n",
            "Epoch 547. Training loss: 0.7759428024291992. Validation loss: 2.7069082260131836.\n",
            "Epoch 548. Training loss: 0.7759397625923157. Validation loss: 2.7069077491760254.\n",
            "Epoch 549. Training loss: 0.7759367823600769. Validation loss: 2.706907272338867.\n",
            "Epoch 550. Training loss: 0.7759337425231934. Validation loss: 2.70690655708313.\n",
            "Epoch 551. Training loss: 0.7759307026863098. Validation loss: 2.706906318664551.\n",
            "Epoch 552. Training loss: 0.775927722454071. Validation loss: 2.7069060802459717.\n",
            "Epoch 553. Training loss: 0.7759246826171875. Validation loss: 2.7069056034088135.\n",
            "Epoch 554. Training loss: 0.7759218215942383. Validation loss: 2.7069051265716553.\n",
            "Epoch 555. Training loss: 0.77591872215271. Validation loss: 2.706904411315918.\n",
            "Epoch 556. Training loss: 0.775915801525116. Validation loss: 2.706904411315918.\n",
            "Epoch 557. Training loss: 0.7759127616882324. Validation loss: 2.7069039344787598.\n",
            "Epoch 558. Training loss: 0.7759097218513489. Validation loss: 2.7069036960601807.\n",
            "Epoch 559. Training loss: 0.7759066224098206. Validation loss: 2.7069034576416016.\n",
            "Epoch 560. Training loss: 0.7759037017822266. Validation loss: 2.7069029808044434.\n",
            "Epoch 561. Training loss: 0.775900661945343. Validation loss: 2.7069027423858643.\n",
            "Epoch 562. Training loss: 0.7758976817131042. Validation loss: 2.706902027130127.\n",
            "Epoch 563. Training loss: 0.7758947014808655. Validation loss: 2.706901788711548.\n",
            "Epoch 564. Training loss: 0.7758916020393372. Validation loss: 2.7069013118743896.\n",
            "Epoch 565. Training loss: 0.7758886218070984. Validation loss: 2.7069010734558105.\n",
            "Epoch 566. Training loss: 0.7758856415748596. Validation loss: 2.7069005966186523.\n",
            "Epoch 567. Training loss: 0.7758825421333313. Validation loss: 2.7069003582000732.\n",
            "Epoch 568. Training loss: 0.7758796215057373. Validation loss: 2.706899881362915.\n",
            "Epoch 569. Training loss: 0.7758765816688538. Validation loss: 2.706899642944336.\n",
            "Epoch 570. Training loss: 0.7758734822273254. Validation loss: 2.7068991661071777.\n",
            "Epoch 571. Training loss: 0.7758705615997314. Validation loss: 2.7068986892700195.\n",
            "Epoch 572. Training loss: 0.7758674621582031. Validation loss: 2.7068984508514404.\n",
            "Epoch 573. Training loss: 0.7758645415306091. Validation loss: 2.706897735595703.\n",
            "Epoch 574. Training loss: 0.7758615016937256. Validation loss: 2.706897735595703.\n",
            "Epoch 575. Training loss: 0.7758584022521973. Validation loss: 2.706897258758545.\n",
            "Epoch 576. Training loss: 0.7758553624153137. Validation loss: 2.706897258758545.\n",
            "Epoch 577. Training loss: 0.775852382183075. Validation loss: 2.7068963050842285.\n",
            "Epoch 578. Training loss: 0.7758493423461914. Validation loss: 2.7068963050842285.\n",
            "Epoch 579. Training loss: 0.7758463025093079. Validation loss: 2.7068958282470703.\n",
            "Epoch 580. Training loss: 0.7758433222770691. Validation loss: 2.706895112991333.\n",
            "Epoch 581. Training loss: 0.7758403420448303. Validation loss: 2.706895112991333.\n",
            "Epoch 582. Training loss: 0.7758373618125916. Validation loss: 2.7068941593170166.\n",
            "Epoch 583. Training loss: 0.7758342623710632. Validation loss: 2.7068941593170166.\n",
            "Epoch 584. Training loss: 0.7758312821388245. Validation loss: 2.7068934440612793.\n",
            "Epoch 585. Training loss: 0.7758281826972961. Validation loss: 2.706892967224121.\n",
            "Epoch 586. Training loss: 0.7758252620697021. Validation loss: 2.706892967224121.\n",
            "Epoch 587. Training loss: 0.775822103023529. Validation loss: 2.706892490386963.\n",
            "Epoch 588. Training loss: 0.7758191227912903. Validation loss: 2.706892490386963.\n",
            "Epoch 589. Training loss: 0.775816023349762. Validation loss: 2.7068917751312256.\n",
            "Epoch 590. Training loss: 0.7758130431175232. Validation loss: 2.7068920135498047.\n",
            "Epoch 591. Training loss: 0.7758100032806396. Validation loss: 2.7068910598754883.\n",
            "Epoch 592. Training loss: 0.7758069634437561. Validation loss: 2.706890821456909.\n",
            "Epoch 593. Training loss: 0.7758039832115173. Validation loss: 2.706890106201172.\n",
            "Epoch 594. Training loss: 0.7758009433746338. Validation loss: 2.7068898677825928.\n",
            "Epoch 595. Training loss: 0.7757978439331055. Validation loss: 2.7068896293640137.\n",
            "Epoch 596. Training loss: 0.7757949233055115. Validation loss: 2.7068891525268555.\n",
            "Epoch 597. Training loss: 0.7757918238639832. Validation loss: 2.7068889141082764.\n",
            "Epoch 598. Training loss: 0.7757887840270996. Validation loss: 2.7068886756896973.\n",
            "Epoch 599. Training loss: 0.7757856845855713. Validation loss: 2.706888198852539.\n",
            "Epoch 600. Training loss: 0.7757826447486877. Validation loss: 2.706887722015381.\n",
            "Epoch 601. Training loss: 0.7757795453071594. Validation loss: 2.7068874835968018.\n",
            "Epoch 602. Training loss: 0.7757766246795654. Validation loss: 2.7068870067596436.\n",
            "Epoch 603. Training loss: 0.7757734656333923. Validation loss: 2.7068865299224854.\n",
            "Epoch 604. Training loss: 0.7757704854011536. Validation loss: 2.7068862915039062.\n",
            "Epoch 605. Training loss: 0.7757675647735596. Validation loss: 2.706886053085327.\n",
            "Epoch 606. Training loss: 0.7757644653320312. Validation loss: 2.70688533782959.\n",
            "Epoch 607. Training loss: 0.7757614254951477. Validation loss: 2.7068848609924316.\n",
            "Epoch 608. Training loss: 0.7757584452629089. Validation loss: 2.7068848609924316.\n",
            "Epoch 609. Training loss: 0.7757553458213806. Validation loss: 2.7068841457366943.\n",
            "Epoch 610. Training loss: 0.7757523059844971. Validation loss: 2.7068841457366943.\n",
            "Epoch 611. Training loss: 0.7757492661476135. Validation loss: 2.706883668899536.\n",
            "Epoch 612. Training loss: 0.7757462859153748. Validation loss: 2.706882953643799.\n",
            "Epoch 613. Training loss: 0.7757430672645569. Validation loss: 2.7068824768066406.\n",
            "Epoch 614. Training loss: 0.7757401466369629. Validation loss: 2.7068824768066406.\n",
            "Epoch 615. Training loss: 0.7757370471954346. Validation loss: 2.7068819999694824.\n",
            "Epoch 616. Training loss: 0.775734007358551. Validation loss: 2.706881523132324.\n",
            "Epoch 617. Training loss: 0.7757309079170227. Validation loss: 2.706881046295166.\n",
            "Epoch 618. Training loss: 0.7757279276847839. Validation loss: 2.706880807876587.\n",
            "Epoch 619. Training loss: 0.7757248878479004. Validation loss: 2.706880569458008.\n",
            "Epoch 620. Training loss: 0.7757217288017273. Validation loss: 2.7068796157836914.\n",
            "Epoch 621. Training loss: 0.7757186889648438. Validation loss: 2.7068796157836914.\n",
            "Epoch 622. Training loss: 0.7757156491279602. Validation loss: 2.706879138946533.\n",
            "Epoch 623. Training loss: 0.7757126688957214. Validation loss: 2.706878900527954.\n",
            "Epoch 624. Training loss: 0.7757095694541931. Validation loss: 2.706878662109375.\n",
            "Epoch 625. Training loss: 0.7757065296173096. Validation loss: 2.7068779468536377.\n",
            "Epoch 626. Training loss: 0.7757034301757812. Validation loss: 2.7068777084350586.\n",
            "Epoch 627. Training loss: 0.7757003903388977. Validation loss: 2.7068772315979004.\n",
            "Epoch 628. Training loss: 0.7756972908973694. Validation loss: 2.7068769931793213.\n",
            "Epoch 629. Training loss: 0.7756943702697754. Validation loss: 2.706876754760742.\n",
            "Epoch 630. Training loss: 0.7756912112236023. Validation loss: 2.706876039505005.\n",
            "Epoch 631. Training loss: 0.7756882309913635. Validation loss: 2.706876039505005.\n",
            "Epoch 632. Training loss: 0.7756850719451904. Validation loss: 2.7068755626678467.\n",
            "Epoch 633. Training loss: 0.7756820321083069. Validation loss: 2.7068750858306885.\n",
            "Epoch 634. Training loss: 0.7756790518760681. Validation loss: 2.7068750858306885.\n",
            "Epoch 635. Training loss: 0.7756759524345398. Validation loss: 2.706874132156372.\n",
            "Epoch 636. Training loss: 0.7756728529930115. Validation loss: 2.706873893737793.\n",
            "Epoch 637. Training loss: 0.7756698727607727. Validation loss: 2.7068734169006348.\n",
            "Epoch 638. Training loss: 0.7756667733192444. Validation loss: 2.7068731784820557.\n",
            "Epoch 639. Training loss: 0.7756636738777161. Validation loss: 2.7068729400634766.\n",
            "Epoch 640. Training loss: 0.7756606936454773. Validation loss: 2.7068727016448975.\n",
            "Epoch 641. Training loss: 0.7756576538085938. Validation loss: 2.706871747970581.\n",
            "Epoch 642. Training loss: 0.7756545543670654. Validation loss: 2.706871509552002.\n",
            "Epoch 643. Training loss: 0.7756514549255371. Validation loss: 2.7068710327148438.\n",
            "Epoch 644. Training loss: 0.7756483554840088. Validation loss: 2.7068705558776855.\n",
            "Epoch 645. Training loss: 0.7756454348564148. Validation loss: 2.7068705558776855.\n",
            "Epoch 646. Training loss: 0.7756422162055969. Validation loss: 2.7068698406219482.\n",
            "Epoch 647. Training loss: 0.7756392359733582. Validation loss: 2.706869602203369.\n",
            "Epoch 648. Training loss: 0.7756361365318298. Validation loss: 2.70686936378479.\n",
            "Epoch 649. Training loss: 0.775632917881012. Validation loss: 2.706868886947632.\n",
            "Epoch 650. Training loss: 0.775629997253418. Validation loss: 2.7068686485290527.\n",
            "Epoch 651. Training loss: 0.7756268382072449. Validation loss: 2.7068681716918945.\n",
            "Epoch 652. Training loss: 0.7756238579750061. Validation loss: 2.7068676948547363.\n",
            "Epoch 653. Training loss: 0.7756207585334778. Validation loss: 2.7068674564361572.\n",
            "Epoch 654. Training loss: 0.7756176590919495. Validation loss: 2.706866979598999.\n",
            "Epoch 655. Training loss: 0.7756145596504211. Validation loss: 2.7068662643432617.\n",
            "Epoch 656. Training loss: 0.7756115794181824. Validation loss: 2.7068660259246826.\n",
            "Epoch 657. Training loss: 0.7756083607673645. Validation loss: 2.7068660259246826.\n",
            "Epoch 658. Training loss: 0.7756053805351257. Validation loss: 2.7068653106689453.\n",
            "Epoch 659. Training loss: 0.7756023406982422. Validation loss: 2.706865072250366.\n",
            "Epoch 660. Training loss: 0.7755992412567139. Validation loss: 2.706864833831787.\n",
            "Epoch 661. Training loss: 0.7755962014198303. Validation loss: 2.706864356994629.\n",
            "Epoch 662. Training loss: 0.775593101978302. Validation loss: 2.70686411857605.\n",
            "Epoch 663. Training loss: 0.7755901217460632. Validation loss: 2.7068636417388916.\n",
            "Epoch 664. Training loss: 0.7755870223045349. Validation loss: 2.7068634033203125.\n",
            "Epoch 665. Training loss: 0.7755839228630066. Validation loss: 2.706862688064575.\n",
            "Epoch 666. Training loss: 0.7755808234214783. Validation loss: 2.706862211227417.\n",
            "Epoch 667. Training loss: 0.7755776047706604. Validation loss: 2.706861734390259.\n",
            "Epoch 668. Training loss: 0.7755746245384216. Validation loss: 2.7068614959716797.\n",
            "Epoch 669. Training loss: 0.7755715250968933. Validation loss: 2.7068610191345215.\n",
            "Epoch 670. Training loss: 0.775568425655365. Validation loss: 2.7068605422973633.\n",
            "Epoch 671. Training loss: 0.7755653858184814. Validation loss: 2.7068605422973633.\n",
            "Epoch 672. Training loss: 0.7755622863769531. Validation loss: 2.706860065460205.\n",
            "Epoch 673. Training loss: 0.7755591869354248. Validation loss: 2.7068593502044678.\n",
            "Epoch 674. Training loss: 0.7755560874938965. Validation loss: 2.7068591117858887.\n",
            "Epoch 675. Training loss: 0.7755530476570129. Validation loss: 2.7068588733673096.\n",
            "Epoch 676. Training loss: 0.7755498290061951. Validation loss: 2.7068583965301514.\n",
            "Epoch 677. Training loss: 0.7755467891693115. Validation loss: 2.7068581581115723.\n",
            "Epoch 678. Training loss: 0.7755436897277832. Validation loss: 2.706857681274414.\n",
            "Epoch 679. Training loss: 0.7755406498908997. Validation loss: 2.706857204437256.\n",
            "Epoch 680. Training loss: 0.7755375504493713. Validation loss: 2.7068567276000977.\n",
            "Epoch 681. Training loss: 0.775534451007843. Validation loss: 2.7068564891815186.\n",
            "Epoch 682. Training loss: 0.7755313515663147. Validation loss: 2.7068560123443604.\n",
            "Epoch 683. Training loss: 0.7755283713340759. Validation loss: 2.7068557739257812.\n",
            "Epoch 684. Training loss: 0.7755252718925476. Validation loss: 2.706855297088623.\n",
            "Epoch 685. Training loss: 0.7755220532417297. Validation loss: 2.706854820251465.\n",
            "Epoch 686. Training loss: 0.7755189538002014. Validation loss: 2.7068545818328857.\n",
            "Epoch 687. Training loss: 0.7755159735679626. Validation loss: 2.7068541049957275.\n",
            "Epoch 688. Training loss: 0.7755127549171448. Validation loss: 2.7068536281585693.\n",
            "Epoch 689. Training loss: 0.7755096554756165. Validation loss: 2.7068536281585693.\n",
            "Epoch 690. Training loss: 0.7755066752433777. Validation loss: 2.706853151321411.\n",
            "Epoch 691. Training loss: 0.7755035758018494. Validation loss: 2.706852674484253.\n",
            "Epoch 692. Training loss: 0.775500476360321. Validation loss: 2.7068519592285156.\n",
            "Epoch 693. Training loss: 0.7754972577095032. Validation loss: 2.7068519592285156.\n",
            "Epoch 694. Training loss: 0.7754942774772644. Validation loss: 2.7068514823913574.\n",
            "Epoch 695. Training loss: 0.7754910588264465. Validation loss: 2.706851005554199.\n",
            "Epoch 696. Training loss: 0.7754880785942078. Validation loss: 2.706850290298462.\n",
            "Epoch 697. Training loss: 0.7754848599433899. Validation loss: 2.706850290298462.\n",
            "Epoch 698. Training loss: 0.7754818797111511. Validation loss: 2.7068498134613037.\n",
            "Epoch 699. Training loss: 0.7754787802696228. Validation loss: 2.7068498134613037.\n",
            "Epoch 700. Training loss: 0.7754756808280945. Validation loss: 2.7068490982055664.\n",
            "Epoch 701. Training loss: 0.7754725813865662. Validation loss: 2.706848621368408.\n",
            "Epoch 702. Training loss: 0.7754694819450378. Validation loss: 2.70684814453125.\n",
            "Epoch 703. Training loss: 0.77546626329422. Validation loss: 2.706847667694092.\n",
            "Epoch 704. Training loss: 0.7754631638526917. Validation loss: 2.706847667694092.\n",
            "Epoch 705. Training loss: 0.7754600644111633. Validation loss: 2.7068471908569336.\n",
            "Epoch 706. Training loss: 0.775456964969635. Validation loss: 2.7068467140197754.\n",
            "Epoch 707. Training loss: 0.7754539847373962. Validation loss: 2.706846237182617.\n",
            "Epoch 708. Training loss: 0.7754507660865784. Validation loss: 2.706845760345459.\n",
            "Epoch 709. Training loss: 0.7754477858543396. Validation loss: 2.70684552192688.\n",
            "Epoch 710. Training loss: 0.7754446864128113. Validation loss: 2.7068450450897217.\n",
            "Epoch 711. Training loss: 0.7754414677619934. Validation loss: 2.7068448066711426.\n",
            "Epoch 712. Training loss: 0.7754383087158203. Validation loss: 2.7068443298339844.\n",
            "Epoch 713. Training loss: 0.7754352688789368. Validation loss: 2.7068440914154053.\n",
            "Epoch 714. Training loss: 0.7754321098327637. Validation loss: 2.706843376159668.\n",
            "Epoch 715. Training loss: 0.7754290699958801. Validation loss: 2.706843376159668.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 716. Training loss: 0.775425910949707. Validation loss: 2.7068428993225098.\n",
            "Epoch 717. Training loss: 0.7754228115081787. Validation loss: 2.7068424224853516.\n",
            "Epoch 718. Training loss: 0.7754197120666504. Validation loss: 2.7068419456481934.\n",
            "Epoch 719. Training loss: 0.7754166126251221. Validation loss: 2.7068417072296143.\n",
            "Epoch 720. Training loss: 0.7754133343696594. Validation loss: 2.706841468811035.\n",
            "Epoch 721. Training loss: 0.7754104137420654. Validation loss: 2.706840991973877.\n",
            "Epoch 722. Training loss: 0.7754073143005371. Validation loss: 2.7068402767181396.\n",
            "Epoch 723. Training loss: 0.7754040360450745. Validation loss: 2.7068402767181396.\n",
            "Epoch 724. Training loss: 0.7754010558128357. Validation loss: 2.7068397998809814.\n",
            "Epoch 725. Training loss: 0.7753979563713074. Validation loss: 2.7068395614624023.\n",
            "Epoch 726. Training loss: 0.7753947377204895. Validation loss: 2.706838846206665.\n",
            "Epoch 727. Training loss: 0.7753916382789612. Validation loss: 2.706838846206665.\n",
            "Epoch 728. Training loss: 0.7753884792327881. Validation loss: 2.706838369369507.\n",
            "Epoch 729. Training loss: 0.775385320186615. Validation loss: 2.7068376541137695.\n",
            "Epoch 730. Training loss: 0.7753822803497314. Validation loss: 2.7068374156951904.\n",
            "Epoch 731. Training loss: 0.7753791213035583. Validation loss: 2.706836700439453.\n",
            "Epoch 732. Training loss: 0.77537602186203. Validation loss: 2.706836462020874.\n",
            "Epoch 733. Training loss: 0.7753729224205017. Validation loss: 2.706836223602295.\n",
            "Epoch 734. Training loss: 0.7753698229789734. Validation loss: 2.7068357467651367.\n",
            "Epoch 735. Training loss: 0.7753667235374451. Validation loss: 2.7068355083465576.\n",
            "Epoch 736. Training loss: 0.7753635048866272. Validation loss: 2.7068352699279785.\n",
            "Epoch 737. Training loss: 0.7753605246543884. Validation loss: 2.706834316253662.\n",
            "Epoch 738. Training loss: 0.7753573060035706. Validation loss: 2.706834077835083.\n",
            "Epoch 739. Training loss: 0.7753542065620422. Validation loss: 2.706833600997925.\n",
            "Epoch 740. Training loss: 0.7753511071205139. Validation loss: 2.7068333625793457.\n",
            "Epoch 741. Training loss: 0.7753479480743408. Validation loss: 2.7068328857421875.\n",
            "Epoch 742. Training loss: 0.7753448486328125. Validation loss: 2.7068326473236084.\n",
            "Epoch 743. Training loss: 0.7753416895866394. Validation loss: 2.7068324089050293.\n",
            "Epoch 744. Training loss: 0.7753385901451111. Validation loss: 2.706831932067871.\n",
            "Epoch 745. Training loss: 0.7753354907035828. Validation loss: 2.706831216812134.\n",
            "Epoch 746. Training loss: 0.7753322720527649. Validation loss: 2.706831216812134.\n",
            "Epoch 747. Training loss: 0.7753291726112366. Validation loss: 2.7068307399749756.\n",
            "Epoch 748. Training loss: 0.7753261923789978. Validation loss: 2.7068305015563965.\n",
            "Epoch 749. Training loss: 0.7753229737281799. Validation loss: 2.7068300247192383.\n",
            "Epoch 750. Training loss: 0.7753197550773621. Validation loss: 2.70682954788208.\n",
            "Epoch 751. Training loss: 0.7753166556358337. Validation loss: 2.706829071044922.\n",
            "Epoch 752. Training loss: 0.7753135561943054. Validation loss: 2.7068285942077637.\n",
            "Epoch 753. Training loss: 0.7753103375434875. Validation loss: 2.7068283557891846.\n",
            "Epoch 754. Training loss: 0.7753072381019592. Validation loss: 2.7068281173706055.\n",
            "Epoch 755. Training loss: 0.7753041386604309. Validation loss: 2.706827402114868.\n",
            "Epoch 756. Training loss: 0.7753009796142578. Validation loss: 2.706827163696289.\n",
            "Epoch 757. Training loss: 0.7752978801727295. Validation loss: 2.70682692527771.\n",
            "Epoch 758. Training loss: 0.7752947807312012. Validation loss: 2.7068262100219727.\n",
            "Epoch 759. Training loss: 0.7752916216850281. Validation loss: 2.7068262100219727.\n",
            "Epoch 760. Training loss: 0.7752884030342102. Validation loss: 2.7068254947662354.\n",
            "Epoch 761. Training loss: 0.7752852439880371. Validation loss: 2.706825017929077.\n",
            "Epoch 762. Training loss: 0.7752821445465088. Validation loss: 2.706825017929077.\n",
            "Epoch 763. Training loss: 0.7752790451049805. Validation loss: 2.70682430267334.\n",
            "Epoch 764. Training loss: 0.7752758860588074. Validation loss: 2.7068240642547607.\n",
            "Epoch 765. Training loss: 0.775272786617279. Validation loss: 2.7068231105804443.\n",
            "Epoch 766. Training loss: 0.7752695083618164. Validation loss: 2.7068228721618652.\n",
            "Epoch 767. Training loss: 0.7752664089202881. Validation loss: 2.706822633743286.\n",
            "Epoch 768. Training loss: 0.775263249874115. Validation loss: 2.706822156906128.\n",
            "Epoch 769. Training loss: 0.7752601504325867. Validation loss: 2.706821918487549.\n",
            "Epoch 770. Training loss: 0.7752571105957031. Validation loss: 2.7068216800689697.\n",
            "Epoch 771. Training loss: 0.7752537727355957. Validation loss: 2.7068214416503906.\n",
            "Epoch 772. Training loss: 0.7752506732940674. Validation loss: 2.7068209648132324.\n",
            "Epoch 773. Training loss: 0.7752475738525391. Validation loss: 2.706820487976074.\n",
            "Epoch 774. Training loss: 0.775244414806366. Validation loss: 2.706820011138916.\n",
            "Epoch 775. Training loss: 0.7752411961555481. Validation loss: 2.706819534301758.\n",
            "Epoch 776. Training loss: 0.7752380967140198. Validation loss: 2.7068192958831787.\n",
            "Epoch 777. Training loss: 0.7752349376678467. Validation loss: 2.7068185806274414.\n",
            "Epoch 778. Training loss: 0.775231659412384. Validation loss: 2.706818103790283.\n",
            "Epoch 779. Training loss: 0.7752285599708557. Validation loss: 2.706818103790283.\n",
            "Epoch 780. Training loss: 0.7752255797386169. Validation loss: 2.706817626953125.\n",
            "Epoch 781. Training loss: 0.7752223014831543. Validation loss: 2.706817150115967.\n",
            "Epoch 782. Training loss: 0.7752192616462708. Validation loss: 2.7068169116973877.\n",
            "Epoch 783. Training loss: 0.7752160429954529. Validation loss: 2.7068164348602295.\n",
            "Epoch 784. Training loss: 0.7752129435539246. Validation loss: 2.706815719604492.\n",
            "Epoch 785. Training loss: 0.7752097249031067. Validation loss: 2.7068159580230713.\n",
            "Epoch 786. Training loss: 0.7752066254615784. Validation loss: 2.706815242767334.\n",
            "Epoch 787. Training loss: 0.7752034068107605. Validation loss: 2.706814765930176.\n",
            "Epoch 788. Training loss: 0.7752001881599426. Validation loss: 2.7068142890930176.\n",
            "Epoch 789. Training loss: 0.7751970887184143. Validation loss: 2.7068140506744385.\n",
            "Epoch 790. Training loss: 0.7751939296722412. Validation loss: 2.7068138122558594.\n",
            "Epoch 791. Training loss: 0.7751907706260681. Validation loss: 2.706813335418701.\n",
            "Epoch 792. Training loss: 0.7751876711845398. Validation loss: 2.706812858581543.\n",
            "Epoch 793. Training loss: 0.7751844525337219. Validation loss: 2.7068123817443848.\n",
            "Epoch 794. Training loss: 0.7751812934875488. Validation loss: 2.7068119049072266.\n",
            "Epoch 795. Training loss: 0.7751781344413757. Validation loss: 2.7068114280700684.\n",
            "Epoch 796. Training loss: 0.7751749157905579. Validation loss: 2.70681095123291.\n",
            "Epoch 797. Training loss: 0.7751719355583191. Validation loss: 2.706810712814331.\n",
            "Epoch 798. Training loss: 0.7751687169075012. Validation loss: 2.706810474395752.\n",
            "Epoch 799. Training loss: 0.7751654982566833. Validation loss: 2.7068099975585938.\n",
            "Epoch 800. Training loss: 0.7751622796058655. Validation loss: 2.7068097591400146.\n",
            "Epoch 801. Training loss: 0.7751591801643372. Validation loss: 2.7068090438842773.\n",
            "Epoch 802. Training loss: 0.7751560211181641. Validation loss: 2.7068088054656982.\n",
            "Epoch 803. Training loss: 0.775152862071991. Validation loss: 2.706808567047119.\n",
            "Epoch 804. Training loss: 0.7751496434211731. Validation loss: 2.706807851791382.\n",
            "Epoch 805. Training loss: 0.7751465439796448. Validation loss: 2.7068076133728027.\n",
            "Epoch 806. Training loss: 0.7751433253288269. Validation loss: 2.7068073749542236.\n",
            "Epoch 807. Training loss: 0.7751402258872986. Validation loss: 2.7068071365356445.\n",
            "Epoch 808. Training loss: 0.7751370072364807. Validation loss: 2.7068068981170654.\n",
            "Epoch 809. Training loss: 0.7751338481903076. Validation loss: 2.706806182861328.\n",
            "Epoch 810. Training loss: 0.7751306891441345. Validation loss: 2.70680570602417.\n",
            "Epoch 811. Training loss: 0.7751274108886719. Validation loss: 2.706805467605591.\n",
            "Epoch 812. Training loss: 0.7751243114471436. Validation loss: 2.7068049907684326.\n",
            "Epoch 813. Training loss: 0.7751212120056152. Validation loss: 2.7068045139312744.\n",
            "Epoch 814. Training loss: 0.7751179337501526. Validation loss: 2.706804037094116.\n",
            "Epoch 815. Training loss: 0.7751148343086243. Validation loss: 2.706803321838379.\n",
            "Epoch 816. Training loss: 0.7751116752624512. Validation loss: 2.706803321838379.\n",
            "Epoch 817. Training loss: 0.7751085162162781. Validation loss: 2.7068028450012207.\n",
            "Epoch 818. Training loss: 0.7751052975654602. Validation loss: 2.7068023681640625.\n",
            "Epoch 819. Training loss: 0.7751020789146423. Validation loss: 2.7068023681640625.\n",
            "Epoch 820. Training loss: 0.775098979473114. Validation loss: 2.706801414489746.\n",
            "Epoch 821. Training loss: 0.7750957608222961. Validation loss: 2.706801414489746.\n",
            "Epoch 822. Training loss: 0.775092601776123. Validation loss: 2.706800937652588.\n",
            "Epoch 823. Training loss: 0.7750893235206604. Validation loss: 2.7068004608154297.\n",
            "Epoch 824. Training loss: 0.7750863432884216. Validation loss: 2.7068002223968506.\n",
            "Epoch 825. Training loss: 0.775083065032959. Validation loss: 2.7067997455596924.\n",
            "Epoch 826. Training loss: 0.7750799059867859. Validation loss: 2.706799268722534.\n",
            "Epoch 827. Training loss: 0.775076687335968. Validation loss: 2.706798791885376.\n",
            "Epoch 828. Training loss: 0.7750734686851501. Validation loss: 2.706798791885376.\n",
            "Epoch 829. Training loss: 0.7750703692436218. Validation loss: 2.7067983150482178.\n",
            "Epoch 830. Training loss: 0.7750670909881592. Validation loss: 2.7067975997924805.\n",
            "Epoch 831. Training loss: 0.7750639319419861. Validation loss: 2.7067971229553223.\n",
            "Epoch 832. Training loss: 0.7750607132911682. Validation loss: 2.706796646118164.\n",
            "Epoch 833. Training loss: 0.7750576138496399. Validation loss: 2.706796407699585.\n",
            "Epoch 834. Training loss: 0.775054395198822. Validation loss: 2.706796169281006.\n",
            "Epoch 835. Training loss: 0.7750511169433594. Validation loss: 2.7067956924438477.\n",
            "Epoch 836. Training loss: 0.7750479578971863. Validation loss: 2.7067956924438477.\n",
            "Epoch 837. Training loss: 0.7750447392463684. Validation loss: 2.7067952156066895.\n",
            "Epoch 838. Training loss: 0.7750415802001953. Validation loss: 2.7067947387695312.\n",
            "Epoch 839. Training loss: 0.7750384211540222. Validation loss: 2.706794023513794.\n",
            "Epoch 840. Training loss: 0.7750352025032043. Validation loss: 2.7067935466766357.\n",
            "Epoch 841. Training loss: 0.7750320434570312. Validation loss: 2.7067933082580566.\n",
            "Epoch 842. Training loss: 0.7750288844108582. Validation loss: 2.7067930698394775.\n",
            "Epoch 843. Training loss: 0.7750256061553955. Validation loss: 2.7067925930023193.\n",
            "Epoch 844. Training loss: 0.7750225067138672. Validation loss: 2.706791877746582.\n",
            "Epoch 845. Training loss: 0.7750192284584045. Validation loss: 2.706791877746582.\n",
            "Epoch 846. Training loss: 0.7750160098075867. Validation loss: 2.706791400909424.\n",
            "Epoch 847. Training loss: 0.7750127911567688. Validation loss: 2.7067906856536865.\n",
            "Epoch 848. Training loss: 0.7750096321105957. Validation loss: 2.7067904472351074.\n",
            "Epoch 849. Training loss: 0.7750064730644226. Validation loss: 2.7067902088165283.\n",
            "Epoch 850. Training loss: 0.77500319480896. Validation loss: 2.706789493560791.\n",
            "Epoch 851. Training loss: 0.7750000357627869. Validation loss: 2.706789493560791.\n",
            "Epoch 852. Training loss: 0.774996817111969. Validation loss: 2.706789016723633.\n",
            "Epoch 853. Training loss: 0.7749936580657959. Validation loss: 2.7067883014678955.\n",
            "Epoch 854. Training loss: 0.7749905586242676. Validation loss: 2.7067880630493164.\n",
            "Epoch 855. Training loss: 0.7749872803688049. Validation loss: 2.706787586212158.\n",
            "Epoch 856. Training loss: 0.7749840617179871. Validation loss: 2.706787109375.\n",
            "Epoch 857. Training loss: 0.7749808430671692. Validation loss: 2.706787109375.\n",
            "Epoch 858. Training loss: 0.7749776244163513. Validation loss: 2.706786632537842.\n",
            "Epoch 859. Training loss: 0.7749744057655334. Validation loss: 2.7067861557006836.\n",
            "Epoch 860. Training loss: 0.7749712467193604. Validation loss: 2.7067856788635254.\n",
            "Epoch 861. Training loss: 0.7749679684638977. Validation loss: 2.706785202026367.\n",
            "Epoch 862. Training loss: 0.7749648094177246. Validation loss: 2.706784725189209.\n",
            "Epoch 863. Training loss: 0.7749616503715515. Validation loss: 2.70678448677063.\n",
            "Epoch 864. Training loss: 0.7749583721160889. Validation loss: 2.706784248352051.\n",
            "Epoch 865. Training loss: 0.7749552130699158. Validation loss: 2.7067837715148926.\n",
            "Epoch 866. Training loss: 0.7749519348144531. Validation loss: 2.7067832946777344.\n",
            "Epoch 867. Training loss: 0.77494877576828. Validation loss: 2.706782817840576.\n",
            "Epoch 868. Training loss: 0.7749455571174622. Validation loss: 2.706782341003418.\n",
            "Epoch 869. Training loss: 0.7749423980712891. Validation loss: 2.706782102584839.\n",
            "Epoch 870. Training loss: 0.774939239025116. Validation loss: 2.7067818641662598.\n",
            "Epoch 871. Training loss: 0.7749359607696533. Validation loss: 2.7067813873291016.\n",
            "Epoch 872. Training loss: 0.7749326825141907. Validation loss: 2.7067809104919434.\n",
            "Epoch 873. Training loss: 0.7749295234680176. Validation loss: 2.706780433654785.\n",
            "Epoch 874. Training loss: 0.7749262452125549. Validation loss: 2.706780195236206.\n",
            "Epoch 875. Training loss: 0.7749230861663818. Validation loss: 2.7067794799804688.\n",
            "Epoch 876. Training loss: 0.7749199271202087. Validation loss: 2.7067790031433105.\n",
            "Epoch 877. Training loss: 0.7749166488647461. Validation loss: 2.7067787647247314.\n",
            "Epoch 878. Training loss: 0.774913489818573. Validation loss: 2.7067785263061523.\n",
            "Epoch 879. Training loss: 0.7749102711677551. Validation loss: 2.706777572631836.\n",
            "Epoch 880. Training loss: 0.7749070525169373. Validation loss: 2.706777334213257.\n",
            "Epoch 881. Training loss: 0.7749037742614746. Validation loss: 2.7067766189575195.\n",
            "Epoch 882. Training loss: 0.7749004364013672. Validation loss: 2.7067763805389404.\n",
            "Epoch 883. Training loss: 0.7748973369598389. Validation loss: 2.7067759037017822.\n",
            "Epoch 884. Training loss: 0.7748940587043762. Validation loss: 2.706775188446045.\n",
            "Epoch 885. Training loss: 0.7748908400535583. Validation loss: 2.706774950027466.\n",
            "Epoch 886. Training loss: 0.77488774061203. Validation loss: 2.7067742347717285.\n",
            "Epoch 887. Training loss: 0.7748845219612122. Validation loss: 2.7067742347717285.\n",
            "Epoch 888. Training loss: 0.7748811841011047. Validation loss: 2.706773281097412.\n",
            "Epoch 889. Training loss: 0.7748780250549316. Validation loss: 2.706773042678833.\n",
            "Epoch 890. Training loss: 0.7748746871948242. Validation loss: 2.7067720890045166.\n",
            "Epoch 891. Training loss: 0.7748715281486511. Validation loss: 2.7067718505859375.\n",
            "Epoch 892. Training loss: 0.7748683094978333. Validation loss: 2.7067713737487793.\n",
            "Epoch 893. Training loss: 0.7748650908470154. Validation loss: 2.706770896911621.\n",
            "Epoch 894. Training loss: 0.7748618721961975. Validation loss: 2.706770420074463.\n",
            "Epoch 895. Training loss: 0.7748586535453796. Validation loss: 2.7067699432373047.\n",
            "Epoch 896. Training loss: 0.7748554348945618. Validation loss: 2.7067694664001465.\n",
            "Epoch 897. Training loss: 0.7748522162437439. Validation loss: 2.7067689895629883.\n",
            "Epoch 898. Training loss: 0.774848997592926. Validation loss: 2.70676851272583.\n",
            "Epoch 899. Training loss: 0.7748458385467529. Validation loss: 2.7067677974700928.\n",
            "Epoch 900. Training loss: 0.7748425006866455. Validation loss: 2.7067673206329346.\n",
            "Epoch 901. Training loss: 0.7748392224311829. Validation loss: 2.7067666053771973.\n",
            "Epoch 902. Training loss: 0.774836003780365. Validation loss: 2.706766366958618.\n",
            "Epoch 903. Training loss: 0.7748329043388367. Validation loss: 2.70676589012146.\n",
            "Epoch 904. Training loss: 0.774829626083374. Validation loss: 2.7067651748657227.\n",
            "Epoch 905. Training loss: 0.7748263478279114. Validation loss: 2.7067646980285645.\n",
            "Epoch 906. Training loss: 0.7748231887817383. Validation loss: 2.7067642211914062.\n",
            "Epoch 907. Training loss: 0.7748198509216309. Validation loss: 2.706763744354248.\n",
            "Epoch 908. Training loss: 0.7748166918754578. Validation loss: 2.70676326751709.\n",
            "Epoch 909. Training loss: 0.7748134732246399. Validation loss: 2.7067627906799316.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 910. Training loss: 0.774810254573822. Validation loss: 2.7067620754241943.\n",
            "Epoch 911. Training loss: 0.7748069763183594. Validation loss: 2.7067618370056152.\n",
            "Epoch 912. Training loss: 0.7748036980628967. Validation loss: 2.706761360168457.\n",
            "Epoch 913. Training loss: 0.7748003602027893. Validation loss: 2.706760883331299.\n",
            "Epoch 914. Training loss: 0.774797260761261. Validation loss: 2.7067604064941406.\n",
            "Epoch 915. Training loss: 0.7747940421104431. Validation loss: 2.7067599296569824.\n",
            "Epoch 916. Training loss: 0.7747907042503357. Validation loss: 2.706759214401245.\n",
            "Epoch 917. Training loss: 0.7747876048088074. Validation loss: 2.706758975982666.\n",
            "Epoch 918. Training loss: 0.7747843265533447. Validation loss: 2.7067582607269287.\n",
            "Epoch 919. Training loss: 0.7747810482978821. Validation loss: 2.7067575454711914.\n",
            "Epoch 920. Training loss: 0.7747777104377747. Validation loss: 2.7067573070526123.\n",
            "Epoch 921. Training loss: 0.7747745513916016. Validation loss: 2.706756591796875.\n",
            "Epoch 922. Training loss: 0.7747712731361389. Validation loss: 2.706756353378296.\n",
            "Epoch 923. Training loss: 0.7747681140899658. Validation loss: 2.7067556381225586.\n",
            "Epoch 924. Training loss: 0.7747648358345032. Validation loss: 2.7067549228668213.\n",
            "Epoch 925. Training loss: 0.7747616767883301. Validation loss: 2.706754684448242.\n",
            "Epoch 926. Training loss: 0.7747583985328674. Validation loss: 2.706753969192505.\n",
            "Epoch 927. Training loss: 0.7747551798820496. Validation loss: 2.706753730773926.\n",
            "Epoch 928. Training loss: 0.7747518420219421. Validation loss: 2.7067532539367676.\n",
            "Epoch 929. Training loss: 0.7747485637664795. Validation loss: 2.7067525386810303.\n",
            "Epoch 930. Training loss: 0.7747454643249512. Validation loss: 2.706752061843872.\n",
            "Epoch 931. Training loss: 0.7747421264648438. Validation loss: 2.706751585006714.\n",
            "Epoch 932. Training loss: 0.7747388482093811. Validation loss: 2.7067511081695557.\n",
            "Epoch 933. Training loss: 0.7747356295585632. Validation loss: 2.7067506313323975.\n",
            "Epoch 934. Training loss: 0.7747324109077454. Validation loss: 2.70674991607666.\n",
            "Epoch 935. Training loss: 0.7747290730476379. Validation loss: 2.706749677658081.\n",
            "Epoch 936. Training loss: 0.7747259140014648. Validation loss: 2.706749200820923.\n",
            "Epoch 937. Training loss: 0.7747226357460022. Validation loss: 2.7067487239837646.\n",
            "Epoch 938. Training loss: 0.7747194170951843. Validation loss: 2.7067480087280273.\n",
            "Epoch 939. Training loss: 0.7747160792350769. Validation loss: 2.7067480087280273.\n",
            "Epoch 940. Training loss: 0.774712860584259. Validation loss: 2.706747055053711.\n",
            "Epoch 941. Training loss: 0.7747096419334412. Validation loss: 2.7067465782165527.\n",
            "Epoch 942. Training loss: 0.7747063636779785. Validation loss: 2.7067463397979736.\n",
            "Epoch 943. Training loss: 0.7747032046318054. Validation loss: 2.7067456245422363.\n",
            "Epoch 944. Training loss: 0.774699866771698. Validation loss: 2.706745147705078.\n",
            "Epoch 945. Training loss: 0.7746965885162354. Validation loss: 2.70674467086792.\n",
            "Epoch 946. Training loss: 0.7746934294700623. Validation loss: 2.7067441940307617.\n",
            "Epoch 947. Training loss: 0.7746900916099548. Validation loss: 2.7067434787750244.\n",
            "Epoch 948. Training loss: 0.7746868133544922. Validation loss: 2.7067432403564453.\n",
            "Epoch 949. Training loss: 0.7746836543083191. Validation loss: 2.706742763519287.\n",
            "Epoch 950. Training loss: 0.7746803164482117. Validation loss: 2.70674204826355.\n",
            "Epoch 951. Training loss: 0.774677038192749. Validation loss: 2.7067418098449707.\n",
            "Epoch 952. Training loss: 0.7746737599372864. Validation loss: 2.7067413330078125.\n",
            "Epoch 953. Training loss: 0.7746705412864685. Validation loss: 2.706740617752075.\n",
            "Epoch 954. Training loss: 0.7746673226356506. Validation loss: 2.706740140914917.\n",
            "Epoch 955. Training loss: 0.7746641039848328. Validation loss: 2.706739902496338.\n",
            "Epoch 956. Training loss: 0.7746607661247253. Validation loss: 2.7067389488220215.\n",
            "Epoch 957. Training loss: 0.7746575474739075. Validation loss: 2.7067384719848633.\n",
            "Epoch 958. Training loss: 0.7746543288230896. Validation loss: 2.706737995147705.\n",
            "Epoch 959. Training loss: 0.7746509909629822. Validation loss: 2.706737756729126.\n",
            "Epoch 960. Training loss: 0.7746477127075195. Validation loss: 2.7067370414733887.\n",
            "Epoch 961. Training loss: 0.7746445536613464. Validation loss: 2.7067365646362305.\n",
            "Epoch 962. Training loss: 0.7746412754058838. Validation loss: 2.7067360877990723.\n",
            "Epoch 963. Training loss: 0.7746379971504211. Validation loss: 2.706735610961914.\n",
            "Epoch 964. Training loss: 0.7746346592903137. Validation loss: 2.7067348957061768.\n",
            "Epoch 965. Training loss: 0.7746314406394958. Validation loss: 2.7067346572875977.\n",
            "Epoch 966. Training loss: 0.7746281623840332. Validation loss: 2.7067341804504395.\n",
            "Epoch 967. Training loss: 0.7746248245239258. Validation loss: 2.706733226776123.\n",
            "Epoch 968. Training loss: 0.7746216654777527. Validation loss: 2.706733226776123.\n",
            "Epoch 969. Training loss: 0.77461838722229. Validation loss: 2.706732749938965.\n",
            "Epoch 970. Training loss: 0.7746150493621826. Validation loss: 2.7067320346832275.\n",
            "Epoch 971. Training loss: 0.7746118903160095. Validation loss: 2.7067313194274902.\n",
            "Epoch 972. Training loss: 0.7746085524559021. Validation loss: 2.706730842590332.\n",
            "Epoch 973. Training loss: 0.7746053338050842. Validation loss: 2.706730365753174.\n",
            "Epoch 974. Training loss: 0.7746019959449768. Validation loss: 2.7067298889160156.\n",
            "Epoch 975. Training loss: 0.7745987772941589. Validation loss: 2.7067294120788574.\n",
            "Epoch 976. Training loss: 0.7745954394340515. Validation loss: 2.706728935241699.\n",
            "Epoch 977. Training loss: 0.7745922207832336. Validation loss: 2.706727981567383.\n",
            "Epoch 978. Training loss: 0.7745888829231262. Validation loss: 2.7067277431488037.\n",
            "Epoch 979. Training loss: 0.7745855450630188. Validation loss: 2.7067275047302246.\n",
            "Epoch 980. Training loss: 0.7745823264122009. Validation loss: 2.7067267894744873.\n",
            "Epoch 981. Training loss: 0.7745791077613831. Validation loss: 2.706726312637329.\n",
            "Epoch 982. Training loss: 0.7745757699012756. Validation loss: 2.706725597381592.\n",
            "Epoch 983. Training loss: 0.7745725512504578. Validation loss: 2.7067253589630127.\n",
            "Epoch 984. Training loss: 0.7745692133903503. Validation loss: 2.7067248821258545.\n",
            "Epoch 985. Training loss: 0.7745659351348877. Validation loss: 2.706724166870117.\n",
            "Epoch 986. Training loss: 0.774562656879425. Validation loss: 2.706723928451538.\n",
            "Epoch 987. Training loss: 0.7745594382286072. Validation loss: 2.706723213195801.\n",
            "Epoch 988. Training loss: 0.7745561599731445. Validation loss: 2.7067224979400635.\n",
            "Epoch 989. Training loss: 0.7745528817176819. Validation loss: 2.7067222595214844.\n",
            "Epoch 990. Training loss: 0.774549663066864. Validation loss: 2.706721782684326.\n",
            "Epoch 991. Training loss: 0.7745463252067566. Validation loss: 2.706721067428589.\n",
            "Epoch 992. Training loss: 0.7745429873466492. Validation loss: 2.7067205905914307.\n",
            "Epoch 993. Training loss: 0.7745397090911865. Validation loss: 2.7067198753356934.\n",
            "Epoch 994. Training loss: 0.7745364308357239. Validation loss: 2.7067196369171143.\n",
            "Epoch 995. Training loss: 0.7745330929756165. Validation loss: 2.706719398498535.\n",
            "Epoch 996. Training loss: 0.7745298743247986. Validation loss: 2.706718921661377.\n",
            "Epoch 997. Training loss: 0.7745265960693359. Validation loss: 2.7067182064056396.\n",
            "Epoch 998. Training loss: 0.7745232582092285. Validation loss: 2.7067177295684814.\n",
            "Epoch 999. Training loss: 0.7745199203491211. Validation loss: 2.706717014312744.\n",
            "Epoch 1000. Training loss: 0.774516761302948. Validation loss: 2.706716537475586.\n",
            "Epoch 1001. Training loss: 0.7745134234428406. Validation loss: 2.7067160606384277.\n",
            "Epoch 1002. Training loss: 0.7745101451873779. Validation loss: 2.7067155838012695.\n",
            "Epoch 1003. Training loss: 0.7745068073272705. Validation loss: 2.7067151069641113.\n",
            "Epoch 1004. Training loss: 0.7745035290718079. Validation loss: 2.706714630126953.\n",
            "Epoch 1005. Training loss: 0.77450031042099. Validation loss: 2.706713914871216.\n",
            "Epoch 1006. Training loss: 0.7744969725608826. Validation loss: 2.7067131996154785.\n",
            "Epoch 1007. Training loss: 0.7744935154914856. Validation loss: 2.7067127227783203.\n",
            "Epoch 1008. Training loss: 0.7744903564453125. Validation loss: 2.706712007522583.\n",
            "Epoch 1009. Training loss: 0.7744870781898499. Validation loss: 2.706711769104004.\n",
            "Epoch 1010. Training loss: 0.774483859539032. Validation loss: 2.7067108154296875.\n",
            "Epoch 1011. Training loss: 0.7744805216789246. Validation loss: 2.7067105770111084.\n",
            "Epoch 1012. Training loss: 0.7744770646095276. Validation loss: 2.706709384918213.\n",
            "Epoch 1013. Training loss: 0.7744738459587097. Validation loss: 2.7067086696624756.\n",
            "Epoch 1014. Training loss: 0.7744703888893127. Validation loss: 2.7067081928253174.\n",
            "Epoch 1015. Training loss: 0.7744671702384949. Validation loss: 2.706707715988159.\n",
            "Epoch 1016. Training loss: 0.7744638323783875. Validation loss: 2.706707000732422.\n",
            "Epoch 1017. Training loss: 0.77446049451828. Validation loss: 2.7067065238952637.\n",
            "Epoch 1018. Training loss: 0.7744571566581726. Validation loss: 2.7067058086395264.\n",
            "Epoch 1019. Training loss: 0.77445387840271. Validation loss: 2.706705093383789.\n",
            "Epoch 1020. Training loss: 0.7744505405426025. Validation loss: 2.706704616546631.\n",
            "Epoch 1021. Training loss: 0.7744472026824951. Validation loss: 2.7067041397094727.\n",
            "Epoch 1022. Training loss: 0.7744438648223877. Validation loss: 2.7067036628723145.\n",
            "Epoch 1023. Training loss: 0.7744405269622803. Validation loss: 2.706702947616577.\n",
            "Epoch 1024. Training loss: 0.7744371294975281. Validation loss: 2.7067019939422607.\n",
            "Epoch 1025. Training loss: 0.7744338512420654. Validation loss: 2.7067015171051025.\n",
            "Epoch 1026. Training loss: 0.774430513381958. Validation loss: 2.7067008018493652.\n",
            "Epoch 1027. Training loss: 0.7744271159172058. Validation loss: 2.706700086593628.\n",
            "Epoch 1028. Training loss: 0.7744238376617432. Validation loss: 2.7066996097564697.\n",
            "Epoch 1029. Training loss: 0.7744204998016357. Validation loss: 2.7066988945007324.\n",
            "Epoch 1030. Training loss: 0.7744171023368835. Validation loss: 2.706698417663574.\n",
            "Epoch 1031. Training loss: 0.7744138240814209. Validation loss: 2.706697702407837.\n",
            "Epoch 1032. Training loss: 0.7744103074073792. Validation loss: 2.7066969871520996.\n",
            "Epoch 1033. Training loss: 0.774407148361206. Validation loss: 2.7066965103149414.\n",
            "Epoch 1034. Training loss: 0.7744036316871643. Validation loss: 2.706695556640625.\n",
            "Epoch 1035. Training loss: 0.7744004130363464. Validation loss: 2.706695079803467.\n",
            "Epoch 1036. Training loss: 0.7743969559669495. Validation loss: 2.7066946029663086.\n",
            "Epoch 1037. Training loss: 0.774393618106842. Validation loss: 2.706693649291992.\n",
            "Epoch 1038. Training loss: 0.7743902802467346. Validation loss: 2.706693172454834.\n",
            "Epoch 1039. Training loss: 0.7743868827819824. Validation loss: 2.7066922187805176.\n",
            "Epoch 1040. Training loss: 0.7743836045265198. Validation loss: 2.7066919803619385.\n",
            "Epoch 1041. Training loss: 0.7743802666664124. Validation loss: 2.706691026687622.\n",
            "Epoch 1042. Training loss: 0.7743768692016602. Validation loss: 2.706690549850464.\n",
            "Epoch 1043. Training loss: 0.7743735313415527. Validation loss: 2.7066900730133057.\n",
            "Epoch 1044. Training loss: 0.7743701934814453. Validation loss: 2.7066893577575684.\n",
            "Epoch 1045. Training loss: 0.7743668556213379. Validation loss: 2.706688642501831.\n",
            "Epoch 1046. Training loss: 0.7743634581565857. Validation loss: 2.706688165664673.\n",
            "Epoch 1047. Training loss: 0.7743601202964783. Validation loss: 2.7066872119903564.\n",
            "Epoch 1048. Training loss: 0.7743567824363708. Validation loss: 2.7066869735717773.\n",
            "Epoch 1049. Training loss: 0.7743533253669739. Validation loss: 2.70668625831604.\n",
            "Epoch 1050. Training loss: 0.774350106716156. Validation loss: 2.7066853046417236.\n",
            "Epoch 1051. Training loss: 0.774346649646759. Validation loss: 2.7066848278045654.\n",
            "Epoch 1052. Training loss: 0.7743432521820068. Validation loss: 2.706684112548828.\n",
            "Epoch 1053. Training loss: 0.7743399143218994. Validation loss: 2.70668363571167.\n",
            "Epoch 1054. Training loss: 0.774336576461792. Validation loss: 2.7066831588745117.\n",
            "Epoch 1055. Training loss: 0.7743332386016846. Validation loss: 2.7066822052001953.\n",
            "Epoch 1056. Training loss: 0.7743299007415771. Validation loss: 2.706681728363037.\n",
            "Epoch 1057. Training loss: 0.774326503276825. Validation loss: 2.7066810131073.\n",
            "Epoch 1058. Training loss: 0.7743231654167175. Validation loss: 2.7066802978515625.\n",
            "Epoch 1059. Training loss: 0.7743197083473206. Validation loss: 2.7066798210144043.\n",
            "Epoch 1060. Training loss: 0.7743163704872131. Validation loss: 2.706679105758667.\n",
            "Epoch 1061. Training loss: 0.7743129730224609. Validation loss: 2.7066781520843506.\n",
            "Epoch 1062. Training loss: 0.7743096351623535. Validation loss: 2.7066776752471924.\n",
            "Epoch 1063. Training loss: 0.7743062973022461. Validation loss: 2.7066774368286133.\n",
            "Epoch 1064. Training loss: 0.7743027806282043. Validation loss: 2.706676483154297.\n",
            "Epoch 1065. Training loss: 0.7742994427680969. Validation loss: 2.7066760063171387.\n",
            "Epoch 1066. Training loss: 0.7742961049079895. Validation loss: 2.7066752910614014.\n",
            "Epoch 1067. Training loss: 0.7742927074432373. Validation loss: 2.706674814224243.\n",
            "Epoch 1068. Training loss: 0.7742893695831299. Validation loss: 2.706674098968506.\n",
            "Epoch 1069. Training loss: 0.7742860317230225. Validation loss: 2.7066733837127686.\n",
            "Epoch 1070. Training loss: 0.7742826342582703. Validation loss: 2.7066726684570312.\n",
            "Epoch 1071. Training loss: 0.7742792963981628. Validation loss: 2.706671953201294.\n",
            "Epoch 1072. Training loss: 0.7742759585380554. Validation loss: 2.7066714763641357.\n",
            "Epoch 1073. Training loss: 0.7742725014686584. Validation loss: 2.7066705226898193.\n",
            "Epoch 1074. Training loss: 0.7742691040039062. Validation loss: 2.706669807434082.\n",
            "Epoch 1075. Training loss: 0.7742657661437988. Validation loss: 2.7066690921783447.\n",
            "Epoch 1076. Training loss: 0.7742623686790466. Validation loss: 2.7066686153411865.\n",
            "Epoch 1077. Training loss: 0.7742590308189392. Validation loss: 2.706667900085449.\n",
            "Epoch 1078. Training loss: 0.7742555737495422. Validation loss: 2.706667184829712.\n",
            "Epoch 1079. Training loss: 0.7742522358894348. Validation loss: 2.706666946411133.\n",
            "Epoch 1080. Training loss: 0.7742488980293274. Validation loss: 2.7066659927368164.\n",
            "Epoch 1081. Training loss: 0.7742455005645752. Validation loss: 2.706665277481079.\n",
            "Epoch 1082. Training loss: 0.7742419838905334. Validation loss: 2.7066650390625.\n",
            "Epoch 1083. Training loss: 0.774238646030426. Validation loss: 2.706664562225342.\n",
            "Epoch 1084. Training loss: 0.7742353081703186. Validation loss: 2.7066636085510254.\n",
            "Epoch 1085. Training loss: 0.7742319107055664. Validation loss: 2.706663131713867.\n",
            "Epoch 1086. Training loss: 0.7742285132408142. Validation loss: 2.706662178039551.\n",
            "Epoch 1087. Training loss: 0.7742252349853516. Validation loss: 2.7066617012023926.\n",
            "Epoch 1088. Training loss: 0.7742217183113098. Validation loss: 2.7066612243652344.\n",
            "Epoch 1089. Training loss: 0.7742183208465576. Validation loss: 2.706660270690918.\n",
            "Epoch 1090. Training loss: 0.7742149829864502. Validation loss: 2.7066597938537598.\n",
            "Epoch 1091. Training loss: 0.7742116451263428. Validation loss: 2.7066593170166016.\n",
            "Epoch 1092. Training loss: 0.7742082476615906. Validation loss: 2.706658363342285.\n",
            "Epoch 1093. Training loss: 0.7742047905921936. Validation loss: 2.706657648086548.\n",
            "Epoch 1094. Training loss: 0.7742014527320862. Validation loss: 2.7066569328308105.\n",
            "Epoch 1095. Training loss: 0.7741979956626892. Validation loss: 2.7066564559936523.\n",
            "Epoch 1096. Training loss: 0.7741946578025818. Validation loss: 2.706655740737915.\n",
            "Epoch 1097. Training loss: 0.7741913199424744. Validation loss: 2.706655263900757.\n",
            "Epoch 1098. Training loss: 0.7741878628730774. Validation loss: 2.7066543102264404.\n",
            "Epoch 1099. Training loss: 0.7741844654083252. Validation loss: 2.7066538333892822.\n",
            "Epoch 1100. Training loss: 0.774181067943573. Validation loss: 2.706653118133545.\n",
            "Epoch 1101. Training loss: 0.7741777300834656. Validation loss: 2.7066524028778076.\n",
            "Epoch 1102. Training loss: 0.7741742730140686. Validation loss: 2.7066519260406494.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1103. Training loss: 0.7741708755493164. Validation loss: 2.706651210784912.\n",
            "Epoch 1104. Training loss: 0.774167537689209. Validation loss: 2.706650495529175.\n",
            "Epoch 1105. Training loss: 0.7741641402244568. Validation loss: 2.7066500186920166.\n",
            "Epoch 1106. Training loss: 0.7741608023643494. Validation loss: 2.7066493034362793.\n",
            "Epoch 1107. Training loss: 0.7741573452949524. Validation loss: 2.706648588180542.\n",
            "Epoch 1108. Training loss: 0.7741539478302002. Validation loss: 2.7066478729248047.\n",
            "Epoch 1109. Training loss: 0.774150550365448. Validation loss: 2.7066471576690674.\n",
            "Epoch 1110. Training loss: 0.774147093296051. Validation loss: 2.706646680831909.\n",
            "Epoch 1111. Training loss: 0.7741437554359436. Validation loss: 2.706645965576172.\n",
            "Epoch 1112. Training loss: 0.7741403579711914. Validation loss: 2.7066452503204346.\n",
            "Epoch 1113. Training loss: 0.7741369605064392. Validation loss: 2.706644296646118.\n",
            "Epoch 1114. Training loss: 0.7741336226463318. Validation loss: 2.706644058227539.\n",
            "Epoch 1115. Training loss: 0.7741300463676453. Validation loss: 2.7066431045532227.\n",
            "Epoch 1116. Training loss: 0.7741267681121826. Validation loss: 2.7066426277160645.\n",
            "Epoch 1117. Training loss: 0.7741233706474304. Validation loss: 2.7066421508789062.\n",
            "Epoch 1118. Training loss: 0.774120032787323. Validation loss: 2.706641435623169.\n",
            "Epoch 1119. Training loss: 0.7741165161132812. Validation loss: 2.7066407203674316.\n",
            "Epoch 1120. Training loss: 0.774113118648529. Validation loss: 2.7066397666931152.\n",
            "Epoch 1121. Training loss: 0.7741096615791321. Validation loss: 2.706639289855957.\n",
            "Epoch 1122. Training loss: 0.7741062641143799. Validation loss: 2.7066385746002197.\n",
            "Epoch 1123. Training loss: 0.7741029262542725. Validation loss: 2.7066380977630615.\n",
            "Epoch 1124. Training loss: 0.7740995287895203. Validation loss: 2.7066376209259033.\n",
            "Epoch 1125. Training loss: 0.7740960717201233. Validation loss: 2.706636428833008.\n",
            "Epoch 1126. Training loss: 0.7740927338600159. Validation loss: 2.7066359519958496.\n",
            "Epoch 1127. Training loss: 0.7740892767906189. Validation loss: 2.7066352367401123.\n",
            "Epoch 1128. Training loss: 0.7740859389305115. Validation loss: 2.706634998321533.\n",
            "Epoch 1129. Training loss: 0.7740824222564697. Validation loss: 2.706634044647217.\n",
            "Epoch 1130. Training loss: 0.7740790843963623. Validation loss: 2.7066335678100586.\n",
            "Epoch 1131. Training loss: 0.7740755677223206. Validation loss: 2.7066328525543213.\n",
            "Epoch 1132. Training loss: 0.7740721702575684. Validation loss: 2.706632137298584.\n",
            "Epoch 1133. Training loss: 0.7740688323974609. Validation loss: 2.706631660461426.\n",
            "Epoch 1134. Training loss: 0.7740653157234192. Validation loss: 2.7066309452056885.\n",
            "Epoch 1135. Training loss: 0.7740619778633118. Validation loss: 2.706630229949951.\n",
            "Epoch 1136. Training loss: 0.7740585207939148. Validation loss: 2.706629753112793.\n",
            "Epoch 1137. Training loss: 0.7740551829338074. Validation loss: 2.7066287994384766.\n",
            "Epoch 1138. Training loss: 0.7740516662597656. Validation loss: 2.7066283226013184.\n",
            "Epoch 1139. Training loss: 0.7740483283996582. Validation loss: 2.706627130508423.\n",
            "Epoch 1140. Training loss: 0.774044930934906. Validation loss: 2.7066268920898438.\n",
            "Epoch 1141. Training loss: 0.7740415930747986. Validation loss: 2.7066261768341064.\n",
            "Epoch 1142. Training loss: 0.7740380764007568. Validation loss: 2.7066256999969482.\n",
            "Epoch 1143. Training loss: 0.7740346789360046. Validation loss: 2.706624746322632.\n",
            "Epoch 1144. Training loss: 0.7740312218666077. Validation loss: 2.7066240310668945.\n",
            "Epoch 1145. Training loss: 0.7740278244018555. Validation loss: 2.7066235542297363.\n",
            "Epoch 1146. Training loss: 0.7740243077278137. Validation loss: 2.706622838973999.\n",
            "Epoch 1147. Training loss: 0.7740209102630615. Validation loss: 2.7066221237182617.\n",
            "Epoch 1148. Training loss: 0.7740175127983093. Validation loss: 2.7066216468811035.\n",
            "Epoch 1149. Training loss: 0.7740142345428467. Validation loss: 2.706620931625366.\n",
            "Epoch 1150. Training loss: 0.7740106582641602. Validation loss: 2.706620693206787.\n",
            "Epoch 1151. Training loss: 0.7740073204040527. Validation loss: 2.7066195011138916.\n",
            "Epoch 1152. Training loss: 0.7740039229393005. Validation loss: 2.7066192626953125.\n",
            "Epoch 1153. Training loss: 0.7740004062652588. Validation loss: 2.706618309020996.\n",
            "Epoch 1154. Training loss: 0.7739970684051514. Validation loss: 2.706617593765259.\n",
            "Epoch 1155. Training loss: 0.7739934921264648. Validation loss: 2.7066168785095215.\n",
            "Epoch 1156. Training loss: 0.7739901542663574. Validation loss: 2.706616163253784.\n",
            "Epoch 1157. Training loss: 0.7739867568016052. Validation loss: 2.706615447998047.\n",
            "Epoch 1158. Training loss: 0.7739832997322083. Validation loss: 2.7066147327423096.\n",
            "Epoch 1159. Training loss: 0.7739798426628113. Validation loss: 2.7066140174865723.\n",
            "Epoch 1160. Training loss: 0.7739765048027039. Validation loss: 2.706613779067993.\n",
            "Epoch 1161. Training loss: 0.7739729881286621. Validation loss: 2.706613063812256.\n",
            "Epoch 1162. Training loss: 0.7739694714546204. Validation loss: 2.7066121101379395.\n",
            "Epoch 1163. Training loss: 0.7739661335945129. Validation loss: 2.7066116333007812.\n",
            "Epoch 1164. Training loss: 0.7739627361297607. Validation loss: 2.706610918045044.\n",
            "Epoch 1165. Training loss: 0.773959219455719. Validation loss: 2.7066104412078857.\n",
            "Epoch 1166. Training loss: 0.7739558219909668. Validation loss: 2.7066097259521484.\n",
            "Epoch 1167. Training loss: 0.7739524245262146. Validation loss: 2.706608772277832.\n",
            "Epoch 1168. Training loss: 0.7739489674568176. Validation loss: 2.706608295440674.\n",
            "Epoch 1169. Training loss: 0.7739455699920654. Validation loss: 2.7066073417663574.\n",
            "Epoch 1170. Training loss: 0.7739421725273132. Validation loss: 2.706606864929199.\n",
            "Epoch 1171. Training loss: 0.7739388346672058. Validation loss: 2.706606388092041.\n",
            "Epoch 1172. Training loss: 0.7739354968070984. Validation loss: 2.7066054344177246.\n",
            "Epoch 1173. Training loss: 0.7739322185516357. Validation loss: 2.7066049575805664.\n",
            "Epoch 1174. Training loss: 0.7739288210868835. Validation loss: 2.706604242324829.\n",
            "Epoch 1175. Training loss: 0.7739254832267761. Validation loss: 2.706603527069092.\n",
            "Epoch 1176. Training loss: 0.7739221453666687. Validation loss: 2.7066030502319336.\n",
            "Epoch 1177. Training loss: 0.7739188075065613. Validation loss: 2.706602096557617.\n",
            "Epoch 1178. Training loss: 0.7739155292510986. Validation loss: 2.706601619720459.\n",
            "Epoch 1179. Training loss: 0.7739121317863464. Validation loss: 2.706601142883301.\n",
            "Epoch 1180. Training loss: 0.773908793926239. Validation loss: 2.7066001892089844.\n",
            "Epoch 1181. Training loss: 0.7739054560661316. Validation loss: 2.706599712371826.\n",
            "Epoch 1182. Training loss: 0.7739021182060242. Validation loss: 2.706599235534668.\n",
            "Epoch 1183. Training loss: 0.7738987803459167. Validation loss: 2.7065982818603516.\n",
            "Epoch 1184. Training loss: 0.7738954424858093. Validation loss: 2.7065978050231934.\n",
            "Epoch 1185. Training loss: 0.7738921642303467. Validation loss: 2.706596851348877.\n",
            "Epoch 1186. Training loss: 0.7738888263702393. Validation loss: 2.7065958976745605.\n",
            "Epoch 1187. Training loss: 0.7738854289054871. Validation loss: 2.7065958976745605.\n",
            "Epoch 1188. Training loss: 0.7738820910453796. Validation loss: 2.7065951824188232.\n",
            "Epoch 1189. Training loss: 0.773878812789917. Validation loss: 2.706594467163086.\n",
            "Epoch 1190. Training loss: 0.7738754749298096. Validation loss: 2.7065935134887695.\n",
            "Epoch 1191. Training loss: 0.7738721966743469. Validation loss: 2.7065930366516113.\n",
            "Epoch 1192. Training loss: 0.7738688588142395. Validation loss: 2.706592559814453.\n",
            "Epoch 1193. Training loss: 0.7738655209541321. Validation loss: 2.706592082977295.\n",
            "Epoch 1194. Training loss: 0.7738621234893799. Validation loss: 2.7065911293029785.\n",
            "Epoch 1195. Training loss: 0.7738588452339172. Validation loss: 2.7065906524658203.\n",
            "Epoch 1196. Training loss: 0.7738555073738098. Validation loss: 2.706589698791504.\n",
            "Epoch 1197. Training loss: 0.7738521695137024. Validation loss: 2.7065889835357666.\n",
            "Epoch 1198. Training loss: 0.773848831653595. Validation loss: 2.7065882682800293.\n",
            "Epoch 1199. Training loss: 0.7738456130027771. Validation loss: 2.706587791442871.\n",
            "Epoch 1200. Training loss: 0.7738422751426697. Validation loss: 2.706587076187134.\n",
            "Epoch 1201. Training loss: 0.7738388180732727. Validation loss: 2.7065863609313965.\n",
            "Epoch 1202. Training loss: 0.7738356590270996. Validation loss: 2.7065858840942383.\n",
            "Epoch 1203. Training loss: 0.7738321423530579. Validation loss: 2.70658540725708.\n",
            "Epoch 1204. Training loss: 0.77382892370224. Validation loss: 2.7065844535827637.\n",
            "Epoch 1205. Training loss: 0.7738255858421326. Validation loss: 2.7065839767456055.\n",
            "Epoch 1206. Training loss: 0.7738222479820251. Validation loss: 2.706583023071289.\n",
            "Epoch 1207. Training loss: 0.7738189101219177. Validation loss: 2.706582546234131.\n",
            "Epoch 1208. Training loss: 0.7738155722618103. Validation loss: 2.7065818309783936.\n",
            "Epoch 1209. Training loss: 0.7738122940063477. Validation loss: 2.7065811157226562.\n",
            "Epoch 1210. Training loss: 0.7738088965415955. Validation loss: 2.706580400466919.\n",
            "Epoch 1211. Training loss: 0.773805558681488. Validation loss: 2.7065799236297607.\n",
            "Epoch 1212. Training loss: 0.7738022208213806. Validation loss: 2.7065792083740234.\n",
            "Epoch 1213. Training loss: 0.773798942565918. Validation loss: 2.7065787315368652.\n",
            "Epoch 1214. Training loss: 0.7737956047058105. Validation loss: 2.706578016281128.\n",
            "Epoch 1215. Training loss: 0.7737922072410583. Validation loss: 2.7065773010253906.\n",
            "Epoch 1216. Training loss: 0.7737889289855957. Validation loss: 2.706576347351074.\n",
            "Epoch 1217. Training loss: 0.7737855911254883. Validation loss: 2.706575870513916.\n",
            "Epoch 1218. Training loss: 0.7737821936607361. Validation loss: 2.7065751552581787.\n",
            "Epoch 1219. Training loss: 0.7737788558006287. Validation loss: 2.7065744400024414.\n",
            "Epoch 1220. Training loss: 0.773775577545166. Validation loss: 2.706573724746704.\n",
            "Epoch 1221. Training loss: 0.7737722396850586. Validation loss: 2.706573247909546.\n",
            "Epoch 1222. Training loss: 0.7737689018249512. Validation loss: 2.7065725326538086.\n",
            "Epoch 1223. Training loss: 0.7737655639648438. Validation loss: 2.7065720558166504.\n",
            "Epoch 1224. Training loss: 0.7737622261047363. Validation loss: 2.706571340560913.\n",
            "Epoch 1225. Training loss: 0.7737589478492737. Validation loss: 2.706570625305176.\n",
            "Epoch 1226. Training loss: 0.7737556099891663. Validation loss: 2.7065699100494385.\n",
            "Epoch 1227. Training loss: 0.7737522721290588. Validation loss: 2.706569194793701.\n",
            "Epoch 1228. Training loss: 0.773749053478241. Validation loss: 2.706568479537964.\n",
            "Epoch 1229. Training loss: 0.7737457156181335. Validation loss: 2.7065680027008057.\n",
            "Epoch 1230. Training loss: 0.7737423777580261. Validation loss: 2.7065675258636475.\n",
            "Epoch 1231. Training loss: 0.7737390398979187. Validation loss: 2.706566572189331.\n",
            "Epoch 1232. Training loss: 0.7737357020378113. Validation loss: 2.706566095352173.\n",
            "Epoch 1233. Training loss: 0.7737324237823486. Validation loss: 2.7065653800964355.\n",
            "Epoch 1234. Training loss: 0.7737290859222412. Validation loss: 2.7065646648406982.\n",
            "Epoch 1235. Training loss: 0.773725688457489. Validation loss: 2.706563949584961.\n",
            "Epoch 1236. Training loss: 0.7737224102020264. Validation loss: 2.7065632343292236.\n",
            "Epoch 1237. Training loss: 0.7737191319465637. Validation loss: 2.7065627574920654.\n",
            "Epoch 1238. Training loss: 0.7737157940864563. Validation loss: 2.706561803817749.\n",
            "Epoch 1239. Training loss: 0.7737125754356384. Validation loss: 2.706561326980591.\n",
            "Epoch 1240. Training loss: 0.773709237575531. Validation loss: 2.7065608501434326.\n",
            "Epoch 1241. Training loss: 0.7737058997154236. Validation loss: 2.7065601348876953.\n",
            "Epoch 1242. Training loss: 0.7737026214599609. Validation loss: 2.706559658050537.\n",
            "Epoch 1243. Training loss: 0.7736992835998535. Validation loss: 2.7065587043762207.\n",
            "Epoch 1244. Training loss: 0.7736959457397461. Validation loss: 2.7065579891204834.\n",
            "Epoch 1245. Training loss: 0.7736926078796387. Validation loss: 2.706557273864746.\n",
            "Epoch 1246. Training loss: 0.7736892700195312. Validation loss: 2.706556797027588.\n",
            "Epoch 1247. Training loss: 0.7736859321594238. Validation loss: 2.7065563201904297.\n",
            "Epoch 1248. Training loss: 0.7736826539039612. Validation loss: 2.7065556049346924.\n",
            "Epoch 1249. Training loss: 0.7736794352531433. Validation loss: 2.706554889678955.\n",
            "Epoch 1250. Training loss: 0.7736760973930359. Validation loss: 2.7065541744232178.\n",
            "Epoch 1251. Training loss: 0.7736726403236389. Validation loss: 2.7065536975860596.\n",
            "Epoch 1252. Training loss: 0.773669421672821. Validation loss: 2.7065529823303223.\n",
            "Epoch 1253. Training loss: 0.7736660838127136. Validation loss: 2.706552267074585.\n",
            "Epoch 1254. Training loss: 0.7736627459526062. Validation loss: 2.7065513134002686.\n",
            "Epoch 1255. Training loss: 0.7736594080924988. Validation loss: 2.7065508365631104.\n",
            "Epoch 1256. Training loss: 0.7736560702323914. Validation loss: 2.706550359725952.\n",
            "Epoch 1257. Training loss: 0.7736527323722839. Validation loss: 2.706549882888794.\n",
            "Epoch 1258. Training loss: 0.7736494541168213. Validation loss: 2.7065486907958984.\n",
            "Epoch 1259. Training loss: 0.7736460566520691. Validation loss: 2.7065482139587402.\n",
            "Epoch 1260. Training loss: 0.7736427783966064. Validation loss: 2.706547737121582.\n",
            "Epoch 1261. Training loss: 0.773639440536499. Validation loss: 2.7065467834472656.\n",
            "Epoch 1262. Training loss: 0.7736360430717468. Validation loss: 2.7065465450286865.\n",
            "Epoch 1263. Training loss: 0.7736327648162842. Validation loss: 2.706545829772949.\n",
            "Epoch 1264. Training loss: 0.7736294269561768. Validation loss: 2.706545114517212.\n",
            "Epoch 1265. Training loss: 0.7736261487007141. Validation loss: 2.7065443992614746.\n",
            "Epoch 1266. Training loss: 0.7736227512359619. Validation loss: 2.7065436840057373.\n",
            "Epoch 1267. Training loss: 0.7736194133758545. Validation loss: 2.706543207168579.\n",
            "Epoch 1268. Training loss: 0.7736162543296814. Validation loss: 2.7065422534942627.\n",
            "Epoch 1269. Training loss: 0.7736127376556396. Validation loss: 2.7065415382385254.\n",
            "Epoch 1270. Training loss: 0.773609459400177. Validation loss: 2.706540822982788.\n",
            "Epoch 1271. Training loss: 0.7736061215400696. Validation loss: 2.70654034614563.\n",
            "Epoch 1272. Training loss: 0.7736029028892517. Validation loss: 2.7065396308898926.\n",
            "Epoch 1273. Training loss: 0.7735994458198547. Validation loss: 2.7065391540527344.\n",
            "Epoch 1274. Training loss: 0.7735962271690369. Validation loss: 2.706538200378418.\n",
            "Epoch 1275. Training loss: 0.7735927700996399. Validation loss: 2.706537961959839.\n",
            "Epoch 1276. Training loss: 0.7735894322395325. Validation loss: 2.7065370082855225.\n",
            "Epoch 1277. Training loss: 0.773586094379425. Validation loss: 2.7065367698669434.\n",
            "Epoch 1278. Training loss: 0.7735827565193176. Validation loss: 2.706536054611206.\n",
            "Epoch 1279. Training loss: 0.7735795378684998. Validation loss: 2.7065351009368896.\n",
            "Epoch 1280. Training loss: 0.7735760807991028. Validation loss: 2.7065343856811523.\n",
            "Epoch 1281. Training loss: 0.7735727429389954. Validation loss: 2.706533908843994.\n",
            "Epoch 1282. Training loss: 0.7735695242881775. Validation loss: 2.706533193588257.\n",
            "Epoch 1283. Training loss: 0.7735660672187805. Validation loss: 2.7065324783325195.\n",
            "Epoch 1284. Training loss: 0.7735627293586731. Validation loss: 2.7065322399139404.\n",
            "Epoch 1285. Training loss: 0.7735593914985657. Validation loss: 2.706531047821045.\n",
            "Epoch 1286. Training loss: 0.7735560536384583. Validation loss: 2.706530809402466.\n",
            "Epoch 1287. Training loss: 0.7735528349876404. Validation loss: 2.7065298557281494.\n",
            "Epoch 1288. Training loss: 0.7735493779182434. Validation loss: 2.706529378890991.\n",
            "Epoch 1289. Training loss: 0.773546040058136. Validation loss: 2.706528425216675.\n",
            "Epoch 1290. Training loss: 0.7735428214073181. Validation loss: 2.7065277099609375.\n",
            "Epoch 1291. Training loss: 0.7735393643379211. Validation loss: 2.7065272331237793.\n",
            "Epoch 1292. Training loss: 0.7735360264778137. Validation loss: 2.706526756286621.\n",
            "Epoch 1293. Training loss: 0.7735326886177063. Validation loss: 2.706526041030884.\n",
            "Epoch 1294. Training loss: 0.7735293507575989. Validation loss: 2.7065253257751465.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1295. Training loss: 0.7735260128974915. Validation loss: 2.7065248489379883.\n",
            "Epoch 1296. Training loss: 0.773522675037384. Validation loss: 2.706523895263672.\n",
            "Epoch 1297. Training loss: 0.7735192775726318. Validation loss: 2.7065231800079346.\n",
            "Epoch 1298. Training loss: 0.7735159993171692. Validation loss: 2.7065227031707764.\n",
            "Epoch 1299. Training loss: 0.7735126614570618. Validation loss: 2.70652174949646.\n",
            "Epoch 1300. Training loss: 0.7735092639923096. Validation loss: 2.7065212726593018.\n",
            "Epoch 1301. Training loss: 0.7735059261322021. Validation loss: 2.7065205574035645.\n",
            "Epoch 1302. Training loss: 0.7735025882720947. Validation loss: 2.7065203189849854.\n",
            "Epoch 1303. Training loss: 0.7734993100166321. Validation loss: 2.706519365310669.\n",
            "Epoch 1304. Training loss: 0.7734959125518799. Validation loss: 2.7065186500549316.\n",
            "Epoch 1305. Training loss: 0.7734925150871277. Validation loss: 2.7065179347991943.\n",
            "Epoch 1306. Training loss: 0.773489236831665. Validation loss: 2.706517457962036.\n",
            "Epoch 1307. Training loss: 0.7734858393669128. Validation loss: 2.7065162658691406.\n",
            "Epoch 1308. Training loss: 0.7734825015068054. Validation loss: 2.7065162658691406.\n",
            "Epoch 1309. Training loss: 0.773479163646698. Validation loss: 2.706515312194824.\n",
            "Epoch 1310. Training loss: 0.7734758257865906. Validation loss: 2.706514835357666.\n",
            "Epoch 1311. Training loss: 0.7734724879264832. Validation loss: 2.7065141201019287.\n",
            "Epoch 1312. Training loss: 0.7734691500663757. Validation loss: 2.7065131664276123.\n",
            "Epoch 1313. Training loss: 0.7734656929969788. Validation loss: 2.706512689590454.\n",
            "Epoch 1314. Training loss: 0.7734623551368713. Validation loss: 2.706511974334717.\n",
            "Epoch 1315. Training loss: 0.7734590172767639. Validation loss: 2.7065112590789795.\n",
            "Epoch 1316. Training loss: 0.7734556794166565. Validation loss: 2.7065107822418213.\n",
            "Epoch 1317. Training loss: 0.7734523415565491. Validation loss: 2.706510066986084.\n",
            "Epoch 1318. Training loss: 0.7734490036964417. Validation loss: 2.706509590148926.\n",
            "Epoch 1319. Training loss: 0.7734456062316895. Validation loss: 2.7065086364746094.\n",
            "Epoch 1320. Training loss: 0.773442268371582. Validation loss: 2.706508159637451.\n",
            "Epoch 1321. Training loss: 0.7734389305114746. Validation loss: 2.706507682800293.\n",
            "Epoch 1322. Training loss: 0.7734355926513672. Validation loss: 2.7065069675445557.\n",
            "Epoch 1323. Training loss: 0.773432195186615. Validation loss: 2.7065062522888184.\n",
            "Epoch 1324. Training loss: 0.7734288573265076. Validation loss: 2.706505537033081.\n",
            "Epoch 1325. Training loss: 0.7734255194664001. Validation loss: 2.7065048217773438.\n",
            "Epoch 1326. Training loss: 0.7734220623970032. Validation loss: 2.7065041065216064.\n",
            "Epoch 1327. Training loss: 0.7734187245368958. Validation loss: 2.7065036296844482.\n",
            "Epoch 1328. Training loss: 0.7734153866767883. Validation loss: 2.706502676010132.\n",
            "Epoch 1329. Training loss: 0.7734119892120361. Validation loss: 2.7065019607543945.\n",
            "Epoch 1330. Training loss: 0.7734087109565735. Validation loss: 2.7065014839172363.\n",
            "Epoch 1331. Training loss: 0.7734053730964661. Validation loss: 2.70650053024292.\n",
            "Epoch 1332. Training loss: 0.7734019756317139. Validation loss: 2.706500291824341.\n",
            "Epoch 1333. Training loss: 0.7733986377716064. Validation loss: 2.7064993381500244.\n",
            "Epoch 1334. Training loss: 0.7733951210975647. Validation loss: 2.706498622894287.\n",
            "Epoch 1335. Training loss: 0.7733917832374573. Validation loss: 2.706498146057129.\n",
            "Epoch 1336. Training loss: 0.7733884453773499. Validation loss: 2.7064976692199707.\n",
            "Epoch 1337. Training loss: 0.7733851075172424. Validation loss: 2.706496477127075.\n",
            "Epoch 1338. Training loss: 0.773381769657135. Validation loss: 2.706496000289917.\n",
            "Epoch 1339. Training loss: 0.7733783721923828. Validation loss: 2.7064952850341797.\n",
            "Epoch 1340. Training loss: 0.7733750343322754. Validation loss: 2.7064948081970215.\n",
            "Epoch 1341. Training loss: 0.7733716368675232. Validation loss: 2.7064943313598633.\n",
            "Epoch 1342. Training loss: 0.7733682990074158. Validation loss: 2.706493377685547.\n",
            "Epoch 1343. Training loss: 0.7733649611473083. Validation loss: 2.7064926624298096.\n",
            "Epoch 1344. Training loss: 0.7733615040779114. Validation loss: 2.7064919471740723.\n",
            "Epoch 1345. Training loss: 0.7733582854270935. Validation loss: 2.706490993499756.\n",
            "Epoch 1346. Training loss: 0.7733548283576965. Validation loss: 2.706490993499756.\n",
            "Epoch 1347. Training loss: 0.7733514904975891. Validation loss: 2.7064902782440186.\n",
            "Epoch 1348. Training loss: 0.7733481526374817. Validation loss: 2.7064895629882812.\n",
            "Epoch 1349. Training loss: 0.7733446955680847. Validation loss: 2.706489086151123.\n",
            "Epoch 1350. Training loss: 0.7733413577079773. Validation loss: 2.7064881324768066.\n",
            "Epoch 1351. Training loss: 0.7733380198478699. Validation loss: 2.7064876556396484.\n",
            "Epoch 1352. Training loss: 0.7733345627784729. Validation loss: 2.706486940383911.\n",
            "Epoch 1353. Training loss: 0.7733311653137207. Validation loss: 2.706486225128174.\n",
            "Epoch 1354. Training loss: 0.7733278274536133. Validation loss: 2.7064852714538574.\n",
            "Epoch 1355. Training loss: 0.7733244895935059. Validation loss: 2.706484794616699.\n",
            "Epoch 1356. Training loss: 0.7733210921287537. Validation loss: 2.706484079360962.\n",
            "Epoch 1357. Training loss: 0.7733177542686462. Validation loss: 2.706483840942383.\n",
            "Epoch 1358. Training loss: 0.7733142971992493. Validation loss: 2.7064826488494873.\n",
            "Epoch 1359. Training loss: 0.7733110785484314. Validation loss: 2.706482172012329.\n",
            "Epoch 1360. Training loss: 0.7733076214790344. Validation loss: 2.7064812183380127.\n",
            "Epoch 1361. Training loss: 0.773304283618927. Validation loss: 2.7064809799194336.\n",
            "Epoch 1362. Training loss: 0.7733008861541748. Validation loss: 2.7064805030822754.\n",
            "Epoch 1363. Training loss: 0.7732974886894226. Validation loss: 2.706479549407959.\n",
            "Epoch 1364. Training loss: 0.7732940316200256. Validation loss: 2.7064788341522217.\n",
            "Epoch 1365. Training loss: 0.7732906937599182. Validation loss: 2.7064783573150635.\n",
            "Epoch 1366. Training loss: 0.773287296295166. Validation loss: 2.7064778804779053.\n",
            "Epoch 1367. Training loss: 0.7732839584350586. Validation loss: 2.706476926803589.\n",
            "Epoch 1368. Training loss: 0.7732805609703064. Validation loss: 2.7064762115478516.\n",
            "Epoch 1369. Training loss: 0.773277223110199. Validation loss: 2.706475257873535.\n",
            "Epoch 1370. Training loss: 0.7732738852500916. Validation loss: 2.706474781036377.\n",
            "Epoch 1371. Training loss: 0.7732704281806946. Validation loss: 2.7064743041992188.\n",
            "Epoch 1372. Training loss: 0.7732670903205872. Validation loss: 2.7064733505249023.\n",
            "Epoch 1373. Training loss: 0.7732636332511902. Validation loss: 2.706472873687744.\n",
            "Epoch 1374. Training loss: 0.7732602953910828. Validation loss: 2.706472158432007.\n",
            "Epoch 1375. Training loss: 0.7732569575309753. Validation loss: 2.7064716815948486.\n",
            "Epoch 1376. Training loss: 0.7732535004615784. Validation loss: 2.7064707279205322.\n",
            "Epoch 1377. Training loss: 0.7732500433921814. Validation loss: 2.706470251083374.\n",
            "Epoch 1378. Training loss: 0.7732467651367188. Validation loss: 2.7064695358276367.\n",
            "Epoch 1379. Training loss: 0.7732433676719666. Validation loss: 2.7064685821533203.\n",
            "Epoch 1380. Training loss: 0.7732398509979248. Validation loss: 2.706468105316162.\n",
            "Epoch 1381. Training loss: 0.7732365727424622. Validation loss: 2.706467390060425.\n",
            "Epoch 1382. Training loss: 0.77323317527771. Validation loss: 2.7064671516418457.\n",
            "Epoch 1383. Training loss: 0.7732297778129578. Validation loss: 2.7064661979675293.\n",
            "Epoch 1384. Training loss: 0.7732264399528503. Validation loss: 2.706465244293213.\n",
            "Epoch 1385. Training loss: 0.7732231020927429. Validation loss: 2.7064647674560547.\n",
            "Epoch 1386. Training loss: 0.7732195854187012. Validation loss: 2.7064638137817383.\n",
            "Epoch 1387. Training loss: 0.7732162475585938. Validation loss: 2.706462860107422.\n",
            "Epoch 1388. Training loss: 0.773212730884552. Validation loss: 2.7064621448516846.\n",
            "Epoch 1389. Training loss: 0.7732093930244446. Validation loss: 2.7064614295959473.\n",
            "Epoch 1390. Training loss: 0.7732060551643372. Validation loss: 2.70646071434021.\n",
            "Epoch 1391. Training loss: 0.7732027173042297. Validation loss: 2.7064597606658936.\n",
            "Epoch 1392. Training loss: 0.7731993198394775. Validation loss: 2.7064590454101562.\n",
            "Epoch 1393. Training loss: 0.7731959223747253. Validation loss: 2.7064578533172607.\n",
            "Epoch 1394. Training loss: 0.7731924653053284. Validation loss: 2.7064571380615234.\n",
            "Epoch 1395. Training loss: 0.7731890678405762. Validation loss: 2.706456422805786.\n",
            "Epoch 1396. Training loss: 0.7731857299804688. Validation loss: 2.7064552307128906.\n",
            "Epoch 1397. Training loss: 0.7731823325157166. Validation loss: 2.7064545154571533.\n",
            "Epoch 1398. Training loss: 0.7731788754463196. Validation loss: 2.706453800201416.\n",
            "Epoch 1399. Training loss: 0.7731755375862122. Validation loss: 2.7064526081085205.\n",
            "Epoch 1400. Training loss: 0.77317214012146. Validation loss: 2.7064521312713623.\n",
            "Epoch 1401. Training loss: 0.7731688022613525. Validation loss: 2.706450939178467.\n",
            "Epoch 1402. Training loss: 0.7731654047966003. Validation loss: 2.7064497470855713.\n",
            "Epoch 1403. Training loss: 0.7731620669364929. Validation loss: 2.706449031829834.\n",
            "Epoch 1404. Training loss: 0.773158609867096. Validation loss: 2.7064480781555176.\n",
            "Epoch 1405. Training loss: 0.7731552124023438. Validation loss: 2.706447124481201.\n",
            "Epoch 1406. Training loss: 0.7731518745422363. Validation loss: 2.7064461708068848.\n",
            "Epoch 1407. Training loss: 0.7731484770774841. Validation loss: 2.7064454555511475.\n",
            "Epoch 1408. Training loss: 0.7731451392173767. Validation loss: 2.706444263458252.\n",
            "Epoch 1409. Training loss: 0.7731416821479797. Validation loss: 2.7064435482025146.\n",
            "Epoch 1410. Training loss: 0.7731383442878723. Validation loss: 2.7064428329467773.\n",
            "Epoch 1411. Training loss: 0.7731349468231201. Validation loss: 2.706441879272461.\n",
            "Epoch 1412. Training loss: 0.7731315493583679. Validation loss: 2.7064411640167236.\n",
            "Epoch 1413. Training loss: 0.7731282114982605. Validation loss: 2.7064404487609863.\n",
            "Epoch 1414. Training loss: 0.7731247544288635. Validation loss: 2.7064390182495117.\n",
            "Epoch 1415. Training loss: 0.7731215357780457. Validation loss: 2.7064383029937744.\n",
            "Epoch 1416. Training loss: 0.7731180787086487. Validation loss: 2.706437587738037.\n",
            "Epoch 1417. Training loss: 0.7731146812438965. Validation loss: 2.7064366340637207.\n",
            "Epoch 1418. Training loss: 0.7731113433837891. Validation loss: 2.7064356803894043.\n",
            "Epoch 1419. Training loss: 0.7731079459190369. Validation loss: 2.706434965133667.\n",
            "Epoch 1420. Training loss: 0.7731044888496399. Validation loss: 2.7064340114593506.\n",
            "Epoch 1421. Training loss: 0.773101270198822. Validation loss: 2.706433057785034.\n",
            "Epoch 1422. Training loss: 0.773097813129425. Validation loss: 2.7064321041107178.\n",
            "Epoch 1423. Training loss: 0.7730944156646729. Validation loss: 2.7064313888549805.\n",
            "Epoch 1424. Training loss: 0.7730910778045654. Validation loss: 2.706430435180664.\n",
            "Epoch 1425. Training loss: 0.7730876803398132. Validation loss: 2.7064294815063477.\n",
            "Epoch 1426. Training loss: 0.7730844020843506. Validation loss: 2.7064287662506104.\n",
            "Epoch 1427. Training loss: 0.7730810642242432. Validation loss: 2.706428050994873.\n",
            "Epoch 1428. Training loss: 0.7730775475502014. Validation loss: 2.7064268589019775.\n",
            "Epoch 1429. Training loss: 0.773074209690094. Validation loss: 2.7064261436462402.\n",
            "Epoch 1430. Training loss: 0.7730708718299866. Validation loss: 2.7064249515533447.\n",
            "Epoch 1431. Training loss: 0.7730674743652344. Validation loss: 2.7064242362976074.\n",
            "Epoch 1432. Training loss: 0.7730640769004822. Validation loss: 2.706423044204712.\n",
            "Epoch 1433. Training loss: 0.7730607390403748. Validation loss: 2.7064223289489746.\n",
            "Epoch 1434. Training loss: 0.7730574011802673. Validation loss: 2.706421375274658.\n",
            "Epoch 1435. Training loss: 0.7730539441108704. Validation loss: 2.706420660018921.\n",
            "Epoch 1436. Training loss: 0.7730505466461182. Validation loss: 2.7064199447631836.\n",
            "Epoch 1437. Training loss: 0.7730472087860107. Validation loss: 2.7064194679260254.\n",
            "Epoch 1438. Training loss: 0.7730438113212585. Validation loss: 2.706418514251709.\n",
            "Epoch 1439. Training loss: 0.7730404734611511. Validation loss: 2.7064177989959717.\n",
            "Epoch 1440. Training loss: 0.7730371356010437. Validation loss: 2.7064173221588135.\n",
            "Epoch 1441. Training loss: 0.7730336785316467. Validation loss: 2.706416606903076.\n",
            "Epoch 1442. Training loss: 0.7730302810668945. Validation loss: 2.7064154148101807.\n",
            "Epoch 1443. Training loss: 0.7730269432067871. Validation loss: 2.7064146995544434.\n",
            "Epoch 1444. Training loss: 0.7730236053466797. Validation loss: 2.706413984298706.\n",
            "Epoch 1445. Training loss: 0.7730202078819275. Validation loss: 2.7064132690429688.\n",
            "Epoch 1446. Training loss: 0.7730167508125305. Validation loss: 2.7064123153686523.\n",
            "Epoch 1447. Training loss: 0.7730134129524231. Validation loss: 2.706411838531494.\n",
            "Epoch 1448. Training loss: 0.7730100750923157. Validation loss: 2.706411361694336.\n",
            "Epoch 1449. Training loss: 0.7730066776275635. Validation loss: 2.7064104080200195.\n",
            "Epoch 1450. Training loss: 0.7730032801628113. Validation loss: 2.706409454345703.\n",
            "Epoch 1451. Training loss: 0.7729999423027039. Validation loss: 2.706408739089966.\n",
            "Epoch 1452. Training loss: 0.7729964256286621. Validation loss: 2.7064082622528076.\n",
            "Epoch 1453. Training loss: 0.7729930877685547. Validation loss: 2.706407308578491.\n",
            "Epoch 1454. Training loss: 0.7729897499084473. Validation loss: 2.706406593322754.\n",
            "Epoch 1455. Training loss: 0.7729863524436951. Validation loss: 2.7064058780670166.\n",
            "Epoch 1456. Training loss: 0.7729830145835876. Validation loss: 2.7064049243927.\n",
            "Epoch 1457. Training loss: 0.7729795575141907. Validation loss: 2.706403970718384.\n",
            "Epoch 1458. Training loss: 0.7729761600494385. Validation loss: 2.7064034938812256.\n",
            "Epoch 1459. Training loss: 0.772972822189331. Validation loss: 2.7064027786254883.\n",
            "Epoch 1460. Training loss: 0.7729694843292236. Validation loss: 2.70640230178833.\n",
            "Epoch 1461. Training loss: 0.7729659676551819. Validation loss: 2.7064011096954346.\n",
            "Epoch 1462. Training loss: 0.772962749004364. Validation loss: 2.706400156021118.\n",
            "Epoch 1463. Training loss: 0.772959291934967. Validation loss: 2.70639967918396.\n",
            "Epoch 1464. Training loss: 0.7729558944702148. Validation loss: 2.7063987255096436.\n",
            "Epoch 1465. Training loss: 0.7729525566101074. Validation loss: 2.7063982486724854.\n",
            "Epoch 1466. Training loss: 0.7729490399360657. Validation loss: 2.706397533416748.\n",
            "Epoch 1467. Training loss: 0.7729456424713135. Validation loss: 2.7063968181610107.\n",
            "Epoch 1468. Training loss: 0.772942304611206. Validation loss: 2.7063961029052734.\n",
            "Epoch 1469. Training loss: 0.7729389667510986. Validation loss: 2.706395149230957.\n",
            "Epoch 1470. Training loss: 0.7729356288909912. Validation loss: 2.706394672393799.\n",
            "Epoch 1471. Training loss: 0.7729321122169495. Validation loss: 2.7063937187194824.\n",
            "Epoch 1472. Training loss: 0.7729287147521973. Validation loss: 2.706392765045166.\n",
            "Epoch 1473. Training loss: 0.7729253768920898. Validation loss: 2.7063920497894287.\n",
            "Epoch 1474. Training loss: 0.7729220390319824. Validation loss: 2.7063913345336914.\n",
            "Epoch 1475. Training loss: 0.7729185223579407. Validation loss: 2.706390857696533.\n",
            "Epoch 1476. Training loss: 0.7729151844978333. Validation loss: 2.706389904022217.\n",
            "Epoch 1477. Training loss: 0.7729117274284363. Validation loss: 2.7063891887664795.\n",
            "Epoch 1478. Training loss: 0.7729082703590393. Validation loss: 2.706388473510742.\n",
            "Epoch 1479. Training loss: 0.7729050517082214. Validation loss: 2.706387519836426.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1480. Training loss: 0.7729015946388245. Validation loss: 2.7063865661621094.\n",
            "Epoch 1481. Training loss: 0.772898256778717. Validation loss: 2.706385850906372.\n",
            "Epoch 1482. Training loss: 0.7728947997093201. Validation loss: 2.706385374069214.\n",
            "Epoch 1483. Training loss: 0.7728913426399231. Validation loss: 2.7063846588134766.\n",
            "Epoch 1484. Training loss: 0.7728879451751709. Validation loss: 2.70638370513916.\n",
            "Epoch 1485. Training loss: 0.7728845477104187. Validation loss: 2.7063827514648438.\n",
            "Epoch 1486. Training loss: 0.7728812098503113. Validation loss: 2.7063825130462646.\n",
            "Epoch 1487. Training loss: 0.7728778719902039. Validation loss: 2.706381320953369.\n",
            "Epoch 1488. Training loss: 0.7728743553161621. Validation loss: 2.706380844116211.\n",
            "Epoch 1489. Training loss: 0.7728710174560547. Validation loss: 2.7063796520233154.\n",
            "Epoch 1490. Training loss: 0.7728675007820129. Validation loss: 2.7063791751861572.\n",
            "Epoch 1491. Training loss: 0.7728641629219055. Validation loss: 2.70637845993042.\n",
            "Epoch 1492. Training loss: 0.7728608250617981. Validation loss: 2.7063777446746826.\n",
            "Epoch 1493. Training loss: 0.7728574275970459. Validation loss: 2.706376791000366.\n",
            "Epoch 1494. Training loss: 0.7728539109230042. Validation loss: 2.70637583732605.\n",
            "Epoch 1495. Training loss: 0.772850513458252. Validation loss: 2.7063753604888916.\n",
            "Epoch 1496. Training loss: 0.7728471755981445. Validation loss: 2.7063746452331543.\n",
            "Epoch 1497. Training loss: 0.7728436589241028. Validation loss: 2.706373929977417.\n",
            "Epoch 1498. Training loss: 0.7728403210639954. Validation loss: 2.7063729763031006.\n",
            "Epoch 1499. Training loss: 0.7728368639945984. Validation loss: 2.7063722610473633.\n",
            "Epoch 1500. Training loss: 0.772833526134491. Validation loss: 2.706371545791626.\n",
            "Epoch 1501. Training loss: 0.772830069065094. Validation loss: 2.7063705921173096.\n",
            "Epoch 1502. Training loss: 0.7728266716003418. Validation loss: 2.7063698768615723.\n",
            "Epoch 1503. Training loss: 0.7728232741355896. Validation loss: 2.706369400024414.\n",
            "Epoch 1504. Training loss: 0.7728198170661926. Validation loss: 2.7063684463500977.\n",
            "Epoch 1505. Training loss: 0.7728164792060852. Validation loss: 2.7063677310943604.\n",
            "Epoch 1506. Training loss: 0.7728130221366882. Validation loss: 2.706367015838623.\n",
            "Epoch 1507. Training loss: 0.7728096842765808. Validation loss: 2.7063658237457275.\n",
            "Epoch 1508. Training loss: 0.7728063464164734. Validation loss: 2.7063653469085693.\n",
            "Epoch 1509. Training loss: 0.7728027701377869. Validation loss: 2.706364631652832.\n",
            "Epoch 1510. Training loss: 0.7727993130683899. Validation loss: 2.7063636779785156.\n",
            "Epoch 1511. Training loss: 0.7727959752082825. Validation loss: 2.7063629627227783.\n",
            "Epoch 1512. Training loss: 0.7727925181388855. Validation loss: 2.706362247467041.\n",
            "Epoch 1513. Training loss: 0.7727891802787781. Validation loss: 2.706361770629883.\n",
            "Epoch 1514. Training loss: 0.7727856636047363. Validation loss: 2.7063608169555664.\n",
            "Epoch 1515. Training loss: 0.7727823257446289. Validation loss: 2.70635986328125.\n",
            "Epoch 1516. Training loss: 0.7727789282798767. Validation loss: 2.706359386444092.\n",
            "Epoch 1517. Training loss: 0.7727754712104797. Validation loss: 2.7063586711883545.\n",
            "Epoch 1518. Training loss: 0.7727720141410828. Validation loss: 2.706357479095459.\n",
            "Epoch 1519. Training loss: 0.7727685570716858. Validation loss: 2.7063567638397217.\n",
            "Epoch 1520. Training loss: 0.7727651596069336. Validation loss: 2.7063558101654053.\n",
            "Epoch 1521. Training loss: 0.7727617621421814. Validation loss: 2.706355094909668.\n",
            "Epoch 1522. Training loss: 0.7727583050727844. Validation loss: 2.7063546180725098.\n",
            "Epoch 1523. Training loss: 0.7727549076080322. Validation loss: 2.7063536643981934.\n",
            "Epoch 1524. Training loss: 0.77275151014328. Validation loss: 2.706352949142456.\n",
            "Epoch 1525. Training loss: 0.7727480530738831. Validation loss: 2.706352472305298.\n",
            "Epoch 1526. Training loss: 0.7727447152137756. Validation loss: 2.7063515186309814.\n",
            "Epoch 1527. Training loss: 0.7727412581443787. Validation loss: 2.706350564956665.\n",
            "Epoch 1528. Training loss: 0.7727379202842712. Validation loss: 2.706350088119507.\n",
            "Epoch 1529. Training loss: 0.7727344036102295. Validation loss: 2.7063488960266113.\n",
            "Epoch 1530. Training loss: 0.7727310061454773. Validation loss: 2.706348419189453.\n",
            "Epoch 1531. Training loss: 0.7727276682853699. Validation loss: 2.7063474655151367.\n",
            "Epoch 1532. Training loss: 0.7727240920066833. Validation loss: 2.7063467502593994.\n",
            "Epoch 1533. Training loss: 0.7727207541465759. Validation loss: 2.706346035003662.\n",
            "Epoch 1534. Training loss: 0.7727172374725342. Validation loss: 2.7063450813293457.\n",
            "Epoch 1535. Training loss: 0.772713840007782. Validation loss: 2.7063446044921875.\n",
            "Epoch 1536. Training loss: 0.7727105021476746. Validation loss: 2.706343412399292.\n",
            "Epoch 1537. Training loss: 0.7727069854736328. Validation loss: 2.706342935562134.\n",
            "Epoch 1538. Training loss: 0.7727035880088806. Validation loss: 2.7063419818878174.\n",
            "Epoch 1539. Training loss: 0.7727001309394836. Validation loss: 2.70634126663208.\n",
            "Epoch 1540. Training loss: 0.7726967334747314. Validation loss: 2.7063403129577637.\n",
            "Epoch 1541. Training loss: 0.7726932168006897. Validation loss: 2.7063395977020264.\n",
            "Epoch 1542. Training loss: 0.7726898193359375. Validation loss: 2.70633864402771.\n",
            "Epoch 1543. Training loss: 0.7726864814758301. Validation loss: 2.706338405609131.\n",
            "Epoch 1544. Training loss: 0.7726829648017883. Validation loss: 2.7063376903533936.\n",
            "Epoch 1545. Training loss: 0.7726795673370361. Validation loss: 2.706336498260498.\n",
            "Epoch 1546. Training loss: 0.7726761698722839. Validation loss: 2.7063357830047607.\n",
            "Epoch 1547. Training loss: 0.7726726531982422. Validation loss: 2.7063350677490234.\n",
            "Epoch 1548. Training loss: 0.77266925573349. Validation loss: 2.706334114074707.\n",
            "Epoch 1549. Training loss: 0.7726659178733826. Validation loss: 2.706333637237549.\n",
            "Epoch 1550. Training loss: 0.7726624608039856. Validation loss: 2.7063326835632324.\n",
            "Epoch 1551. Training loss: 0.7726590037345886. Validation loss: 2.706331729888916.\n",
            "Epoch 1552. Training loss: 0.7726555466651917. Validation loss: 2.7063310146331787.\n",
            "Epoch 1553. Training loss: 0.7726521492004395. Validation loss: 2.7063302993774414.\n",
            "Epoch 1554. Training loss: 0.7726485729217529. Validation loss: 2.706329345703125.\n",
            "Epoch 1555. Training loss: 0.7726451754570007. Validation loss: 2.706328868865967.\n",
            "Epoch 1556. Training loss: 0.7726417183876038. Validation loss: 2.7063281536102295.\n",
            "Epoch 1557. Training loss: 0.7726383805274963. Validation loss: 2.706327199935913.\n",
            "Epoch 1558. Training loss: 0.7726349234580994. Validation loss: 2.706326484680176.\n",
            "Epoch 1559. Training loss: 0.7726314067840576. Validation loss: 2.7063257694244385.\n",
            "Epoch 1560. Training loss: 0.7726280093193054. Validation loss: 2.706324815750122.\n",
            "Epoch 1561. Training loss: 0.7726245522499084. Validation loss: 2.7063238620758057.\n",
            "Epoch 1562. Training loss: 0.7726211547851562. Validation loss: 2.7063231468200684.\n",
            "Epoch 1563. Training loss: 0.772617757320404. Validation loss: 2.706322431564331.\n",
            "Epoch 1564. Training loss: 0.7726142406463623. Validation loss: 2.7063217163085938.\n",
            "Epoch 1565. Training loss: 0.7726109027862549. Validation loss: 2.7063207626342773.\n",
            "Epoch 1566. Training loss: 0.7726073861122131. Validation loss: 2.70632004737854.\n",
            "Epoch 1567. Training loss: 0.7726038098335266. Validation loss: 2.7063193321228027.\n",
            "Epoch 1568. Training loss: 0.7726004123687744. Validation loss: 2.7063186168670654.\n",
            "Epoch 1569. Training loss: 0.7725970149040222. Validation loss: 2.706317901611328.\n",
            "Epoch 1570. Training loss: 0.7725934982299805. Validation loss: 2.7063169479370117.\n",
            "Epoch 1571. Training loss: 0.7725901007652283. Validation loss: 2.7063162326812744.\n",
            "Epoch 1572. Training loss: 0.7725866436958313. Validation loss: 2.706315040588379.\n",
            "Epoch 1573. Training loss: 0.7725832462310791. Validation loss: 2.7063143253326416.\n",
            "Epoch 1574. Training loss: 0.7725796699523926. Validation loss: 2.7063136100769043.\n",
            "Epoch 1575. Training loss: 0.7725763320922852. Validation loss: 2.706312656402588.\n",
            "Epoch 1576. Training loss: 0.7725728154182434. Validation loss: 2.7063119411468506.\n",
            "Epoch 1577. Training loss: 0.7725694179534912. Validation loss: 2.7063112258911133.\n",
            "Epoch 1578. Training loss: 0.7725659012794495. Validation loss: 2.706310272216797.\n",
            "Epoch 1579. Training loss: 0.7725625038146973. Validation loss: 2.7063093185424805.\n",
            "Epoch 1580. Training loss: 0.7725589871406555. Validation loss: 2.706308364868164.\n",
            "Epoch 1581. Training loss: 0.7725555300712585. Validation loss: 2.7063076496124268.\n",
            "Epoch 1582. Training loss: 0.7725521922111511. Validation loss: 2.7063064575195312.\n",
            "Epoch 1583. Training loss: 0.7725487351417542. Validation loss: 2.706305980682373.\n",
            "Epoch 1584. Training loss: 0.7725452780723572. Validation loss: 2.7063050270080566.\n",
            "Epoch 1585. Training loss: 0.7725417613983154. Validation loss: 2.7063040733337402.\n",
            "Epoch 1586. Training loss: 0.7725383639335632. Validation loss: 2.706303358078003.\n",
            "Epoch 1587. Training loss: 0.7725347876548767. Validation loss: 2.7063024044036865.\n",
            "Epoch 1588. Training loss: 0.7725315093994141. Validation loss: 2.70630145072937.\n",
            "Epoch 1589. Training loss: 0.7725279331207275. Validation loss: 2.706300735473633.\n",
            "Epoch 1590. Training loss: 0.7725245356559753. Validation loss: 2.7063000202178955.\n",
            "Epoch 1591. Training loss: 0.7725210189819336. Validation loss: 2.706299304962158.\n",
            "Epoch 1592. Training loss: 0.7725175023078918. Validation loss: 2.706298351287842.\n",
            "Epoch 1593. Training loss: 0.7725141048431396. Validation loss: 2.7062973976135254.\n",
            "Epoch 1594. Training loss: 0.7725105881690979. Validation loss: 2.706296443939209.\n",
            "Epoch 1595. Training loss: 0.7725071310997009. Validation loss: 2.7062954902648926.\n",
            "Epoch 1596. Training loss: 0.772503674030304. Validation loss: 2.7062950134277344.\n",
            "Epoch 1597. Training loss: 0.772500216960907. Validation loss: 2.706293821334839.\n",
            "Epoch 1598. Training loss: 0.7724967002868652. Validation loss: 2.7062933444976807.\n",
            "Epoch 1599. Training loss: 0.772493302822113. Validation loss: 2.7062923908233643.\n",
            "Epoch 1600. Training loss: 0.7724898457527161. Validation loss: 2.706291437149048.\n",
            "Epoch 1601. Training loss: 0.7724863886833191. Validation loss: 2.7062902450561523.\n",
            "Epoch 1602. Training loss: 0.7724828720092773. Validation loss: 2.706289768218994.\n",
            "Epoch 1603. Training loss: 0.7724793553352356. Validation loss: 2.7062885761260986.\n",
            "Epoch 1604. Training loss: 0.7724760174751282. Validation loss: 2.7062883377075195.\n",
            "Epoch 1605. Training loss: 0.7724725604057312. Validation loss: 2.706287384033203.\n",
            "Epoch 1606. Training loss: 0.7724689841270447. Validation loss: 2.7062864303588867.\n",
            "Epoch 1607. Training loss: 0.7724655270576477. Validation loss: 2.7062854766845703.\n",
            "Epoch 1608. Training loss: 0.7724621295928955. Validation loss: 2.706284523010254.\n",
            "Epoch 1609. Training loss: 0.7724586129188538. Validation loss: 2.7062838077545166.\n",
            "Epoch 1610. Training loss: 0.7724552154541016. Validation loss: 2.7062828540802.\n",
            "Epoch 1611. Training loss: 0.7724516987800598. Validation loss: 2.706282377243042.\n",
            "Epoch 1612. Training loss: 0.7724481225013733. Validation loss: 2.7062811851501465.\n",
            "Epoch 1613. Training loss: 0.7724447250366211. Validation loss: 2.706280469894409.\n",
            "Epoch 1614. Training loss: 0.7724411487579346. Validation loss: 2.706279754638672.\n",
            "Epoch 1615. Training loss: 0.7724378108978271. Validation loss: 2.7062783241271973.\n",
            "Epoch 1616. Training loss: 0.7724342942237854. Validation loss: 2.706277847290039.\n",
            "Epoch 1617. Training loss: 0.7724308371543884. Validation loss: 2.7062768936157227.\n",
            "Epoch 1618. Training loss: 0.7724273204803467. Validation loss: 2.7062761783599854.\n",
            "Epoch 1619. Training loss: 0.7724239230155945. Validation loss: 2.706275463104248.\n",
            "Epoch 1620. Training loss: 0.7724204063415527. Validation loss: 2.7062745094299316.\n",
            "Epoch 1621. Training loss: 0.7724170088768005. Validation loss: 2.7062740325927734.\n",
            "Epoch 1622. Training loss: 0.772413432598114. Validation loss: 2.706272840499878.\n",
            "Epoch 1623. Training loss: 0.7724099159240723. Validation loss: 2.7062718868255615.\n",
            "Epoch 1624. Training loss: 0.7724065184593201. Validation loss: 2.706271171569824.\n",
            "Epoch 1625. Training loss: 0.7724030613899231. Validation loss: 2.706270217895508.\n",
            "Epoch 1626. Training loss: 0.7723996043205261. Validation loss: 2.7062695026397705.\n",
            "Epoch 1627. Training loss: 0.7723960876464844. Validation loss: 2.706268787384033.\n",
            "Epoch 1628. Training loss: 0.7723925709724426. Validation loss: 2.706267833709717.\n",
            "Epoch 1629. Training loss: 0.7723891139030457. Validation loss: 2.7062668800354004.\n",
            "Epoch 1630. Training loss: 0.7723855972290039. Validation loss: 2.706266164779663.\n",
            "Epoch 1631. Training loss: 0.7723822593688965. Validation loss: 2.706265449523926.\n",
            "Epoch 1632. Training loss: 0.7723785042762756. Validation loss: 2.7062642574310303.\n",
            "Epoch 1633. Training loss: 0.7723751068115234. Validation loss: 2.706263303756714.\n",
            "Epoch 1634. Training loss: 0.7723717093467712. Validation loss: 2.7062628269195557.\n",
            "Epoch 1635. Training loss: 0.7723681330680847. Validation loss: 2.70626163482666.\n",
            "Epoch 1636. Training loss: 0.7723646759986877. Validation loss: 2.7062606811523438.\n",
            "Epoch 1637. Training loss: 0.7723612785339355. Validation loss: 2.7062602043151855.\n",
            "Epoch 1638. Training loss: 0.7723577618598938. Validation loss: 2.706259250640869.\n",
            "Epoch 1639. Training loss: 0.7723543047904968. Validation loss: 2.706258535385132.\n",
            "Epoch 1640. Training loss: 0.7723507881164551. Validation loss: 2.7062580585479736.\n",
            "Epoch 1641. Training loss: 0.7723472714424133. Validation loss: 2.7062573432922363.\n",
            "Epoch 1642. Training loss: 0.7723438143730164. Validation loss: 2.706256866455078.\n",
            "Epoch 1643. Training loss: 0.7723403573036194. Validation loss: 2.706256151199341.\n",
            "Epoch 1644. Training loss: 0.7723367810249329. Validation loss: 2.7062556743621826.\n",
            "Epoch 1645. Training loss: 0.7723333835601807. Validation loss: 2.7062549591064453.\n",
            "Epoch 1646. Training loss: 0.7723298668861389. Validation loss: 2.706254243850708.\n",
            "Epoch 1647. Training loss: 0.7723264098167419. Validation loss: 2.7062532901763916.\n",
            "Epoch 1648. Training loss: 0.7723228335380554. Validation loss: 2.7062528133392334.\n",
            "Epoch 1649. Training loss: 0.772319495677948. Validation loss: 2.706252098083496.\n",
            "Epoch 1650. Training loss: 0.7723159193992615. Validation loss: 2.706251621246338.\n",
            "Epoch 1651. Training loss: 0.7723124623298645. Validation loss: 2.7062511444091797.\n",
            "Epoch 1652. Training loss: 0.772308886051178. Validation loss: 2.7062501907348633.\n",
            "Epoch 1653. Training loss: 0.772305428981781. Validation loss: 2.706249713897705.\n",
            "Epoch 1654. Training loss: 0.7723019123077393. Validation loss: 2.7062489986419678.\n",
            "Epoch 1655. Training loss: 0.7722983956336975. Validation loss: 2.7062482833862305.\n",
            "Epoch 1656. Training loss: 0.772294819355011. Validation loss: 2.706247568130493.\n",
            "Epoch 1657. Training loss: 0.7722914814949036. Validation loss: 2.7062466144561768.\n",
            "Epoch 1658. Training loss: 0.7722880244255066. Validation loss: 2.7062458992004395.\n",
            "Epoch 1659. Training loss: 0.7722844481468201. Validation loss: 2.7062456607818604.\n",
            "Epoch 1660. Training loss: 0.7722809910774231. Validation loss: 2.706244945526123.\n",
            "Epoch 1661. Training loss: 0.7722773551940918. Validation loss: 2.7062439918518066.\n",
            "Epoch 1662. Training loss: 0.7722739577293396. Validation loss: 2.7062437534332275.\n",
            "Epoch 1663. Training loss: 0.7722704410552979. Validation loss: 2.7062432765960693.\n",
            "Epoch 1664. Training loss: 0.7722668647766113. Validation loss: 2.706242561340332.\n",
            "Epoch 1665. Training loss: 0.7722634673118591. Validation loss: 2.7062416076660156.\n",
            "Epoch 1666. Training loss: 0.7722599506378174. Validation loss: 2.7062411308288574.\n",
            "Epoch 1667. Training loss: 0.7722564339637756. Validation loss: 2.706240653991699.\n",
            "Epoch 1668. Training loss: 0.7722529768943787. Validation loss: 2.706239938735962.\n",
            "Epoch 1669. Training loss: 0.7722494006156921. Validation loss: 2.7062394618988037.\n",
            "Epoch 1670. Training loss: 0.7722459435462952. Validation loss: 2.706238269805908.\n",
            "Epoch 1671. Training loss: 0.7722423672676086. Validation loss: 2.706237554550171.\n",
            "Epoch 1672. Training loss: 0.7722389101982117. Validation loss: 2.7062370777130127.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1673. Training loss: 0.7722353935241699. Validation loss: 2.7062363624572754.\n",
            "Epoch 1674. Training loss: 0.7722318768501282. Validation loss: 2.706235885620117.\n",
            "Epoch 1675. Training loss: 0.7722284197807312. Validation loss: 2.70623517036438.\n",
            "Epoch 1676. Training loss: 0.7722249031066895. Validation loss: 2.7062346935272217.\n",
            "Epoch 1677. Training loss: 0.7722213864326477. Validation loss: 2.7062337398529053.\n",
            "Epoch 1678. Training loss: 0.7722178101539612. Validation loss: 2.706233263015747.\n",
            "Epoch 1679. Training loss: 0.772214412689209. Validation loss: 2.7062325477600098.\n",
            "Epoch 1680. Training loss: 0.7722108364105225. Validation loss: 2.7062318325042725.\n",
            "Epoch 1681. Training loss: 0.7722073197364807. Validation loss: 2.7062313556671143.\n",
            "Epoch 1682. Training loss: 0.7722038626670837. Validation loss: 2.706230640411377.\n",
            "Epoch 1683. Training loss: 0.7722002863883972. Validation loss: 2.7062296867370605.\n",
            "Epoch 1684. Training loss: 0.7721967697143555. Validation loss: 2.7062292098999023.\n",
            "Epoch 1685. Training loss: 0.7721932530403137. Validation loss: 2.706228733062744.\n",
            "Epoch 1686. Training loss: 0.7721898555755615. Validation loss: 2.7062277793884277.\n",
            "Epoch 1687. Training loss: 0.7721863389015198. Validation loss: 2.7062275409698486.\n",
            "Epoch 1688. Training loss: 0.7721827626228333. Validation loss: 2.7062268257141113.\n",
            "Epoch 1689. Training loss: 0.7721793055534363. Validation loss: 2.706225872039795.\n",
            "Epoch 1690. Training loss: 0.7721757292747498. Validation loss: 2.7062253952026367.\n",
            "Epoch 1691. Training loss: 0.772172212600708. Validation loss: 2.7062249183654785.\n",
            "Epoch 1692. Training loss: 0.7721686959266663. Validation loss: 2.706224203109741.\n",
            "Epoch 1693. Training loss: 0.7721651196479797. Validation loss: 2.706223487854004.\n",
            "Epoch 1694. Training loss: 0.7721617221832275. Validation loss: 2.7062225341796875.\n",
            "Epoch 1695. Training loss: 0.772158145904541. Validation loss: 2.7062220573425293.\n",
            "Epoch 1696. Training loss: 0.7721546292304993. Validation loss: 2.706221342086792.\n",
            "Epoch 1697. Training loss: 0.7721511721611023. Validation loss: 2.7062206268310547.\n",
            "Epoch 1698. Training loss: 0.7721476554870605. Validation loss: 2.7062201499938965.\n",
            "Epoch 1699. Training loss: 0.7721441388130188. Validation loss: 2.706219434738159.\n",
            "Epoch 1700. Training loss: 0.7721405625343323. Validation loss: 2.7062184810638428.\n",
            "Epoch 1701. Training loss: 0.7721371054649353. Validation loss: 2.7062182426452637.\n",
            "Epoch 1702. Training loss: 0.7721335887908936. Validation loss: 2.7062175273895264.\n",
            "Epoch 1703. Training loss: 0.772130012512207. Validation loss: 2.706216812133789.\n",
            "Epoch 1704. Training loss: 0.7721264362335205. Validation loss: 2.7062160968780518.\n",
            "Epoch 1705. Training loss: 0.7721230387687683. Validation loss: 2.7062153816223145.\n",
            "Epoch 1706. Training loss: 0.7721195220947266. Validation loss: 2.706214666366577.\n",
            "Epoch 1707. Training loss: 0.7721160054206848. Validation loss: 2.706214189529419.\n",
            "Epoch 1708. Training loss: 0.7721123695373535. Validation loss: 2.7062134742736816.\n",
            "Epoch 1709. Training loss: 0.7721089720726013. Validation loss: 2.7062127590179443.\n",
            "Epoch 1710. Training loss: 0.7721054553985596. Validation loss: 2.706212043762207.\n",
            "Epoch 1711. Training loss: 0.772101879119873. Validation loss: 2.7062113285064697.\n",
            "Epoch 1712. Training loss: 0.7720983028411865. Validation loss: 2.7062110900878906.\n",
            "Epoch 1713. Training loss: 0.7720947861671448. Validation loss: 2.706210136413574.\n",
            "Epoch 1714. Training loss: 0.7720913290977478. Validation loss: 2.706209659576416.\n",
            "Epoch 1715. Training loss: 0.772087812423706. Validation loss: 2.706209182739258.\n",
            "Epoch 1716. Training loss: 0.7720842361450195. Validation loss: 2.7062079906463623.\n",
            "Epoch 1717. Training loss: 0.7720807194709778. Validation loss: 2.706207513809204.\n",
            "Epoch 1718. Training loss: 0.7720771431922913. Validation loss: 2.706206798553467.\n",
            "Epoch 1719. Training loss: 0.7720737457275391. Validation loss: 2.7062063217163086.\n",
            "Epoch 1720. Training loss: 0.7720701694488525. Validation loss: 2.7062056064605713.\n",
            "Epoch 1721. Training loss: 0.7720666527748108. Validation loss: 2.706204891204834.\n",
            "Epoch 1722. Training loss: 0.7720631957054138. Validation loss: 2.7062041759490967.\n",
            "Epoch 1723. Training loss: 0.7720596790313721. Validation loss: 2.7062034606933594.\n",
            "Epoch 1724. Training loss: 0.7720560431480408. Validation loss: 2.706202507019043.\n",
            "Epoch 1725. Training loss: 0.772052526473999. Validation loss: 2.7062020301818848.\n",
            "Epoch 1726. Training loss: 0.7720490097999573. Validation loss: 2.7062013149261475.\n",
            "Epoch 1727. Training loss: 0.7720454335212708. Validation loss: 2.70620059967041.\n",
            "Epoch 1728. Training loss: 0.7720418572425842. Validation loss: 2.706200122833252.\n",
            "Epoch 1729. Training loss: 0.772038459777832. Validation loss: 2.7061991691589355.\n",
            "Epoch 1730. Training loss: 0.7720348238945007. Validation loss: 2.7061989307403564.\n",
            "Epoch 1731. Training loss: 0.772031307220459. Validation loss: 2.706197738647461.\n",
            "Epoch 1732. Training loss: 0.7720277905464172. Validation loss: 2.7061972618103027.\n",
            "Epoch 1733. Training loss: 0.7720243334770203. Validation loss: 2.7061965465545654.\n",
            "Epoch 1734. Training loss: 0.7720207571983337. Validation loss: 2.706195831298828.\n",
            "Epoch 1735. Training loss: 0.772017240524292. Validation loss: 2.706195116043091.\n",
            "Epoch 1736. Training loss: 0.7720136642456055. Validation loss: 2.7061946392059326.\n",
            "Epoch 1737. Training loss: 0.7720101475715637. Validation loss: 2.7061939239501953.\n",
            "Epoch 1738. Training loss: 0.7720065712928772. Validation loss: 2.706193447113037.\n",
            "Epoch 1739. Training loss: 0.7720031142234802. Validation loss: 2.7061920166015625.\n",
            "Epoch 1740. Training loss: 0.7719995379447937. Validation loss: 2.7061917781829834.\n",
            "Epoch 1741. Training loss: 0.7719959616661072. Validation loss: 2.706191062927246.\n",
            "Epoch 1742. Training loss: 0.7719924449920654. Validation loss: 2.706190347671509.\n",
            "Epoch 1743. Training loss: 0.7719888687133789. Validation loss: 2.7061896324157715.\n",
            "Epoch 1744. Training loss: 0.7719853520393372. Validation loss: 2.7061893939971924.\n",
            "Epoch 1745. Training loss: 0.7719818949699402. Validation loss: 2.706188440322876.\n",
            "Epoch 1746. Training loss: 0.7719781994819641. Validation loss: 2.7061879634857178.\n",
            "Epoch 1747. Training loss: 0.7719747424125671. Validation loss: 2.7061874866485596.\n",
            "Epoch 1748. Training loss: 0.7719711661338806. Validation loss: 2.706186532974243.\n",
            "Epoch 1749. Training loss: 0.7719677090644836. Validation loss: 2.706186056137085.\n",
            "Epoch 1750. Training loss: 0.7719641327857971. Validation loss: 2.7061851024627686.\n",
            "Epoch 1751. Training loss: 0.7719604969024658. Validation loss: 2.7061846256256104.\n",
            "Epoch 1752. Training loss: 0.7719569206237793. Validation loss: 2.706183671951294.\n",
            "Epoch 1753. Training loss: 0.7719535827636719. Validation loss: 2.706183433532715.\n",
            "Epoch 1754. Training loss: 0.7719499468803406. Validation loss: 2.7061824798583984.\n",
            "Epoch 1755. Training loss: 0.7719464302062988. Validation loss: 2.7061822414398193.\n",
            "Epoch 1756. Training loss: 0.7719427943229675. Validation loss: 2.706181526184082.\n",
            "Epoch 1757. Training loss: 0.7719393372535706. Validation loss: 2.7061805725097656.\n",
            "Epoch 1758. Training loss: 0.771935760974884. Validation loss: 2.7061800956726074.\n",
            "Epoch 1759. Training loss: 0.7719321250915527. Validation loss: 2.706179141998291.\n",
            "Epoch 1760. Training loss: 0.771928608417511. Validation loss: 2.706178665161133.\n",
            "Epoch 1761. Training loss: 0.7719250321388245. Validation loss: 2.7061781883239746.\n",
            "Epoch 1762. Training loss: 0.7719214558601379. Validation loss: 2.706177234649658.\n",
            "Epoch 1763. Training loss: 0.7719180583953857. Validation loss: 2.706176519393921.\n",
            "Epoch 1764. Training loss: 0.7719143033027649. Validation loss: 2.706176280975342.\n",
            "Epoch 1765. Training loss: 0.7719109654426575. Validation loss: 2.7061750888824463.\n",
            "Epoch 1766. Training loss: 0.7719073295593262. Validation loss: 2.706174612045288.\n",
            "Epoch 1767. Training loss: 0.7719038128852844. Validation loss: 2.70617413520813.\n",
            "Epoch 1768. Training loss: 0.7719003558158875. Validation loss: 2.7061731815338135.\n",
            "Epoch 1769. Training loss: 0.7718966603279114. Validation loss: 2.7061727046966553.\n",
            "Epoch 1770. Training loss: 0.7718930840492249. Validation loss: 2.706171989440918.\n",
            "Epoch 1771. Training loss: 0.7718896269798279. Validation loss: 2.7061715126037598.\n",
            "Epoch 1772. Training loss: 0.7718861103057861. Validation loss: 2.7061707973480225.\n",
            "Epoch 1773. Training loss: 0.7718825340270996. Validation loss: 2.706169605255127.\n",
            "Epoch 1774. Training loss: 0.7718789577484131. Validation loss: 2.7061691284179688.\n",
            "Epoch 1775. Training loss: 0.7718753814697266. Validation loss: 2.7061686515808105.\n",
            "Epoch 1776. Training loss: 0.77187180519104. Validation loss: 2.7061679363250732.\n",
            "Epoch 1777. Training loss: 0.7718682289123535. Validation loss: 2.706167221069336.\n",
            "Epoch 1778. Training loss: 0.7718647122383118. Validation loss: 2.7061662673950195.\n",
            "Epoch 1779. Training loss: 0.7718611359596252. Validation loss: 2.7061660289764404.\n",
            "Epoch 1780. Training loss: 0.7718575596809387. Validation loss: 2.706165313720703.\n",
            "Epoch 1781. Training loss: 0.7718539834022522. Validation loss: 2.706164598464966.\n",
            "Epoch 1782. Training loss: 0.7718505263328552. Validation loss: 2.7061641216278076.\n",
            "Epoch 1783. Training loss: 0.7718470096588135. Validation loss: 2.706163167953491.\n",
            "Epoch 1784. Training loss: 0.771843433380127. Validation loss: 2.706162929534912.\n",
            "Epoch 1785. Training loss: 0.7718397974967957. Validation loss: 2.7061617374420166.\n",
            "Epoch 1786. Training loss: 0.7718362212181091. Validation loss: 2.7061612606048584.\n",
            "Epoch 1787. Training loss: 0.7718327045440674. Validation loss: 2.7061607837677.\n",
            "Epoch 1788. Training loss: 0.7718291282653809. Validation loss: 2.706160068511963.\n",
            "Epoch 1789. Training loss: 0.7718255519866943. Validation loss: 2.7061595916748047.\n",
            "Epoch 1790. Training loss: 0.7718219757080078. Validation loss: 2.7061586380004883.\n",
            "Epoch 1791. Training loss: 0.7718184590339661. Validation loss: 2.70615816116333.\n",
            "Epoch 1792. Training loss: 0.7718148827552795. Validation loss: 2.7061572074890137.\n",
            "Epoch 1793. Training loss: 0.7718112468719482. Validation loss: 2.7061564922332764.\n",
            "Epoch 1794. Training loss: 0.7718077301979065. Validation loss: 2.706156015396118.\n",
            "Epoch 1795. Training loss: 0.77180415391922. Validation loss: 2.7061550617218018.\n",
            "Epoch 1796. Training loss: 0.7718005776405334. Validation loss: 2.7061548233032227.\n",
            "Epoch 1797. Training loss: 0.7717971205711365. Validation loss: 2.7061541080474854.\n",
            "Epoch 1798. Training loss: 0.7717934250831604. Validation loss: 2.706153154373169.\n",
            "Epoch 1799. Training loss: 0.7717898488044739. Validation loss: 2.7061526775360107.\n",
            "Epoch 1800. Training loss: 0.7717862725257874. Validation loss: 2.7061519622802734.\n",
            "Epoch 1801. Training loss: 0.7717828154563904. Validation loss: 2.706151247024536.\n",
            "Epoch 1802. Training loss: 0.7717792391777039. Validation loss: 2.706151008605957.\n",
            "Epoch 1803. Training loss: 0.7717756628990173. Validation loss: 2.7061500549316406.\n",
            "Epoch 1804. Training loss: 0.7717720866203308. Validation loss: 2.7061493396759033.\n",
            "Epoch 1805. Training loss: 0.7717683911323547. Validation loss: 2.706148862838745.\n",
            "Epoch 1806. Training loss: 0.7717649936676025. Validation loss: 2.7061479091644287.\n",
            "Epoch 1807. Training loss: 0.771761417388916. Validation loss: 2.7061471939086914.\n",
            "Epoch 1808. Training loss: 0.7717578411102295. Validation loss: 2.706146717071533.\n",
            "Epoch 1809. Training loss: 0.771754264831543. Validation loss: 2.706145763397217.\n",
            "Epoch 1810. Training loss: 0.7717506885528564. Validation loss: 2.7061455249786377.\n",
            "Epoch 1811. Training loss: 0.7717471122741699. Validation loss: 2.706144332885742.\n",
            "Epoch 1812. Training loss: 0.7717435956001282. Validation loss: 2.706144094467163.\n",
            "Epoch 1813. Training loss: 0.7717399597167969. Validation loss: 2.7061431407928467.\n",
            "Epoch 1814. Training loss: 0.7717363834381104. Validation loss: 2.7061429023742676.\n",
            "Epoch 1815. Training loss: 0.7717328071594238. Validation loss: 2.706141948699951.\n",
            "Epoch 1816. Training loss: 0.7717292308807373. Validation loss: 2.706141233444214.\n",
            "Epoch 1817. Training loss: 0.7717256546020508. Validation loss: 2.7061402797698975.\n",
            "Epoch 1818. Training loss: 0.7717220783233643. Validation loss: 2.7061398029327393.\n",
            "Epoch 1819. Training loss: 0.7717185020446777. Validation loss: 2.706139326095581.\n",
            "Epoch 1820. Training loss: 0.7717149257659912. Validation loss: 2.7061386108398438.\n",
            "Epoch 1821. Training loss: 0.7717113494873047. Validation loss: 2.7061378955841064.\n",
            "Epoch 1822. Training loss: 0.7717077732086182. Validation loss: 2.7061376571655273.\n",
            "Epoch 1823. Training loss: 0.7717041969299316. Validation loss: 2.706136703491211.\n",
            "Epoch 1824. Training loss: 0.7717006206512451. Validation loss: 2.7061357498168945.\n",
            "Epoch 1825. Training loss: 0.7716970443725586. Validation loss: 2.706134796142578.\n",
            "Epoch 1826. Training loss: 0.7716934680938721. Validation loss: 2.706134557723999.\n",
            "Epoch 1827. Training loss: 0.7716898918151855. Validation loss: 2.706134080886841.\n",
            "Epoch 1828. Training loss: 0.7716862559318542. Validation loss: 2.7061333656311035.\n",
            "Epoch 1829. Training loss: 0.7716827392578125. Validation loss: 2.706132411956787.\n",
            "Epoch 1830. Training loss: 0.7716791033744812. Validation loss: 2.706131935119629.\n",
            "Epoch 1831. Training loss: 0.7716755270957947. Validation loss: 2.7061312198638916.\n",
            "Epoch 1832. Training loss: 0.7716720104217529. Validation loss: 2.7061305046081543.\n",
            "Epoch 1833. Training loss: 0.7716684341430664. Validation loss: 2.706129550933838.\n",
            "Epoch 1834. Training loss: 0.7716646790504456. Validation loss: 2.706129312515259.\n",
            "Epoch 1835. Training loss: 0.7716612219810486. Validation loss: 2.7061285972595215.\n",
            "Epoch 1836. Training loss: 0.7716576457023621. Validation loss: 2.7061281204223633.\n",
            "Epoch 1837. Training loss: 0.7716540694236755. Validation loss: 2.706127166748047.\n",
            "Epoch 1838. Training loss: 0.771650493144989. Validation loss: 2.7061266899108887.\n",
            "Epoch 1839. Training loss: 0.7716467976570129. Validation loss: 2.7061257362365723.\n",
            "Epoch 1840. Training loss: 0.771643340587616. Validation loss: 2.706125020980835.\n",
            "Epoch 1841. Training loss: 0.7716396450996399. Validation loss: 2.706124782562256.\n",
            "Epoch 1842. Training loss: 0.7716360688209534. Validation loss: 2.7061238288879395.\n",
            "Epoch 1843. Training loss: 0.7716324925422668. Validation loss: 2.706123113632202.\n",
            "Epoch 1844. Training loss: 0.7716289162635803. Validation loss: 2.706122398376465.\n",
            "Epoch 1845. Training loss: 0.771625280380249. Validation loss: 2.7061219215393066.\n",
            "Epoch 1846. Training loss: 0.7716217041015625. Validation loss: 2.7061209678649902.\n",
            "Epoch 1847. Training loss: 0.771618127822876. Validation loss: 2.706120491027832.\n",
            "Epoch 1848. Training loss: 0.7716146111488342. Validation loss: 2.7061197757720947.\n",
            "Epoch 1849. Training loss: 0.7716109156608582. Validation loss: 2.7061190605163574.\n",
            "Epoch 1850. Training loss: 0.7716073989868164. Validation loss: 2.706118583679199.\n",
            "Epoch 1851. Training loss: 0.7716037631034851. Validation loss: 2.706117868423462.\n",
            "Epoch 1852. Training loss: 0.7716001868247986. Validation loss: 2.7061169147491455.\n",
            "Epoch 1853. Training loss: 0.7715966105461121. Validation loss: 2.7061164379119873.\n",
            "Epoch 1854. Training loss: 0.7715928554534912. Validation loss: 2.70611572265625.\n",
            "Epoch 1855. Training loss: 0.7715892791748047. Validation loss: 2.706115245819092.\n",
            "Epoch 1856. Training loss: 0.7715857625007629. Validation loss: 2.7061142921447754.\n",
            "Epoch 1857. Training loss: 0.7715821266174316. Validation loss: 2.706113576889038.\n",
            "Epoch 1858. Training loss: 0.7715784907341003. Validation loss: 2.706112861633301.\n",
            "Epoch 1859. Training loss: 0.7715749740600586. Validation loss: 2.7061123847961426.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1860. Training loss: 0.7715713977813721. Validation loss: 2.7061116695404053.\n",
            "Epoch 1861. Training loss: 0.7715677618980408. Validation loss: 2.706110954284668.\n",
            "Epoch 1862. Training loss: 0.771564245223999. Validation loss: 2.7061102390289307.\n",
            "Epoch 1863. Training loss: 0.7715606093406677. Validation loss: 2.7061097621917725.\n",
            "Epoch 1864. Training loss: 0.7715569138526917. Validation loss: 2.706109046936035.\n",
            "Epoch 1865. Training loss: 0.7715532779693604. Validation loss: 2.706108570098877.\n",
            "Epoch 1866. Training loss: 0.7715497612953186. Validation loss: 2.7061071395874023.\n",
            "Epoch 1867. Training loss: 0.7715460658073425. Validation loss: 2.7061069011688232.\n",
            "Epoch 1868. Training loss: 0.771542489528656. Validation loss: 2.706105947494507.\n",
            "Epoch 1869. Training loss: 0.7715389728546143. Validation loss: 2.7061054706573486.\n",
            "Epoch 1870. Training loss: 0.7715353965759277. Validation loss: 2.7061047554016113.\n",
            "Epoch 1871. Training loss: 0.7715317606925964. Validation loss: 2.706104278564453.\n",
            "Epoch 1872. Training loss: 0.7715280652046204. Validation loss: 2.706103563308716.\n",
            "Epoch 1873. Training loss: 0.7715244889259338. Validation loss: 2.7061030864715576.\n",
            "Epoch 1874. Training loss: 0.7715208530426025. Validation loss: 2.706101894378662.\n",
            "Epoch 1875. Training loss: 0.771517276763916. Validation loss: 2.706101417541504.\n",
            "Epoch 1876. Training loss: 0.7715136408805847. Validation loss: 2.7061007022857666.\n",
            "Epoch 1877. Training loss: 0.771510124206543. Validation loss: 2.7061002254486084.\n",
            "Epoch 1878. Training loss: 0.7715064883232117. Validation loss: 2.706099510192871.\n",
            "Epoch 1879. Training loss: 0.7715027928352356. Validation loss: 2.706098794937134.\n",
            "Epoch 1880. Training loss: 0.7714992165565491. Validation loss: 2.7060978412628174.\n",
            "Epoch 1881. Training loss: 0.7714955806732178. Validation loss: 2.706097364425659.\n",
            "Epoch 1882. Training loss: 0.7714920043945312. Validation loss: 2.706096649169922.\n",
            "Epoch 1883. Training loss: 0.7714883685112. Validation loss: 2.7060959339141846.\n",
            "Epoch 1884. Training loss: 0.7714846730232239. Validation loss: 2.7060954570770264.\n",
            "Epoch 1885. Training loss: 0.7714812159538269. Validation loss: 2.70609450340271.\n",
            "Epoch 1886. Training loss: 0.7714776396751404. Validation loss: 2.706094264984131.\n",
            "Epoch 1887. Training loss: 0.7714739441871643. Validation loss: 2.7060933113098145.\n",
            "Epoch 1888. Training loss: 0.7714703679084778. Validation loss: 2.706092596054077.\n",
            "Epoch 1889. Training loss: 0.7714667320251465. Validation loss: 2.70609188079834.\n",
            "Epoch 1890. Training loss: 0.77146315574646. Validation loss: 2.7060914039611816.\n",
            "Epoch 1891. Training loss: 0.7714595198631287. Validation loss: 2.7060906887054443.\n",
            "Epoch 1892. Training loss: 0.7714559435844421. Validation loss: 2.706089496612549.\n",
            "Epoch 1893. Training loss: 0.7714522480964661. Validation loss: 2.7060892581939697.\n",
            "Epoch 1894. Training loss: 0.7714486122131348. Validation loss: 2.7060885429382324.\n",
            "Epoch 1895. Training loss: 0.7714450359344482. Validation loss: 2.706087827682495.\n",
            "Epoch 1896. Training loss: 0.7714412808418274. Validation loss: 2.706087112426758.\n",
            "Epoch 1897. Training loss: 0.7714378237724304. Validation loss: 2.7060863971710205.\n",
            "Epoch 1898. Training loss: 0.7714341282844543. Validation loss: 2.7060859203338623.\n",
            "Epoch 1899. Training loss: 0.771430492401123. Validation loss: 2.706085205078125.\n",
            "Epoch 1900. Training loss: 0.7714269161224365. Validation loss: 2.7060844898223877.\n",
            "Epoch 1901. Training loss: 0.77142333984375. Validation loss: 2.7060837745666504.\n",
            "Epoch 1902. Training loss: 0.7714197635650635. Validation loss: 2.706083059310913.\n",
            "Epoch 1903. Training loss: 0.7714160084724426. Validation loss: 2.706082344055176.\n",
            "Epoch 1904. Training loss: 0.7714123725891113. Validation loss: 2.7060818672180176.\n",
            "Epoch 1905. Training loss: 0.7714088559150696. Validation loss: 2.706080913543701.\n",
            "Epoch 1906. Training loss: 0.7714051604270935. Validation loss: 2.706080913543701.\n",
            "Epoch 1907. Training loss: 0.7714014649391174. Validation loss: 2.7060797214508057.\n",
            "Epoch 1908. Training loss: 0.7713978886604309. Validation loss: 2.7060790061950684.\n",
            "Epoch 1909. Training loss: 0.7713943123817444. Validation loss: 2.706078290939331.\n",
            "Epoch 1910. Training loss: 0.7713906764984131. Validation loss: 2.7060775756835938.\n",
            "Epoch 1911. Training loss: 0.7713870406150818. Validation loss: 2.7060770988464355.\n",
            "Epoch 1912. Training loss: 0.7713833451271057. Validation loss: 2.7060763835906982.\n",
            "Epoch 1913. Training loss: 0.7713797688484192. Validation loss: 2.70607590675354.\n",
            "Epoch 1914. Training loss: 0.7713761329650879. Validation loss: 2.7060747146606445.\n",
            "Epoch 1915. Training loss: 0.7713724970817566. Validation loss: 2.7060742378234863.\n",
            "Epoch 1916. Training loss: 0.7713689804077148. Validation loss: 2.706073522567749.\n",
            "Epoch 1917. Training loss: 0.7713653445243835. Validation loss: 2.7060728073120117.\n",
            "Epoch 1918. Training loss: 0.7713615894317627. Validation loss: 2.7060723304748535.\n",
            "Epoch 1919. Training loss: 0.7713580131530762. Validation loss: 2.706071376800537.\n",
            "Epoch 1920. Training loss: 0.7713543772697449. Validation loss: 2.706070899963379.\n",
            "Epoch 1921. Training loss: 0.7713508009910583. Validation loss: 2.7060704231262207.\n",
            "Epoch 1922. Training loss: 0.7713470458984375. Validation loss: 2.706069231033325.\n",
            "Epoch 1923. Training loss: 0.771343469619751. Validation loss: 2.706068754196167.\n",
            "Epoch 1924. Training loss: 0.7713398933410645. Validation loss: 2.7060680389404297.\n",
            "Epoch 1925. Training loss: 0.7713362574577332. Validation loss: 2.7060673236846924.\n",
            "Epoch 1926. Training loss: 0.7713327407836914. Validation loss: 2.706066846847534.\n",
            "Epoch 1927. Training loss: 0.771329402923584. Validation loss: 2.706066131591797.\n",
            "Epoch 1928. Training loss: 0.7713260054588318. Validation loss: 2.7060654163360596.\n",
            "Epoch 1929. Training loss: 0.7713226675987244. Validation loss: 2.7060649394989014.\n",
            "Epoch 1930. Training loss: 0.7713192105293274. Validation loss: 2.706063985824585.\n",
            "Epoch 1931. Training loss: 0.7713158130645752. Validation loss: 2.7060632705688477.\n",
            "Epoch 1932. Training loss: 0.771312415599823. Validation loss: 2.7060627937316895.\n",
            "Epoch 1933. Training loss: 0.7713090777397156. Validation loss: 2.706061840057373.\n",
            "Epoch 1934. Training loss: 0.7713057398796082. Validation loss: 2.706061601638794.\n",
            "Epoch 1935. Training loss: 0.7713022828102112. Validation loss: 2.7060608863830566.\n",
            "Epoch 1936. Training loss: 0.771298885345459. Validation loss: 2.7060601711273193.\n",
            "Epoch 1937. Training loss: 0.7712955474853516. Validation loss: 2.706059455871582.\n",
            "Epoch 1938. Training loss: 0.7712921500205994. Validation loss: 2.706058979034424.\n",
            "Epoch 1939. Training loss: 0.7712886929512024. Validation loss: 2.7060580253601074.\n",
            "Epoch 1940. Training loss: 0.771285355091095. Validation loss: 2.70605731010437.\n",
            "Epoch 1941. Training loss: 0.7712820172309875. Validation loss: 2.706057071685791.\n",
            "Epoch 1942. Training loss: 0.7712785601615906. Validation loss: 2.7060558795928955.\n",
            "Epoch 1943. Training loss: 0.7712752223014832. Validation loss: 2.7060556411743164.\n",
            "Epoch 1944. Training loss: 0.7712717056274414. Validation loss: 2.706054925918579.\n",
            "Epoch 1945. Training loss: 0.7712685465812683. Validation loss: 2.706054449081421.\n",
            "Epoch 1946. Training loss: 0.7712650299072266. Validation loss: 2.7060537338256836.\n",
            "Epoch 1947. Training loss: 0.7712616920471191. Validation loss: 2.706052780151367.\n",
            "Epoch 1948. Training loss: 0.7712581753730774. Validation loss: 2.706052303314209.\n",
            "Epoch 1949. Training loss: 0.7712549567222595. Validation loss: 2.7060513496398926.\n",
            "Epoch 1950. Training loss: 0.7712514996528625. Validation loss: 2.7060508728027344.\n",
            "Epoch 1951. Training loss: 0.7712481021881104. Validation loss: 2.706050157546997.\n",
            "Epoch 1952. Training loss: 0.7712447047233582. Validation loss: 2.7060494422912598.\n",
            "Epoch 1953. Training loss: 0.7712412476539612. Validation loss: 2.7060489654541016.\n",
            "Epoch 1954. Training loss: 0.7712379097938538. Validation loss: 2.7060482501983643.\n",
            "Epoch 1955. Training loss: 0.7712346911430359. Validation loss: 2.706047296524048.\n",
            "Epoch 1956. Training loss: 0.7712311744689941. Validation loss: 2.7060465812683105.\n",
            "Epoch 1957. Training loss: 0.7712277770042419. Validation loss: 2.7060461044311523.\n",
            "Epoch 1958. Training loss: 0.771224319934845. Validation loss: 2.706045627593994.\n",
            "Epoch 1959. Training loss: 0.7712209820747375. Validation loss: 2.706045150756836.\n",
            "Epoch 1960. Training loss: 0.7712176442146301. Validation loss: 2.7060446739196777.\n",
            "Epoch 1961. Training loss: 0.7712142467498779. Validation loss: 2.7060437202453613.\n",
            "Epoch 1962. Training loss: 0.7712109088897705. Validation loss: 2.706043243408203.\n",
            "Epoch 1963. Training loss: 0.7712073922157288. Validation loss: 2.7060422897338867.\n",
            "Epoch 1964. Training loss: 0.7712040543556213. Validation loss: 2.7060420513153076.\n",
            "Epoch 1965. Training loss: 0.7712006568908691. Validation loss: 2.7060413360595703.\n",
            "Epoch 1966. Training loss: 0.7711972594261169. Validation loss: 2.706040382385254.\n",
            "Epoch 1967. Training loss: 0.7711939215660095. Validation loss: 2.7060396671295166.\n",
            "Epoch 1968. Training loss: 0.7711905837059021. Validation loss: 2.7060389518737793.\n",
            "Epoch 1969. Training loss: 0.7711870074272156. Validation loss: 2.706038475036621.\n",
            "Epoch 1970. Training loss: 0.7711837291717529. Validation loss: 2.706037998199463.\n",
            "Epoch 1971. Training loss: 0.7711803317070007. Validation loss: 2.7060370445251465.\n",
            "Epoch 1972. Training loss: 0.7711769938468933. Validation loss: 2.7060365676879883.\n",
            "Epoch 1973. Training loss: 0.7711736559867859. Validation loss: 2.70603609085083.\n",
            "Epoch 1974. Training loss: 0.7711701989173889. Validation loss: 2.7060348987579346.\n",
            "Epoch 1975. Training loss: 0.7711668610572815. Validation loss: 2.7060346603393555.\n",
            "Epoch 1976. Training loss: 0.7711634635925293. Validation loss: 2.706033706665039.\n",
            "Epoch 1977. Training loss: 0.7711601257324219. Validation loss: 2.7060329914093018.\n",
            "Epoch 1978. Training loss: 0.7711567878723145. Validation loss: 2.7060322761535645.\n",
            "Epoch 1979. Training loss: 0.771153450012207. Validation loss: 2.706031560897827.\n",
            "Epoch 1980. Training loss: 0.7711499333381653. Validation loss: 2.706031084060669.\n",
            "Epoch 1981. Training loss: 0.7711465954780579. Validation loss: 2.7060303688049316.\n",
            "Epoch 1982. Training loss: 0.7711432576179504. Validation loss: 2.7060298919677734.\n",
            "Epoch 1983. Training loss: 0.771139919757843. Validation loss: 2.706029176712036.\n",
            "Epoch 1984. Training loss: 0.7711365222930908. Validation loss: 2.706028938293457.\n",
            "Epoch 1985. Training loss: 0.7711331844329834. Validation loss: 2.706028461456299.\n",
            "Epoch 1986. Training loss: 0.7711297869682312. Validation loss: 2.7060279846191406.\n",
            "Epoch 1987. Training loss: 0.7711265087127686. Validation loss: 2.706027030944824.\n",
            "Epoch 1988. Training loss: 0.7711231112480164. Validation loss: 2.706026554107666.\n",
            "Epoch 1989. Training loss: 0.7711197733879089. Validation loss: 2.706026077270508.\n",
            "Epoch 1990. Training loss: 0.771116316318512. Validation loss: 2.7060256004333496.\n",
            "Epoch 1991. Training loss: 0.7711129188537598. Validation loss: 2.7060251235961914.\n",
            "Epoch 1992. Training loss: 0.7711095809936523. Validation loss: 2.7060248851776123.\n",
            "Epoch 1993. Training loss: 0.7711062431335449. Validation loss: 2.706023931503296.\n",
            "Epoch 1994. Training loss: 0.7711028456687927. Validation loss: 2.706023693084717.\n",
            "Epoch 1995. Training loss: 0.7710995078086853. Validation loss: 2.7060229778289795.\n",
            "Epoch 1996. Training loss: 0.7710960507392883. Validation loss: 2.7060225009918213.\n",
            "Epoch 1997. Training loss: 0.7710926532745361. Validation loss: 2.706022024154663.\n",
            "Epoch 1998. Training loss: 0.7710893154144287. Validation loss: 2.706021547317505.\n",
            "Epoch 1999. Training loss: 0.7710859775543213. Validation loss: 2.7060208320617676.\n",
            "Epoch 2000. Training loss: 0.7710826396942139. Validation loss: 2.7060205936431885.\n",
            "Epoch 2001. Training loss: 0.7710793018341064. Validation loss: 2.7060201168060303.\n",
            "Epoch 2002. Training loss: 0.7710759043693542. Validation loss: 2.706019401550293.\n",
            "Epoch 2003. Training loss: 0.7710724472999573. Validation loss: 2.7060189247131348.\n",
            "Epoch 2004. Training loss: 0.7710690498352051. Validation loss: 2.7060184478759766.\n",
            "Epoch 2005. Training loss: 0.7710656523704529. Validation loss: 2.7060179710388184.\n",
            "Epoch 2006. Training loss: 0.7710623741149902. Validation loss: 2.70601749420166.\n",
            "Epoch 2007. Training loss: 0.7710588574409485. Validation loss: 2.706016778945923.\n",
            "Epoch 2008. Training loss: 0.7710554599761963. Validation loss: 2.7060160636901855.\n",
            "Epoch 2009. Training loss: 0.7710521817207336. Validation loss: 2.7060155868530273.\n",
            "Epoch 2010. Training loss: 0.7710488438606262. Validation loss: 2.706015110015869.\n",
            "Epoch 2011. Training loss: 0.771045446395874. Validation loss: 2.706014633178711.\n",
            "Epoch 2012. Training loss: 0.7710420489311218. Validation loss: 2.7060141563415527.\n",
            "Epoch 2013. Training loss: 0.7710387110710144. Validation loss: 2.7060136795043945.\n",
            "Epoch 2014. Training loss: 0.7710352540016174. Validation loss: 2.7060132026672363.\n",
            "Epoch 2015. Training loss: 0.77103191614151. Validation loss: 2.706012725830078.\n",
            "Epoch 2016. Training loss: 0.7710285186767578. Validation loss: 2.70601224899292.\n",
            "Epoch 2017. Training loss: 0.7710251808166504. Validation loss: 2.7060117721557617.\n",
            "Epoch 2018. Training loss: 0.771021842956543. Validation loss: 2.7060110569000244.\n",
            "Epoch 2019. Training loss: 0.7710184454917908. Validation loss: 2.706010341644287.\n",
            "Epoch 2020. Training loss: 0.7710149884223938. Validation loss: 2.706010103225708.\n",
            "Epoch 2021. Training loss: 0.7710115909576416. Validation loss: 2.7060093879699707.\n",
            "Epoch 2022. Training loss: 0.7710082530975342. Validation loss: 2.7060089111328125.\n",
            "Epoch 2023. Training loss: 0.771004855632782. Validation loss: 2.7060084342956543.\n",
            "Epoch 2024. Training loss: 0.7710015177726746. Validation loss: 2.706007957458496.\n",
            "Epoch 2025. Training loss: 0.7709980607032776. Validation loss: 2.706007242202759.\n",
            "Epoch 2026. Training loss: 0.7709946632385254. Validation loss: 2.7060067653656006.\n",
            "Epoch 2027. Training loss: 0.7709912657737732. Validation loss: 2.7060062885284424.\n",
            "Epoch 2028. Training loss: 0.7709879279136658. Validation loss: 2.706005811691284.\n",
            "Epoch 2029. Training loss: 0.7709845900535583. Validation loss: 2.706005096435547.\n",
            "Epoch 2030. Training loss: 0.7709811329841614. Validation loss: 2.7060043811798096.\n",
            "Epoch 2031. Training loss: 0.770977795124054. Validation loss: 2.7060039043426514.\n",
            "Epoch 2032. Training loss: 0.7709743976593018. Validation loss: 2.7060036659240723.\n",
            "Epoch 2033. Training loss: 0.7709710001945496. Validation loss: 2.706003189086914.\n",
            "Epoch 2034. Training loss: 0.7709676623344421. Validation loss: 2.7060022354125977.\n",
            "Epoch 2035. Training loss: 0.7709642052650452. Validation loss: 2.7060019969940186.\n",
            "Epoch 2036. Training loss: 0.770960807800293. Validation loss: 2.7060017585754395.\n",
            "Epoch 2037. Training loss: 0.7709574699401855. Validation loss: 2.706000804901123.\n",
            "Epoch 2038. Training loss: 0.7709540724754333. Validation loss: 2.706000328063965.\n",
            "Epoch 2039. Training loss: 0.7709507346153259. Validation loss: 2.7059996128082275.\n",
            "Epoch 2040. Training loss: 0.770947277545929. Validation loss: 2.7059991359710693.\n",
            "Epoch 2041. Training loss: 0.7709439396858215. Validation loss: 2.7059988975524902.\n",
            "Epoch 2042. Training loss: 0.7709404826164246. Validation loss: 2.705998420715332.\n",
            "Epoch 2043. Training loss: 0.7709372043609619. Validation loss: 2.705997943878174.\n",
            "Epoch 2044. Training loss: 0.7709336876869202. Validation loss: 2.7059974670410156.\n",
            "Epoch 2045. Training loss: 0.770930290222168. Validation loss: 2.705996513366699.\n",
            "Epoch 2046. Training loss: 0.7709269523620605. Validation loss: 2.705996036529541.\n",
            "Epoch 2047. Training loss: 0.7709235548973083. Validation loss: 2.705995559692383.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2048. Training loss: 0.7709202170372009. Validation loss: 2.7059950828552246.\n",
            "Epoch 2049. Training loss: 0.770916759967804. Validation loss: 2.7059948444366455.\n",
            "Epoch 2050. Training loss: 0.7709133625030518. Validation loss: 2.70599365234375.\n",
            "Epoch 2051. Training loss: 0.7709100246429443. Validation loss: 2.70599365234375.\n",
            "Epoch 2052. Training loss: 0.7709066867828369. Validation loss: 2.705993175506592.\n",
            "Epoch 2053. Training loss: 0.7709031105041504. Validation loss: 2.7059926986694336.\n",
            "Epoch 2054. Training loss: 0.770899772644043. Validation loss: 2.7059919834136963.\n",
            "Epoch 2055. Training loss: 0.7708964347839355. Validation loss: 2.705991506576538.\n",
            "Epoch 2056. Training loss: 0.7708929181098938. Validation loss: 2.70599102973938.\n",
            "Epoch 2057. Training loss: 0.7708895802497864. Validation loss: 2.7059903144836426.\n",
            "Epoch 2058. Training loss: 0.7708861827850342. Validation loss: 2.7059895992279053.\n",
            "Epoch 2059. Training loss: 0.7708828449249268. Validation loss: 2.705989360809326.\n",
            "Epoch 2060. Training loss: 0.7708794474601746. Validation loss: 2.705988883972168.\n",
            "Epoch 2061. Training loss: 0.7708759903907776. Validation loss: 2.7059884071350098.\n",
            "Epoch 2062. Training loss: 0.7708725929260254. Validation loss: 2.7059879302978516.\n",
            "Epoch 2063. Training loss: 0.7708691954612732. Validation loss: 2.7059874534606934.\n",
            "Epoch 2064. Training loss: 0.7708657383918762. Validation loss: 2.705986499786377.\n",
            "Epoch 2065. Training loss: 0.7708624005317688. Validation loss: 2.7059860229492188.\n",
            "Epoch 2066. Training loss: 0.7708589434623718. Validation loss: 2.7059855461120605.\n",
            "Epoch 2067. Training loss: 0.7708556056022644. Validation loss: 2.7059850692749023.\n",
            "Epoch 2068. Training loss: 0.7708521485328674. Validation loss: 2.7059848308563232.\n",
            "Epoch 2069. Training loss: 0.77084881067276. Validation loss: 2.705984115600586.\n",
            "Epoch 2070. Training loss: 0.7708454132080078. Validation loss: 2.7059834003448486.\n",
            "Epoch 2071. Training loss: 0.7708418965339661. Validation loss: 2.7059831619262695.\n",
            "Epoch 2072. Training loss: 0.7708384990692139. Validation loss: 2.7059824466705322.\n",
            "Epoch 2073. Training loss: 0.7708352208137512. Validation loss: 2.705981969833374.\n",
            "Epoch 2074. Training loss: 0.7708317637443542. Validation loss: 2.705981492996216.\n",
            "Epoch 2075. Training loss: 0.7708284258842468. Validation loss: 2.7059812545776367.\n",
            "Epoch 2076. Training loss: 0.7708249092102051. Validation loss: 2.705980062484741.\n",
            "Epoch 2077. Training loss: 0.7708215713500977. Validation loss: 2.705980062484741.\n",
            "Epoch 2078. Training loss: 0.7708181738853455. Validation loss: 2.705979585647583.\n",
            "Epoch 2079. Training loss: 0.7708147168159485. Validation loss: 2.705979108810425.\n",
            "Epoch 2080. Training loss: 0.7708113193511963. Validation loss: 2.7059783935546875.\n",
            "Epoch 2081. Training loss: 0.7708079218864441. Validation loss: 2.70597767829895.\n",
            "Epoch 2082. Training loss: 0.7708044648170471. Validation loss: 2.705977439880371.\n",
            "Epoch 2083. Training loss: 0.7708010673522949. Validation loss: 2.705976963043213.\n",
            "Epoch 2084. Training loss: 0.7707976698875427. Validation loss: 2.7059764862060547.\n",
            "Epoch 2085. Training loss: 0.7707943320274353. Validation loss: 2.7059757709503174.\n",
            "Epoch 2086. Training loss: 0.7707908749580383. Validation loss: 2.705975294113159.\n",
            "Epoch 2087. Training loss: 0.7707875370979309. Validation loss: 2.705974817276001.\n",
            "Epoch 2088. Training loss: 0.7707840800285339. Validation loss: 2.7059741020202637.\n",
            "Epoch 2089. Training loss: 0.770780622959137. Validation loss: 2.7059736251831055.\n",
            "Epoch 2090. Training loss: 0.7707772254943848. Validation loss: 2.7059731483459473.\n",
            "Epoch 2091. Training loss: 0.7707738876342773. Validation loss: 2.705972671508789.\n",
            "Epoch 2092. Training loss: 0.7707703709602356. Validation loss: 2.7059719562530518.\n",
            "Epoch 2093. Training loss: 0.7707669734954834. Validation loss: 2.7059714794158936.\n",
            "Epoch 2094. Training loss: 0.7707635760307312. Validation loss: 2.7059710025787354.\n",
            "Epoch 2095. Training loss: 0.7707600593566895. Validation loss: 2.7059707641601562.\n",
            "Epoch 2096. Training loss: 0.7707567811012268. Validation loss: 2.70596981048584.\n",
            "Epoch 2097. Training loss: 0.7707532048225403. Validation loss: 2.7059693336486816.\n",
            "Epoch 2098. Training loss: 0.7707499861717224. Validation loss: 2.7059688568115234.\n",
            "Epoch 2099. Training loss: 0.7707464694976807. Validation loss: 2.7059683799743652.\n",
            "Epoch 2100. Training loss: 0.7707431316375732. Validation loss: 2.705967903137207.\n",
            "Epoch 2101. Training loss: 0.770739734172821. Validation loss: 2.705967426300049.\n",
            "Epoch 2102. Training loss: 0.7707362174987793. Validation loss: 2.7059669494628906.\n",
            "Epoch 2103. Training loss: 0.7707328796386719. Validation loss: 2.705965995788574.\n",
            "Epoch 2104. Training loss: 0.7707294821739197. Validation loss: 2.705965518951416.\n",
            "Epoch 2105. Training loss: 0.7707259654998779. Validation loss: 2.705965280532837.\n",
            "Epoch 2106. Training loss: 0.7707225680351257. Validation loss: 2.7059648036956787.\n",
            "Epoch 2107. Training loss: 0.7707191109657288. Validation loss: 2.7059640884399414.\n",
            "Epoch 2108. Training loss: 0.7707157135009766. Validation loss: 2.705963611602783.\n",
            "Epoch 2109. Training loss: 0.7707123160362244. Validation loss: 2.705963373184204.\n",
            "Epoch 2110. Training loss: 0.7707088589668274. Validation loss: 2.705962896347046.\n",
            "Epoch 2111. Training loss: 0.7707054615020752. Validation loss: 2.7059619426727295.\n",
            "Epoch 2112. Training loss: 0.7707021236419678. Validation loss: 2.7059614658355713.\n",
            "Epoch 2113. Training loss: 0.770698606967926. Validation loss: 2.705960988998413.\n",
            "Epoch 2114. Training loss: 0.770695149898529. Validation loss: 2.705960750579834.\n",
            "Epoch 2115. Training loss: 0.7706918120384216. Validation loss: 2.7059600353240967.\n",
            "Epoch 2116. Training loss: 0.7706883549690247. Validation loss: 2.7059595584869385.\n",
            "Epoch 2117. Training loss: 0.7706849575042725. Validation loss: 2.705958843231201.\n",
            "Epoch 2118. Training loss: 0.7706815600395203. Validation loss: 2.705958366394043.\n",
            "Epoch 2119. Training loss: 0.7706781029701233. Validation loss: 2.7059578895568848.\n",
            "Epoch 2120. Training loss: 0.7706746459007263. Validation loss: 2.7059574127197266.\n",
            "Epoch 2121. Training loss: 0.7706711888313293. Validation loss: 2.7059566974639893.\n",
            "Epoch 2122. Training loss: 0.7706678509712219. Validation loss: 2.70595645904541.\n",
            "Epoch 2123. Training loss: 0.770664393901825. Validation loss: 2.705955743789673.\n",
            "Epoch 2124. Training loss: 0.7706610560417175. Validation loss: 2.7059552669525146.\n",
            "Epoch 2125. Training loss: 0.7706575393676758. Validation loss: 2.7059547901153564.\n",
            "Epoch 2126. Training loss: 0.7706542015075684. Validation loss: 2.705954074859619.\n",
            "Epoch 2127. Training loss: 0.7706506848335266. Validation loss: 2.705953598022461.\n",
            "Epoch 2128. Training loss: 0.7706472873687744. Validation loss: 2.705953359603882.\n",
            "Epoch 2129. Training loss: 0.7706437706947327. Validation loss: 2.7059526443481445.\n",
            "Epoch 2130. Training loss: 0.7706403732299805. Validation loss: 2.7059524059295654.\n",
            "Epoch 2131. Training loss: 0.770637035369873. Validation loss: 2.705951452255249.\n",
            "Epoch 2132. Training loss: 0.7706335186958313. Validation loss: 2.7059507369995117.\n",
            "Epoch 2133. Training loss: 0.7706301808357239. Validation loss: 2.7059504985809326.\n",
            "Epoch 2134. Training loss: 0.7706267237663269. Validation loss: 2.7059500217437744.\n",
            "Epoch 2135. Training loss: 0.7706232666969299. Validation loss: 2.705949306488037.\n",
            "Epoch 2136. Training loss: 0.770619809627533. Validation loss: 2.705948829650879.\n",
            "Epoch 2137. Training loss: 0.770616352558136. Validation loss: 2.7059483528137207.\n",
            "Epoch 2138. Training loss: 0.770612895488739. Validation loss: 2.7059478759765625.\n",
            "Epoch 2139. Training loss: 0.7706095576286316. Validation loss: 2.7059473991394043.\n",
            "Epoch 2140. Training loss: 0.7706060409545898. Validation loss: 2.705946922302246.\n",
            "Epoch 2141. Training loss: 0.7706027030944824. Validation loss: 2.705946445465088.\n",
            "Epoch 2142. Training loss: 0.7705991864204407. Validation loss: 2.7059454917907715.\n",
            "Epoch 2143. Training loss: 0.7705957293510437. Validation loss: 2.7059450149536133.\n",
            "Epoch 2144. Training loss: 0.7705923914909363. Validation loss: 2.705944538116455.\n",
            "Epoch 2145. Training loss: 0.7705888748168945. Validation loss: 2.705944538116455.\n",
            "Epoch 2146. Training loss: 0.7705854773521423. Validation loss: 2.7059435844421387.\n",
            "Epoch 2147. Training loss: 0.7705821394920349. Validation loss: 2.7059433460235596.\n",
            "Epoch 2148. Training loss: 0.7705785632133484. Validation loss: 2.7059428691864014.\n",
            "Epoch 2149. Training loss: 0.770575225353241. Validation loss: 2.705942153930664.\n",
            "Epoch 2150. Training loss: 0.7705717086791992. Validation loss: 2.7059414386749268.\n",
            "Epoch 2151. Training loss: 0.7705681920051575. Validation loss: 2.7059407234191895.\n",
            "Epoch 2152. Training loss: 0.77056485414505. Validation loss: 2.7059402465820312.\n",
            "Epoch 2153. Training loss: 0.7705613970756531. Validation loss: 2.705939769744873.\n",
            "Epoch 2154. Training loss: 0.7705578804016113. Validation loss: 2.705939292907715.\n",
            "Epoch 2155. Training loss: 0.7705545425415039. Validation loss: 2.7059388160705566.\n",
            "Epoch 2156. Training loss: 0.7705510258674622. Validation loss: 2.7059381008148193.\n",
            "Epoch 2157. Training loss: 0.77054762840271. Validation loss: 2.7059378623962402.\n",
            "Epoch 2158. Training loss: 0.7705442309379578. Validation loss: 2.705937147140503.\n",
            "Epoch 2159. Training loss: 0.770540714263916. Validation loss: 2.705936908721924.\n",
            "Epoch 2160. Training loss: 0.7705373167991638. Validation loss: 2.7059359550476074.\n",
            "Epoch 2161. Training loss: 0.7705338001251221. Validation loss: 2.705935478210449.\n",
            "Epoch 2162. Training loss: 0.7705304026603699. Validation loss: 2.70593523979187.\n",
            "Epoch 2163. Training loss: 0.7705269455909729. Validation loss: 2.705934524536133.\n",
            "Epoch 2164. Training loss: 0.7705234885215759. Validation loss: 2.7059340476989746.\n",
            "Epoch 2165. Training loss: 0.770520031452179. Validation loss: 2.7059333324432373.\n",
            "Epoch 2166. Training loss: 0.7705166339874268. Validation loss: 2.705932855606079.\n",
            "Epoch 2167. Training loss: 0.770513117313385. Validation loss: 2.7059326171875.\n",
            "Epoch 2168. Training loss: 0.7705097794532776. Validation loss: 2.705932140350342.\n",
            "Epoch 2169. Training loss: 0.7705063223838806. Validation loss: 2.7059314250946045.\n",
            "Epoch 2170. Training loss: 0.7705028057098389. Validation loss: 2.705930709838867.\n",
            "Epoch 2171. Training loss: 0.7704992890357971. Validation loss: 2.705930471420288.\n",
            "Epoch 2172. Training loss: 0.7704959511756897. Validation loss: 2.705929756164551.\n",
            "Epoch 2173. Training loss: 0.7704924941062927. Validation loss: 2.7059292793273926.\n",
            "Epoch 2174. Training loss: 0.7704890370368958. Validation loss: 2.7059288024902344.\n",
            "Epoch 2175. Training loss: 0.7704855799674988. Validation loss: 2.705928087234497.\n",
            "Epoch 2176. Training loss: 0.770482063293457. Validation loss: 2.705927848815918.\n",
            "Epoch 2177. Training loss: 0.7704786658287048. Validation loss: 2.7059268951416016.\n",
            "Epoch 2178. Training loss: 0.7704751491546631. Validation loss: 2.7059266567230225.\n",
            "Epoch 2179. Training loss: 0.7704717516899109. Validation loss: 2.705925941467285.\n",
            "Epoch 2180. Training loss: 0.7704682946205139. Validation loss: 2.705925703048706.\n",
            "Epoch 2181. Training loss: 0.7704648971557617. Validation loss: 2.7059249877929688.\n",
            "Epoch 2182. Training loss: 0.77046138048172. Validation loss: 2.7059245109558105.\n",
            "Epoch 2183. Training loss: 0.770457923412323. Validation loss: 2.7059240341186523.\n",
            "Epoch 2184. Training loss: 0.770454466342926. Validation loss: 2.705923080444336.\n",
            "Epoch 2185. Training loss: 0.770451009273529. Validation loss: 2.705922842025757.\n",
            "Epoch 2186. Training loss: 0.7704475522041321. Validation loss: 2.7059226036071777.\n",
            "Epoch 2187. Training loss: 0.7704440951347351. Validation loss: 2.7059218883514404.\n",
            "Epoch 2188. Training loss: 0.7704406380653381. Validation loss: 2.705921173095703.\n",
            "Epoch 2189. Training loss: 0.7704372406005859. Validation loss: 2.705920696258545.\n",
            "Epoch 2190. Training loss: 0.7704337239265442. Validation loss: 2.7059202194213867.\n",
            "Epoch 2191. Training loss: 0.7704302668571472. Validation loss: 2.7059197425842285.\n",
            "Epoch 2192. Training loss: 0.7704268097877502. Validation loss: 2.705919027328491.\n",
            "Epoch 2193. Training loss: 0.770423412322998. Validation loss: 2.705918788909912.\n",
            "Epoch 2194. Training loss: 0.7704198956489563. Validation loss: 2.7059178352355957.\n",
            "Epoch 2195. Training loss: 0.7704164385795593. Validation loss: 2.7059173583984375.\n",
            "Epoch 2196. Training loss: 0.7704129815101624. Validation loss: 2.7059171199798584.\n",
            "Epoch 2197. Training loss: 0.7704095244407654. Validation loss: 2.705916404724121.\n",
            "Epoch 2198. Training loss: 0.7704060077667236. Validation loss: 2.705915689468384.\n",
            "Epoch 2199. Training loss: 0.7704026103019714. Validation loss: 2.7059152126312256.\n",
            "Epoch 2200. Training loss: 0.7703990936279297. Validation loss: 2.7059149742126465.\n",
            "Epoch 2201. Training loss: 0.7703956961631775. Validation loss: 2.7059144973754883.\n",
            "Epoch 2202. Training loss: 0.7703921794891357. Validation loss: 2.705913543701172.\n",
            "Epoch 2203. Training loss: 0.7703887820243835. Validation loss: 2.7059130668640137.\n",
            "Epoch 2204. Training loss: 0.7703852653503418. Validation loss: 2.7059128284454346.\n",
            "Epoch 2205. Training loss: 0.7703818678855896. Validation loss: 2.7059123516082764.\n",
            "Epoch 2206. Training loss: 0.7703783512115479. Validation loss: 2.705911636352539.\n",
            "Epoch 2207. Training loss: 0.7703748345375061. Validation loss: 2.7059109210968018.\n",
            "Epoch 2208. Training loss: 0.7703714370727539. Validation loss: 2.7059106826782227.\n",
            "Epoch 2209. Training loss: 0.7703679203987122. Validation loss: 2.7059097290039062.\n",
            "Epoch 2210. Training loss: 0.77036452293396. Validation loss: 2.705909490585327.\n",
            "Epoch 2211. Training loss: 0.7703609466552734. Validation loss: 2.705909252166748.\n",
            "Epoch 2212. Training loss: 0.7703574299812317. Validation loss: 2.7059082984924316.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2213. Training loss: 0.7703542113304138. Validation loss: 2.7059080600738525.\n",
            "Epoch 2214. Training loss: 0.7703506350517273. Validation loss: 2.7059073448181152.\n",
            "Epoch 2215. Training loss: 0.7703471183776855. Validation loss: 2.705906867980957.\n",
            "Epoch 2216. Training loss: 0.7703437209129333. Validation loss: 2.705906391143799.\n",
            "Epoch 2217. Training loss: 0.7703402042388916. Validation loss: 2.7059059143066406.\n",
            "Epoch 2218. Training loss: 0.7703366875648499. Validation loss: 2.7059051990509033.\n",
            "Epoch 2219. Training loss: 0.7703332304954529. Validation loss: 2.705904483795166.\n",
            "Epoch 2220. Training loss: 0.7703297138214111. Validation loss: 2.705904006958008.\n",
            "Epoch 2221. Training loss: 0.7703263163566589. Validation loss: 2.7059035301208496.\n",
            "Epoch 2222. Training loss: 0.770322859287262. Validation loss: 2.7059030532836914.\n",
            "Epoch 2223. Training loss: 0.7703192830085754. Validation loss: 2.705902576446533.\n",
            "Epoch 2224. Training loss: 0.7703158259391785. Validation loss: 2.705902099609375.\n",
            "Epoch 2225. Training loss: 0.770312488079071. Validation loss: 2.705901622772217.\n",
            "Epoch 2226. Training loss: 0.7703089118003845. Validation loss: 2.7059009075164795.\n",
            "Epoch 2227. Training loss: 0.7703053951263428. Validation loss: 2.705900192260742.\n",
            "Epoch 2228. Training loss: 0.7703019976615906. Validation loss: 2.705899715423584.\n",
            "Epoch 2229. Training loss: 0.7702984809875488. Validation loss: 2.705899238586426.\n",
            "Epoch 2230. Training loss: 0.7702949643135071. Validation loss: 2.7058987617492676.\n",
            "Epoch 2231. Training loss: 0.7702915668487549. Validation loss: 2.7058982849121094.\n",
            "Epoch 2232. Training loss: 0.7702880501747131. Validation loss: 2.705897808074951.\n",
            "Epoch 2233. Training loss: 0.7702845931053162. Validation loss: 2.705897092819214.\n",
            "Epoch 2234. Training loss: 0.7702811360359192. Validation loss: 2.7058966159820557.\n",
            "Epoch 2235. Training loss: 0.7702775597572327. Validation loss: 2.7058959007263184.\n",
            "Epoch 2236. Training loss: 0.7702741622924805. Validation loss: 2.70589542388916.\n",
            "Epoch 2237. Training loss: 0.7702706456184387. Validation loss: 2.705895185470581.\n",
            "Epoch 2238. Training loss: 0.7702671885490417. Validation loss: 2.705894708633423.\n",
            "Epoch 2239. Training loss: 0.770263671875. Validation loss: 2.7058937549591064.\n",
            "Epoch 2240. Training loss: 0.7702601552009583. Validation loss: 2.7058932781219482.\n",
            "Epoch 2241. Training loss: 0.7702566981315613. Validation loss: 2.70589280128479.\n",
            "Epoch 2242. Training loss: 0.7702532410621643. Validation loss: 2.705892562866211.\n",
            "Epoch 2243. Training loss: 0.7702497839927673. Validation loss: 2.7058916091918945.\n",
            "Epoch 2244. Training loss: 0.7702462077140808. Validation loss: 2.7058911323547363.\n",
            "Epoch 2245. Training loss: 0.7702427506446838. Validation loss: 2.7058908939361572.\n",
            "Epoch 2246. Training loss: 0.7702393531799316. Validation loss: 2.70589017868042.\n",
            "Epoch 2247. Training loss: 0.7702358365058899. Validation loss: 2.7058897018432617.\n",
            "Epoch 2248. Training loss: 0.7702323794364929. Validation loss: 2.7058892250061035.\n",
            "Epoch 2249. Training loss: 0.7702288031578064. Validation loss: 2.705888509750366.\n",
            "Epoch 2250. Training loss: 0.7702253460884094. Validation loss: 2.705888271331787.\n",
            "Epoch 2251. Training loss: 0.7702218890190125. Validation loss: 2.70588755607605.\n",
            "Epoch 2252. Training loss: 0.7702184319496155. Validation loss: 2.7058868408203125.\n",
            "Epoch 2253. Training loss: 0.770214855670929. Validation loss: 2.7058863639831543.\n",
            "Epoch 2254. Training loss: 0.770211398601532. Validation loss: 2.705885887145996.\n",
            "Epoch 2255. Training loss: 0.7702078819274902. Validation loss: 2.705885171890259.\n",
            "Epoch 2256. Training loss: 0.770204484462738. Validation loss: 2.7058849334716797.\n",
            "Epoch 2257. Training loss: 0.7702009081840515. Validation loss: 2.7058844566345215.\n",
            "Epoch 2258. Training loss: 0.7701973915100098. Validation loss: 2.705883741378784.\n",
            "Epoch 2259. Training loss: 0.7701939940452576. Validation loss: 2.705883264541626.\n",
            "Epoch 2260. Training loss: 0.7701904773712158. Validation loss: 2.7058825492858887.\n",
            "Epoch 2261. Training loss: 0.7701869010925293. Validation loss: 2.7058820724487305.\n",
            "Epoch 2262. Training loss: 0.7701835036277771. Validation loss: 2.7058815956115723.\n",
            "Epoch 2263. Training loss: 0.7701799273490906. Validation loss: 2.705881118774414.\n",
            "Epoch 2264. Training loss: 0.7701764106750488. Validation loss: 2.7058804035186768.\n",
            "Epoch 2265. Training loss: 0.7701728940010071. Validation loss: 2.7058796882629395.\n",
            "Epoch 2266. Training loss: 0.7701694965362549. Validation loss: 2.7058792114257812.\n",
            "Epoch 2267. Training loss: 0.7701659202575684. Validation loss: 2.705878973007202.\n",
            "Epoch 2268. Training loss: 0.7701624035835266. Validation loss: 2.705878257751465.\n",
            "Epoch 2269. Training loss: 0.7701589465141296. Validation loss: 2.7058777809143066.\n",
            "Epoch 2270. Training loss: 0.7701554298400879. Validation loss: 2.7058770656585693.\n",
            "Epoch 2271. Training loss: 0.7701519131660461. Validation loss: 2.705876350402832.\n",
            "Epoch 2272. Training loss: 0.7701484560966492. Validation loss: 2.705875873565674.\n",
            "Epoch 2273. Training loss: 0.7701449394226074. Validation loss: 2.7058756351470947.\n",
            "Epoch 2274. Training loss: 0.7701414227485657. Validation loss: 2.7058749198913574.\n",
            "Epoch 2275. Training loss: 0.7701379656791687. Validation loss: 2.7058746814727783.\n",
            "Epoch 2276. Training loss: 0.770134449005127. Validation loss: 2.705873966217041.\n",
            "Epoch 2277. Training loss: 0.7701308727264404. Validation loss: 2.7058732509613037.\n",
            "Epoch 2278. Training loss: 0.7701273560523987. Validation loss: 2.7058730125427246.\n",
            "Epoch 2279. Training loss: 0.7701239585876465. Validation loss: 2.7058722972869873.\n",
            "Epoch 2280. Training loss: 0.7701204419136047. Validation loss: 2.70587158203125.\n",
            "Epoch 2281. Training loss: 0.7701168656349182. Validation loss: 2.705871105194092.\n",
            "Epoch 2282. Training loss: 0.7701134085655212. Validation loss: 2.7058706283569336.\n",
            "Epoch 2283. Training loss: 0.7701098918914795. Validation loss: 2.7058701515197754.\n",
            "Epoch 2284. Training loss: 0.7701063752174377. Validation loss: 2.705869674682617.\n",
            "Epoch 2285. Training loss: 0.7701029181480408. Validation loss: 2.705869197845459.\n",
            "Epoch 2286. Training loss: 0.7700993418693542. Validation loss: 2.7058684825897217.\n",
            "Epoch 2287. Training loss: 0.7700958847999573. Validation loss: 2.7058682441711426.\n",
            "Epoch 2288. Training loss: 0.7700923085212708. Validation loss: 2.705867290496826.\n",
            "Epoch 2289. Training loss: 0.7700889110565186. Validation loss: 2.705867052078247.\n",
            "Epoch 2290. Training loss: 0.770085334777832. Validation loss: 2.7058663368225098.\n",
            "Epoch 2291. Training loss: 0.7700818181037903. Validation loss: 2.7058658599853516.\n",
            "Epoch 2292. Training loss: 0.7700782418251038. Validation loss: 2.7058651447296143.\n",
            "Epoch 2293. Training loss: 0.7700748443603516. Validation loss: 2.705864906311035.\n",
            "Epoch 2294. Training loss: 0.7700713276863098. Validation loss: 2.705864429473877.\n",
            "Epoch 2295. Training loss: 0.7700677514076233. Validation loss: 2.7058632373809814.\n",
            "Epoch 2296. Training loss: 0.770064115524292. Validation loss: 2.7058632373809814.\n",
            "Epoch 2297. Training loss: 0.7700607180595398. Validation loss: 2.705862522125244.\n",
            "Epoch 2298. Training loss: 0.7700571417808533. Validation loss: 2.705862283706665.\n",
            "Epoch 2299. Training loss: 0.7700536847114563. Validation loss: 2.7058615684509277.\n",
            "Epoch 2300. Training loss: 0.7700502276420593. Validation loss: 2.7058610916137695.\n",
            "Epoch 2301. Training loss: 0.7700466513633728. Validation loss: 2.7058606147766113.\n",
            "Epoch 2302. Training loss: 0.7700431942939758. Validation loss: 2.705859899520874.\n",
            "Epoch 2303. Training loss: 0.7700395584106445. Validation loss: 2.7058591842651367.\n",
            "Epoch 2304. Training loss: 0.7700360417366028. Validation loss: 2.7058587074279785.\n",
            "Epoch 2305. Training loss: 0.7700325846672058. Validation loss: 2.7058582305908203.\n",
            "Epoch 2306. Training loss: 0.7700290679931641. Validation loss: 2.705857515335083.\n",
            "Epoch 2307. Training loss: 0.7700254917144775. Validation loss: 2.705857276916504.\n",
            "Epoch 2308. Training loss: 0.7700220942497253. Validation loss: 2.7058568000793457.\n",
            "Epoch 2309. Training loss: 0.7700185775756836. Validation loss: 2.7058560848236084.\n",
            "Epoch 2310. Training loss: 0.7700149416923523. Validation loss: 2.705855369567871.\n",
            "Epoch 2311. Training loss: 0.7700114846229553. Validation loss: 2.705855131149292.\n",
            "Epoch 2312. Training loss: 0.7700079083442688. Validation loss: 2.705854654312134.\n",
            "Epoch 2313. Training loss: 0.7700044512748718. Validation loss: 2.7058539390563965.\n",
            "Epoch 2314. Training loss: 0.7700009346008301. Validation loss: 2.705853223800659.\n",
            "Epoch 2315. Training loss: 0.7699974179267883. Validation loss: 2.705852508544922.\n",
            "Epoch 2316. Training loss: 0.7699939608573914. Validation loss: 2.7058522701263428.\n",
            "Epoch 2317. Training loss: 0.7699902653694153. Validation loss: 2.7058515548706055.\n",
            "Epoch 2318. Training loss: 0.7699868679046631. Validation loss: 2.7058513164520264.\n",
            "Epoch 2319. Training loss: 0.7699833512306213. Validation loss: 2.705850839614868.\n",
            "Epoch 2320. Training loss: 0.7699797749519348. Validation loss: 2.7058498859405518.\n",
            "Epoch 2321. Training loss: 0.7699763178825378. Validation loss: 2.7058491706848145.\n",
            "Epoch 2322. Training loss: 0.7699728012084961. Validation loss: 2.7058486938476562.\n",
            "Epoch 2323. Training loss: 0.7699691653251648. Validation loss: 2.7058486938476562.\n",
            "Epoch 2324. Training loss: 0.7699657082557678. Validation loss: 2.70584774017334.\n",
            "Epoch 2325. Training loss: 0.7699621319770813. Validation loss: 2.7058470249176025.\n",
            "Epoch 2326. Training loss: 0.7699586749076843. Validation loss: 2.7058467864990234.\n",
            "Epoch 2327. Training loss: 0.7699549794197083. Validation loss: 2.7058463096618652.\n",
            "Epoch 2328. Training loss: 0.769951581954956. Validation loss: 2.705845832824707.\n",
            "Epoch 2329. Training loss: 0.7699480652809143. Validation loss: 2.7058451175689697.\n",
            "Epoch 2330. Training loss: 0.769944429397583. Validation loss: 2.7058446407318115.\n",
            "Epoch 2331. Training loss: 0.7699409127235413. Validation loss: 2.705843925476074.\n",
            "Epoch 2332. Training loss: 0.7699373364448547. Validation loss: 2.705843210220337.\n",
            "Epoch 2333. Training loss: 0.7699339389801025. Validation loss: 2.7058427333831787.\n",
            "Epoch 2334. Training loss: 0.769930362701416. Validation loss: 2.7058424949645996.\n",
            "Epoch 2335. Training loss: 0.7699268460273743. Validation loss: 2.705841541290283.\n",
            "Epoch 2336. Training loss: 0.7699233889579773. Validation loss: 2.705841541290283.\n",
            "Epoch 2337. Training loss: 0.7699198126792908. Validation loss: 2.705841064453125.\n",
            "Epoch 2338. Training loss: 0.769916296005249. Validation loss: 2.7058398723602295.\n",
            "Epoch 2339. Training loss: 0.7699127197265625. Validation loss: 2.7058396339416504.\n",
            "Epoch 2340. Training loss: 0.7699090838432312. Validation loss: 2.705839157104492.\n",
            "Epoch 2341. Training loss: 0.7699056267738342. Validation loss: 2.705838680267334.\n",
            "Epoch 2342. Training loss: 0.7699021697044373. Validation loss: 2.7058377265930176.\n",
            "Epoch 2343. Training loss: 0.7698984742164612. Validation loss: 2.7058374881744385.\n",
            "Epoch 2344. Training loss: 0.7698948979377747. Validation loss: 2.705836772918701.\n",
            "Epoch 2345. Training loss: 0.7698915004730225. Validation loss: 2.705836296081543.\n",
            "Epoch 2346. Training loss: 0.7698879241943359. Validation loss: 2.7058355808258057.\n",
            "Epoch 2347. Training loss: 0.7698845267295837. Validation loss: 2.7058353424072266.\n",
            "Epoch 2348. Training loss: 0.7698807716369629. Validation loss: 2.7058346271514893.\n",
            "Epoch 2349. Training loss: 0.7698772549629211. Validation loss: 2.705833911895752.\n",
            "Epoch 2350. Training loss: 0.7698737978935242. Validation loss: 2.7058334350585938.\n",
            "Epoch 2351. Training loss: 0.7698702812194824. Validation loss: 2.7058329582214355.\n",
            "Epoch 2352. Training loss: 0.7698666453361511. Validation loss: 2.7058324813842773.\n",
            "Epoch 2353. Training loss: 0.7698631286621094. Validation loss: 2.705832004547119.\n",
            "Epoch 2354. Training loss: 0.7698596119880676. Validation loss: 2.705831527709961.\n",
            "Epoch 2355. Training loss: 0.7698560357093811. Validation loss: 2.7058310508728027.\n",
            "Epoch 2356. Training loss: 0.7698525786399841. Validation loss: 2.7058300971984863.\n",
            "Epoch 2357. Training loss: 0.7698490023612976. Validation loss: 2.705829620361328.\n",
            "Epoch 2358. Training loss: 0.7698454260826111. Validation loss: 2.70582914352417.\n",
            "Epoch 2359. Training loss: 0.7698418498039246. Validation loss: 2.7058286666870117.\n",
            "Epoch 2360. Training loss: 0.7698383331298828. Validation loss: 2.7058281898498535.\n",
            "Epoch 2361. Training loss: 0.7698347568511963. Validation loss: 2.705827474594116.\n",
            "Epoch 2362. Training loss: 0.7698311805725098. Validation loss: 2.705826997756958.\n",
            "Epoch 2363. Training loss: 0.769827663898468. Validation loss: 2.7058262825012207.\n",
            "Epoch 2364. Training loss: 0.7698240876197815. Validation loss: 2.7058260440826416.\n",
            "Epoch 2365. Training loss: 0.769820511341095. Validation loss: 2.705825090408325.\n",
            "Epoch 2366. Training loss: 0.769817054271698. Validation loss: 2.705824851989746.\n",
            "Epoch 2367. Training loss: 0.7698135375976562. Validation loss: 2.705824613571167.\n",
            "Epoch 2368. Training loss: 0.769809901714325. Validation loss: 2.7058234214782715.\n",
            "Epoch 2369. Training loss: 0.7698063850402832. Validation loss: 2.7058231830596924.\n",
            "Epoch 2370. Training loss: 0.7698028087615967. Validation loss: 2.705822467803955.\n",
            "Epoch 2371. Training loss: 0.7697992324829102. Validation loss: 2.705821990966797.\n",
            "Epoch 2372. Training loss: 0.7697957158088684. Validation loss: 2.7058215141296387.\n",
            "Epoch 2373. Training loss: 0.7697922587394714. Validation loss: 2.7058210372924805.\n",
            "Epoch 2374. Training loss: 0.7697885632514954. Validation loss: 2.705820083618164.\n",
            "Epoch 2375. Training loss: 0.7697849869728088. Validation loss: 2.705819845199585.\n",
            "Epoch 2376. Training loss: 0.7697815299034119. Validation loss: 2.7058191299438477.\n",
            "Epoch 2377. Training loss: 0.7697778344154358. Validation loss: 2.7058186531066895.\n",
            "Epoch 2378. Training loss: 0.7697743773460388. Validation loss: 2.705817937850952.\n",
            "Epoch 2379. Training loss: 0.7697708606719971. Validation loss: 2.705817699432373.\n",
            "Epoch 2380. Training loss: 0.7697672843933105. Validation loss: 2.7058169841766357.\n",
            "Epoch 2381. Training loss: 0.7697637677192688. Validation loss: 2.7058162689208984.\n",
            "Epoch 2382. Training loss: 0.7697601318359375. Validation loss: 2.7058160305023193.\n",
            "Epoch 2383. Training loss: 0.769756555557251. Validation loss: 2.705815315246582.\n",
            "Epoch 2384. Training loss: 0.7697530388832092. Validation loss: 2.705815076828003.\n",
            "Epoch 2385. Training loss: 0.7697494626045227. Validation loss: 2.7058143615722656.\n",
            "Epoch 2386. Training loss: 0.7697458863258362. Validation loss: 2.7058138847351074.\n",
            "Epoch 2387. Training loss: 0.7697423100471497. Validation loss: 2.70581316947937.\n",
            "Epoch 2388. Training loss: 0.7697387337684631. Validation loss: 2.705812692642212.\n",
            "Epoch 2389. Training loss: 0.7697351574897766. Validation loss: 2.7058119773864746.\n",
            "Epoch 2390. Training loss: 0.7697315812110901. Validation loss: 2.7058115005493164.\n",
            "Epoch 2391. Training loss: 0.7697281837463379. Validation loss: 2.7058112621307373.\n",
            "Epoch 2392. Training loss: 0.7697245478630066. Validation loss: 2.705810546875.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2393. Training loss: 0.7697209715843201. Validation loss: 2.7058098316192627.\n",
            "Epoch 2394. Training loss: 0.7697173953056335. Validation loss: 2.7058091163635254.\n",
            "Epoch 2395. Training loss: 0.769713819026947. Validation loss: 2.7058088779449463.\n",
            "Epoch 2396. Training loss: 0.7697103023529053. Validation loss: 2.705808639526367.\n",
            "Epoch 2397. Training loss: 0.769706666469574. Validation loss: 2.7058074474334717.\n",
            "Epoch 2398. Training loss: 0.7697030901908875. Validation loss: 2.7058074474334717.\n",
            "Epoch 2399. Training loss: 0.7696996331214905. Validation loss: 2.7058067321777344.\n",
            "Epoch 2400. Training loss: 0.7696959972381592. Validation loss: 2.705806016921997.\n",
            "Epoch 2401. Training loss: 0.7696924209594727. Validation loss: 2.705805540084839.\n",
            "Epoch 2402. Training loss: 0.7696889042854309. Validation loss: 2.7058048248291016.\n",
            "Epoch 2403. Training loss: 0.7696852684020996. Validation loss: 2.7058048248291016.\n",
            "Epoch 2404. Training loss: 0.7696816921234131. Validation loss: 2.705803632736206.\n",
            "Epoch 2405. Training loss: 0.7696781754493713. Validation loss: 2.705803155899048.\n",
            "Epoch 2406. Training loss: 0.7696745991706848. Validation loss: 2.7058024406433105.\n",
            "Epoch 2407. Training loss: 0.7696710228919983. Validation loss: 2.7058022022247314.\n",
            "Epoch 2408. Training loss: 0.7696674466133118. Validation loss: 2.7058017253875732.\n",
            "Epoch 2409. Training loss: 0.7696638107299805. Validation loss: 2.705801486968994.\n",
            "Epoch 2410. Training loss: 0.7696602940559387. Validation loss: 2.7058005332946777.\n",
            "Epoch 2411. Training loss: 0.7696567177772522. Validation loss: 2.7057998180389404.\n",
            "Epoch 2412. Training loss: 0.7696530818939209. Validation loss: 2.705799102783203.\n",
            "Epoch 2413. Training loss: 0.7696495056152344. Validation loss: 2.7057993412017822.\n",
            "Epoch 2414. Training loss: 0.7696459889411926. Validation loss: 2.705798864364624.\n",
            "Epoch 2415. Training loss: 0.7696424126625061. Validation loss: 2.7057981491088867.\n",
            "Epoch 2416. Training loss: 0.7696388363838196. Validation loss: 2.7057976722717285.\n",
            "Epoch 2417. Training loss: 0.7696352601051331. Validation loss: 2.7057974338531494.\n",
            "Epoch 2418. Training loss: 0.7696316838264465. Validation loss: 2.7057974338531494.\n",
            "Epoch 2419. Training loss: 0.7696280479431152. Validation loss: 2.7057971954345703.\n",
            "Epoch 2420. Training loss: 0.7696245312690735. Validation loss: 2.705796241760254.\n",
            "Epoch 2421. Training loss: 0.769620954990387. Validation loss: 2.7057957649230957.\n",
            "Epoch 2422. Training loss: 0.7696173787117004. Validation loss: 2.7057957649230957.\n",
            "Epoch 2423. Training loss: 0.7696137428283691. Validation loss: 2.7057952880859375.\n",
            "Epoch 2424. Training loss: 0.7696102261543274. Validation loss: 2.7057945728302.\n",
            "Epoch 2425. Training loss: 0.7696065902709961. Validation loss: 2.7057945728302.\n",
            "Epoch 2426. Training loss: 0.7696030139923096. Validation loss: 2.705794334411621.\n",
            "Epoch 2427. Training loss: 0.769599437713623. Validation loss: 2.705793857574463.\n",
            "Epoch 2428. Training loss: 0.7695958018302917. Validation loss: 2.7057933807373047.\n",
            "Epoch 2429. Training loss: 0.76959228515625. Validation loss: 2.7057931423187256.\n",
            "Epoch 2430. Training loss: 0.7695886492729187. Validation loss: 2.7057926654815674.\n",
            "Epoch 2431. Training loss: 0.7695850729942322. Validation loss: 2.705792188644409.\n",
            "Epoch 2432. Training loss: 0.7695815563201904. Validation loss: 2.70579195022583.\n",
            "Epoch 2433. Training loss: 0.7695779204368591. Validation loss: 2.705791473388672.\n",
            "Epoch 2434. Training loss: 0.7695743441581726. Validation loss: 2.705791473388672.\n",
            "Epoch 2435. Training loss: 0.7695707678794861. Validation loss: 2.7057909965515137.\n",
            "Epoch 2436. Training loss: 0.7695671916007996. Validation loss: 2.7057909965515137.\n",
            "Epoch 2437. Training loss: 0.769563615322113. Validation loss: 2.7057900428771973.\n",
            "Epoch 2438. Training loss: 0.7695600390434265. Validation loss: 2.705789566040039.\n",
            "Epoch 2439. Training loss: 0.7695563435554504. Validation loss: 2.70578932762146.\n",
            "Epoch 2440. Training loss: 0.7695527672767639. Validation loss: 2.705789089202881.\n",
            "Epoch 2441. Training loss: 0.7695493102073669. Validation loss: 2.7057888507843018.\n",
            "Epoch 2442. Training loss: 0.7695456147193909. Validation loss: 2.7057883739471436.\n",
            "Epoch 2443. Training loss: 0.7695419788360596. Validation loss: 2.7057881355285645.\n",
            "Epoch 2444. Training loss: 0.7695384621620178. Validation loss: 2.7057876586914062.\n",
            "Epoch 2445. Training loss: 0.7695348262786865. Validation loss: 2.705787420272827.\n",
            "Epoch 2446. Training loss: 0.76953125. Validation loss: 2.70578670501709.\n",
            "Epoch 2447. Training loss: 0.7695276141166687. Validation loss: 2.70578670501709.\n",
            "Epoch 2448. Training loss: 0.7695240378379822. Validation loss: 2.7057862281799316.\n",
            "Epoch 2449. Training loss: 0.7695204615592957. Validation loss: 2.7057857513427734.\n",
            "Epoch 2450. Training loss: 0.7695167660713196. Validation loss: 2.705785036087036.\n",
            "Epoch 2451. Training loss: 0.7695133090019226. Validation loss: 2.705784797668457.\n",
            "Epoch 2452. Training loss: 0.7695096135139465. Validation loss: 2.705784797668457.\n",
            "Epoch 2453. Training loss: 0.76950603723526. Validation loss: 2.7057840824127197.\n",
            "Epoch 2454. Training loss: 0.7695024013519287. Validation loss: 2.7057838439941406.\n",
            "Epoch 2455. Training loss: 0.769498884677887. Validation loss: 2.7057836055755615.\n",
            "Epoch 2456. Training loss: 0.7694952487945557. Validation loss: 2.7057833671569824.\n",
            "Epoch 2457. Training loss: 0.7694916129112244. Validation loss: 2.705782413482666.\n",
            "Epoch 2458. Training loss: 0.7694880366325378. Validation loss: 2.705782413482666.\n",
            "Epoch 2459. Training loss: 0.7694844603538513. Validation loss: 2.705781936645508.\n",
            "Epoch 2460. Training loss: 0.7694807648658752. Validation loss: 2.7057814598083496.\n",
            "Epoch 2461. Training loss: 0.7694771885871887. Validation loss: 2.7057814598083496.\n",
            "Epoch 2462. Training loss: 0.7694736123085022. Validation loss: 2.7057809829711914.\n",
            "Epoch 2463. Training loss: 0.7694700360298157. Validation loss: 2.7057807445526123.\n",
            "Epoch 2464. Training loss: 0.7694664001464844. Validation loss: 2.705780267715454.\n",
            "Epoch 2465. Training loss: 0.7694627642631531. Validation loss: 2.705780029296875.\n",
            "Epoch 2466. Training loss: 0.7694591879844666. Validation loss: 2.705779552459717.\n",
            "Epoch 2467. Training loss: 0.76945561170578. Validation loss: 2.7057788372039795.\n",
            "Epoch 2468. Training loss: 0.7694520950317383. Validation loss: 2.7057785987854004.\n",
            "Epoch 2469. Training loss: 0.769448459148407. Validation loss: 2.7057785987854004.\n",
            "Epoch 2470. Training loss: 0.7694447636604309. Validation loss: 2.705778121948242.\n",
            "Epoch 2471. Training loss: 0.7694411873817444. Validation loss: 2.705777883529663.\n",
            "Epoch 2472. Training loss: 0.7694374918937683. Validation loss: 2.7057769298553467.\n",
            "Epoch 2473. Training loss: 0.7694337964057922. Validation loss: 2.7057766914367676.\n",
            "Epoch 2474. Training loss: 0.7694303393363953. Validation loss: 2.7057762145996094.\n",
            "Epoch 2475. Training loss: 0.7694266438484192. Validation loss: 2.7057759761810303.\n",
            "Epoch 2476. Training loss: 0.7694231867790222. Validation loss: 2.705775737762451.\n",
            "Epoch 2477. Training loss: 0.7694194912910461. Validation loss: 2.705775499343872.\n",
            "Epoch 2478. Training loss: 0.7694158554077148. Validation loss: 2.7057747840881348.\n",
            "Epoch 2479. Training loss: 0.7694122791290283. Validation loss: 2.705775260925293.\n",
            "Epoch 2480. Training loss: 0.769408643245697. Validation loss: 2.7057740688323975.\n",
            "Epoch 2481. Training loss: 0.7694050669670105. Validation loss: 2.7057738304138184.\n",
            "Epoch 2482. Training loss: 0.769401490688324. Validation loss: 2.7057735919952393.\n",
            "Epoch 2483. Training loss: 0.7693977355957031. Validation loss: 2.705773115158081.\n",
            "Epoch 2484. Training loss: 0.7693941593170166. Validation loss: 2.705773115158081.\n",
            "Epoch 2485. Training loss: 0.7693905830383301. Validation loss: 2.705772638320923.\n",
            "Epoch 2486. Training loss: 0.7693870663642883. Validation loss: 2.7057723999023438.\n",
            "Epoch 2487. Training loss: 0.7693833708763123. Validation loss: 2.7057719230651855.\n",
            "Epoch 2488. Training loss: 0.7693797945976257. Validation loss: 2.7057714462280273.\n",
            "Epoch 2489. Training loss: 0.7693760991096497. Validation loss: 2.705770969390869.\n",
            "Epoch 2490. Training loss: 0.7693725228309631. Validation loss: 2.705770492553711.\n",
            "Epoch 2491. Training loss: 0.7693689465522766. Validation loss: 2.705770254135132.\n",
            "Epoch 2492. Training loss: 0.7693652510643005. Validation loss: 2.7057695388793945.\n",
            "Epoch 2493. Training loss: 0.769361674785614. Validation loss: 2.7057695388793945.\n",
            "Epoch 2494. Training loss: 0.7693580985069275. Validation loss: 2.7057690620422363.\n",
            "Epoch 2495. Training loss: 0.7693544030189514. Validation loss: 2.7057688236236572.\n",
            "Epoch 2496. Training loss: 0.7693507075309753. Validation loss: 2.705768346786499.\n",
            "Epoch 2497. Training loss: 0.7693471908569336. Validation loss: 2.70576810836792.\n",
            "Epoch 2498. Training loss: 0.7693434357643127. Validation loss: 2.7057676315307617.\n",
            "Epoch 2499. Training loss: 0.7693398594856262. Validation loss: 2.7057673931121826.\n",
            "Epoch 2500. Training loss: 0.7693362832069397. Validation loss: 2.7057669162750244.\n",
            "Epoch 2501. Training loss: 0.7693325877189636. Validation loss: 2.705766201019287.\n",
            "Epoch 2502. Training loss: 0.7693290114402771. Validation loss: 2.705766201019287.\n",
            "Epoch 2503. Training loss: 0.769325315952301. Validation loss: 2.705765724182129.\n",
            "Epoch 2504. Training loss: 0.7693216800689697. Validation loss: 2.70576548576355.\n",
            "Epoch 2505. Training loss: 0.769318163394928. Validation loss: 2.7057652473449707.\n",
            "Epoch 2506. Training loss: 0.7693145275115967. Validation loss: 2.7057647705078125.\n",
            "Epoch 2507. Training loss: 0.7693107724189758. Validation loss: 2.7057642936706543.\n",
            "Epoch 2508. Training loss: 0.7693071961402893. Validation loss: 2.7057642936706543.\n",
            "Epoch 2509. Training loss: 0.769303560256958. Validation loss: 2.705763578414917.\n",
            "Epoch 2510. Training loss: 0.7692999839782715. Validation loss: 2.705763339996338.\n",
            "Epoch 2511. Training loss: 0.7692963480949402. Validation loss: 2.705763101577759.\n",
            "Epoch 2512. Training loss: 0.7692927718162537. Validation loss: 2.7057623863220215.\n",
            "Epoch 2513. Training loss: 0.7692890763282776. Validation loss: 2.7057621479034424.\n",
            "Epoch 2514. Training loss: 0.7692854404449463. Validation loss: 2.7057619094848633.\n",
            "Epoch 2515. Training loss: 0.7692818641662598. Validation loss: 2.705760955810547.\n",
            "Epoch 2516. Training loss: 0.7692780494689941. Validation loss: 2.705760955810547.\n",
            "Epoch 2517. Training loss: 0.7692745327949524. Validation loss: 2.7057602405548096.\n",
            "Epoch 2518. Training loss: 0.7692708373069763. Validation loss: 2.7057602405548096.\n",
            "Epoch 2519. Training loss: 0.7692671418190002. Validation loss: 2.7057597637176514.\n",
            "Epoch 2520. Training loss: 0.7692635655403137. Validation loss: 2.7057597637176514.\n",
            "Epoch 2521. Training loss: 0.7692599892616272. Validation loss: 2.705759048461914.\n",
            "Epoch 2522. Training loss: 0.7692562937736511. Validation loss: 2.705758810043335.\n",
            "Epoch 2523. Training loss: 0.7692527770996094. Validation loss: 2.7057583332061768.\n",
            "Epoch 2524. Training loss: 0.7692490220069885. Validation loss: 2.7057578563690186.\n",
            "Epoch 2525. Training loss: 0.7692453861236572. Validation loss: 2.7057576179504395.\n",
            "Epoch 2526. Training loss: 0.7692418098449707. Validation loss: 2.7057573795318604.\n",
            "Epoch 2527. Training loss: 0.7692381739616394. Validation loss: 2.705756902694702.\n",
            "Epoch 2528. Training loss: 0.7692344188690186. Validation loss: 2.705756425857544.\n",
            "Epoch 2529. Training loss: 0.769230842590332. Validation loss: 2.705756187438965.\n",
            "Epoch 2530. Training loss: 0.7692272067070007. Validation loss: 2.7057557106018066.\n",
            "Epoch 2531. Training loss: 0.7692236304283142. Validation loss: 2.7057552337646484.\n",
            "Epoch 2532. Training loss: 0.7692198753356934. Validation loss: 2.7057549953460693.\n",
            "Epoch 2533. Training loss: 0.7692162990570068. Validation loss: 2.7057547569274902.\n",
            "Epoch 2534. Training loss: 0.769212543964386. Validation loss: 2.705754280090332.\n",
            "Epoch 2535. Training loss: 0.7692089676856995. Validation loss: 2.705754041671753.\n",
            "Epoch 2536. Training loss: 0.7692053318023682. Validation loss: 2.7057533264160156.\n",
            "Epoch 2537. Training loss: 0.7692016959190369. Validation loss: 2.7057530879974365.\n",
            "Epoch 2538. Training loss: 0.7691981196403503. Validation loss: 2.7057526111602783.\n",
            "Epoch 2539. Training loss: 0.7691944241523743. Validation loss: 2.70575213432312.\n",
            "Epoch 2540. Training loss: 0.7691907286643982. Validation loss: 2.70575213432312.\n",
            "Epoch 2541. Training loss: 0.7691871523857117. Validation loss: 2.705751657485962.\n",
            "Epoch 2542. Training loss: 0.7691833972930908. Validation loss: 2.7057509422302246.\n",
            "Epoch 2543. Training loss: 0.7691797614097595. Validation loss: 2.7057509422302246.\n",
            "Epoch 2544. Training loss: 0.769176185131073. Validation loss: 2.7057502269744873.\n",
            "Epoch 2545. Training loss: 0.7691724896430969. Validation loss: 2.705749750137329.\n",
            "Epoch 2546. Training loss: 0.7691688537597656. Validation loss: 2.705749750137329.\n",
            "Epoch 2547. Training loss: 0.7691652178764343. Validation loss: 2.705749273300171.\n",
            "Epoch 2548. Training loss: 0.7691615223884583. Validation loss: 2.705749273300171.\n",
            "Epoch 2549. Training loss: 0.769157886505127. Validation loss: 2.7057485580444336.\n",
            "Epoch 2550. Training loss: 0.7691541314125061. Validation loss: 2.7057480812072754.\n",
            "Epoch 2551. Training loss: 0.7691505551338196. Validation loss: 2.7057478427886963.\n",
            "Epoch 2552. Training loss: 0.7691468596458435. Validation loss: 2.705747604370117.\n",
            "Epoch 2553. Training loss: 0.7691431641578674. Validation loss: 2.705747127532959.\n",
            "Epoch 2554. Training loss: 0.7691397070884705. Validation loss: 2.70574688911438.\n",
            "Epoch 2555. Training loss: 0.7691357731819153. Validation loss: 2.7057459354400635.\n",
            "Epoch 2556. Training loss: 0.7691323161125183. Validation loss: 2.7057461738586426.\n",
            "Epoch 2557. Training loss: 0.7691285610198975. Validation loss: 2.7057456970214844.\n",
            "Epoch 2558. Training loss: 0.7691249251365662. Validation loss: 2.705744981765747.\n",
            "Epoch 2559. Training loss: 0.7691212296485901. Validation loss: 2.705744743347168.\n",
            "Epoch 2560. Training loss: 0.7691176533699036. Validation loss: 2.705744504928589.\n",
            "Epoch 2561. Training loss: 0.7691140174865723. Validation loss: 2.7057442665100098.\n",
            "Epoch 2562. Training loss: 0.769110381603241. Validation loss: 2.7057437896728516.\n",
            "Epoch 2563. Training loss: 0.7691066861152649. Validation loss: 2.7057435512542725.\n",
            "Epoch 2564. Training loss: 0.7691029906272888. Validation loss: 2.7057430744171143.\n",
            "Epoch 2565. Training loss: 0.7690992951393127. Validation loss: 2.705742597579956.\n",
            "Epoch 2566. Training loss: 0.7690955996513367. Validation loss: 2.705742359161377.\n",
            "Epoch 2567. Training loss: 0.7690920829772949. Validation loss: 2.705742120742798.\n",
            "Epoch 2568. Training loss: 0.7690882682800293. Validation loss: 2.7057416439056396.\n",
            "Epoch 2569. Training loss: 0.769084632396698. Validation loss: 2.7057411670684814.\n",
            "Epoch 2570. Training loss: 0.7690810561180115. Validation loss: 2.7057409286499023.\n",
            "Epoch 2571. Training loss: 0.7690773010253906. Validation loss: 2.705739974975586.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2572. Training loss: 0.7690736651420593. Validation loss: 2.705739736557007.\n",
            "Epoch 2573. Training loss: 0.7690699696540833. Validation loss: 2.7057394981384277.\n",
            "Epoch 2574. Training loss: 0.769066333770752. Validation loss: 2.7057392597198486.\n",
            "Epoch 2575. Training loss: 0.7690625786781311. Validation loss: 2.7057390213012695.\n",
            "Epoch 2576. Training loss: 0.7690590023994446. Validation loss: 2.7057387828826904.\n",
            "Epoch 2577. Training loss: 0.7690553069114685. Validation loss: 2.705738067626953.\n",
            "Epoch 2578. Training loss: 0.769051730632782. Validation loss: 2.705737829208374.\n",
            "Epoch 2579. Training loss: 0.7690480351448059. Validation loss: 2.705737352371216.\n",
            "Epoch 2580. Training loss: 0.7690442204475403. Validation loss: 2.7057368755340576.\n",
            "Epoch 2581. Training loss: 0.769040584564209. Validation loss: 2.7057363986968994.\n",
            "Epoch 2582. Training loss: 0.7690368294715881. Validation loss: 2.7057363986968994.\n",
            "Epoch 2583. Training loss: 0.7690333724021912. Validation loss: 2.7057361602783203.\n",
            "Epoch 2584. Training loss: 0.7690296173095703. Validation loss: 2.705735445022583.\n",
            "Epoch 2585. Training loss: 0.769025981426239. Validation loss: 2.705735206604004.\n",
            "Epoch 2586. Training loss: 0.7690222859382629. Validation loss: 2.7057347297668457.\n",
            "Epoch 2587. Training loss: 0.7690185904502869. Validation loss: 2.7057342529296875.\n",
            "Epoch 2588. Training loss: 0.7690148949623108. Validation loss: 2.7057340145111084.\n",
            "Epoch 2589. Training loss: 0.7690111994743347. Validation loss: 2.7057337760925293.\n",
            "Epoch 2590. Training loss: 0.7690076231956482. Validation loss: 2.705733060836792.\n",
            "Epoch 2591. Training loss: 0.7690039277076721. Validation loss: 2.705733060836792.\n",
            "Epoch 2592. Training loss: 0.7690001130104065. Validation loss: 2.705732822418213.\n",
            "Epoch 2593. Training loss: 0.76899653673172. Validation loss: 2.7057321071624756.\n",
            "Epoch 2594. Training loss: 0.7689927220344543. Validation loss: 2.7057316303253174.\n",
            "Epoch 2595. Training loss: 0.7689892649650574. Validation loss: 2.705731153488159.\n",
            "Epoch 2596. Training loss: 0.7689855098724365. Validation loss: 2.705731153488159.\n",
            "Epoch 2597. Training loss: 0.7689817547798157. Validation loss: 2.705730676651001.\n",
            "Epoch 2598. Training loss: 0.7689781188964844. Validation loss: 2.7057301998138428.\n",
            "Epoch 2599. Training loss: 0.7689743638038635. Validation loss: 2.7057299613952637.\n",
            "Epoch 2600. Training loss: 0.7689707279205322. Validation loss: 2.7057294845581055.\n",
            "Epoch 2601. Training loss: 0.7689670920372009. Validation loss: 2.7057292461395264.\n",
            "Epoch 2602. Training loss: 0.7689633369445801. Validation loss: 2.705728769302368.\n",
            "Epoch 2603. Training loss: 0.7689597010612488. Validation loss: 2.70572829246521.\n",
            "Epoch 2604. Training loss: 0.7689560055732727. Validation loss: 2.705728054046631.\n",
            "Epoch 2605. Training loss: 0.7689523100852966. Validation loss: 2.7057275772094727.\n",
            "Epoch 2606. Training loss: 0.7689486145973206. Validation loss: 2.7057271003723145.\n",
            "Epoch 2607. Training loss: 0.7689449787139893. Validation loss: 2.7057268619537354.\n",
            "Epoch 2608. Training loss: 0.7689412236213684. Validation loss: 2.705726385116577.\n",
            "Epoch 2609. Training loss: 0.7689375877380371. Validation loss: 2.705725908279419.\n",
            "Epoch 2610. Training loss: 0.7689338326454163. Validation loss: 2.70572566986084.\n",
            "Epoch 2611. Training loss: 0.768930196762085. Validation loss: 2.7057254314422607.\n",
            "Epoch 2612. Training loss: 0.7689265608787537. Validation loss: 2.7057249546051025.\n",
            "Epoch 2613. Training loss: 0.7689228057861328. Validation loss: 2.7057247161865234.\n",
            "Epoch 2614. Training loss: 0.768919050693512. Validation loss: 2.705724000930786.\n",
            "Epoch 2615. Training loss: 0.7689153552055359. Validation loss: 2.705723762512207.\n",
            "Epoch 2616. Training loss: 0.7689117789268494. Validation loss: 2.705723285675049.\n",
            "Epoch 2617. Training loss: 0.7689080238342285. Validation loss: 2.705723285675049.\n",
            "Epoch 2618. Training loss: 0.7689043879508972. Validation loss: 2.7057230472564697.\n",
            "Epoch 2619. Training loss: 0.7689006328582764. Validation loss: 2.7057223320007324.\n",
            "Epoch 2620. Training loss: 0.7688969969749451. Validation loss: 2.705721855163574.\n",
            "Epoch 2621. Training loss: 0.7688932418823242. Validation loss: 2.705721378326416.\n",
            "Epoch 2622. Training loss: 0.7688894867897034. Validation loss: 2.705721139907837.\n",
            "Epoch 2623. Training loss: 0.7688858509063721. Validation loss: 2.7057206630706787.\n",
            "Epoch 2624. Training loss: 0.7688820958137512. Validation loss: 2.7057201862335205.\n",
            "Epoch 2625. Training loss: 0.7688784599304199. Validation loss: 2.7057199478149414.\n",
            "Epoch 2626. Training loss: 0.7688748240470886. Validation loss: 2.705719470977783.\n",
            "Epoch 2627. Training loss: 0.768871009349823. Validation loss: 2.705718994140625.\n",
            "Epoch 2628. Training loss: 0.7688673138618469. Validation loss: 2.705718994140625.\n",
            "Epoch 2629. Training loss: 0.7688636779785156. Validation loss: 2.705718517303467.\n",
            "Epoch 2630. Training loss: 0.7688599228858948. Validation loss: 2.7057180404663086.\n",
            "Epoch 2631. Training loss: 0.7688562870025635. Validation loss: 2.7057178020477295.\n",
            "Epoch 2632. Training loss: 0.7688525319099426. Validation loss: 2.7057173252105713.\n",
            "Epoch 2633. Training loss: 0.768848717212677. Validation loss: 2.705717086791992.\n",
            "Epoch 2634. Training loss: 0.7688451409339905. Validation loss: 2.705716609954834.\n",
            "Epoch 2635. Training loss: 0.7688414454460144. Validation loss: 2.705716371536255.\n",
            "Epoch 2636. Training loss: 0.7688376903533936. Validation loss: 2.7057156562805176.\n",
            "Epoch 2637. Training loss: 0.7688340544700623. Validation loss: 2.7057154178619385.\n",
            "Epoch 2638. Training loss: 0.7688302993774414. Validation loss: 2.7057149410247803.\n",
            "Epoch 2639. Training loss: 0.7688266634941101. Validation loss: 2.7057149410247803.\n",
            "Epoch 2640. Training loss: 0.7688229084014893. Validation loss: 2.705714464187622.\n",
            "Epoch 2641. Training loss: 0.7688191533088684. Validation loss: 2.7057137489318848.\n",
            "Epoch 2642. Training loss: 0.7688153386116028. Validation loss: 2.7057137489318848.\n",
            "Epoch 2643. Training loss: 0.7688117623329163. Validation loss: 2.7057132720947266.\n",
            "Epoch 2644. Training loss: 0.7688080668449402. Validation loss: 2.7057127952575684.\n",
            "Epoch 2645. Training loss: 0.7688043713569641. Validation loss: 2.70571231842041.\n",
            "Epoch 2646. Training loss: 0.768800675868988. Validation loss: 2.705712080001831.\n",
            "Epoch 2647. Training loss: 0.7687969207763672. Validation loss: 2.705711841583252.\n",
            "Epoch 2648. Training loss: 0.7687932848930359. Validation loss: 2.7057111263275146.\n",
            "Epoch 2649. Training loss: 0.7687894701957703. Validation loss: 2.7057108879089355.\n",
            "Epoch 2650. Training loss: 0.7687857151031494. Validation loss: 2.7057106494903564.\n",
            "Epoch 2651. Training loss: 0.7687819600105286. Validation loss: 2.7057101726531982.\n",
            "Epoch 2652. Training loss: 0.7687783241271973. Validation loss: 2.705709934234619.\n",
            "Epoch 2653. Training loss: 0.768774688243866. Validation loss: 2.705709457397461.\n",
            "Epoch 2654. Training loss: 0.7687708735466003. Validation loss: 2.7057089805603027.\n",
            "Epoch 2655. Training loss: 0.7687671780586243. Validation loss: 2.7057085037231445.\n",
            "Epoch 2656. Training loss: 0.7687634825706482. Validation loss: 2.7057082653045654.\n",
            "Epoch 2657. Training loss: 0.7687597274780273. Validation loss: 2.7057077884674072.\n",
            "Epoch 2658. Training loss: 0.768756091594696. Validation loss: 2.705707550048828.\n",
            "Epoch 2659. Training loss: 0.7687523365020752. Validation loss: 2.705707311630249.\n",
            "Epoch 2660. Training loss: 0.7687485814094543. Validation loss: 2.7057065963745117.\n",
            "Epoch 2661. Training loss: 0.7687448859214783. Validation loss: 2.7057065963745117.\n",
            "Epoch 2662. Training loss: 0.7687411308288574. Validation loss: 2.7057061195373535.\n",
            "Epoch 2663. Training loss: 0.7687374949455261. Validation loss: 2.705705404281616.\n",
            "Epoch 2664. Training loss: 0.7687336802482605. Validation loss: 2.705704927444458.\n",
            "Epoch 2665. Training loss: 0.768730103969574. Validation loss: 2.705704689025879.\n",
            "Epoch 2666. Training loss: 0.7687262892723083. Validation loss: 2.7057044506073.\n",
            "Epoch 2667. Training loss: 0.7687225341796875. Validation loss: 2.7057039737701416.\n",
            "Epoch 2668. Training loss: 0.7687188982963562. Validation loss: 2.7057037353515625.\n",
            "Epoch 2669. Training loss: 0.7687151432037354. Validation loss: 2.7057032585144043.\n",
            "Epoch 2670. Training loss: 0.7687113881111145. Validation loss: 2.705702781677246.\n",
            "Epoch 2671. Training loss: 0.7687077522277832. Validation loss: 2.705702543258667.\n",
            "Epoch 2672. Training loss: 0.7687039375305176. Validation loss: 2.7057018280029297.\n",
            "Epoch 2673. Training loss: 0.7687003016471863. Validation loss: 2.7057018280029297.\n",
            "Epoch 2674. Training loss: 0.7686965465545654. Validation loss: 2.7057015895843506.\n",
            "Epoch 2675. Training loss: 0.7686927318572998. Validation loss: 2.7057013511657715.\n",
            "Epoch 2676. Training loss: 0.7686890959739685. Validation loss: 2.7057008743286133.\n",
            "Epoch 2677. Training loss: 0.7686853408813477. Validation loss: 2.705700159072876.\n",
            "Epoch 2678. Training loss: 0.768681526184082. Validation loss: 2.7056996822357178.\n",
            "Epoch 2679. Training loss: 0.7686778903007507. Validation loss: 2.7056994438171387.\n",
            "Epoch 2680. Training loss: 0.7686741352081299. Validation loss: 2.7056989669799805.\n",
            "Epoch 2681. Training loss: 0.768670380115509. Validation loss: 2.7056987285614014.\n",
            "Epoch 2682. Training loss: 0.7686665654182434. Validation loss: 2.7056984901428223.\n",
            "Epoch 2683. Training loss: 0.7686628699302673. Validation loss: 2.705698013305664.\n",
            "Epoch 2684. Training loss: 0.7686591148376465. Validation loss: 2.705697536468506.\n",
            "Epoch 2685. Training loss: 0.76865553855896. Validation loss: 2.7056972980499268.\n",
            "Epoch 2686. Training loss: 0.7686516642570496. Validation loss: 2.7056965827941895.\n",
            "Epoch 2687. Training loss: 0.7686479687690735. Validation loss: 2.7056965827941895.\n",
            "Epoch 2688. Training loss: 0.7686442732810974. Validation loss: 2.7056961059570312.\n",
            "Epoch 2689. Training loss: 0.7686405181884766. Validation loss: 2.705695629119873.\n",
            "Epoch 2690. Training loss: 0.7686367630958557. Validation loss: 2.705695629119873.\n",
            "Epoch 2691. Training loss: 0.7686330676078796. Validation loss: 2.7056949138641357.\n",
            "Epoch 2692. Training loss: 0.7686293125152588. Validation loss: 2.7056944370269775.\n",
            "Epoch 2693. Training loss: 0.7686255574226379. Validation loss: 2.7056941986083984.\n",
            "Epoch 2694. Training loss: 0.7686217427253723. Validation loss: 2.705693483352661.\n",
            "Epoch 2695. Training loss: 0.768618106842041. Validation loss: 2.705693244934082.\n",
            "Epoch 2696. Training loss: 0.7686142921447754. Validation loss: 2.705692768096924.\n",
            "Epoch 2697. Training loss: 0.7686106562614441. Validation loss: 2.7056922912597656.\n",
            "Epoch 2698. Training loss: 0.7686067223548889. Validation loss: 2.7056922912597656.\n",
            "Epoch 2699. Training loss: 0.7686031460762024. Validation loss: 2.7056918144226074.\n",
            "Epoch 2700. Training loss: 0.7685994505882263. Validation loss: 2.705691337585449.\n",
            "Epoch 2701. Training loss: 0.7685956358909607. Validation loss: 2.705690860748291.\n",
            "Epoch 2702. Training loss: 0.7685918807983398. Validation loss: 2.705690860748291.\n",
            "Epoch 2703. Training loss: 0.768588125705719. Validation loss: 2.7056899070739746.\n",
            "Epoch 2704. Training loss: 0.7685844302177429. Validation loss: 2.7056899070739746.\n",
            "Epoch 2705. Training loss: 0.7685806751251221. Validation loss: 2.7056894302368164.\n",
            "Epoch 2706. Training loss: 0.7685769200325012. Validation loss: 2.705688953399658.\n",
            "Epoch 2707. Training loss: 0.7685731053352356. Validation loss: 2.705688714981079.\n",
            "Epoch 2708. Training loss: 0.7685694694519043. Validation loss: 2.7056884765625.\n",
            "Epoch 2709. Training loss: 0.7685656547546387. Validation loss: 2.705687999725342.\n",
            "Epoch 2710. Training loss: 0.7685618996620178. Validation loss: 2.7056875228881836.\n",
            "Epoch 2711. Training loss: 0.7685582041740417. Validation loss: 2.7056875228881836.\n",
            "Epoch 2712. Training loss: 0.7685543894767761. Validation loss: 2.7056870460510254.\n",
            "Epoch 2713. Training loss: 0.7685506343841553. Validation loss: 2.705686330795288.\n",
            "Epoch 2714. Training loss: 0.768546998500824. Validation loss: 2.70568585395813.\n",
            "Epoch 2715. Training loss: 0.7685431838035583. Validation loss: 2.70568585395813.\n",
            "Epoch 2716. Training loss: 0.7685393691062927. Validation loss: 2.7056851387023926.\n",
            "Epoch 2717. Training loss: 0.7685357928276062. Validation loss: 2.7056846618652344.\n",
            "Epoch 2718. Training loss: 0.7685319781303406. Validation loss: 2.7056846618652344.\n",
            "Epoch 2719. Training loss: 0.768528163433075. Validation loss: 2.705684185028076.\n",
            "Epoch 2720. Training loss: 0.7685244083404541. Validation loss: 2.705683708190918.\n",
            "Epoch 2721. Training loss: 0.7685205936431885. Validation loss: 2.705683469772339.\n",
            "Epoch 2722. Training loss: 0.7685168385505676. Validation loss: 2.7056827545166016.\n",
            "Epoch 2723. Training loss: 0.768513023853302. Validation loss: 2.7056827545166016.\n",
            "Epoch 2724. Training loss: 0.7685093879699707. Validation loss: 2.7056825160980225.\n",
            "Epoch 2725. Training loss: 0.7685055732727051. Validation loss: 2.705681800842285.\n",
            "Epoch 2726. Training loss: 0.7685018181800842. Validation loss: 2.705681324005127.\n",
            "Epoch 2727. Training loss: 0.7684980034828186. Validation loss: 2.7056808471679688.\n",
            "Epoch 2728. Training loss: 0.7684943675994873. Validation loss: 2.7056808471679688.\n",
            "Epoch 2729. Training loss: 0.7684905529022217. Validation loss: 2.7056803703308105.\n",
            "Epoch 2730. Training loss: 0.7684867978096008. Validation loss: 2.7056798934936523.\n",
            "Epoch 2731. Training loss: 0.7684829831123352. Validation loss: 2.705679416656494.\n",
            "Epoch 2732. Training loss: 0.7684792876243591. Validation loss: 2.705678939819336.\n",
            "Epoch 2733. Training loss: 0.7684755325317383. Validation loss: 2.7056784629821777.\n",
            "Epoch 2734. Training loss: 0.7684717178344727. Validation loss: 2.7056782245635986.\n",
            "Epoch 2735. Training loss: 0.768467903137207. Validation loss: 2.7056779861450195.\n",
            "Epoch 2736. Training loss: 0.7684642672538757. Validation loss: 2.7056775093078613.\n",
            "Epoch 2737. Training loss: 0.7684605121612549. Validation loss: 2.7056775093078613.\n",
            "Epoch 2738. Training loss: 0.7684566378593445. Validation loss: 2.705676555633545.\n",
            "Epoch 2739. Training loss: 0.7684529423713684. Validation loss: 2.705676555633545.\n",
            "Epoch 2740. Training loss: 0.7684491276741028. Validation loss: 2.7056760787963867.\n",
            "Epoch 2741. Training loss: 0.7684454321861267. Validation loss: 2.7056756019592285.\n",
            "Epoch 2742. Training loss: 0.7684416770935059. Validation loss: 2.7056756019592285.\n",
            "Epoch 2743. Training loss: 0.7684378027915955. Validation loss: 2.705674886703491.\n",
            "Epoch 2744. Training loss: 0.7684341073036194. Validation loss: 2.705674648284912.\n",
            "Epoch 2745. Training loss: 0.768430233001709. Validation loss: 2.705674171447754.\n",
            "Epoch 2746. Training loss: 0.7684265971183777. Validation loss: 2.7056736946105957.\n",
            "Epoch 2747. Training loss: 0.7684228420257568. Validation loss: 2.7056732177734375.\n",
            "Epoch 2748. Training loss: 0.768419086933136. Validation loss: 2.7056729793548584.\n",
            "Epoch 2749. Training loss: 0.7684152722358704. Validation loss: 2.7056727409362793.\n",
            "Epoch 2750. Training loss: 0.7684114575386047. Validation loss: 2.705672264099121.\n",
            "Epoch 2751. Training loss: 0.7684076428413391. Validation loss: 2.705671787261963.\n",
            "Epoch 2752. Training loss: 0.7684040069580078. Validation loss: 2.705671787261963.\n",
            "Epoch 2753. Training loss: 0.7684000134468079. Validation loss: 2.7056708335876465.\n",
            "Epoch 2754. Training loss: 0.7683963179588318. Validation loss: 2.7056708335876465.\n",
            "Epoch 2755. Training loss: 0.7683925628662109. Validation loss: 2.705670118331909.\n",
            "Epoch 2756. Training loss: 0.7683888077735901. Validation loss: 2.70566987991333.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2757. Training loss: 0.7683849930763245. Validation loss: 2.705669641494751.\n",
            "Epoch 2758. Training loss: 0.7683812975883484. Validation loss: 2.7056689262390137.\n",
            "Epoch 2759. Training loss: 0.7683773636817932. Validation loss: 2.7056689262390137.\n",
            "Epoch 2760. Training loss: 0.7683737874031067. Validation loss: 2.7056682109832764.\n",
            "Epoch 2761. Training loss: 0.7683699131011963. Validation loss: 2.7056679725646973.\n",
            "Epoch 2762. Training loss: 0.7683660984039307. Validation loss: 2.705667495727539.\n",
            "Epoch 2763. Training loss: 0.7683623433113098. Validation loss: 2.70566725730896.\n",
            "Epoch 2764. Training loss: 0.7683585286140442. Validation loss: 2.7056665420532227.\n",
            "Epoch 2765. Training loss: 0.7683547139167786. Validation loss: 2.7056660652160645.\n",
            "Epoch 2766. Training loss: 0.7683510184288025. Validation loss: 2.7056660652160645.\n",
            "Epoch 2767. Training loss: 0.7683472633361816. Validation loss: 2.7056655883789062.\n",
            "Epoch 2768. Training loss: 0.768343448638916. Validation loss: 2.705665111541748.\n",
            "Epoch 2769. Training loss: 0.7683396339416504. Validation loss: 2.705664873123169.\n",
            "Epoch 2770. Training loss: 0.7683358788490295. Validation loss: 2.7056643962860107.\n",
            "Epoch 2771. Training loss: 0.7683320045471191. Validation loss: 2.7056636810302734.\n",
            "Epoch 2772. Training loss: 0.7683283686637878. Validation loss: 2.7056634426116943.\n",
            "Epoch 2773. Training loss: 0.7683245539665222. Validation loss: 2.7056632041931152.\n",
            "Epoch 2774. Training loss: 0.7683207392692566. Validation loss: 2.705662965774536.\n",
            "Epoch 2775. Training loss: 0.7683169841766357. Validation loss: 2.705662488937378.\n",
            "Epoch 2776. Training loss: 0.7683131694793701. Validation loss: 2.705662250518799.\n",
            "Epoch 2777. Training loss: 0.7683093547821045. Validation loss: 2.7056617736816406.\n",
            "Epoch 2778. Training loss: 0.7683054804801941. Validation loss: 2.7056612968444824.\n",
            "Epoch 2779. Training loss: 0.7683017253875732. Validation loss: 2.705660820007324.\n",
            "Epoch 2780. Training loss: 0.7682979702949524. Validation loss: 2.705660820007324.\n",
            "Epoch 2781. Training loss: 0.7682941555976868. Validation loss: 2.705660104751587.\n",
            "Epoch 2782. Training loss: 0.7682904601097107. Validation loss: 2.7056596279144287.\n",
            "Epoch 2783. Training loss: 0.7682866454124451. Validation loss: 2.7056596279144287.\n",
            "Epoch 2784. Training loss: 0.7682827115058899. Validation loss: 2.7056591510772705.\n",
            "Epoch 2785. Training loss: 0.7682790756225586. Validation loss: 2.7056589126586914.\n",
            "Epoch 2786. Training loss: 0.7682752013206482. Validation loss: 2.705657958984375.\n",
            "Epoch 2787. Training loss: 0.7682714462280273. Validation loss: 2.705657958984375.\n",
            "Epoch 2788. Training loss: 0.7682676315307617. Validation loss: 2.705657482147217.\n",
            "Epoch 2789. Training loss: 0.7682638168334961. Validation loss: 2.705657482147217.\n",
            "Epoch 2790. Training loss: 0.7682600617408752. Validation loss: 2.7056570053100586.\n",
            "Epoch 2791. Training loss: 0.7682562470436096. Validation loss: 2.7056562900543213.\n",
            "Epoch 2792. Training loss: 0.768252432346344. Validation loss: 2.705656051635742.\n",
            "Epoch 2793. Training loss: 0.7682485580444336. Validation loss: 2.705655574798584.\n",
            "Epoch 2794. Training loss: 0.768244743347168. Validation loss: 2.705655097961426.\n",
            "Epoch 2795. Training loss: 0.7682409286499023. Validation loss: 2.7056546211242676.\n",
            "Epoch 2796. Training loss: 0.7682371735572815. Validation loss: 2.7056541442871094.\n",
            "Epoch 2797. Training loss: 0.7682333588600159. Validation loss: 2.7056541442871094.\n",
            "Epoch 2798. Training loss: 0.7682295441627502. Validation loss: 2.705653667449951.\n",
            "Epoch 2799. Training loss: 0.7682257294654846. Validation loss: 2.705653190612793.\n",
            "Epoch 2800. Training loss: 0.768221914768219. Validation loss: 2.705653190612793.\n",
            "Epoch 2801. Training loss: 0.7682181000709534. Validation loss: 2.7056524753570557.\n",
            "Epoch 2802. Training loss: 0.768214225769043. Validation loss: 2.7056519985198975.\n",
            "Epoch 2803. Training loss: 0.7682104110717773. Validation loss: 2.7056517601013184.\n",
            "Epoch 2804. Training loss: 0.7682065963745117. Validation loss: 2.7056515216827393.\n",
            "Epoch 2805. Training loss: 0.7682028412818909. Validation loss: 2.705651044845581.\n",
            "Epoch 2806. Training loss: 0.7681990265846252. Validation loss: 2.7056500911712646.\n",
            "Epoch 2807. Training loss: 0.7681951522827148. Validation loss: 2.7056500911712646.\n",
            "Epoch 2808. Training loss: 0.7681913375854492. Validation loss: 2.7056498527526855.\n",
            "Epoch 2809. Training loss: 0.7681875228881836. Validation loss: 2.7056491374969482.\n",
            "Epoch 2810. Training loss: 0.7681836485862732. Validation loss: 2.7056491374969482.\n",
            "Epoch 2811. Training loss: 0.7681798338890076. Validation loss: 2.70564866065979.\n",
            "Epoch 2812. Training loss: 0.7681760191917419. Validation loss: 2.705648422241211.\n",
            "Epoch 2813. Training loss: 0.7681722044944763. Validation loss: 2.7056479454040527.\n",
            "Epoch 2814. Training loss: 0.7681682705879211. Validation loss: 2.7056474685668945.\n",
            "Epoch 2815. Training loss: 0.7681645750999451. Validation loss: 2.7056467533111572.\n",
            "Epoch 2816. Training loss: 0.7681606411933899. Validation loss: 2.7056469917297363.\n",
            "Epoch 2817. Training loss: 0.7681568264961243. Validation loss: 2.705646276473999.\n",
            "Epoch 2818. Training loss: 0.7681530117988586. Validation loss: 2.705645799636841.\n",
            "Epoch 2819. Training loss: 0.7681491374969482. Validation loss: 2.7056453227996826.\n",
            "Epoch 2820. Training loss: 0.7681453227996826. Validation loss: 2.7056450843811035.\n",
            "Epoch 2821. Training loss: 0.7681414484977722. Validation loss: 2.705644369125366.\n",
            "Epoch 2822. Training loss: 0.7681376338005066. Validation loss: 2.705644130706787.\n",
            "Epoch 2823. Training loss: 0.7681336998939514. Validation loss: 2.705643892288208.\n",
            "Epoch 2824. Training loss: 0.7681298851966858. Validation loss: 2.705643653869629.\n",
            "Epoch 2825. Training loss: 0.7681260108947754. Validation loss: 2.7056431770324707.\n",
            "Epoch 2826. Training loss: 0.7681222558021545. Validation loss: 2.7056427001953125.\n",
            "Epoch 2827. Training loss: 0.7681183815002441. Validation loss: 2.7056427001953125.\n",
            "Epoch 2828. Training loss: 0.7681145668029785. Validation loss: 2.705641984939575.\n",
            "Epoch 2829. Training loss: 0.7681107521057129. Validation loss: 2.705641508102417.\n",
            "Epoch 2830. Training loss: 0.7681069374084473. Validation loss: 2.705641269683838.\n",
            "Epoch 2831. Training loss: 0.7681029438972473. Validation loss: 2.7056407928466797.\n",
            "Epoch 2832. Training loss: 0.7680991291999817. Validation loss: 2.7056405544281006.\n",
            "Epoch 2833. Training loss: 0.7680953145027161. Validation loss: 2.7056398391723633.\n",
            "Epoch 2834. Training loss: 0.7680914402008057. Validation loss: 2.7056398391723633.\n",
            "Epoch 2835. Training loss: 0.76808762550354. Validation loss: 2.705639123916626.\n",
            "Epoch 2836. Training loss: 0.7680837512016296. Validation loss: 2.7056386470794678.\n",
            "Epoch 2837. Training loss: 0.768079936504364. Validation loss: 2.7056386470794678.\n",
            "Epoch 2838. Training loss: 0.7680761218070984. Validation loss: 2.7056381702423096.\n",
            "Epoch 2839. Training loss: 0.7680721879005432. Validation loss: 2.7056374549865723.\n",
            "Epoch 2840. Training loss: 0.7680683135986328. Validation loss: 2.705636978149414.\n",
            "Epoch 2841. Training loss: 0.7680644989013672. Validation loss: 2.705636978149414.\n",
            "Epoch 2842. Training loss: 0.7680606245994568. Validation loss: 2.7056362628936768.\n",
            "Epoch 2843. Training loss: 0.7680566906929016. Validation loss: 2.7056360244750977.\n",
            "Epoch 2844. Training loss: 0.768052875995636. Validation loss: 2.7056357860565186.\n",
            "Epoch 2845. Training loss: 0.7680490612983704. Validation loss: 2.7056355476379395.\n",
            "Epoch 2846. Training loss: 0.76804518699646. Validation loss: 2.7056350708007812.\n",
            "Epoch 2847. Training loss: 0.7680413722991943. Validation loss: 2.705634355545044.\n",
            "Epoch 2848. Training loss: 0.7680374979972839. Validation loss: 2.7056338787078857.\n",
            "Epoch 2849. Training loss: 0.7680335640907288. Validation loss: 2.7056336402893066.\n",
            "Epoch 2850. Training loss: 0.7680296897888184. Validation loss: 2.7056331634521484.\n",
            "Epoch 2851. Training loss: 0.7680258750915527. Validation loss: 2.7056331634521484.\n",
            "Epoch 2852. Training loss: 0.7680221199989319. Validation loss: 2.7056326866149902.\n",
            "Epoch 2853. Training loss: 0.7680180668830872. Validation loss: 2.705632209777832.\n",
            "Epoch 2854. Training loss: 0.7680143713951111. Validation loss: 2.705632209777832.\n",
            "Epoch 2855. Training loss: 0.7680104374885559. Validation loss: 2.7056314945220947.\n",
            "Epoch 2856. Training loss: 0.7680065035820007. Validation loss: 2.7056310176849365.\n",
            "Epoch 2857. Training loss: 0.7680026888847351. Validation loss: 2.7056305408477783.\n",
            "Epoch 2858. Training loss: 0.7679988741874695. Validation loss: 2.70563006401062.\n",
            "Epoch 2859. Training loss: 0.7679950594902039. Validation loss: 2.705629587173462.\n",
            "Epoch 2860. Training loss: 0.7679912447929382. Validation loss: 2.705629348754883.\n",
            "Epoch 2861. Training loss: 0.7679874300956726. Validation loss: 2.7056291103363037.\n",
            "Epoch 2862. Training loss: 0.7679833769798279. Validation loss: 2.7056283950805664.\n",
            "Epoch 2863. Training loss: 0.7679794430732727. Validation loss: 2.7056283950805664.\n",
            "Epoch 2864. Training loss: 0.7679757475852966. Validation loss: 2.705627918243408.\n",
            "Epoch 2865. Training loss: 0.7679718136787415. Validation loss: 2.705627679824829.\n",
            "Epoch 2866. Training loss: 0.7679678797721863. Validation loss: 2.705626964569092.\n",
            "Epoch 2867. Training loss: 0.7679640650749207. Validation loss: 2.705626964569092.\n",
            "Epoch 2868. Training loss: 0.7679601311683655. Validation loss: 2.7056264877319336.\n",
            "Epoch 2869. Training loss: 0.7679562568664551. Validation loss: 2.7056260108947754.\n",
            "Epoch 2870. Training loss: 0.7679525017738342. Validation loss: 2.705625534057617.\n",
            "Epoch 2871. Training loss: 0.7679484486579895. Validation loss: 2.70562481880188.\n",
            "Epoch 2872. Training loss: 0.7679446339607239. Validation loss: 2.70562481880188.\n",
            "Epoch 2873. Training loss: 0.7679407596588135. Validation loss: 2.7056243419647217.\n",
            "Epoch 2874. Training loss: 0.7679368853569031. Validation loss: 2.7056238651275635.\n",
            "Epoch 2875. Training loss: 0.7679329514503479. Validation loss: 2.7056236267089844.\n",
            "Epoch 2876. Training loss: 0.7679290771484375. Validation loss: 2.705623149871826.\n",
            "Epoch 2877. Training loss: 0.7679252624511719. Validation loss: 2.705622911453247.\n",
            "Epoch 2878. Training loss: 0.7679213881492615. Validation loss: 2.705622434616089.\n",
            "Epoch 2879. Training loss: 0.7679175734519958. Validation loss: 2.7056221961975098.\n",
            "Epoch 2880. Training loss: 0.7679136395454407. Validation loss: 2.7056217193603516.\n",
            "Epoch 2881. Training loss: 0.7679097652435303. Validation loss: 2.7056212425231934.\n",
            "Epoch 2882. Training loss: 0.7679058909416199. Validation loss: 2.7056210041046143.\n",
            "Epoch 2883. Training loss: 0.7679019570350647. Validation loss: 2.705620050430298.\n",
            "Epoch 2884. Training loss: 0.7678981423377991. Validation loss: 2.705620050430298.\n",
            "Epoch 2885. Training loss: 0.7678942084312439. Validation loss: 2.7056195735931396.\n",
            "Epoch 2886. Training loss: 0.7678903937339783. Validation loss: 2.7056193351745605.\n",
            "Epoch 2887. Training loss: 0.7678864002227783. Validation loss: 2.7056190967559814.\n",
            "Epoch 2888. Training loss: 0.7678825855255127. Validation loss: 2.7056186199188232.\n",
            "Epoch 2889. Training loss: 0.7678785920143127. Validation loss: 2.705618381500244.\n",
            "Epoch 2890. Training loss: 0.7678747773170471. Validation loss: 2.705617666244507.\n",
            "Epoch 2891. Training loss: 0.7678709030151367. Validation loss: 2.7056174278259277.\n",
            "Epoch 2892. Training loss: 0.7678670883178711. Validation loss: 2.7056171894073486.\n",
            "Epoch 2893. Training loss: 0.7678630948066711. Validation loss: 2.7056164741516113.\n",
            "Epoch 2894. Training loss: 0.7678592801094055. Validation loss: 2.7056164741516113.\n",
            "Epoch 2895. Training loss: 0.7678553462028503. Validation loss: 2.705615520477295.\n",
            "Epoch 2896. Training loss: 0.7678515315055847. Validation loss: 2.705615520477295.\n",
            "Epoch 2897. Training loss: 0.7678475975990295. Validation loss: 2.7056150436401367.\n",
            "Epoch 2898. Training loss: 0.7678437232971191. Validation loss: 2.7056145668029785.\n",
            "Epoch 2899. Training loss: 0.7678397297859192. Validation loss: 2.7056140899658203.\n",
            "Epoch 2900. Training loss: 0.7678358554840088. Validation loss: 2.705613851547241.\n",
            "Epoch 2901. Training loss: 0.7678319811820984. Validation loss: 2.705613613128662.\n",
            "Epoch 2902. Training loss: 0.7678281664848328. Validation loss: 2.705613136291504.\n",
            "Epoch 2903. Training loss: 0.7678241729736328. Validation loss: 2.7056126594543457.\n",
            "Epoch 2904. Training loss: 0.7678202986717224. Validation loss: 2.7056126594543457.\n",
            "Epoch 2905. Training loss: 0.7678163647651672. Validation loss: 2.7056119441986084.\n",
            "Epoch 2906. Training loss: 0.7678125500679016. Validation loss: 2.7056117057800293.\n",
            "Epoch 2907. Training loss: 0.7678086161613464. Validation loss: 2.705611228942871.\n",
            "Epoch 2908. Training loss: 0.7678048014640808. Validation loss: 2.705610752105713.\n",
            "Epoch 2909. Training loss: 0.7678008675575256. Validation loss: 2.7056100368499756.\n",
            "Epoch 2910. Training loss: 0.7677969932556152. Validation loss: 2.7056097984313965.\n",
            "Epoch 2911. Training loss: 0.7677929997444153. Validation loss: 2.7056095600128174.\n",
            "Epoch 2912. Training loss: 0.7677890658378601. Validation loss: 2.70560884475708.\n",
            "Epoch 2913. Training loss: 0.7677851319313049. Validation loss: 2.705608367919922.\n",
            "Epoch 2914. Training loss: 0.7677813172340393. Validation loss: 2.705608367919922.\n",
            "Epoch 2915. Training loss: 0.7677774429321289. Validation loss: 2.7056078910827637.\n",
            "Epoch 2916. Training loss: 0.7677735686302185. Validation loss: 2.7056076526641846.\n",
            "Epoch 2917. Training loss: 0.7677695751190186. Validation loss: 2.7056071758270264.\n",
            "Epoch 2918. Training loss: 0.7677658200263977. Validation loss: 2.705606460571289.\n",
            "Epoch 2919. Training loss: 0.767761766910553. Validation loss: 2.705605983734131.\n",
            "Epoch 2920. Training loss: 0.7677578926086426. Validation loss: 2.70560622215271.\n",
            "Epoch 2921. Training loss: 0.7677538990974426. Validation loss: 2.7056055068969727.\n",
            "Epoch 2922. Training loss: 0.7677500247955322. Validation loss: 2.7056047916412354.\n",
            "Epoch 2923. Training loss: 0.7677461504936218. Validation loss: 2.7056047916412354.\n",
            "Epoch 2924. Training loss: 0.7677422165870667. Validation loss: 2.705604314804077.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2925. Training loss: 0.7677383422851562. Validation loss: 2.705603837966919.\n",
            "Epoch 2926. Training loss: 0.7677344679832458. Validation loss: 2.705603837966919.\n",
            "Epoch 2927. Training loss: 0.7677305340766907. Validation loss: 2.7056033611297607.\n",
            "Epoch 2928. Training loss: 0.767726480960846. Validation loss: 2.7056026458740234.\n",
            "Epoch 2929. Training loss: 0.7677226662635803. Validation loss: 2.7056021690368652.\n",
            "Epoch 2930. Training loss: 0.7677187919616699. Validation loss: 2.705601930618286.\n",
            "Epoch 2931. Training loss: 0.7677149176597595. Validation loss: 2.705601692199707.\n",
            "Epoch 2932. Training loss: 0.7677109241485596. Validation loss: 2.705601453781128.\n",
            "Epoch 2933. Training loss: 0.7677070498466492. Validation loss: 2.7056009769439697.\n",
            "Epoch 2934. Training loss: 0.767703115940094. Validation loss: 2.7056007385253906.\n",
            "Epoch 2935. Training loss: 0.7676992416381836. Validation loss: 2.7056000232696533.\n",
            "Epoch 2936. Training loss: 0.7676952481269836. Validation loss: 2.705599546432495.\n",
            "Epoch 2937. Training loss: 0.7676913738250732. Validation loss: 2.705599308013916.\n",
            "Epoch 2938. Training loss: 0.7676874995231628. Validation loss: 2.705598831176758.\n",
            "Epoch 2939. Training loss: 0.7676835060119629. Validation loss: 2.7055983543395996.\n",
            "Epoch 2940. Training loss: 0.7676796317100525. Validation loss: 2.7055981159210205.\n",
            "Epoch 2941. Training loss: 0.7676756381988525. Validation loss: 2.7055976390838623.\n",
            "Epoch 2942. Training loss: 0.7676717638969421. Validation loss: 2.705597400665283.\n",
            "Epoch 2943. Training loss: 0.7676677703857422. Validation loss: 2.705596685409546.\n",
            "Epoch 2944. Training loss: 0.7676639556884766. Validation loss: 2.7055962085723877.\n",
            "Epoch 2945. Training loss: 0.7676600813865662. Validation loss: 2.7055959701538086.\n",
            "Epoch 2946. Training loss: 0.7676560878753662. Validation loss: 2.7055959701538086.\n",
            "Epoch 2947. Training loss: 0.7676520943641663. Validation loss: 2.7055952548980713.\n",
            "Epoch 2948. Training loss: 0.7676482200622559. Validation loss: 2.705594778060913.\n",
            "Epoch 2949. Training loss: 0.7676443457603455. Validation loss: 2.705594301223755.\n",
            "Epoch 2950. Training loss: 0.7676402926445007. Validation loss: 2.705594062805176.\n",
            "Epoch 2951. Training loss: 0.7676363587379456. Validation loss: 2.7055938243865967.\n",
            "Epoch 2952. Training loss: 0.7676324844360352. Validation loss: 2.7055933475494385.\n",
            "Epoch 2953. Training loss: 0.7676286101341248. Validation loss: 2.7055928707122803.\n",
            "Epoch 2954. Training loss: 0.7676246762275696. Validation loss: 2.705592632293701.\n",
            "Epoch 2955. Training loss: 0.7676207423210144. Validation loss: 2.705592155456543.\n",
            "Epoch 2956. Training loss: 0.7676167488098145. Validation loss: 2.7055916786193848.\n",
            "Epoch 2957. Training loss: 0.7676129341125488. Validation loss: 2.7055912017822266.\n",
            "Epoch 2958. Training loss: 0.7676088809967041. Validation loss: 2.7055907249450684.\n",
            "Epoch 2959. Training loss: 0.7676050662994385. Validation loss: 2.7055907249450684.\n",
            "Epoch 2960. Training loss: 0.7676010131835938. Validation loss: 2.70559024810791.\n",
            "Epoch 2961. Training loss: 0.7675971388816833. Validation loss: 2.705589771270752.\n",
            "Epoch 2962. Training loss: 0.7675932049751282. Validation loss: 2.7055892944335938.\n",
            "Epoch 2963. Training loss: 0.767589271068573. Validation loss: 2.7055888175964355.\n",
            "Epoch 2964. Training loss: 0.7675853371620178. Validation loss: 2.7055883407592773.\n",
            "Epoch 2965. Training loss: 0.7675812840461731. Validation loss: 2.7055881023406982.\n",
            "Epoch 2966. Training loss: 0.7675773501396179. Validation loss: 2.705587863922119.\n",
            "Epoch 2967. Training loss: 0.7675734162330627. Validation loss: 2.705587387084961.\n",
            "Epoch 2968. Training loss: 0.7675694823265076. Validation loss: 2.7055869102478027.\n",
            "Epoch 2969. Training loss: 0.7675655484199524. Validation loss: 2.7055864334106445.\n",
            "Epoch 2970. Training loss: 0.767561674118042. Validation loss: 2.7055859565734863.\n",
            "Epoch 2971. Training loss: 0.767557680606842. Validation loss: 2.705585479736328.\n",
            "Epoch 2972. Training loss: 0.7675537467002869. Validation loss: 2.705585241317749.\n",
            "Epoch 2973. Training loss: 0.7675498127937317. Validation loss: 2.70558500289917.\n",
            "Epoch 2974. Training loss: 0.7675458788871765. Validation loss: 2.7055845260620117.\n",
            "Epoch 2975. Training loss: 0.7675419449806213. Validation loss: 2.7055840492248535.\n",
            "Epoch 2976. Training loss: 0.7675380110740662. Validation loss: 2.7055835723876953.\n",
            "Epoch 2977. Training loss: 0.7675340175628662. Validation loss: 2.705583095550537.\n",
            "Epoch 2978. Training loss: 0.7675300240516663. Validation loss: 2.705582857131958.\n",
            "Epoch 2979. Training loss: 0.7675261497497559. Validation loss: 2.705582618713379.\n",
            "Epoch 2980. Training loss: 0.7675221562385559. Validation loss: 2.7055821418762207.\n",
            "Epoch 2981. Training loss: 0.7675182223320007. Validation loss: 2.7055816650390625.\n",
            "Epoch 2982. Training loss: 0.7675142288208008. Validation loss: 2.7055816650390625.\n",
            "Epoch 2983. Training loss: 0.7675103545188904. Validation loss: 2.705580949783325.\n",
            "Epoch 2984. Training loss: 0.7675063014030457. Validation loss: 2.705580711364746.\n",
            "Epoch 2985. Training loss: 0.7675023674964905. Validation loss: 2.705579996109009.\n",
            "Epoch 2986. Training loss: 0.7674984931945801. Validation loss: 2.7055797576904297.\n",
            "Epoch 2987. Training loss: 0.7674944996833801. Validation loss: 2.7055792808532715.\n",
            "Epoch 2988. Training loss: 0.767490565776825. Validation loss: 2.7055788040161133.\n",
            "Epoch 2989. Training loss: 0.767486572265625. Validation loss: 2.705578327178955.\n",
            "Epoch 2990. Training loss: 0.767482578754425. Validation loss: 2.705578088760376.\n",
            "Epoch 2991. Training loss: 0.7674786448478699. Validation loss: 2.7055773735046387.\n",
            "Epoch 2992. Training loss: 0.7674747109413147. Validation loss: 2.7055771350860596.\n",
            "Epoch 2993. Training loss: 0.7674707770347595. Validation loss: 2.7055771350860596.\n",
            "Epoch 2994. Training loss: 0.7674667835235596. Validation loss: 2.7055764198303223.\n",
            "Epoch 2995. Training loss: 0.7674627900123596. Validation loss: 2.705575942993164.\n",
            "Epoch 2996. Training loss: 0.7674589157104492. Validation loss: 2.705575466156006.\n",
            "Epoch 2997. Training loss: 0.7674550414085388. Validation loss: 2.7055752277374268.\n",
            "Epoch 2998. Training loss: 0.7674509882926941. Validation loss: 2.7055749893188477.\n",
            "Epoch 2999. Training loss: 0.7674470543861389. Validation loss: 2.7055745124816895.\n",
            "Epoch 3000. Training loss: 0.7674431204795837. Validation loss: 2.7055742740631104.\n",
            "Epoch 3001. Training loss: 0.7674391269683838. Validation loss: 2.7055740356445312.\n",
            "Epoch 3002. Training loss: 0.7674350738525391. Validation loss: 2.705573081970215.\n",
            "Epoch 3003. Training loss: 0.7674311995506287. Validation loss: 2.7055728435516357.\n",
            "Epoch 3004. Training loss: 0.7674272060394287. Validation loss: 2.7055726051330566.\n",
            "Epoch 3005. Training loss: 0.7674232125282288. Validation loss: 2.7055721282958984.\n",
            "Epoch 3006. Training loss: 0.7674192786216736. Validation loss: 2.7055718898773193.\n",
            "Epoch 3007. Training loss: 0.7674152255058289. Validation loss: 2.705571174621582.\n",
            "Epoch 3008. Training loss: 0.7674112915992737. Validation loss: 2.705570697784424.\n",
            "Epoch 3009. Training loss: 0.7674073576927185. Validation loss: 2.7055702209472656.\n",
            "Epoch 3010. Training loss: 0.7674035429954529. Validation loss: 2.7055702209472656.\n",
            "Epoch 3011. Training loss: 0.7673996090888977. Validation loss: 2.7055697441101074.\n",
            "Epoch 3012. Training loss: 0.7673957943916321. Validation loss: 2.7055695056915283.\n",
            "Epoch 3013. Training loss: 0.7673919200897217. Validation loss: 2.70556902885437.\n",
            "Epoch 3014. Training loss: 0.767388105392456. Validation loss: 2.705568313598633.\n",
            "Epoch 3015. Training loss: 0.7673844695091248. Validation loss: 2.705568313598633.\n",
            "Epoch 3016. Training loss: 0.7673805356025696. Validation loss: 2.7055675983428955.\n",
            "Epoch 3017. Training loss: 0.7673768401145935. Validation loss: 2.7055673599243164.\n",
            "Epoch 3018. Training loss: 0.7673729062080383. Validation loss: 2.7055671215057373.\n",
            "Epoch 3019. Training loss: 0.7673692107200623. Validation loss: 2.705566644668579.\n",
            "Epoch 3020. Training loss: 0.7673653960227966. Validation loss: 2.705566167831421.\n",
            "Epoch 3021. Training loss: 0.767361581325531. Validation loss: 2.7055654525756836.\n",
            "Epoch 3022. Training loss: 0.7673578262329102. Validation loss: 2.7055649757385254.\n",
            "Epoch 3023. Training loss: 0.7673540115356445. Validation loss: 2.7055647373199463.\n",
            "Epoch 3024. Training loss: 0.7673502564430237. Validation loss: 2.705564498901367.\n",
            "Epoch 3025. Training loss: 0.7673464417457581. Validation loss: 2.705564022064209.\n",
            "Epoch 3026. Training loss: 0.767342746257782. Validation loss: 2.705563545227051.\n",
            "Epoch 3027. Training loss: 0.7673389315605164. Validation loss: 2.7055633068084717.\n",
            "Epoch 3028. Training loss: 0.7673351764678955. Validation loss: 2.7055625915527344.\n",
            "Epoch 3029. Training loss: 0.7673313617706299. Validation loss: 2.7055623531341553.\n",
            "Epoch 3030. Training loss: 0.767327606678009. Validation loss: 2.705561876296997.\n",
            "Epoch 3031. Training loss: 0.7673237919807434. Validation loss: 2.705561399459839.\n",
            "Epoch 3032. Training loss: 0.7673200964927673. Validation loss: 2.7055611610412598.\n",
            "Epoch 3033. Training loss: 0.7673162817955017. Validation loss: 2.7055606842041016.\n",
            "Epoch 3034. Training loss: 0.7673125267028809. Validation loss: 2.7055604457855225.\n",
            "Epoch 3035. Training loss: 0.7673088908195496. Validation loss: 2.705559492111206.\n",
            "Epoch 3036. Training loss: 0.7673051357269287. Validation loss: 2.705559253692627.\n",
            "Epoch 3037. Training loss: 0.7673015594482422. Validation loss: 2.7055587768554688.\n",
            "Epoch 3038. Training loss: 0.7672978043556213. Validation loss: 2.7055585384368896.\n",
            "Epoch 3039. Training loss: 0.7672942280769348. Validation loss: 2.7055583000183105.\n",
            "Epoch 3040. Training loss: 0.7672905921936035. Validation loss: 2.7055578231811523.\n",
            "Epoch 3041. Training loss: 0.7672869563102722. Validation loss: 2.705557346343994.\n",
            "Epoch 3042. Training loss: 0.7672834396362305. Validation loss: 2.705556869506836.\n",
            "Epoch 3043. Training loss: 0.7672798037528992. Validation loss: 2.705556631088257.\n",
            "Epoch 3044. Training loss: 0.7672761082649231. Validation loss: 2.7055561542510986.\n",
            "Epoch 3045. Training loss: 0.7672724723815918. Validation loss: 2.7055556774139404.\n",
            "Epoch 3046. Training loss: 0.7672688961029053. Validation loss: 2.7055552005767822.\n",
            "Epoch 3047. Training loss: 0.767265260219574. Validation loss: 2.705554485321045.\n",
            "Epoch 3048. Training loss: 0.7672617435455322. Validation loss: 2.705554246902466.\n",
            "Epoch 3049. Training loss: 0.7672581672668457. Validation loss: 2.7055535316467285.\n",
            "Epoch 3050. Training loss: 0.7672544121742249. Validation loss: 2.7055535316467285.\n",
            "Epoch 3051. Training loss: 0.7672510147094727. Validation loss: 2.7055530548095703.\n",
            "Epoch 3052. Training loss: 0.7672473788261414. Validation loss: 2.705552816390991.\n",
            "Epoch 3053. Training loss: 0.7672436833381653. Validation loss: 2.705552101135254.\n",
            "Epoch 3054. Training loss: 0.7672401070594788. Validation loss: 2.705551862716675.\n",
            "Epoch 3055. Training loss: 0.7672364711761475. Validation loss: 2.7055513858795166.\n",
            "Epoch 3056. Training loss: 0.7672328948974609. Validation loss: 2.7055506706237793.\n",
            "Epoch 3057. Training loss: 0.7672293782234192. Validation loss: 2.7055504322052.\n",
            "Epoch 3058. Training loss: 0.7672258019447327. Validation loss: 2.705549716949463.\n",
            "Epoch 3059. Training loss: 0.7672221660614014. Validation loss: 2.705549478530884.\n",
            "Epoch 3060. Training loss: 0.7672185897827148. Validation loss: 2.7055490016937256.\n",
            "Epoch 3061. Training loss: 0.7672149538993835. Validation loss: 2.7055487632751465.\n",
            "Epoch 3062. Training loss: 0.7672112584114075. Validation loss: 2.705548048019409.\n",
            "Epoch 3063. Training loss: 0.7672078013420105. Validation loss: 2.70554780960083.\n",
            "Epoch 3064. Training loss: 0.7672041058540344. Validation loss: 2.705547332763672.\n",
            "Epoch 3065. Training loss: 0.7672005295753479. Validation loss: 2.7055466175079346.\n",
            "Epoch 3066. Training loss: 0.7671969532966614. Validation loss: 2.7055461406707764.\n",
            "Epoch 3067. Training loss: 0.7671933770179749. Validation loss: 2.7055461406707764.\n",
            "Epoch 3068. Training loss: 0.7671898007392883. Validation loss: 2.705545425415039.\n",
            "Epoch 3069. Training loss: 0.767186164855957. Validation loss: 2.70554518699646.\n",
            "Epoch 3070. Training loss: 0.7671825289726257. Validation loss: 2.705544948577881.\n",
            "Epoch 3071. Training loss: 0.767179012298584. Validation loss: 2.7055444717407227.\n",
            "Epoch 3072. Training loss: 0.7671754360198975. Validation loss: 2.7055437564849854.\n",
            "Epoch 3073. Training loss: 0.7671718001365662. Validation loss: 2.7055435180664062.\n",
            "Epoch 3074. Training loss: 0.7671682238578796. Validation loss: 2.705543041229248.\n",
            "Epoch 3075. Training loss: 0.7671645283699036. Validation loss: 2.70554256439209.\n",
            "Epoch 3076. Training loss: 0.767160952091217. Validation loss: 2.7055418491363525.\n",
            "Epoch 3077. Training loss: 0.7671573758125305. Validation loss: 2.7055413722991943.\n",
            "Epoch 3078. Training loss: 0.7671537399291992. Validation loss: 2.705540895462036.\n",
            "Epoch 3079. Training loss: 0.7671501636505127. Validation loss: 2.705540895462036.\n",
            "Epoch 3080. Training loss: 0.7671465873718262. Validation loss: 2.705540418624878.\n",
            "Epoch 3081. Training loss: 0.7671429514884949. Validation loss: 2.7055399417877197.\n",
            "Epoch 3082. Training loss: 0.7671394348144531. Validation loss: 2.7055394649505615.\n",
            "Epoch 3083. Training loss: 0.7671357989311218. Validation loss: 2.7055392265319824.\n",
            "Epoch 3084. Training loss: 0.7671321034431458. Validation loss: 2.705538511276245.\n",
            "Epoch 3085. Training loss: 0.7671284675598145. Validation loss: 2.705538034439087.\n",
            "Epoch 3086. Training loss: 0.7671249508857727. Validation loss: 2.7055375576019287.\n",
            "Epoch 3087. Training loss: 0.7671214938163757. Validation loss: 2.7055373191833496.\n",
            "Epoch 3088. Training loss: 0.7671177387237549. Validation loss: 2.7055368423461914.\n",
            "Epoch 3089. Training loss: 0.7671141624450684. Validation loss: 2.705536365509033.\n",
            "Epoch 3090. Training loss: 0.7671105265617371. Validation loss: 2.705536127090454.\n",
            "Epoch 3091. Training loss: 0.7671069502830505. Validation loss: 2.705535411834717.\n",
            "Epoch 3092. Training loss: 0.767103374004364. Validation loss: 2.7055349349975586.\n",
            "Epoch 3093. Training loss: 0.7670996785163879. Validation loss: 2.7055346965789795.\n",
            "Epoch 3094. Training loss: 0.7670961022377014. Validation loss: 2.7055344581604004.\n",
            "Epoch 3095. Training loss: 0.7670925259590149. Validation loss: 2.705533504486084.\n",
            "Epoch 3096. Training loss: 0.7670887112617493. Validation loss: 2.705533027648926.\n",
            "Epoch 3097. Training loss: 0.7670853137969971. Validation loss: 2.705533027648926.\n",
            "Epoch 3098. Training loss: 0.7670816779136658. Validation loss: 2.7055325508117676.\n",
            "Epoch 3099. Training loss: 0.7670779824256897. Validation loss: 2.7055320739746094.\n",
            "Epoch 3100. Training loss: 0.7670743465423584. Validation loss: 2.7055318355560303.\n",
            "Epoch 3101. Training loss: 0.7670707702636719. Validation loss: 2.705531358718872.\n",
            "Epoch 3102. Training loss: 0.7670671343803406. Validation loss: 2.7055306434631348.\n",
            "Epoch 3103. Training loss: 0.7670636177062988. Validation loss: 2.7055304050445557.\n",
            "Epoch 3104. Training loss: 0.767059862613678. Validation loss: 2.7055299282073975.\n",
            "Epoch 3105. Training loss: 0.7670562863349915. Validation loss: 2.7055294513702393.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3106. Training loss: 0.7670526504516602. Validation loss: 2.705528736114502.\n",
            "Epoch 3107. Training loss: 0.7670490741729736. Validation loss: 2.705528497695923.\n",
            "Epoch 3108. Training loss: 0.7670454978942871. Validation loss: 2.7055280208587646.\n",
            "Epoch 3109. Training loss: 0.7670416831970215. Validation loss: 2.7055275440216064.\n",
            "Epoch 3110. Training loss: 0.767038106918335. Validation loss: 2.7055273056030273.\n",
            "Epoch 3111. Training loss: 0.7670345306396484. Validation loss: 2.705526828765869.\n",
            "Epoch 3112. Training loss: 0.7670309543609619. Validation loss: 2.705526351928711.\n",
            "Epoch 3113. Training loss: 0.7670273184776306. Validation loss: 2.7055256366729736.\n",
            "Epoch 3114. Training loss: 0.7670236229896545. Validation loss: 2.7055253982543945.\n",
            "Epoch 3115. Training loss: 0.767020046710968. Validation loss: 2.7055249214172363.\n",
            "Epoch 3116. Training loss: 0.7670164704322815. Validation loss: 2.7055246829986572.\n",
            "Epoch 3117. Training loss: 0.7670127749443054. Validation loss: 2.705524206161499.\n",
            "Epoch 3118. Training loss: 0.7670091986656189. Validation loss: 2.70552396774292.\n",
            "Epoch 3119. Training loss: 0.7670055031776428. Validation loss: 2.7055234909057617.\n",
            "Epoch 3120. Training loss: 0.7670019268989563. Validation loss: 2.7055227756500244.\n",
            "Epoch 3121. Training loss: 0.7669982314109802. Validation loss: 2.705522298812866.\n",
            "Epoch 3122. Training loss: 0.7669946551322937. Validation loss: 2.705522060394287.\n",
            "Epoch 3123. Training loss: 0.7669910788536072. Validation loss: 2.705521583557129.\n",
            "Epoch 3124. Training loss: 0.7669873833656311. Validation loss: 2.7055211067199707.\n",
            "Epoch 3125. Training loss: 0.7669838070869446. Validation loss: 2.7055208683013916.\n",
            "Epoch 3126. Training loss: 0.7669801115989685. Validation loss: 2.705519914627075.\n",
            "Epoch 3127. Training loss: 0.766976535320282. Validation loss: 2.705519676208496.\n",
            "Epoch 3128. Training loss: 0.7669728398323059. Validation loss: 2.705519199371338.\n",
            "Epoch 3129. Training loss: 0.7669692039489746. Validation loss: 2.705518960952759.\n",
            "Epoch 3130. Training loss: 0.7669656276702881. Validation loss: 2.7055187225341797.\n",
            "Epoch 3131. Training loss: 0.7669619917869568. Validation loss: 2.7055180072784424.\n",
            "Epoch 3132. Training loss: 0.7669584155082703. Validation loss: 2.705517530441284.\n",
            "Epoch 3133. Training loss: 0.7669547200202942. Validation loss: 2.705517292022705.\n",
            "Epoch 3134. Training loss: 0.7669510245323181. Validation loss: 2.705516815185547.\n",
            "Epoch 3135. Training loss: 0.7669474482536316. Validation loss: 2.7055165767669678.\n",
            "Epoch 3136. Training loss: 0.7669438719749451. Validation loss: 2.7055158615112305.\n",
            "Epoch 3137. Training loss: 0.7669401168823242. Validation loss: 2.705515146255493.\n",
            "Epoch 3138. Training loss: 0.7669365406036377. Validation loss: 2.705514907836914.\n",
            "Epoch 3139. Training loss: 0.7669329047203064. Validation loss: 2.705514430999756.\n",
            "Epoch 3140. Training loss: 0.7669292092323303. Validation loss: 2.7055139541625977.\n",
            "Epoch 3141. Training loss: 0.7669256329536438. Validation loss: 2.7055137157440186.\n",
            "Epoch 3142. Training loss: 0.7669219970703125. Validation loss: 2.7055132389068604.\n",
            "Epoch 3143. Training loss: 0.7669183611869812. Validation loss: 2.705512762069702.\n",
            "Epoch 3144. Training loss: 0.7669146656990051. Validation loss: 2.705512046813965.\n",
            "Epoch 3145. Training loss: 0.7669110894203186. Validation loss: 2.705512046813965.\n",
            "Epoch 3146. Training loss: 0.7669073939323425. Validation loss: 2.7055118083953857.\n",
            "Epoch 3147. Training loss: 0.7669036984443665. Validation loss: 2.7055110931396484.\n",
            "Epoch 3148. Training loss: 0.7669001221656799. Validation loss: 2.7055110931396484.\n",
            "Epoch 3149. Training loss: 0.7668963074684143. Validation loss: 2.705510139465332.\n",
            "Epoch 3150. Training loss: 0.7668927311897278. Validation loss: 2.7055094242095947.\n",
            "Epoch 3151. Training loss: 0.7668890357017517. Validation loss: 2.7055091857910156.\n",
            "Epoch 3152. Training loss: 0.7668853402137756. Validation loss: 2.7055089473724365.\n",
            "Epoch 3153. Training loss: 0.7668817639350891. Validation loss: 2.7055084705352783.\n",
            "Epoch 3154. Training loss: 0.7668781876564026. Validation loss: 2.70550799369812.\n",
            "Epoch 3155. Training loss: 0.7668744921684265. Validation loss: 2.705507278442383.\n",
            "Epoch 3156. Training loss: 0.7668707966804504. Validation loss: 2.7055068016052246.\n",
            "Epoch 3157. Training loss: 0.7668672204017639. Validation loss: 2.7055068016052246.\n",
            "Epoch 3158. Training loss: 0.7668635845184326. Validation loss: 2.7055063247680664.\n",
            "Epoch 3159. Training loss: 0.7668598294258118. Validation loss: 2.705505847930908.\n",
            "Epoch 3160. Training loss: 0.7668562531471252. Validation loss: 2.70550537109375.\n",
            "Epoch 3161. Training loss: 0.7668525576591492. Validation loss: 2.705504894256592.\n",
            "Epoch 3162. Training loss: 0.7668488621711731. Validation loss: 2.7055041790008545.\n",
            "Epoch 3163. Training loss: 0.7668452262878418. Validation loss: 2.7055039405822754.\n",
            "Epoch 3164. Training loss: 0.7668415904045105. Validation loss: 2.7055039405822754.\n",
            "Epoch 3165. Training loss: 0.766838014125824. Validation loss: 2.705503225326538.\n",
            "Epoch 3166. Training loss: 0.7668342590332031. Validation loss: 2.705502510070801.\n",
            "Epoch 3167. Training loss: 0.7668306827545166. Validation loss: 2.7055022716522217.\n",
            "Epoch 3168. Training loss: 0.7668269276618958. Validation loss: 2.7055020332336426.\n",
            "Epoch 3169. Training loss: 0.7668232917785645. Validation loss: 2.7055015563964844.\n",
            "Epoch 3170. Training loss: 0.7668196558952332. Validation loss: 2.705501079559326.\n",
            "Epoch 3171. Training loss: 0.7668159604072571. Validation loss: 2.705500364303589.\n",
            "Epoch 3172. Training loss: 0.7668123245239258. Validation loss: 2.7055001258850098.\n",
            "Epoch 3173. Training loss: 0.7668086886405945. Validation loss: 2.7054996490478516.\n",
            "Epoch 3174. Training loss: 0.7668049931526184. Validation loss: 2.7054991722106934.\n",
            "Epoch 3175. Training loss: 0.7668012976646423. Validation loss: 2.705498695373535.\n",
            "Epoch 3176. Training loss: 0.7667976021766663. Validation loss: 2.705498218536377.\n",
            "Epoch 3177. Training loss: 0.766793966293335. Validation loss: 2.705497980117798.\n",
            "Epoch 3178. Training loss: 0.7667903304100037. Validation loss: 2.7054972648620605.\n",
            "Epoch 3179. Training loss: 0.7667866349220276. Validation loss: 2.7054967880249023.\n",
            "Epoch 3180. Training loss: 0.7667829990386963. Validation loss: 2.7054967880249023.\n",
            "Epoch 3181. Training loss: 0.766779363155365. Validation loss: 2.705496311187744.\n",
            "Epoch 3182. Training loss: 0.7667756080627441. Validation loss: 2.705495834350586.\n",
            "Epoch 3183. Training loss: 0.7667720317840576. Validation loss: 2.7054953575134277.\n",
            "Epoch 3184. Training loss: 0.7667682766914368. Validation loss: 2.7054948806762695.\n",
            "Epoch 3185. Training loss: 0.7667645812034607. Validation loss: 2.7054944038391113.\n",
            "Epoch 3186. Training loss: 0.7667610049247742. Validation loss: 2.705493927001953.\n",
            "Epoch 3187. Training loss: 0.7667573094367981. Validation loss: 2.705493688583374.\n",
            "Epoch 3188. Training loss: 0.7667536735534668. Validation loss: 2.7054929733276367.\n",
            "Epoch 3189. Training loss: 0.7667500376701355. Validation loss: 2.7054927349090576.\n",
            "Epoch 3190. Training loss: 0.7667462229728699. Validation loss: 2.7054920196533203.\n",
            "Epoch 3191. Training loss: 0.7667426466941833. Validation loss: 2.705491542816162.\n",
            "Epoch 3192. Training loss: 0.7667388916015625. Validation loss: 2.705491065979004.\n",
            "Epoch 3193. Training loss: 0.7667352557182312. Validation loss: 2.705491065979004.\n",
            "Epoch 3194. Training loss: 0.7667315602302551. Validation loss: 2.7054905891418457.\n",
            "Epoch 3195. Training loss: 0.766727864742279. Validation loss: 2.7054901123046875.\n",
            "Epoch 3196. Training loss: 0.766724169254303. Validation loss: 2.7054896354675293.\n",
            "Epoch 3197. Training loss: 0.7667205333709717. Validation loss: 2.705489158630371.\n",
            "Epoch 3198. Training loss: 0.7667168974876404. Validation loss: 2.705488681793213.\n",
            "Epoch 3199. Training loss: 0.7667131423950195. Validation loss: 2.705488681793213.\n",
            "Epoch 3200. Training loss: 0.7667093873023987. Validation loss: 2.7054877281188965.\n",
            "Epoch 3201. Training loss: 0.7667058110237122. Validation loss: 2.7054874897003174.\n",
            "Epoch 3202. Training loss: 0.7667021155357361. Validation loss: 2.705487012863159.\n",
            "Epoch 3203. Training loss: 0.76669842004776. Validation loss: 2.705486297607422.\n",
            "Epoch 3204. Training loss: 0.7666947245597839. Validation loss: 2.7054858207702637.\n",
            "Epoch 3205. Training loss: 0.7666911482810974. Validation loss: 2.7054855823516846.\n",
            "Epoch 3206. Training loss: 0.7666873931884766. Validation loss: 2.7054848670959473.\n",
            "Epoch 3207. Training loss: 0.7666837573051453. Validation loss: 2.7054848670959473.\n",
            "Epoch 3208. Training loss: 0.7666800022125244. Validation loss: 2.705484390258789.\n",
            "Epoch 3209. Training loss: 0.7666762471199036. Validation loss: 2.705483913421631.\n",
            "Epoch 3210. Training loss: 0.7666726112365723. Validation loss: 2.7054834365844727.\n",
            "Epoch 3211. Training loss: 0.7666690349578857. Validation loss: 2.7054831981658936.\n",
            "Epoch 3212. Training loss: 0.7666653990745544. Validation loss: 2.7054824829101562.\n",
            "Epoch 3213. Training loss: 0.7666616439819336. Validation loss: 2.705482006072998.\n",
            "Epoch 3214. Training loss: 0.7666578888893127. Validation loss: 2.705482006072998.\n",
            "Epoch 3215. Training loss: 0.7666541934013367. Validation loss: 2.7054810523986816.\n",
            "Epoch 3216. Training loss: 0.7666504979133606. Validation loss: 2.7054805755615234.\n",
            "Epoch 3217. Training loss: 0.7666468620300293. Validation loss: 2.7054803371429443.\n",
            "Epoch 3218. Training loss: 0.7666431069374084. Validation loss: 2.705479860305786.\n",
            "Epoch 3219. Training loss: 0.7666394114494324. Validation loss: 2.705479383468628.\n",
            "Epoch 3220. Training loss: 0.7666357159614563. Validation loss: 2.705479145050049.\n",
            "Epoch 3221. Training loss: 0.766632080078125. Validation loss: 2.7054786682128906.\n",
            "Epoch 3222. Training loss: 0.7666282653808594. Validation loss: 2.7054781913757324.\n",
            "Epoch 3223. Training loss: 0.7666246294975281. Validation loss: 2.7054779529571533.\n",
            "Epoch 3224. Training loss: 0.766620934009552. Validation loss: 2.705477476119995.\n",
            "Epoch 3225. Training loss: 0.7666172385215759. Validation loss: 2.705476999282837.\n",
            "Epoch 3226. Training loss: 0.7666135430335999. Validation loss: 2.7054765224456787.\n",
            "Epoch 3227. Training loss: 0.7666098475456238. Validation loss: 2.7054758071899414.\n",
            "Epoch 3228. Training loss: 0.7666061520576477. Validation loss: 2.705475330352783.\n",
            "Epoch 3229. Training loss: 0.7666024565696716. Validation loss: 2.705475091934204.\n",
            "Epoch 3230. Training loss: 0.7665987610816956. Validation loss: 2.705474853515625.\n",
            "Epoch 3231. Training loss: 0.7665950655937195. Validation loss: 2.7054741382598877.\n",
            "Epoch 3232. Training loss: 0.7665913701057434. Validation loss: 2.7054738998413086.\n",
            "Epoch 3233. Training loss: 0.7665877342224121. Validation loss: 2.7054734230041504.\n",
            "Epoch 3234. Training loss: 0.7665839195251465. Validation loss: 2.705472707748413.\n",
            "Epoch 3235. Training loss: 0.7665802836418152. Validation loss: 2.705472469329834.\n",
            "Epoch 3236. Training loss: 0.7665765881538391. Validation loss: 2.705471992492676.\n",
            "Epoch 3237. Training loss: 0.7665729522705078. Validation loss: 2.7054717540740967.\n",
            "Epoch 3238. Training loss: 0.7665691375732422. Validation loss: 2.7054715156555176.\n",
            "Epoch 3239. Training loss: 0.7665653824806213. Validation loss: 2.705470561981201.\n",
            "Epoch 3240. Training loss: 0.7665616869926453. Validation loss: 2.705470323562622.\n",
            "Epoch 3241. Training loss: 0.7665579319000244. Validation loss: 2.705469846725464.\n",
            "Epoch 3242. Training loss: 0.7665542960166931. Validation loss: 2.7054693698883057.\n",
            "Epoch 3243. Training loss: 0.7665505409240723. Validation loss: 2.7054688930511475.\n",
            "Epoch 3244. Training loss: 0.7665467858314514. Validation loss: 2.7054684162139893.\n",
            "Epoch 3245. Training loss: 0.7665430903434753. Validation loss: 2.705467939376831.\n",
            "Epoch 3246. Training loss: 0.7665393948554993. Validation loss: 2.70546817779541.\n",
            "Epoch 3247. Training loss: 0.7665356993675232. Validation loss: 2.7054672241210938.\n",
            "Epoch 3248. Training loss: 0.7665321230888367. Validation loss: 2.7054669857025146.\n",
            "Epoch 3249. Training loss: 0.766528308391571. Validation loss: 2.7054665088653564.\n",
            "Epoch 3250. Training loss: 0.7665245532989502. Validation loss: 2.7054660320281982.\n",
            "Epoch 3251. Training loss: 0.7665207982063293. Validation loss: 2.705465316772461.\n",
            "Epoch 3252. Training loss: 0.7665171027183533. Validation loss: 2.7054648399353027.\n",
            "Epoch 3253. Training loss: 0.7665134072303772. Validation loss: 2.7054646015167236.\n",
            "Epoch 3254. Training loss: 0.7665095925331116. Validation loss: 2.7054638862609863.\n",
            "Epoch 3255. Training loss: 0.766506016254425. Validation loss: 2.7054636478424072.\n",
            "Epoch 3256. Training loss: 0.766502320766449. Validation loss: 2.705463409423828.\n",
            "Epoch 3257. Training loss: 0.7664985656738281. Validation loss: 2.70546293258667.\n",
            "Epoch 3258. Training loss: 0.7664947509765625. Validation loss: 2.7054622173309326.\n",
            "Epoch 3259. Training loss: 0.7664909958839417. Validation loss: 2.7054617404937744.\n",
            "Epoch 3260. Training loss: 0.7664874196052551. Validation loss: 2.7054615020751953.\n",
            "Epoch 3261. Training loss: 0.766483724117279. Validation loss: 2.705461025238037.\n",
            "Epoch 3262. Training loss: 0.7664799094200134. Validation loss: 2.705460548400879.\n",
            "Epoch 3263. Training loss: 0.7664761543273926. Validation loss: 2.7054603099823.\n",
            "Epoch 3264. Training loss: 0.7664723992347717. Validation loss: 2.7054595947265625.\n",
            "Epoch 3265. Training loss: 0.7664687037467957. Validation loss: 2.7054595947265625.\n",
            "Epoch 3266. Training loss: 0.7664650082588196. Validation loss: 2.705458641052246.\n",
            "Epoch 3267. Training loss: 0.7664613723754883. Validation loss: 2.705458641052246.\n",
            "Epoch 3268. Training loss: 0.7664575576782227. Validation loss: 2.705458164215088.\n",
            "Epoch 3269. Training loss: 0.766453742980957. Validation loss: 2.7054576873779297.\n",
            "Epoch 3270. Training loss: 0.7664501070976257. Validation loss: 2.7054569721221924.\n",
            "Epoch 3271. Training loss: 0.7664463520050049. Validation loss: 2.705456495285034.\n",
            "Epoch 3272. Training loss: 0.766442596912384. Validation loss: 2.705456018447876.\n",
            "Epoch 3273. Training loss: 0.766438901424408. Validation loss: 2.7054555416107178.\n",
            "Epoch 3274. Training loss: 0.7664351463317871. Validation loss: 2.7054553031921387.\n",
            "Epoch 3275. Training loss: 0.7664313912391663. Validation loss: 2.7054553031921387.\n",
            "Epoch 3276. Training loss: 0.7664276957511902. Validation loss: 2.7054543495178223.\n",
            "Epoch 3277. Training loss: 0.7664239406585693. Validation loss: 2.705453872680664.\n",
            "Epoch 3278. Training loss: 0.766420304775238. Validation loss: 2.705453634262085.\n",
            "Epoch 3279. Training loss: 0.7664164900779724. Validation loss: 2.7054529190063477.\n",
            "Epoch 3280. Training loss: 0.7664127349853516. Validation loss: 2.7054529190063477.\n",
            "Epoch 3281. Training loss: 0.7664089202880859. Validation loss: 2.7054522037506104.\n",
            "Epoch 3282. Training loss: 0.7664052844047546. Validation loss: 2.7054519653320312.\n",
            "Epoch 3283. Training loss: 0.7664015889167786. Validation loss: 2.705451488494873.\n",
            "Epoch 3284. Training loss: 0.7663977742195129. Validation loss: 2.7054507732391357.\n",
            "Epoch 3285. Training loss: 0.7663940787315369. Validation loss: 2.7054505348205566.\n",
            "Epoch 3286. Training loss: 0.766390323638916. Validation loss: 2.7054502964019775.\n",
            "Epoch 3287. Training loss: 0.7663865089416504. Validation loss: 2.7054498195648193.\n",
            "Epoch 3288. Training loss: 0.7663828730583191. Validation loss: 2.705448865890503.\n",
            "Epoch 3289. Training loss: 0.7663790583610535. Validation loss: 2.705448627471924.\n",
            "Epoch 3290. Training loss: 0.7663753628730774. Validation loss: 2.7054481506347656.\n",
            "Epoch 3291. Training loss: 0.7663716673851013. Validation loss: 2.7054479122161865.\n",
            "Epoch 3292. Training loss: 0.7663679122924805. Validation loss: 2.705447196960449.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3293. Training loss: 0.7663640975952148. Validation loss: 2.705447196960449.\n",
            "Epoch 3294. Training loss: 0.7663602828979492. Validation loss: 2.705446481704712.\n",
            "Epoch 3295. Training loss: 0.7663565278053284. Validation loss: 2.7054460048675537.\n",
            "Epoch 3296. Training loss: 0.7663528919219971. Validation loss: 2.7054455280303955.\n",
            "Epoch 3297. Training loss: 0.7663490772247314. Validation loss: 2.7054452896118164.\n",
            "Epoch 3298. Training loss: 0.7663453221321106. Validation loss: 2.705444812774658.\n",
            "Epoch 3299. Training loss: 0.7663416266441345. Validation loss: 2.7054443359375.\n",
            "Epoch 3300. Training loss: 0.7663378715515137. Validation loss: 2.705443859100342.\n",
            "Epoch 3301. Training loss: 0.7663341164588928. Validation loss: 2.7054433822631836.\n",
            "Epoch 3302. Training loss: 0.7663303017616272. Validation loss: 2.7054429054260254.\n",
            "Epoch 3303. Training loss: 0.7663266658782959. Validation loss: 2.705442428588867.\n",
            "Epoch 3304. Training loss: 0.7663228511810303. Validation loss: 2.705441951751709.\n",
            "Epoch 3305. Training loss: 0.7663190364837646. Validation loss: 2.705441474914551.\n",
            "Epoch 3306. Training loss: 0.7663154006004333. Validation loss: 2.7054409980773926.\n",
            "Epoch 3307. Training loss: 0.7663115859031677. Validation loss: 2.7054405212402344.\n",
            "Epoch 3308. Training loss: 0.7663078308105469. Validation loss: 2.705440044403076.\n",
            "Epoch 3309. Training loss: 0.766304075717926. Validation loss: 2.705440044403076.\n",
            "Epoch 3310. Training loss: 0.7663002610206604. Validation loss: 2.705439329147339.\n",
            "Epoch 3311. Training loss: 0.7662965655326843. Validation loss: 2.7054390907287598.\n",
            "Epoch 3312. Training loss: 0.7662927508354187. Validation loss: 2.7054386138916016.\n",
            "Epoch 3313. Training loss: 0.7662889957427979. Validation loss: 2.7054383754730225.\n",
            "Epoch 3314. Training loss: 0.7662851810455322. Validation loss: 2.7054378986358643.\n",
            "Epoch 3315. Training loss: 0.7662814259529114. Validation loss: 2.705437183380127.\n",
            "Epoch 3316. Training loss: 0.7662776112556458. Validation loss: 2.705436944961548.\n",
            "Epoch 3317. Training loss: 0.7662739753723145. Validation loss: 2.7054362297058105.\n",
            "Epoch 3318. Training loss: 0.7662701606750488. Validation loss: 2.7054357528686523.\n",
            "Epoch 3319. Training loss: 0.7662665247917175. Validation loss: 2.7054355144500732.\n",
            "Epoch 3320. Training loss: 0.7662627100944519. Validation loss: 2.705434799194336.\n",
            "Epoch 3321. Training loss: 0.7662588953971863. Validation loss: 2.705434560775757.\n",
            "Epoch 3322. Training loss: 0.7662551403045654. Validation loss: 2.7054338455200195.\n",
            "Epoch 3323. Training loss: 0.7662513852119446. Validation loss: 2.7054338455200195.\n",
            "Epoch 3324. Training loss: 0.7662476897239685. Validation loss: 2.7054333686828613.\n",
            "Epoch 3325. Training loss: 0.7662438750267029. Validation loss: 2.705432891845703.\n",
            "Epoch 3326. Training loss: 0.7662400603294373. Validation loss: 2.705432415008545.\n",
            "Epoch 3327. Training loss: 0.7662363052368164. Validation loss: 2.705432176589966.\n",
            "Epoch 3328. Training loss: 0.7662325501441956. Validation loss: 2.7054314613342285.\n",
            "Epoch 3329. Training loss: 0.7662288546562195. Validation loss: 2.7054312229156494.\n",
            "Epoch 3330. Training loss: 0.7662250399589539. Validation loss: 2.705430507659912.\n",
            "Epoch 3331. Training loss: 0.7662212252616882. Validation loss: 2.705430030822754.\n",
            "Epoch 3332. Training loss: 0.7662174105644226. Validation loss: 2.705429792404175.\n",
            "Epoch 3333. Training loss: 0.7662136554718018. Validation loss: 2.7054293155670166.\n",
            "Epoch 3334. Training loss: 0.7662099003791809. Validation loss: 2.7054288387298584.\n",
            "Epoch 3335. Training loss: 0.7662060856819153. Validation loss: 2.7054286003112793.\n",
            "Epoch 3336. Training loss: 0.7662022113800049. Validation loss: 2.705428123474121.\n",
            "Epoch 3337. Training loss: 0.7661985754966736. Validation loss: 2.705427646636963.\n",
            "Epoch 3338. Training loss: 0.7661946415901184. Validation loss: 2.705427408218384.\n",
            "Epoch 3339. Training loss: 0.7661910057067871. Validation loss: 2.7054266929626465.\n",
            "Epoch 3340. Training loss: 0.7661871910095215. Validation loss: 2.7054262161254883.\n",
            "Epoch 3341. Training loss: 0.7661834359169006. Validation loss: 2.70542573928833.\n",
            "Epoch 3342. Training loss: 0.766179621219635. Validation loss: 2.7054250240325928.\n",
            "Epoch 3343. Training loss: 0.7661758065223694. Validation loss: 2.7054247856140137.\n",
            "Epoch 3344. Training loss: 0.7661719918251038. Validation loss: 2.7054243087768555.\n",
            "Epoch 3345. Training loss: 0.7661682963371277. Validation loss: 2.7054238319396973.\n",
            "Epoch 3346. Training loss: 0.7661644816398621. Validation loss: 2.705423355102539.\n",
            "Epoch 3347. Training loss: 0.7661607265472412. Validation loss: 2.705422878265381.\n",
            "Epoch 3348. Training loss: 0.7661569118499756. Validation loss: 2.7054226398468018.\n",
            "Epoch 3349. Training loss: 0.7661531567573547. Validation loss: 2.7054219245910645.\n",
            "Epoch 3350. Training loss: 0.7661493420600891. Validation loss: 2.7054216861724854.\n",
            "Epoch 3351. Training loss: 0.7661455273628235. Validation loss: 2.7054214477539062.\n",
            "Epoch 3352. Training loss: 0.7661417126655579. Validation loss: 2.705420970916748.\n",
            "Epoch 3353. Training loss: 0.7661378979682922. Validation loss: 2.7054200172424316.\n",
            "Epoch 3354. Training loss: 0.7661342620849609. Validation loss: 2.7054200172424316.\n",
            "Epoch 3355. Training loss: 0.7661304473876953. Validation loss: 2.7054195404052734.\n",
            "Epoch 3356. Training loss: 0.7661266326904297. Validation loss: 2.7054190635681152.\n",
            "Epoch 3357. Training loss: 0.7661228179931641. Validation loss: 2.705418825149536.\n",
            "Epoch 3358. Training loss: 0.7661190032958984. Validation loss: 2.705418348312378.\n",
            "Epoch 3359. Training loss: 0.7661152482032776. Validation loss: 2.7054176330566406.\n",
            "Epoch 3360. Training loss: 0.7661113739013672. Validation loss: 2.7054171562194824.\n",
            "Epoch 3361. Training loss: 0.7661077380180359. Validation loss: 2.705416679382324.\n",
            "Epoch 3362. Training loss: 0.7661038041114807. Validation loss: 2.705416440963745.\n",
            "Epoch 3363. Training loss: 0.7660999894142151. Validation loss: 2.705415725708008.\n",
            "Epoch 3364. Training loss: 0.766096293926239. Validation loss: 2.7054154872894287.\n",
            "Epoch 3365. Training loss: 0.7660924792289734. Validation loss: 2.7054152488708496.\n",
            "Epoch 3366. Training loss: 0.7660887241363525. Validation loss: 2.7054147720336914.\n",
            "Epoch 3367. Training loss: 0.7660848498344421. Validation loss: 2.705414056777954.\n",
            "Epoch 3368. Training loss: 0.7660810351371765. Validation loss: 2.705413818359375.\n",
            "Epoch 3369. Training loss: 0.7660772800445557. Validation loss: 2.705413341522217.\n",
            "Epoch 3370. Training loss: 0.76607346534729. Validation loss: 2.7054128646850586.\n",
            "Epoch 3371. Training loss: 0.7660697102546692. Validation loss: 2.7054126262664795.\n",
            "Epoch 3372. Training loss: 0.7660658359527588. Validation loss: 2.705411911010742.\n",
            "Epoch 3373. Training loss: 0.7660620212554932. Validation loss: 2.705411434173584.\n",
            "Epoch 3374. Training loss: 0.7660582661628723. Validation loss: 2.705410957336426.\n",
            "Epoch 3375. Training loss: 0.7660543918609619. Validation loss: 2.7054104804992676.\n",
            "Epoch 3376. Training loss: 0.7660506367683411. Validation loss: 2.7054104804992676.\n",
            "Epoch 3377. Training loss: 0.7660467624664307. Validation loss: 2.7054097652435303.\n",
            "Epoch 3378. Training loss: 0.766042947769165. Validation loss: 2.705409288406372.\n",
            "Epoch 3379. Training loss: 0.7660391926765442. Validation loss: 2.705409049987793.\n",
            "Epoch 3380. Training loss: 0.7660353779792786. Validation loss: 2.7054085731506348.\n",
            "Epoch 3381. Training loss: 0.7660315632820129. Validation loss: 2.7054080963134766.\n",
            "Epoch 3382. Training loss: 0.7660276889801025. Validation loss: 2.7054073810577393.\n",
            "Epoch 3383. Training loss: 0.7660240530967712. Validation loss: 2.70540714263916.\n",
            "Epoch 3384. Training loss: 0.7660202383995056. Validation loss: 2.705406665802002.\n",
            "Epoch 3385. Training loss: 0.7660163044929504. Validation loss: 2.7054061889648438.\n",
            "Epoch 3386. Training loss: 0.7660124897956848. Validation loss: 2.7054057121276855.\n",
            "Epoch 3387. Training loss: 0.7660086750984192. Validation loss: 2.7054054737091064.\n",
            "Epoch 3388. Training loss: 0.7660048604011536. Validation loss: 2.705404758453369.\n",
            "Epoch 3389. Training loss: 0.7660011649131775. Validation loss: 2.705404281616211.\n",
            "Epoch 3390. Training loss: 0.7659974098205566. Validation loss: 2.7054035663604736.\n",
            "Epoch 3391. Training loss: 0.7659935355186462. Validation loss: 2.7054035663604736.\n",
            "Epoch 3392. Training loss: 0.7659896016120911. Validation loss: 2.7054030895233154.\n",
            "Epoch 3393. Training loss: 0.7659857869148254. Validation loss: 2.7054028511047363.\n",
            "Epoch 3394. Training loss: 0.7659819722175598. Validation loss: 2.705402135848999.\n",
            "Epoch 3395. Training loss: 0.7659781575202942. Validation loss: 2.70540189743042.\n",
            "Epoch 3396. Training loss: 0.7659743428230286. Validation loss: 2.7054009437561035.\n",
            "Epoch 3397. Training loss: 0.7659704685211182. Validation loss: 2.7054007053375244.\n",
            "Epoch 3398. Training loss: 0.7659667134284973. Validation loss: 2.705400228500366.\n",
            "Epoch 3399. Training loss: 0.7659628987312317. Validation loss: 2.705400228500366.\n",
            "Epoch 3400. Training loss: 0.7659590244293213. Validation loss: 2.705399751663208.\n",
            "Epoch 3401. Training loss: 0.7659552693367004. Validation loss: 2.70539927482605.\n",
            "Epoch 3402. Training loss: 0.76595139503479. Validation loss: 2.7053985595703125.\n",
            "Epoch 3403. Training loss: 0.7659477591514587. Validation loss: 2.7053980827331543.\n",
            "Epoch 3404. Training loss: 0.7659438252449036. Validation loss: 2.705397367477417.\n",
            "Epoch 3405. Training loss: 0.7659399509429932. Validation loss: 2.705397367477417.\n",
            "Epoch 3406. Training loss: 0.7659361362457275. Validation loss: 2.705397129058838.\n",
            "Epoch 3407. Training loss: 0.7659323215484619. Validation loss: 2.7053961753845215.\n",
            "Epoch 3408. Training loss: 0.7659284472465515. Validation loss: 2.7053956985473633.\n",
            "Epoch 3409. Training loss: 0.7659246325492859. Validation loss: 2.705395460128784.\n",
            "Epoch 3410. Training loss: 0.765920877456665. Validation loss: 2.705394744873047.\n",
            "Epoch 3411. Training loss: 0.7659168839454651. Validation loss: 2.705394744873047.\n",
            "Epoch 3412. Training loss: 0.765913188457489. Validation loss: 2.7053942680358887.\n",
            "Epoch 3413. Training loss: 0.7659091949462891. Validation loss: 2.7053940296173096.\n",
            "Epoch 3414. Training loss: 0.7659054398536682. Validation loss: 2.7053937911987305.\n",
            "Epoch 3415. Training loss: 0.7659015655517578. Validation loss: 2.705393075942993.\n",
            "Epoch 3416. Training loss: 0.765897810459137. Validation loss: 2.7053921222686768.\n",
            "Epoch 3417. Training loss: 0.7658939957618713. Validation loss: 2.7053921222686768.\n",
            "Epoch 3418. Training loss: 0.7658901214599609. Validation loss: 2.7053916454315186.\n",
            "Epoch 3419. Training loss: 0.7658862471580505. Validation loss: 2.7053909301757812.\n",
            "Epoch 3420. Training loss: 0.7658824324607849. Validation loss: 2.705390691757202.\n",
            "Epoch 3421. Training loss: 0.7658786773681641. Validation loss: 2.705390453338623.\n",
            "Epoch 3422. Training loss: 0.7658748030662537. Validation loss: 2.7053897380828857.\n",
            "Epoch 3423. Training loss: 0.765870988368988. Validation loss: 2.7053892612457275.\n",
            "Epoch 3424. Training loss: 0.7658671736717224. Validation loss: 2.7053890228271484.\n",
            "Epoch 3425. Training loss: 0.7658633589744568. Validation loss: 2.705388307571411.\n",
            "Epoch 3426. Training loss: 0.7658593654632568. Validation loss: 2.705388069152832.\n",
            "Epoch 3427. Training loss: 0.7658555507659912. Validation loss: 2.7053873538970947.\n",
            "Epoch 3428. Training loss: 0.7658517956733704. Validation loss: 2.7053868770599365.\n",
            "Epoch 3429. Training loss: 0.76584792137146. Validation loss: 2.7053864002227783.\n",
            "Epoch 3430. Training loss: 0.7658440470695496. Validation loss: 2.70538592338562.\n",
            "Epoch 3431. Training loss: 0.7658402323722839. Validation loss: 2.705385684967041.\n",
            "Epoch 3432. Training loss: 0.7658364176750183. Validation loss: 2.705385208129883.\n",
            "Epoch 3433. Training loss: 0.7658324837684631. Validation loss: 2.7053847312927246.\n",
            "Epoch 3434. Training loss: 0.7658286690711975. Validation loss: 2.7053842544555664.\n",
            "Epoch 3435. Training loss: 0.7658247947692871. Validation loss: 2.7053842544555664.\n",
            "Epoch 3436. Training loss: 0.7658209800720215. Validation loss: 2.70538330078125.\n",
            "Epoch 3437. Training loss: 0.7658171653747559. Validation loss: 2.70538330078125.\n",
            "Epoch 3438. Training loss: 0.7658132910728455. Validation loss: 2.705382823944092.\n",
            "Epoch 3439. Training loss: 0.7658093571662903. Validation loss: 2.7053821086883545.\n",
            "Epoch 3440. Training loss: 0.7658055424690247. Validation loss: 2.705381393432617.\n",
            "Epoch 3441. Training loss: 0.765801727771759. Validation loss: 2.705381393432617.\n",
            "Epoch 3442. Training loss: 0.7657978534698486. Validation loss: 2.705380916595459.\n",
            "Epoch 3443. Training loss: 0.765794038772583. Validation loss: 2.7053799629211426.\n",
            "Epoch 3444. Training loss: 0.7657902240753174. Validation loss: 2.7053799629211426.\n",
            "Epoch 3445. Training loss: 0.7657862305641174. Validation loss: 2.7053794860839844.\n",
            "Epoch 3446. Training loss: 0.7657824158668518. Validation loss: 2.705379009246826.\n",
            "Epoch 3447. Training loss: 0.7657785415649414. Validation loss: 2.705378770828247.\n",
            "Epoch 3448. Training loss: 0.765774667263031. Validation loss: 2.7053780555725098.\n",
            "Epoch 3449. Training loss: 0.7657708525657654. Validation loss: 2.7053775787353516.\n",
            "Epoch 3450. Training loss: 0.7657669186592102. Validation loss: 2.7053768634796143.\n",
            "Epoch 3451. Training loss: 0.7657632231712341. Validation loss: 2.7053768634796143.\n",
            "Epoch 3452. Training loss: 0.765759289264679. Validation loss: 2.705376386642456.\n",
            "Epoch 3453. Training loss: 0.7657553553581238. Validation loss: 2.705375909805298.\n",
            "Epoch 3454. Training loss: 0.7657515406608582. Validation loss: 2.7053754329681396.\n",
            "Epoch 3455. Training loss: 0.7657477259635925. Validation loss: 2.7053747177124023.\n",
            "Epoch 3456. Training loss: 0.7657437920570374. Validation loss: 2.7053747177124023.\n",
            "Epoch 3457. Training loss: 0.765739917755127. Validation loss: 2.705374002456665.\n",
            "Epoch 3458. Training loss: 0.7657361030578613. Validation loss: 2.705373764038086.\n",
            "Epoch 3459. Training loss: 0.7657321095466614. Validation loss: 2.7053732872009277.\n",
            "Epoch 3460. Training loss: 0.7657284140586853. Validation loss: 2.7053728103637695.\n",
            "Epoch 3461. Training loss: 0.7657244801521301. Validation loss: 2.7053725719451904.\n",
            "Epoch 3462. Training loss: 0.7657206654548645. Validation loss: 2.705371618270874.\n",
            "Epoch 3463. Training loss: 0.7657167315483093. Validation loss: 2.705371379852295.\n",
            "Epoch 3464. Training loss: 0.7657129764556885. Validation loss: 2.705371141433716.\n",
            "Epoch 3465. Training loss: 0.7657091021537781. Validation loss: 2.7053704261779785.\n",
            "Epoch 3466. Training loss: 0.7657051682472229. Validation loss: 2.7053699493408203.\n",
            "Epoch 3467. Training loss: 0.7657012939453125. Validation loss: 2.705369472503662.\n",
            "Epoch 3468. Training loss: 0.7656974792480469. Validation loss: 2.705369234085083.\n",
            "Epoch 3469. Training loss: 0.7656934857368469. Validation loss: 2.705368757247925.\n",
            "Epoch 3470. Training loss: 0.7656896710395813. Validation loss: 2.7053680419921875.\n",
            "Epoch 3471. Training loss: 0.7656857967376709. Validation loss: 2.7053678035736084.\n",
            "Epoch 3472. Training loss: 0.76568204164505. Validation loss: 2.705367088317871.\n",
            "Epoch 3473. Training loss: 0.765678346157074. Validation loss: 2.705367088317871.\n",
            "Epoch 3474. Training loss: 0.7656745910644531. Validation loss: 2.705366373062134.\n",
            "Epoch 3475. Training loss: 0.7656707763671875. Validation loss: 2.7053658962249756.\n",
            "Epoch 3476. Training loss: 0.7656670212745667. Validation loss: 2.7053658962249756.\n",
            "Epoch 3477. Training loss: 0.7656633257865906. Validation loss: 2.7053651809692383.\n",
            "Epoch 3478. Training loss: 0.7656596302986145. Validation loss: 2.705364465713501.\n",
            "Epoch 3479. Training loss: 0.7656558156013489. Validation loss: 2.7053639888763428.\n",
            "Epoch 3480. Training loss: 0.7656521201133728. Validation loss: 2.7053637504577637.\n",
            "Epoch 3481. Training loss: 0.765648365020752. Validation loss: 2.7053632736206055.\n",
            "Epoch 3482. Training loss: 0.7656447291374207. Validation loss: 2.7053627967834473.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3483. Training loss: 0.7656407952308655. Validation loss: 2.705362558364868.\n",
            "Epoch 3484. Training loss: 0.765637218952179. Validation loss: 2.705361843109131.\n",
            "Epoch 3485. Training loss: 0.7656334042549133. Validation loss: 2.7053613662719727.\n",
            "Epoch 3486. Training loss: 0.765629768371582. Validation loss: 2.7053608894348145.\n",
            "Epoch 3487. Training loss: 0.7656260132789612. Validation loss: 2.7053604125976562.\n",
            "Epoch 3488. Training loss: 0.7656223177909851. Validation loss: 2.705359935760498.\n",
            "Epoch 3489. Training loss: 0.7656185030937195. Validation loss: 2.705359935760498.\n",
            "Epoch 3490. Training loss: 0.7656148076057434. Validation loss: 2.7053592205047607.\n",
            "Epoch 3491. Training loss: 0.7656111121177673. Validation loss: 2.7053587436676025.\n",
            "Epoch 3492. Training loss: 0.7656074166297913. Validation loss: 2.7053585052490234.\n",
            "Epoch 3493. Training loss: 0.7656037211418152. Validation loss: 2.705357551574707.\n",
            "Epoch 3494. Training loss: 0.7655999064445496. Validation loss: 2.705357074737549.\n",
            "Epoch 3495. Training loss: 0.7655962109565735. Validation loss: 2.705357074737549.\n",
            "Epoch 3496. Training loss: 0.7655925154685974. Validation loss: 2.7053561210632324.\n",
            "Epoch 3497. Training loss: 0.7655887603759766. Validation loss: 2.705355644226074.\n",
            "Epoch 3498. Training loss: 0.7655850052833557. Validation loss: 2.705355167388916.\n",
            "Epoch 3499. Training loss: 0.7655814290046692. Validation loss: 2.705354690551758.\n",
            "Epoch 3500. Training loss: 0.7655777335166931. Validation loss: 2.7053544521331787.\n",
            "Epoch 3501. Training loss: 0.7655739784240723. Validation loss: 2.7053537368774414.\n",
            "Epoch 3502. Training loss: 0.7655701637268066. Validation loss: 2.7053534984588623.\n",
            "Epoch 3503. Training loss: 0.7655664086341858. Validation loss: 2.705353260040283.\n",
            "Epoch 3504. Training loss: 0.7655627727508545. Validation loss: 2.705352306365967.\n",
            "Epoch 3505. Training loss: 0.7655591368675232. Validation loss: 2.7053520679473877.\n",
            "Epoch 3506. Training loss: 0.7655553817749023. Validation loss: 2.7053515911102295.\n",
            "Epoch 3507. Training loss: 0.7655516266822815. Validation loss: 2.705350875854492.\n",
            "Epoch 3508. Training loss: 0.7655479311943054. Validation loss: 2.705350399017334.\n",
            "Epoch 3509. Training loss: 0.7655441164970398. Validation loss: 2.705350399017334.\n",
            "Epoch 3510. Training loss: 0.7655404210090637. Validation loss: 2.7053496837615967.\n",
            "Epoch 3511. Training loss: 0.7655366063117981. Validation loss: 2.7053494453430176.\n",
            "Epoch 3512. Training loss: 0.7655329704284668. Validation loss: 2.7053489685058594.\n",
            "Epoch 3513. Training loss: 0.765529215335846. Validation loss: 2.705348253250122.\n",
            "Epoch 3514. Training loss: 0.7655255198478699. Validation loss: 2.705348014831543.\n",
            "Epoch 3515. Training loss: 0.7655218243598938. Validation loss: 2.7053470611572266.\n",
            "Epoch 3516. Training loss: 0.7655180096626282. Validation loss: 2.7053470611572266.\n",
            "Epoch 3517. Training loss: 0.7655143737792969. Validation loss: 2.7053465843200684.\n",
            "Epoch 3518. Training loss: 0.7655107378959656. Validation loss: 2.70534610748291.\n",
            "Epoch 3519. Training loss: 0.7655069231987. Validation loss: 2.705345630645752.\n",
            "Epoch 3520. Training loss: 0.7655031681060791. Validation loss: 2.7053449153900146.\n",
            "Epoch 3521. Training loss: 0.7654994130134583. Validation loss: 2.7053444385528564.\n",
            "Epoch 3522. Training loss: 0.7654955983161926. Validation loss: 2.7053439617156982.\n",
            "Epoch 3523. Training loss: 0.7654919028282166. Validation loss: 2.705343723297119.\n",
            "Epoch 3524. Training loss: 0.7654882073402405. Validation loss: 2.705343246459961.\n",
            "Epoch 3525. Training loss: 0.7654845118522644. Validation loss: 2.7053427696228027.\n",
            "Epoch 3526. Training loss: 0.7654808163642883. Validation loss: 2.7053420543670654.\n",
            "Epoch 3527. Training loss: 0.7654771208763123. Validation loss: 2.7053418159484863.\n",
            "Epoch 3528. Training loss: 0.7654731869697571. Validation loss: 2.705341339111328.\n",
            "Epoch 3529. Training loss: 0.7654695510864258. Validation loss: 2.70534086227417.\n",
            "Epoch 3530. Training loss: 0.7654657363891602. Validation loss: 2.7053403854370117.\n",
            "Epoch 3531. Training loss: 0.7654619812965393. Validation loss: 2.7053399085998535.\n",
            "Epoch 3532. Training loss: 0.765458345413208. Validation loss: 2.7053394317626953.\n",
            "Epoch 3533. Training loss: 0.7654545903205872. Validation loss: 2.705338954925537.\n",
            "Epoch 3534. Training loss: 0.7654508948326111. Validation loss: 2.705338716506958.\n",
            "Epoch 3535. Training loss: 0.7654470801353455. Validation loss: 2.7053380012512207.\n",
            "Epoch 3536. Training loss: 0.7654433846473694. Validation loss: 2.7053375244140625.\n",
            "Epoch 3537. Training loss: 0.7654396891593933. Validation loss: 2.705336809158325.\n",
            "Epoch 3538. Training loss: 0.7654359340667725. Validation loss: 2.705336570739746.\n",
            "Epoch 3539. Training loss: 0.7654321789741516. Validation loss: 2.705336093902588.\n",
            "Epoch 3540. Training loss: 0.765428364276886. Validation loss: 2.705336093902588.\n",
            "Epoch 3541. Training loss: 0.7654246687889099. Validation loss: 2.7053351402282715.\n",
            "Epoch 3542. Training loss: 0.7654209733009338. Validation loss: 2.7053346633911133.\n",
            "Epoch 3543. Training loss: 0.7654171586036682. Validation loss: 2.705334424972534.\n",
            "Epoch 3544. Training loss: 0.7654134631156921. Validation loss: 2.7053334712982178.\n",
            "Epoch 3545. Training loss: 0.7654096484184265. Validation loss: 2.7053334712982178.\n",
            "Epoch 3546. Training loss: 0.7654059529304504. Validation loss: 2.7053329944610596.\n",
            "Epoch 3547. Training loss: 0.7654021382331848. Validation loss: 2.7053322792053223.\n",
            "Epoch 3548. Training loss: 0.7653984427452087. Validation loss: 2.705331802368164.\n",
            "Epoch 3549. Training loss: 0.7653946876525879. Validation loss: 2.705331325531006.\n",
            "Epoch 3550. Training loss: 0.765390932559967. Validation loss: 2.705331325531006.\n",
            "Epoch 3551. Training loss: 0.7653871178627014. Validation loss: 2.7053308486938477.\n",
            "Epoch 3552. Training loss: 0.7653834819793701. Validation loss: 2.7053298950195312.\n",
            "Epoch 3553. Training loss: 0.7653797268867493. Validation loss: 2.705329656600952.\n",
            "Epoch 3554. Training loss: 0.7653759121894836. Validation loss: 2.705329179763794.\n",
            "Epoch 3555. Training loss: 0.7653722167015076. Validation loss: 2.7053287029266357.\n",
            "Epoch 3556. Training loss: 0.7653684616088867. Validation loss: 2.7053279876708984.\n",
            "Epoch 3557. Training loss: 0.7653646469116211. Validation loss: 2.7053277492523193.\n",
            "Epoch 3558. Training loss: 0.7653608918190002. Validation loss: 2.705327033996582.\n",
            "Epoch 3559. Training loss: 0.7653570771217346. Validation loss: 2.705326557159424.\n",
            "Epoch 3560. Training loss: 0.7653533816337585. Validation loss: 2.705326557159424.\n",
            "Epoch 3561. Training loss: 0.7653496861457825. Validation loss: 2.7053260803222656.\n",
            "Epoch 3562. Training loss: 0.7653458714485168. Validation loss: 2.7053256034851074.\n",
            "Epoch 3563. Training loss: 0.7653421759605408. Validation loss: 2.705325126647949.\n",
            "Epoch 3564. Training loss: 0.7653384208679199. Validation loss: 2.705324649810791.\n",
            "Epoch 3565. Training loss: 0.7653346061706543. Validation loss: 2.705324172973633.\n",
            "Epoch 3566. Training loss: 0.7653308510780334. Validation loss: 2.7053234577178955.\n",
            "Epoch 3567. Training loss: 0.7653272747993469. Validation loss: 2.7053229808807373.\n",
            "Epoch 3568. Training loss: 0.7653236389160156. Validation loss: 2.705322504043579.\n",
            "Epoch 3569. Training loss: 0.7653200030326843. Validation loss: 2.705322027206421.\n",
            "Epoch 3570. Training loss: 0.7653163075447083. Validation loss: 2.7053215503692627.\n",
            "Epoch 3571. Training loss: 0.7653126120567322. Validation loss: 2.7053210735321045.\n",
            "Epoch 3572. Training loss: 0.7653090357780457. Validation loss: 2.7053205966949463.\n",
            "Epoch 3573. Training loss: 0.7653053402900696. Validation loss: 2.705320358276367.\n",
            "Epoch 3574. Training loss: 0.7653016448020935. Validation loss: 2.705319881439209.\n",
            "Epoch 3575. Training loss: 0.765298068523407. Validation loss: 2.705319404602051.\n",
            "Epoch 3576. Training loss: 0.7652944922447205. Validation loss: 2.7053189277648926.\n",
            "Epoch 3577. Training loss: 0.7652907371520996. Validation loss: 2.7053182125091553.\n",
            "Epoch 3578. Training loss: 0.7652871012687683. Validation loss: 2.7053182125091553.\n",
            "Epoch 3579. Training loss: 0.7652835249900818. Validation loss: 2.705317258834839.\n",
            "Epoch 3580. Training loss: 0.7652798295021057. Validation loss: 2.7053170204162598.\n",
            "Epoch 3581. Training loss: 0.7652761340141296. Validation loss: 2.7053163051605225.\n",
            "Epoch 3582. Training loss: 0.7652725577354431. Validation loss: 2.7053158283233643.\n",
            "Epoch 3583. Training loss: 0.7652689814567566. Validation loss: 2.705315351486206.\n",
            "Epoch 3584. Training loss: 0.7652652263641357. Validation loss: 2.705315113067627.\n",
            "Epoch 3585. Training loss: 0.765261709690094. Validation loss: 2.7053146362304688.\n",
            "Epoch 3586. Training loss: 0.7652578949928284. Validation loss: 2.7053139209747314.\n",
            "Epoch 3587. Training loss: 0.7652542591094971. Validation loss: 2.7053134441375732.\n",
            "Epoch 3588. Training loss: 0.7652506828308105. Validation loss: 2.705312967300415.\n",
            "Epoch 3589. Training loss: 0.7652470469474792. Validation loss: 2.705312728881836.\n",
            "Epoch 3590. Training loss: 0.7652433514595032. Validation loss: 2.7053120136260986.\n",
            "Epoch 3591. Training loss: 0.7652397751808167. Validation loss: 2.7053117752075195.\n",
            "Epoch 3592. Training loss: 0.7652361989021301. Validation loss: 2.7053110599517822.\n",
            "Epoch 3593. Training loss: 0.765232503414154. Validation loss: 2.705310583114624.\n",
            "Epoch 3594. Training loss: 0.7652287483215332. Validation loss: 2.705310106277466.\n",
            "Epoch 3595. Training loss: 0.7652252316474915. Validation loss: 2.7053096294403076.\n",
            "Epoch 3596. Training loss: 0.7652215361595154. Validation loss: 2.7053091526031494.\n",
            "Epoch 3597. Training loss: 0.7652179598808289. Validation loss: 2.7053091526031494.\n",
            "Epoch 3598. Training loss: 0.7652143836021423. Validation loss: 2.705308198928833.\n",
            "Epoch 3599. Training loss: 0.7652106881141663. Validation loss: 2.7053072452545166.\n",
            "Epoch 3600. Training loss: 0.765207052230835. Validation loss: 2.7053070068359375.\n",
            "Epoch 3601. Training loss: 0.7652034163475037. Validation loss: 2.7053067684173584.\n",
            "Epoch 3602. Training loss: 0.7651996612548828. Validation loss: 2.7053062915802.\n",
            "Epoch 3603. Training loss: 0.7651960253715515. Validation loss: 2.705305576324463.\n",
            "Epoch 3604. Training loss: 0.765192449092865. Validation loss: 2.7053050994873047.\n",
            "Epoch 3605. Training loss: 0.7651886940002441. Validation loss: 2.7053046226501465.\n",
            "Epoch 3606. Training loss: 0.7651851177215576. Validation loss: 2.7053041458129883.\n",
            "Epoch 3607. Training loss: 0.7651814818382263. Validation loss: 2.70530366897583.\n",
            "Epoch 3608. Training loss: 0.7651779055595398. Validation loss: 2.705303192138672.\n",
            "Epoch 3609. Training loss: 0.765174150466919. Validation loss: 2.7053027153015137.\n",
            "Epoch 3610. Training loss: 0.7651705741882324. Validation loss: 2.7053024768829346.\n",
            "Epoch 3611. Training loss: 0.7651668190956116. Validation loss: 2.7053017616271973.\n",
            "Epoch 3612. Training loss: 0.7651631236076355. Validation loss: 2.705301284790039.\n",
            "Epoch 3613. Training loss: 0.765159547328949. Validation loss: 2.705300807952881.\n",
            "Epoch 3614. Training loss: 0.7651559710502625. Validation loss: 2.7053003311157227.\n",
            "Epoch 3615. Training loss: 0.7651522159576416. Validation loss: 2.7052998542785645.\n",
            "Epoch 3616. Training loss: 0.7651486396789551. Validation loss: 2.7052993774414062.\n",
            "Epoch 3617. Training loss: 0.7651450037956238. Validation loss: 2.705298900604248.\n",
            "Epoch 3618. Training loss: 0.7651414275169373. Validation loss: 2.705298662185669.\n",
            "Epoch 3619. Training loss: 0.7651377320289612. Validation loss: 2.7052977085113525.\n",
            "Epoch 3620. Training loss: 0.7651340365409851. Validation loss: 2.7052972316741943.\n",
            "Epoch 3621. Training loss: 0.7651302814483643. Validation loss: 2.7052969932556152.\n",
            "Epoch 3622. Training loss: 0.765126645565033. Validation loss: 2.705296516418457.\n",
            "Epoch 3623. Training loss: 0.7651230692863464. Validation loss: 2.705296039581299.\n",
            "Epoch 3624. Training loss: 0.7651193737983704. Validation loss: 2.7052958011627197.\n",
            "Epoch 3625. Training loss: 0.7651157379150391. Validation loss: 2.7052950859069824.\n",
            "Epoch 3626. Training loss: 0.7651121020317078. Validation loss: 2.705294609069824.\n",
            "Epoch 3627. Training loss: 0.7651083469390869. Validation loss: 2.705294132232666.\n",
            "Epoch 3628. Training loss: 0.7651047706604004. Validation loss: 2.705293655395508.\n",
            "Epoch 3629. Training loss: 0.7651011347770691. Validation loss: 2.7052929401397705.\n",
            "Epoch 3630. Training loss: 0.765097439289093. Validation loss: 2.705292224884033.\n",
            "Epoch 3631. Training loss: 0.7650937438011169. Validation loss: 2.705292224884033.\n",
            "Epoch 3632. Training loss: 0.7650901675224304. Validation loss: 2.705291271209717.\n",
            "Epoch 3633. Training loss: 0.7650864124298096. Validation loss: 2.7052910327911377.\n",
            "Epoch 3634. Training loss: 0.7650827765464783. Validation loss: 2.7052907943725586.\n",
            "Epoch 3635. Training loss: 0.7650790214538574. Validation loss: 2.7052903175354004.\n",
            "Epoch 3636. Training loss: 0.7650753855705261. Validation loss: 2.705289840698242.\n",
            "Epoch 3637. Training loss: 0.7650718092918396. Validation loss: 2.705289363861084.\n",
            "Epoch 3638. Training loss: 0.7650681138038635. Validation loss: 2.7052884101867676.\n",
            "Epoch 3639. Training loss: 0.7650644183158875. Validation loss: 2.7052881717681885.\n",
            "Epoch 3640. Training loss: 0.7650607228279114. Validation loss: 2.7052876949310303.\n",
            "Epoch 3641. Training loss: 0.7650571465492249. Validation loss: 2.705286979675293.\n",
            "Epoch 3642. Training loss: 0.7650534510612488. Validation loss: 2.7052865028381348.\n",
            "Epoch 3643. Training loss: 0.7650496959686279. Validation loss: 2.7052860260009766.\n",
            "Epoch 3644. Training loss: 0.7650461196899414. Validation loss: 2.7052857875823975.\n",
            "Epoch 3645. Training loss: 0.7650424838066101. Validation loss: 2.70528507232666.\n",
            "Epoch 3646. Training loss: 0.765038788318634. Validation loss: 2.705284595489502.\n",
            "Epoch 3647. Training loss: 0.7650352120399475. Validation loss: 2.705284357070923.\n",
            "Epoch 3648. Training loss: 0.7650313973426819. Validation loss: 2.7052836418151855.\n",
            "Epoch 3649. Training loss: 0.7650277614593506. Validation loss: 2.7052831649780273.\n",
            "Epoch 3650. Training loss: 0.7650240063667297. Validation loss: 2.705282688140869.\n",
            "Epoch 3651. Training loss: 0.7650203704833984. Validation loss: 2.705282211303711.\n",
            "Epoch 3652. Training loss: 0.7650167346000671. Validation loss: 2.7052817344665527.\n",
            "Epoch 3653. Training loss: 0.7650131583213806. Validation loss: 2.7052812576293945.\n",
            "Epoch 3654. Training loss: 0.7650094032287598. Validation loss: 2.7052807807922363.\n",
            "Epoch 3655. Training loss: 0.7650057673454285. Validation loss: 2.705280303955078.\n",
            "Epoch 3656. Training loss: 0.7650020718574524. Validation loss: 2.705280303955078.\n",
            "Epoch 3657. Training loss: 0.7649984359741211. Validation loss: 2.7052793502807617.\n",
            "Epoch 3658. Training loss: 0.7649946808815002. Validation loss: 2.7052788734436035.\n",
            "Epoch 3659. Training loss: 0.764991044998169. Validation loss: 2.7052783966064453.\n",
            "Epoch 3660. Training loss: 0.7649872899055481. Validation loss: 2.705277681350708.\n",
            "Epoch 3661. Training loss: 0.7649836540222168. Validation loss: 2.705277442932129.\n",
            "Epoch 3662. Training loss: 0.7649800777435303. Validation loss: 2.7052767276763916.\n",
            "Epoch 3663. Training loss: 0.7649763226509094. Validation loss: 2.7052764892578125.\n",
            "Epoch 3664. Training loss: 0.7649726271629333. Validation loss: 2.705275774002075.\n",
            "Epoch 3665. Training loss: 0.7649690508842468. Validation loss: 2.705275535583496.\n",
            "Epoch 3666. Training loss: 0.7649652361869812. Validation loss: 2.705274820327759.\n",
            "Epoch 3667. Training loss: 0.7649615406990051. Validation loss: 2.7052743434906006.\n",
            "Epoch 3668. Training loss: 0.7649579644203186. Validation loss: 2.7052736282348633.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3669. Training loss: 0.7649542689323425. Validation loss: 2.7052736282348633.\n",
            "Epoch 3670. Training loss: 0.7649505138397217. Validation loss: 2.705272912979126.\n",
            "Epoch 3671. Training loss: 0.7649468779563904. Validation loss: 2.7052724361419678.\n",
            "Epoch 3672. Training loss: 0.7649431824684143. Validation loss: 2.7052721977233887.\n",
            "Epoch 3673. Training loss: 0.7649394869804382. Validation loss: 2.7052712440490723.\n",
            "Epoch 3674. Training loss: 0.7649357914924622. Validation loss: 2.7052712440490723.\n",
            "Epoch 3675. Training loss: 0.7649321556091309. Validation loss: 2.705270528793335.\n",
            "Epoch 3676. Training loss: 0.76492840051651. Validation loss: 2.7052700519561768.\n",
            "Epoch 3677. Training loss: 0.7649247646331787. Validation loss: 2.7052695751190186.\n",
            "Epoch 3678. Training loss: 0.7649210095405579. Validation loss: 2.7052688598632812.\n",
            "Epoch 3679. Training loss: 0.7649173140525818. Validation loss: 2.7052688598632812.\n",
            "Epoch 3680. Training loss: 0.7649136185646057. Validation loss: 2.705267906188965.\n",
            "Epoch 3681. Training loss: 0.7649100422859192. Validation loss: 2.705267906188965.\n",
            "Epoch 3682. Training loss: 0.7649063467979431. Validation loss: 2.7052671909332275.\n",
            "Epoch 3683. Training loss: 0.7649025917053223. Validation loss: 2.7052667140960693.\n",
            "Epoch 3684. Training loss: 0.7648988366127014. Validation loss: 2.705266237258911.\n",
            "Epoch 3685. Training loss: 0.7648951411247253. Validation loss: 2.705265760421753.\n",
            "Epoch 3686. Training loss: 0.7648914456367493. Validation loss: 2.7052652835845947.\n",
            "Epoch 3687. Training loss: 0.764887809753418. Validation loss: 2.7052648067474365.\n",
            "Epoch 3688. Training loss: 0.7648840546607971. Validation loss: 2.7052643299102783.\n",
            "Epoch 3689. Training loss: 0.7648804187774658. Validation loss: 2.705263614654541.\n",
            "Epoch 3690. Training loss: 0.7648766040802002. Validation loss: 2.705263137817383.\n",
            "Epoch 3691. Training loss: 0.7648730278015137. Validation loss: 2.7052624225616455.\n",
            "Epoch 3692. Training loss: 0.764869213104248. Validation loss: 2.7052621841430664.\n",
            "Epoch 3693. Training loss: 0.7648656368255615. Validation loss: 2.705261468887329.\n",
            "Epoch 3694. Training loss: 0.7648618817329407. Validation loss: 2.70526123046875.\n",
            "Epoch 3695. Training loss: 0.7648582458496094. Validation loss: 2.705260753631592.\n",
            "Epoch 3696. Training loss: 0.7648544907569885. Validation loss: 2.7052602767944336.\n",
            "Epoch 3697. Training loss: 0.7648507952690125. Validation loss: 2.7052597999572754.\n",
            "Epoch 3698. Training loss: 0.7648470997810364. Validation loss: 2.705259084701538.\n",
            "Epoch 3699. Training loss: 0.7648434042930603. Validation loss: 2.70525860786438.\n",
            "Epoch 3700. Training loss: 0.7648396492004395. Validation loss: 2.705258369445801.\n",
            "Epoch 3701. Training loss: 0.7648360729217529. Validation loss: 2.7052576541900635.\n",
            "Epoch 3702. Training loss: 0.7648322582244873. Validation loss: 2.7052571773529053.\n",
            "Epoch 3703. Training loss: 0.7648285031318665. Validation loss: 2.705256938934326.\n",
            "Epoch 3704. Training loss: 0.7648248672485352. Validation loss: 2.705256462097168.\n",
            "Epoch 3705. Training loss: 0.7648212313652039. Validation loss: 2.7052555084228516.\n",
            "Epoch 3706. Training loss: 0.7648174166679382. Validation loss: 2.7052555084228516.\n",
            "Epoch 3707. Training loss: 0.7648137211799622. Validation loss: 2.7052547931671143.\n",
            "Epoch 3708. Training loss: 0.7648100852966309. Validation loss: 2.705254554748535.\n",
            "Epoch 3709. Training loss: 0.7648062705993652. Validation loss: 2.705254316329956.\n",
            "Epoch 3710. Training loss: 0.7648026347160339. Validation loss: 2.7052533626556396.\n",
            "Epoch 3711. Training loss: 0.7647988200187683. Validation loss: 2.7052526473999023.\n",
            "Epoch 3712. Training loss: 0.7647951245307922. Validation loss: 2.705252170562744.\n",
            "Epoch 3713. Training loss: 0.7647914290428162. Validation loss: 2.705251455307007.\n",
            "Epoch 3714. Training loss: 0.7647876143455505. Validation loss: 2.705251693725586.\n",
            "Epoch 3715. Training loss: 0.764784038066864. Validation loss: 2.7052512168884277.\n",
            "Epoch 3716. Training loss: 0.7647802829742432. Validation loss: 2.7052502632141113.\n",
            "Epoch 3717. Training loss: 0.7647765278816223. Validation loss: 2.705249786376953.\n",
            "Epoch 3718. Training loss: 0.764772891998291. Validation loss: 2.705249309539795.\n",
            "Epoch 3719. Training loss: 0.7647691369056702. Validation loss: 2.705249071121216.\n",
            "Epoch 3720. Training loss: 0.7647654414176941. Validation loss: 2.7052483558654785.\n",
            "Epoch 3721. Training loss: 0.7647616863250732. Validation loss: 2.7052478790283203.\n",
            "Epoch 3722. Training loss: 0.7647581100463867. Validation loss: 2.705247402191162.\n",
            "Epoch 3723. Training loss: 0.7647542357444763. Validation loss: 2.705246925354004.\n",
            "Epoch 3724. Training loss: 0.7647505402565002. Validation loss: 2.705246686935425.\n",
            "Epoch 3725. Training loss: 0.7647468447685242. Validation loss: 2.7052459716796875.\n",
            "Epoch 3726. Training loss: 0.7647431492805481. Validation loss: 2.7052457332611084.\n",
            "Epoch 3727. Training loss: 0.7647393345832825. Validation loss: 2.705245018005371.\n",
            "Epoch 3728. Training loss: 0.7647356390953064. Validation loss: 2.705244302749634.\n",
            "Epoch 3729. Training loss: 0.7647319436073303. Validation loss: 2.7052440643310547.\n",
            "Epoch 3730. Training loss: 0.7647281289100647. Validation loss: 2.7052435874938965.\n",
            "Epoch 3731. Training loss: 0.7647245526313782. Validation loss: 2.705242872238159.\n",
            "Epoch 3732. Training loss: 0.7647208571434021. Validation loss: 2.70524263381958.\n",
            "Epoch 3733. Training loss: 0.7647170424461365. Validation loss: 2.705242156982422.\n",
            "Epoch 3734. Training loss: 0.7647132277488708. Validation loss: 2.7052414417266846.\n",
            "Epoch 3735. Training loss: 0.76470947265625. Validation loss: 2.7052409648895264.\n",
            "Epoch 3736. Training loss: 0.7647057175636292. Validation loss: 2.7052407264709473.\n",
            "Epoch 3737. Training loss: 0.7647020816802979. Validation loss: 2.705239772796631.\n",
            "Epoch 3738. Training loss: 0.7646982669830322. Validation loss: 2.7052395343780518.\n",
            "Epoch 3739. Training loss: 0.7646946310997009. Validation loss: 2.7052390575408936.\n",
            "Epoch 3740. Training loss: 0.7646908760070801. Validation loss: 2.7052383422851562.\n",
            "Epoch 3741. Training loss: 0.7646872401237488. Validation loss: 2.7052383422851562.\n",
            "Epoch 3742. Training loss: 0.7646834254264832. Validation loss: 2.705237865447998.\n",
            "Epoch 3743. Training loss: 0.7646797299385071. Validation loss: 2.7052371501922607.\n",
            "Epoch 3744. Training loss: 0.7646758556365967. Validation loss: 2.7052369117736816.\n",
            "Epoch 3745. Training loss: 0.7646722197532654. Validation loss: 2.7052364349365234.\n",
            "Epoch 3746. Training loss: 0.7646684646606445. Validation loss: 2.7052359580993652.\n",
            "Epoch 3747. Training loss: 0.7646648287773132. Validation loss: 2.705235481262207.\n",
            "Epoch 3748. Training loss: 0.7646610140800476. Validation loss: 2.705235004425049.\n",
            "Epoch 3749. Training loss: 0.7646572589874268. Validation loss: 2.7052340507507324.\n",
            "Epoch 3750. Training loss: 0.7646535038948059. Validation loss: 2.7052338123321533.\n",
            "Epoch 3751. Training loss: 0.7646496891975403. Validation loss: 2.705233097076416.\n",
            "Epoch 3752. Training loss: 0.7646459937095642. Validation loss: 2.705232858657837.\n",
            "Epoch 3753. Training loss: 0.7646422386169434. Validation loss: 2.7052321434020996.\n",
            "Epoch 3754. Training loss: 0.7646386027336121. Validation loss: 2.7052319049835205.\n",
            "Epoch 3755. Training loss: 0.7646348476409912. Validation loss: 2.705231189727783.\n",
            "Epoch 3756. Training loss: 0.7646310925483704. Validation loss: 2.705230474472046.\n",
            "Epoch 3757. Training loss: 0.7646274566650391. Validation loss: 2.705230236053467.\n",
            "Epoch 3758. Training loss: 0.7646236419677734. Validation loss: 2.7052297592163086.\n",
            "Epoch 3759. Training loss: 0.7646198272705078. Validation loss: 2.7052292823791504.\n",
            "Epoch 3760. Training loss: 0.764616072177887. Validation loss: 2.705228805541992.\n",
            "Epoch 3761. Training loss: 0.7646123766899109. Validation loss: 2.705228090286255.\n",
            "Epoch 3762. Training loss: 0.7646085619926453. Validation loss: 2.7052276134490967.\n",
            "Epoch 3763. Training loss: 0.7646048665046692. Validation loss: 2.7052273750305176.\n",
            "Epoch 3764. Training loss: 0.7646010518074036. Validation loss: 2.7052268981933594.\n",
            "Epoch 3765. Training loss: 0.7645973563194275. Validation loss: 2.705226421356201.\n",
            "Epoch 3766. Training loss: 0.7645935416221619. Validation loss: 2.705225944519043.\n",
            "Epoch 3767. Training loss: 0.7645899653434753. Validation loss: 2.7052254676818848.\n",
            "Epoch 3768. Training loss: 0.7645861506462097. Validation loss: 2.7052247524261475.\n",
            "Epoch 3769. Training loss: 0.7645823359489441. Validation loss: 2.70522403717041.\n",
            "Epoch 3770. Training loss: 0.7645785808563232. Validation loss: 2.705223560333252.\n",
            "Epoch 3771. Training loss: 0.7645748257637024. Validation loss: 2.705223560333252.\n",
            "Epoch 3772. Training loss: 0.7645711302757263. Validation loss: 2.7052228450775146.\n",
            "Epoch 3773. Training loss: 0.7645673751831055. Validation loss: 2.7052223682403564.\n",
            "Epoch 3774. Training loss: 0.7645635604858398. Validation loss: 2.7052221298217773.\n",
            "Epoch 3775. Training loss: 0.764559805393219. Validation loss: 2.705221652984619.\n",
            "Epoch 3776. Training loss: 0.7645561099052429. Validation loss: 2.705220937728882.\n",
            "Epoch 3777. Training loss: 0.7645523548126221. Validation loss: 2.7052202224731445.\n",
            "Epoch 3778. Training loss: 0.7645485997200012. Validation loss: 2.7052197456359863.\n",
            "Epoch 3779. Training loss: 0.7645447850227356. Validation loss: 2.705219268798828.\n",
            "Epoch 3780. Training loss: 0.7645410895347595. Validation loss: 2.70521879196167.\n",
            "Epoch 3781. Training loss: 0.7645372748374939. Validation loss: 2.70521879196167.\n",
            "Epoch 3782. Training loss: 0.764533519744873. Validation loss: 2.7052178382873535.\n",
            "Epoch 3783. Training loss: 0.7645297646522522. Validation loss: 2.7052173614501953.\n",
            "Epoch 3784. Training loss: 0.7645259499549866. Validation loss: 2.705216646194458.\n",
            "Epoch 3785. Training loss: 0.7645222544670105. Validation loss: 2.705216646194458.\n",
            "Epoch 3786. Training loss: 0.7645183205604553. Validation loss: 2.7052161693573.\n",
            "Epoch 3787. Training loss: 0.764514684677124. Validation loss: 2.7052154541015625.\n",
            "Epoch 3788. Training loss: 0.7645108699798584. Validation loss: 2.7052149772644043.\n",
            "Epoch 3789. Training loss: 0.7645071148872375. Validation loss: 2.705214262008667.\n",
            "Epoch 3790. Training loss: 0.7645034193992615. Validation loss: 2.705214023590088.\n",
            "Epoch 3791. Training loss: 0.7644996047019958. Validation loss: 2.7052133083343506.\n",
            "Epoch 3792. Training loss: 0.764495849609375. Validation loss: 2.7052130699157715.\n",
            "Epoch 3793. Training loss: 0.7644920945167542. Validation loss: 2.7052125930786133.\n",
            "Epoch 3794. Training loss: 0.7644882798194885. Validation loss: 2.705212116241455.\n",
            "Epoch 3795. Training loss: 0.7644845843315125. Validation loss: 2.7052114009857178.\n",
            "Epoch 3796. Training loss: 0.7644807696342468. Validation loss: 2.7052114009857178.\n",
            "Epoch 3797. Training loss: 0.7644769549369812. Validation loss: 2.7052106857299805.\n",
            "Epoch 3798. Training loss: 0.7644731402397156. Validation loss: 2.7052102088928223.\n",
            "Epoch 3799. Training loss: 0.7644694447517395. Validation loss: 2.705209732055664.\n",
            "Epoch 3800. Training loss: 0.7644656300544739. Validation loss: 2.7052090167999268.\n",
            "Epoch 3801. Training loss: 0.7644619345664978. Validation loss: 2.7052087783813477.\n",
            "Epoch 3802. Training loss: 0.7644581198692322. Validation loss: 2.7052083015441895.\n",
            "Epoch 3803. Training loss: 0.7644543051719666. Validation loss: 2.7052078247070312.\n",
            "Epoch 3804. Training loss: 0.7644505500793457. Validation loss: 2.705206871032715.\n",
            "Epoch 3805. Training loss: 0.7644467949867249. Validation loss: 2.7052066326141357.\n",
            "Epoch 3806. Training loss: 0.7644430994987488. Validation loss: 2.7052061557769775.\n",
            "Epoch 3807. Training loss: 0.7644392848014832. Validation loss: 2.7052056789398193.\n",
            "Epoch 3808. Training loss: 0.7644354701042175. Validation loss: 2.7052054405212402.\n",
            "Epoch 3809. Training loss: 0.7644316554069519. Validation loss: 2.7052042484283447.\n",
            "Epoch 3810. Training loss: 0.7644278407096863. Validation loss: 2.7052040100097656.\n",
            "Epoch 3811. Training loss: 0.7644240856170654. Validation loss: 2.7052035331726074.\n",
            "Epoch 3812. Training loss: 0.7644202709197998. Validation loss: 2.705203056335449.\n",
            "Epoch 3813. Training loss: 0.7644166350364685. Validation loss: 2.705202579498291.\n",
            "Epoch 3814. Training loss: 0.7644128203392029. Validation loss: 2.7052018642425537.\n",
            "Epoch 3815. Training loss: 0.7644088864326477. Validation loss: 2.7052016258239746.\n",
            "Epoch 3816. Training loss: 0.7644050717353821. Validation loss: 2.7052011489868164.\n",
            "Epoch 3817. Training loss: 0.7644012570381165. Validation loss: 2.7052011489868164.\n",
            "Epoch 3818. Training loss: 0.7643976211547852. Validation loss: 2.7052001953125.\n",
            "Epoch 3819. Training loss: 0.7643938660621643. Validation loss: 2.705199718475342.\n",
            "Epoch 3820. Training loss: 0.7643899917602539. Validation loss: 2.7051992416381836.\n",
            "Epoch 3821. Training loss: 0.7643861770629883. Validation loss: 2.7051985263824463.\n",
            "Epoch 3822. Training loss: 0.7643824219703674. Validation loss: 2.705198287963867.\n",
            "Epoch 3823. Training loss: 0.7643786072731018. Validation loss: 2.705197811126709.\n",
            "Epoch 3824. Training loss: 0.7643749117851257. Validation loss: 2.705197334289551.\n",
            "Epoch 3825. Training loss: 0.7643710970878601. Validation loss: 2.7051966190338135.\n",
            "Epoch 3826. Training loss: 0.7643672823905945. Validation loss: 2.7051961421966553.\n",
            "Epoch 3827. Training loss: 0.7643635272979736. Validation loss: 2.705195903778076.\n",
            "Epoch 3828. Training loss: 0.7643596529960632. Validation loss: 2.705195188522339.\n",
            "Epoch 3829. Training loss: 0.7643559575080872. Validation loss: 2.7051947116851807.\n",
            "Epoch 3830. Training loss: 0.764352023601532. Validation loss: 2.7051944732666016.\n",
            "Epoch 3831. Training loss: 0.7643482685089111. Validation loss: 2.705193519592285.\n",
            "Epoch 3832. Training loss: 0.7643444538116455. Validation loss: 2.705193042755127.\n",
            "Epoch 3833. Training loss: 0.7643406987190247. Validation loss: 2.705192804336548.\n",
            "Epoch 3834. Training loss: 0.764336884021759. Validation loss: 2.7051923274993896.\n",
            "Epoch 3835. Training loss: 0.764333188533783. Validation loss: 2.7051916122436523.\n",
            "Epoch 3836. Training loss: 0.7643292546272278. Validation loss: 2.705190896987915.\n",
            "Epoch 3837. Training loss: 0.7643254399299622. Validation loss: 2.705190420150757.\n",
            "Epoch 3838. Training loss: 0.7643216252326965. Validation loss: 2.7051901817321777.\n",
            "Epoch 3839. Training loss: 0.7643179297447205. Validation loss: 2.7051894664764404.\n",
            "Epoch 3840. Training loss: 0.7643141150474548. Validation loss: 2.7051894664764404.\n",
            "Epoch 3841. Training loss: 0.7643103003501892. Validation loss: 2.705188751220703.\n",
            "Epoch 3842. Training loss: 0.7643064856529236. Validation loss: 2.705188274383545.\n",
            "Epoch 3843. Training loss: 0.764302670955658. Validation loss: 2.7051877975463867.\n",
            "Epoch 3844. Training loss: 0.7642988562583923. Validation loss: 2.7051873207092285.\n",
            "Epoch 3845. Training loss: 0.7642950415611267. Validation loss: 2.7051870822906494.\n",
            "Epoch 3846. Training loss: 0.7642912268638611. Validation loss: 2.705186367034912.\n",
            "Epoch 3847. Training loss: 0.7642874717712402. Validation loss: 2.705185651779175.\n",
            "Epoch 3848. Training loss: 0.7642836570739746. Validation loss: 2.7051851749420166.\n",
            "Epoch 3849. Training loss: 0.764279842376709. Validation loss: 2.7051844596862793.\n",
            "Epoch 3850. Training loss: 0.7642759680747986. Validation loss: 2.7051842212677.\n",
            "Epoch 3851. Training loss: 0.7642722725868225. Validation loss: 2.705183982849121.\n",
            "Epoch 3852. Training loss: 0.7642683982849121. Validation loss: 2.705183506011963.\n",
            "Epoch 3853. Training loss: 0.7642645835876465. Validation loss: 2.7051830291748047.\n",
            "Epoch 3854. Training loss: 0.7642607688903809. Validation loss: 2.7051823139190674.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3855. Training loss: 0.7642569541931152. Validation loss: 2.7051820755004883.\n",
            "Epoch 3856. Training loss: 0.7642531394958496. Validation loss: 2.70518159866333.\n",
            "Epoch 3857. Training loss: 0.764249324798584. Validation loss: 2.705181121826172.\n",
            "Epoch 3858. Training loss: 0.7642455101013184. Validation loss: 2.7051804065704346.\n",
            "Epoch 3859. Training loss: 0.764241635799408. Validation loss: 2.7051799297332764.\n",
            "Epoch 3860. Training loss: 0.7642378807067871. Validation loss: 2.705179452896118.\n",
            "Epoch 3861. Training loss: 0.7642340660095215. Validation loss: 2.705178737640381.\n",
            "Epoch 3862. Training loss: 0.7642302513122559. Validation loss: 2.7051782608032227.\n",
            "Epoch 3863. Training loss: 0.7642264366149902. Validation loss: 2.7051777839660645.\n",
            "Epoch 3864. Training loss: 0.7642226219177246. Validation loss: 2.7051773071289062.\n",
            "Epoch 3865. Training loss: 0.764218807220459. Validation loss: 2.705177068710327.\n",
            "Epoch 3866. Training loss: 0.7642149329185486. Validation loss: 2.705176591873169.\n",
            "Epoch 3867. Training loss: 0.764211118221283. Validation loss: 2.7051758766174316.\n",
            "Epoch 3868. Training loss: 0.7642073631286621. Validation loss: 2.7051753997802734.\n",
            "Epoch 3869. Training loss: 0.7642035484313965. Validation loss: 2.705174684524536.\n",
            "Epoch 3870. Training loss: 0.7641996741294861. Validation loss: 2.705174446105957.\n",
            "Epoch 3871. Training loss: 0.7641959190368652. Validation loss: 2.705173969268799.\n",
            "Epoch 3872. Training loss: 0.7641920447349548. Validation loss: 2.7051734924316406.\n",
            "Epoch 3873. Training loss: 0.7641882300376892. Validation loss: 2.7051730155944824.\n",
            "Epoch 3874. Training loss: 0.7641844153404236. Validation loss: 2.705172538757324.\n",
            "Epoch 3875. Training loss: 0.7641804814338684. Validation loss: 2.705172061920166.\n",
            "Epoch 3876. Training loss: 0.7641767859458923. Validation loss: 2.705171585083008.\n",
            "Epoch 3877. Training loss: 0.7641728520393372. Validation loss: 2.7051711082458496.\n",
            "Epoch 3878. Training loss: 0.7641690373420715. Validation loss: 2.7051706314086914.\n",
            "Epoch 3879. Training loss: 0.7641651630401611. Validation loss: 2.705169677734375.\n",
            "Epoch 3880. Training loss: 0.7641613483428955. Validation loss: 2.705169439315796.\n",
            "Epoch 3881. Training loss: 0.7641575336456299. Validation loss: 2.7051689624786377.\n",
            "Epoch 3882. Training loss: 0.7641536593437195. Validation loss: 2.7051684856414795.\n",
            "Epoch 3883. Training loss: 0.7641499042510986. Validation loss: 2.7051680088043213.\n",
            "Epoch 3884. Training loss: 0.7641460299491882. Validation loss: 2.705167293548584.\n",
            "Epoch 3885. Training loss: 0.7641422152519226. Validation loss: 2.705167055130005.\n",
            "Epoch 3886. Training loss: 0.764138400554657. Validation loss: 2.7051663398742676.\n",
            "Epoch 3887. Training loss: 0.7641345858573914. Validation loss: 2.7051663398742676.\n",
            "Epoch 3888. Training loss: 0.7641306519508362. Validation loss: 2.7051658630371094.\n",
            "Epoch 3889. Training loss: 0.7641267776489258. Validation loss: 2.705164909362793.\n",
            "Epoch 3890. Training loss: 0.7641230225563049. Validation loss: 2.7051641941070557.\n",
            "Epoch 3891. Training loss: 0.7641192078590393. Validation loss: 2.7051639556884766.\n",
            "Epoch 3892. Training loss: 0.7641153335571289. Validation loss: 2.7051634788513184.\n",
            "Epoch 3893. Training loss: 0.7641115188598633. Validation loss: 2.7051632404327393.\n",
            "Epoch 3894. Training loss: 0.7641077041625977. Validation loss: 2.7051620483398438.\n",
            "Epoch 3895. Training loss: 0.7641037106513977. Validation loss: 2.7051620483398438.\n",
            "Epoch 3896. Training loss: 0.7640998959541321. Validation loss: 2.7051610946655273.\n",
            "Epoch 3897. Training loss: 0.764096200466156. Validation loss: 2.7051610946655273.\n",
            "Epoch 3898. Training loss: 0.764092206954956. Validation loss: 2.705160140991211.\n",
            "Epoch 3899. Training loss: 0.7640883922576904. Validation loss: 2.705160140991211.\n",
            "Epoch 3900. Training loss: 0.7640845775604248. Validation loss: 2.7051596641540527.\n",
            "Epoch 3901. Training loss: 0.7640807032585144. Validation loss: 2.7051591873168945.\n",
            "Epoch 3902. Training loss: 0.7640769481658936. Validation loss: 2.7051587104797363.\n",
            "Epoch 3903. Training loss: 0.7640731334686279. Validation loss: 2.705157995223999.\n",
            "Epoch 3904. Training loss: 0.7640692591667175. Validation loss: 2.70515775680542.\n",
            "Epoch 3905. Training loss: 0.7640653252601624. Validation loss: 2.7051572799682617.\n",
            "Epoch 3906. Training loss: 0.7640615105628967. Validation loss: 2.7051563262939453.\n",
            "Epoch 3907. Training loss: 0.7640576362609863. Validation loss: 2.705156087875366.\n",
            "Epoch 3908. Training loss: 0.7640538215637207. Validation loss: 2.705155849456787.\n",
            "Epoch 3909. Training loss: 0.7640500068664551. Validation loss: 2.7051548957824707.\n",
            "Epoch 3910. Training loss: 0.7640461325645447. Validation loss: 2.7051548957824707.\n",
            "Epoch 3911. Training loss: 0.764042317867279. Validation loss: 2.7051539421081543.\n",
            "Epoch 3912. Training loss: 0.7640383839607239. Validation loss: 2.705153465270996.\n",
            "Epoch 3913. Training loss: 0.7640345096588135. Validation loss: 2.705152988433838.\n",
            "Epoch 3914. Training loss: 0.7640306949615479. Validation loss: 2.705152750015259.\n",
            "Epoch 3915. Training loss: 0.7640268206596375. Validation loss: 2.7051517963409424.\n",
            "Epoch 3916. Training loss: 0.7640230059623718. Validation loss: 2.7051515579223633.\n",
            "Epoch 3917. Training loss: 0.7640190720558167. Validation loss: 2.705151081085205.\n",
            "Epoch 3918. Training loss: 0.764015257358551. Validation loss: 2.705150604248047.\n",
            "Epoch 3919. Training loss: 0.7640113830566406. Validation loss: 2.7051498889923096.\n",
            "Epoch 3920. Training loss: 0.764007568359375. Validation loss: 2.7051496505737305.\n",
            "Epoch 3921. Training loss: 0.7640036940574646. Validation loss: 2.7051491737365723.\n",
            "Epoch 3922. Training loss: 0.763999879360199. Validation loss: 2.705148696899414.\n",
            "Epoch 3923. Training loss: 0.7639959454536438. Validation loss: 2.7051479816436768.\n",
            "Epoch 3924. Training loss: 0.7639920711517334. Validation loss: 2.7051475048065186.\n",
            "Epoch 3925. Training loss: 0.7639882564544678. Validation loss: 2.7051472663879395.\n",
            "Epoch 3926. Training loss: 0.7639843821525574. Validation loss: 2.705146551132202.\n",
            "Epoch 3927. Training loss: 0.7639804482460022. Validation loss: 2.705146074295044.\n",
            "Epoch 3928. Training loss: 0.7639766335487366. Validation loss: 2.7051453590393066.\n",
            "Epoch 3929. Training loss: 0.7639727592468262. Validation loss: 2.7051448822021484.\n",
            "Epoch 3930. Training loss: 0.7639689445495605. Validation loss: 2.7051444053649902.\n",
            "Epoch 3931. Training loss: 0.7639649510383606. Validation loss: 2.705144166946411.\n",
            "Epoch 3932. Training loss: 0.7639610767364502. Validation loss: 2.705143690109253.\n",
            "Epoch 3933. Training loss: 0.7639572620391846. Validation loss: 2.7051429748535156.\n",
            "Epoch 3934. Training loss: 0.763953447341919. Validation loss: 2.7051424980163574.\n",
            "Epoch 3935. Training loss: 0.7639495730400085. Validation loss: 2.705142021179199.\n",
            "Epoch 3936. Training loss: 0.7639455795288086. Validation loss: 2.705141305923462.\n",
            "Epoch 3937. Training loss: 0.7639418244361877. Validation loss: 2.705141305923462.\n",
            "Epoch 3938. Training loss: 0.7639378905296326. Validation loss: 2.7051408290863037.\n",
            "Epoch 3939. Training loss: 0.7639340758323669. Validation loss: 2.7051401138305664.\n",
            "Epoch 3940. Training loss: 0.763930082321167. Validation loss: 2.705139398574829.\n",
            "Epoch 3941. Training loss: 0.7639262676239014. Validation loss: 2.705138683319092.\n",
            "Epoch 3942. Training loss: 0.7639224529266357. Validation loss: 2.705138683319092.\n",
            "Epoch 3943. Training loss: 0.7639185786247253. Validation loss: 2.7051382064819336.\n",
            "Epoch 3944. Training loss: 0.7639146447181702. Validation loss: 2.705137252807617.\n",
            "Epoch 3945. Training loss: 0.763910710811615. Validation loss: 2.705137014389038.\n",
            "Epoch 3946. Training loss: 0.7639068961143494. Validation loss: 2.705136299133301.\n",
            "Epoch 3947. Training loss: 0.7639030814170837. Validation loss: 2.7051360607147217.\n",
            "Epoch 3948. Training loss: 0.7638990879058838. Validation loss: 2.7051353454589844.\n",
            "Epoch 3949. Training loss: 0.7638952136039734. Validation loss: 2.705134868621826.\n",
            "Epoch 3950. Training loss: 0.7638913989067078. Validation loss: 2.705134391784668.\n",
            "Epoch 3951. Training loss: 0.7638874650001526. Validation loss: 2.7051339149475098.\n",
            "Epoch 3952. Training loss: 0.7638835906982422. Validation loss: 2.7051336765289307.\n",
            "Epoch 3953. Training loss: 0.7638797163963318. Validation loss: 2.7051331996917725.\n",
            "Epoch 3954. Training loss: 0.7638759016990662. Validation loss: 2.7051327228546143.\n",
            "Epoch 3955. Training loss: 0.7638719081878662. Validation loss: 2.705132007598877.\n",
            "Epoch 3956. Training loss: 0.7638680934906006. Validation loss: 2.705131769180298.\n",
            "Epoch 3957. Training loss: 0.7638642191886902. Validation loss: 2.7051310539245605.\n",
            "Epoch 3958. Training loss: 0.763860285282135. Validation loss: 2.7051303386688232.\n",
            "Epoch 3959. Training loss: 0.7638564109802246. Validation loss: 2.7051303386688232.\n",
            "Epoch 3960. Training loss: 0.7638525366783142. Validation loss: 2.705129623413086.\n",
            "Epoch 3961. Training loss: 0.763848602771759. Validation loss: 2.7051291465759277.\n",
            "Epoch 3962. Training loss: 0.7638447284698486. Validation loss: 2.7051284313201904.\n",
            "Epoch 3963. Training loss: 0.7638408541679382. Validation loss: 2.7051281929016113.\n",
            "Epoch 3964. Training loss: 0.7638370394706726. Validation loss: 2.705127716064453.\n",
            "Epoch 3965. Training loss: 0.7638331055641174. Validation loss: 2.705127239227295.\n",
            "Epoch 3966. Training loss: 0.7638290524482727. Validation loss: 2.7051265239715576.\n",
            "Epoch 3967. Training loss: 0.7638252377510071. Validation loss: 2.7051258087158203.\n",
            "Epoch 3968. Training loss: 0.7638214230537415. Validation loss: 2.7051260471343994.\n",
            "Epoch 3969. Training loss: 0.7638174891471863. Validation loss: 2.705124855041504.\n",
            "Epoch 3970. Training loss: 0.7638135552406311. Validation loss: 2.7051243782043457.\n",
            "Epoch 3971. Training loss: 0.7638096809387207. Validation loss: 2.7051239013671875.\n",
            "Epoch 3972. Training loss: 0.7638056874275208. Validation loss: 2.7051234245300293.\n",
            "Epoch 3973. Training loss: 0.7638018727302551. Validation loss: 2.70512318611145.\n",
            "Epoch 3974. Training loss: 0.7637979984283447. Validation loss: 2.705122709274292.\n",
            "Epoch 3975. Training loss: 0.7637941241264343. Validation loss: 2.7051219940185547.\n",
            "Epoch 3976. Training loss: 0.7637901902198792. Validation loss: 2.7051215171813965.\n",
            "Epoch 3977. Training loss: 0.763786256313324. Validation loss: 2.705120801925659.\n",
            "Epoch 3978. Training loss: 0.7637824416160583. Validation loss: 2.705120325088501.\n",
            "Epoch 3979. Training loss: 0.7637785077095032. Validation loss: 2.705120325088501.\n",
            "Epoch 3980. Training loss: 0.763774573802948. Validation loss: 2.7051196098327637.\n",
            "Epoch 3981. Training loss: 0.7637706398963928. Validation loss: 2.7051188945770264.\n",
            "Epoch 3982. Training loss: 0.7637667655944824. Validation loss: 2.7051186561584473.\n",
            "Epoch 3983. Training loss: 0.7637627720832825. Validation loss: 2.705118179321289.\n",
            "Epoch 3984. Training loss: 0.7637589573860168. Validation loss: 2.7051174640655518.\n",
            "Epoch 3985. Training loss: 0.7637550234794617. Validation loss: 2.7051172256469727.\n",
            "Epoch 3986. Training loss: 0.763751208782196. Validation loss: 2.7051167488098145.\n",
            "Epoch 3987. Training loss: 0.7637472152709961. Validation loss: 2.705116033554077.\n",
            "Epoch 3988. Training loss: 0.7637433409690857. Validation loss: 2.70511531829834.\n",
            "Epoch 3989. Training loss: 0.7637393474578857. Validation loss: 2.7051148414611816.\n",
            "Epoch 3990. Training loss: 0.7637355327606201. Validation loss: 2.7051148414611816.\n",
            "Epoch 3991. Training loss: 0.7637314796447754. Validation loss: 2.7051141262054443.\n",
            "Epoch 3992. Training loss: 0.763727605342865. Validation loss: 2.705113410949707.\n",
            "Epoch 3993. Training loss: 0.7637236714363098. Validation loss: 2.705112934112549.\n",
            "Epoch 3994. Training loss: 0.7637198567390442. Validation loss: 2.7051124572753906.\n",
            "Epoch 3995. Training loss: 0.763715922832489. Validation loss: 2.7051119804382324.\n",
            "Epoch 3996. Training loss: 0.7637119293212891. Validation loss: 2.705111503601074.\n",
            "Epoch 3997. Training loss: 0.7637081146240234. Validation loss: 2.705111026763916.\n",
            "Epoch 3998. Training loss: 0.7637041211128235. Validation loss: 2.7051103115081787.\n",
            "Epoch 3999. Training loss: 0.7637001872062683. Validation loss: 2.7051098346710205.\n",
            "Epoch 4000. Training loss: 0.7636963725090027. Validation loss: 2.7051093578338623.\n",
            "Epoch 4001. Training loss: 0.763692319393158. Validation loss: 2.705108880996704.\n",
            "Epoch 4002. Training loss: 0.7636883854866028. Validation loss: 2.705108880996704.\n",
            "Epoch 4003. Training loss: 0.7636845111846924. Validation loss: 2.7051076889038086.\n",
            "Epoch 4004. Training loss: 0.7636805176734924. Validation loss: 2.7051074504852295.\n",
            "Epoch 4005. Training loss: 0.763676643371582. Validation loss: 2.705106735229492.\n",
            "Epoch 4006. Training loss: 0.7636727690696716. Validation loss: 2.705106258392334.\n",
            "Epoch 4007. Training loss: 0.7636688351631165. Validation loss: 2.705106258392334.\n",
            "Epoch 4008. Training loss: 0.7636649012565613. Validation loss: 2.7051053047180176.\n",
            "Epoch 4009. Training loss: 0.7636609077453613. Validation loss: 2.7051048278808594.\n",
            "Epoch 4010. Training loss: 0.7636570930480957. Validation loss: 2.705104351043701.\n",
            "Epoch 4011. Training loss: 0.7636530995368958. Validation loss: 2.705103874206543.\n",
            "Epoch 4012. Training loss: 0.7636492252349854. Validation loss: 2.7051033973693848.\n",
            "Epoch 4013. Training loss: 0.7636452317237854. Validation loss: 2.7051029205322266.\n",
            "Epoch 4014. Training loss: 0.763641357421875. Validation loss: 2.7051022052764893.\n",
            "Epoch 4015. Training loss: 0.7636374831199646. Validation loss: 2.70510196685791.\n",
            "Epoch 4016. Training loss: 0.7636334300041199. Validation loss: 2.705101490020752.\n",
            "Epoch 4017. Training loss: 0.7636296153068542. Validation loss: 2.7051010131835938.\n",
            "Epoch 4018. Training loss: 0.7636256217956543. Validation loss: 2.7050998210906982.\n",
            "Epoch 4019. Training loss: 0.7636216282844543. Validation loss: 2.7051000595092773.\n",
            "Epoch 4020. Training loss: 0.7636176943778992. Validation loss: 2.705099105834961.\n",
            "Epoch 4021. Training loss: 0.763613760471344. Validation loss: 2.705098867416382.\n",
            "Epoch 4022. Training loss: 0.7636098265647888. Validation loss: 2.7050983905792236.\n",
            "Epoch 4023. Training loss: 0.7636058926582336. Validation loss: 2.7050981521606445.\n",
            "Epoch 4024. Training loss: 0.7636020183563232. Validation loss: 2.7050976753234863.\n",
            "Epoch 4025. Training loss: 0.7635980248451233. Validation loss: 2.705096960067749.\n",
            "Epoch 4026. Training loss: 0.7635941505432129. Validation loss: 2.705096483230591.\n",
            "Epoch 4027. Training loss: 0.7635901570320129. Validation loss: 2.7050957679748535.\n",
            "Epoch 4028. Training loss: 0.7635862231254578. Validation loss: 2.7050952911376953.\n",
            "Epoch 4029. Training loss: 0.7635822296142578. Validation loss: 2.705094814300537.\n",
            "Epoch 4030. Training loss: 0.7635783553123474. Validation loss: 2.705094337463379.\n",
            "Epoch 4031. Training loss: 0.7635743021965027. Validation loss: 2.7050938606262207.\n",
            "Epoch 4032. Training loss: 0.7635704874992371. Validation loss: 2.7050931453704834.\n",
            "Epoch 4033. Training loss: 0.7635664939880371. Validation loss: 2.7050929069519043.\n",
            "Epoch 4034. Training loss: 0.7635625004768372. Validation loss: 2.705092430114746.\n",
            "Epoch 4035. Training loss: 0.7635586261749268. Validation loss: 2.705091953277588.\n",
            "Epoch 4036. Training loss: 0.763554573059082. Validation loss: 2.7050909996032715.\n",
            "Epoch 4037. Training loss: 0.7635507583618164. Validation loss: 2.7050907611846924.\n",
            "Epoch 4038. Training loss: 0.7635467648506165. Validation loss: 2.7050905227661133.\n",
            "Epoch 4039. Training loss: 0.7635428309440613. Validation loss: 2.705089807510376.\n",
            "Epoch 4040. Training loss: 0.7635388374328613. Validation loss: 2.705089569091797.\n",
            "Epoch 4041. Training loss: 0.7635349631309509. Validation loss: 2.7050886154174805.\n",
            "Epoch 4042. Training loss: 0.763530969619751. Validation loss: 2.7050883769989014.\n",
            "Epoch 4043. Training loss: 0.7635269165039062. Validation loss: 2.705087900161743.\n",
            "Epoch 4044. Training loss: 0.7635230422019958. Validation loss: 2.705087423324585.\n",
            "Epoch 4045. Training loss: 0.7635190486907959. Validation loss: 2.7050867080688477.\n",
            "Epoch 4046. Training loss: 0.7635151743888855. Validation loss: 2.7050859928131104.\n",
            "Epoch 4047. Training loss: 0.7635111212730408. Validation loss: 2.705085515975952.\n",
            "Epoch 4048. Training loss: 0.7635071873664856. Validation loss: 2.705085277557373.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4049. Training loss: 0.7635032534599304. Validation loss: 2.7050845623016357.\n",
            "Epoch 4050. Training loss: 0.7634992599487305. Validation loss: 2.7050843238830566.\n",
            "Epoch 4051. Training loss: 0.7634952664375305. Validation loss: 2.7050838470458984.\n",
            "Epoch 4052. Training loss: 0.7634913325309753. Validation loss: 2.7050833702087402.\n",
            "Epoch 4053. Training loss: 0.7634873390197754. Validation loss: 2.705082893371582.\n",
            "Epoch 4054. Training loss: 0.7634835243225098. Validation loss: 2.7050821781158447.\n",
            "Epoch 4055. Training loss: 0.763479471206665. Validation loss: 2.7050817012786865.\n",
            "Epoch 4056. Training loss: 0.7634754776954651. Validation loss: 2.7050812244415283.\n",
            "Epoch 4057. Training loss: 0.7634714245796204. Validation loss: 2.70508074760437.\n",
            "Epoch 4058. Training loss: 0.7634676098823547. Validation loss: 2.705080032348633.\n",
            "Epoch 4059. Training loss: 0.7634636759757996. Validation loss: 2.7050795555114746.\n",
            "Epoch 4060. Training loss: 0.7634596824645996. Validation loss: 2.7050793170928955.\n",
            "Epoch 4061. Training loss: 0.7634556293487549. Validation loss: 2.705078601837158.\n",
            "Epoch 4062. Training loss: 0.7634517550468445. Validation loss: 2.705078125.\n",
            "Epoch 4063. Training loss: 0.7634477615356445. Validation loss: 2.705077886581421.\n",
            "Epoch 4064. Training loss: 0.7634438872337341. Validation loss: 2.7050774097442627.\n",
            "Epoch 4065. Training loss: 0.7634398937225342. Validation loss: 2.7050769329071045.\n",
            "Epoch 4066. Training loss: 0.7634358406066895. Validation loss: 2.705075979232788.\n",
            "Epoch 4067. Training loss: 0.7634318470954895. Validation loss: 2.705075740814209.\n",
            "Epoch 4068. Training loss: 0.7634279131889343. Validation loss: 2.705075263977051.\n",
            "Epoch 4069. Training loss: 0.7634239196777344. Validation loss: 2.7050747871398926.\n",
            "Epoch 4070. Training loss: 0.7634199261665344. Validation loss: 2.7050740718841553.\n",
            "Epoch 4071. Training loss: 0.7634159922599792. Validation loss: 2.705073833465576.\n",
            "Epoch 4072. Training loss: 0.7634119987487793. Validation loss: 2.705073595046997.\n",
            "Epoch 4073. Training loss: 0.7634079456329346. Validation loss: 2.7050724029541016.\n",
            "Epoch 4074. Training loss: 0.7634039521217346. Validation loss: 2.7050719261169434.\n",
            "Epoch 4075. Training loss: 0.7634000182151794. Validation loss: 2.7050716876983643.\n",
            "Epoch 4076. Training loss: 0.7633960843086243. Validation loss: 2.705071210861206.\n",
            "Epoch 4077. Training loss: 0.7633921504020691. Validation loss: 2.7050704956054688.\n",
            "Epoch 4078. Training loss: 0.7633881568908691. Validation loss: 2.7050700187683105.\n",
            "Epoch 4079. Training loss: 0.7633841037750244. Validation loss: 2.7050697803497314.\n",
            "Epoch 4080. Training loss: 0.763380229473114. Validation loss: 2.705069065093994.\n",
            "Epoch 4081. Training loss: 0.7633761763572693. Validation loss: 2.705068588256836.\n",
            "Epoch 4082. Training loss: 0.7633722424507141. Validation loss: 2.7050681114196777.\n",
            "Epoch 4083. Training loss: 0.7633681297302246. Validation loss: 2.7050676345825195.\n",
            "Epoch 4084. Training loss: 0.7633642554283142. Validation loss: 2.7050669193267822.\n",
            "Epoch 4085. Training loss: 0.763360321521759. Validation loss: 2.705066442489624.\n",
            "Epoch 4086. Training loss: 0.7633563876152039. Validation loss: 2.705065965652466.\n",
            "Epoch 4087. Training loss: 0.7633523344993591. Validation loss: 2.7050654888153076.\n",
            "Epoch 4088. Training loss: 0.7633482813835144. Validation loss: 2.705064535140991.\n",
            "Epoch 4089. Training loss: 0.7633442878723145. Validation loss: 2.705064058303833.\n",
            "Epoch 4090. Training loss: 0.7633402943611145. Validation loss: 2.705064058303833.\n",
            "Epoch 4091. Training loss: 0.7633363604545593. Validation loss: 2.705063581466675.\n",
            "Epoch 4092. Training loss: 0.7633323073387146. Validation loss: 2.7050628662109375.\n",
            "Epoch 4093. Training loss: 0.7633283138275146. Validation loss: 2.7050626277923584.\n",
            "Epoch 4094. Training loss: 0.7633244395256042. Validation loss: 2.705061912536621.\n",
            "Epoch 4095. Training loss: 0.7633203864097595. Validation loss: 2.705061435699463.\n",
            "Epoch 4096. Training loss: 0.7633164525032043. Validation loss: 2.7050609588623047.\n",
            "Epoch 4097. Training loss: 0.7633123397827148. Validation loss: 2.7050604820251465.\n",
            "Epoch 4098. Training loss: 0.7633083462715149. Validation loss: 2.705059766769409.\n",
            "Epoch 4099. Training loss: 0.7633042931556702. Validation loss: 2.70505952835083.\n",
            "Epoch 4100. Training loss: 0.7633004188537598. Validation loss: 2.705059051513672.\n",
            "Epoch 4101. Training loss: 0.763296365737915. Validation loss: 2.7050583362579346.\n",
            "Epoch 4102. Training loss: 0.7632924914360046. Validation loss: 2.7050580978393555.\n",
            "Epoch 4103. Training loss: 0.7632883191108704. Validation loss: 2.7050576210021973.\n",
            "Epoch 4104. Training loss: 0.76328444480896. Validation loss: 2.705056667327881.\n",
            "Epoch 4105. Training loss: 0.7632803916931152. Validation loss: 2.7050564289093018.\n",
            "Epoch 4106. Training loss: 0.7632763981819153. Validation loss: 2.7050557136535645.\n",
            "Epoch 4107. Training loss: 0.7632724642753601. Validation loss: 2.7050554752349854.\n",
            "Epoch 4108. Training loss: 0.7632684111595154. Validation loss: 2.705054998397827.\n",
            "Epoch 4109. Training loss: 0.7632643580436707. Validation loss: 2.70505428314209.\n",
            "Epoch 4110. Training loss: 0.7632603645324707. Validation loss: 2.7050540447235107.\n",
            "Epoch 4111. Training loss: 0.7632563710212708. Validation loss: 2.7050533294677734.\n",
            "Epoch 4112. Training loss: 0.7632524371147156. Validation loss: 2.705052614212036.\n",
            "Epoch 4113. Training loss: 0.7632483839988708. Validation loss: 2.705052137374878.\n",
            "Epoch 4114. Training loss: 0.7632443904876709. Validation loss: 2.7050514221191406.\n",
            "Epoch 4115. Training loss: 0.7632403373718262. Validation loss: 2.7050514221191406.\n",
            "Epoch 4116. Training loss: 0.7632363438606262. Validation loss: 2.7050507068634033.\n",
            "Epoch 4117. Training loss: 0.763232409954071. Validation loss: 2.705050230026245.\n",
            "Epoch 4118. Training loss: 0.7632283568382263. Validation loss: 2.705049753189087.\n",
            "Epoch 4119. Training loss: 0.7632243037223816. Validation loss: 2.7050492763519287.\n",
            "Epoch 4120. Training loss: 0.7632203102111816. Validation loss: 2.7050485610961914.\n",
            "Epoch 4121. Training loss: 0.7632163166999817. Validation loss: 2.705048084259033.\n",
            "Epoch 4122. Training loss: 0.763212263584137. Validation loss: 2.705047607421875.\n",
            "Epoch 4123. Training loss: 0.7632083296775818. Validation loss: 2.705047130584717.\n",
            "Epoch 4124. Training loss: 0.7632042765617371. Validation loss: 2.705047130584717.\n",
            "Epoch 4125. Training loss: 0.7632002830505371. Validation loss: 2.7050461769104004.\n",
            "Epoch 4126. Training loss: 0.7631962299346924. Validation loss: 2.705045700073242.\n",
            "Epoch 4127. Training loss: 0.7631921768188477. Validation loss: 2.705045223236084.\n",
            "Epoch 4128. Training loss: 0.7631881833076477. Validation loss: 2.705044984817505.\n",
            "Epoch 4129. Training loss: 0.763184130191803. Validation loss: 2.7050445079803467.\n",
            "Epoch 4130. Training loss: 0.7631801962852478. Validation loss: 2.7050435543060303.\n",
            "Epoch 4131. Training loss: 0.7631761431694031. Validation loss: 2.705043077468872.\n",
            "Epoch 4132. Training loss: 0.7631721496582031. Validation loss: 2.705042839050293.\n",
            "Epoch 4133. Training loss: 0.7631680369377136. Validation loss: 2.7050418853759766.\n",
            "Epoch 4134. Training loss: 0.7631641030311584. Validation loss: 2.7050416469573975.\n",
            "Epoch 4135. Training loss: 0.7631600499153137. Validation loss: 2.7050414085388184.\n",
            "Epoch 4136. Training loss: 0.763155996799469. Validation loss: 2.705040693283081.\n",
            "Epoch 4137. Training loss: 0.7631519436836243. Validation loss: 2.7050399780273438.\n",
            "Epoch 4138. Training loss: 0.7631478309631348. Validation loss: 2.7050395011901855.\n",
            "Epoch 4139. Training loss: 0.7631439566612244. Validation loss: 2.7050392627716064.\n",
            "Epoch 4140. Training loss: 0.7631399035453796. Validation loss: 2.70503830909729.\n",
            "Epoch 4141. Training loss: 0.7631359100341797. Validation loss: 2.70503830909729.\n",
            "Epoch 4142. Training loss: 0.763131856918335. Validation loss: 2.7050375938415527.\n",
            "Epoch 4143. Training loss: 0.7631278038024902. Validation loss: 2.7050368785858154.\n",
            "Epoch 4144. Training loss: 0.7631238102912903. Validation loss: 2.7050364017486572.\n",
            "Epoch 4145. Training loss: 0.7631197571754456. Validation loss: 2.705035924911499.\n",
            "Epoch 4146. Training loss: 0.7631157040596008. Validation loss: 2.705035448074341.\n",
            "Epoch 4147. Training loss: 0.7631116509437561. Validation loss: 2.7050349712371826.\n",
            "Epoch 4148. Training loss: 0.7631077170372009. Validation loss: 2.7050347328186035.\n",
            "Epoch 4149. Training loss: 0.763103723526001. Validation loss: 2.705034017562866.\n",
            "Epoch 4150. Training loss: 0.7630996704101562. Validation loss: 2.705033540725708.\n",
            "Epoch 4151. Training loss: 0.7630955576896667. Validation loss: 2.7050328254699707.\n",
            "Epoch 4152. Training loss: 0.763091504573822. Validation loss: 2.7050323486328125.\n",
            "Epoch 4153. Training loss: 0.7630874514579773. Validation loss: 2.705031394958496.\n",
            "Epoch 4154. Training loss: 0.7630834579467773. Validation loss: 2.705031394958496.\n",
            "Epoch 4155. Training loss: 0.7630794048309326. Validation loss: 2.705030918121338.\n",
            "Epoch 4156. Training loss: 0.7630754113197327. Validation loss: 2.7050304412841797.\n",
            "Epoch 4157. Training loss: 0.7630713582038879. Validation loss: 2.7050299644470215.\n",
            "Epoch 4158. Training loss: 0.7630672454833984. Validation loss: 2.7050294876098633.\n",
            "Epoch 4159. Training loss: 0.763063371181488. Validation loss: 2.705028772354126.\n",
            "Epoch 4160. Training loss: 0.7630591988563538. Validation loss: 2.705028533935547.\n",
            "Epoch 4161. Training loss: 0.763055145740509. Validation loss: 2.7050278186798096.\n",
            "Epoch 4162. Training loss: 0.7630510926246643. Validation loss: 2.7050273418426514.\n",
            "Epoch 4163. Training loss: 0.7630471587181091. Validation loss: 2.7050271034240723.\n",
            "Epoch 4164. Training loss: 0.7630431056022644. Validation loss: 2.705026149749756.\n",
            "Epoch 4165. Training loss: 0.7630390524864197. Validation loss: 2.7050259113311768.\n",
            "Epoch 4166. Training loss: 0.763034999370575. Validation loss: 2.7050254344940186.\n",
            "Epoch 4167. Training loss: 0.763031005859375. Validation loss: 2.7050247192382812.\n",
            "Epoch 4168. Training loss: 0.7630269527435303. Validation loss: 2.705024242401123.\n",
            "Epoch 4169. Training loss: 0.7630227208137512. Validation loss: 2.705023765563965.\n",
            "Epoch 4170. Training loss: 0.763018786907196. Validation loss: 2.7050230503082275.\n",
            "Epoch 4171. Training loss: 0.7630147337913513. Validation loss: 2.7050225734710693.\n",
            "Epoch 4172. Training loss: 0.7630106806755066. Validation loss: 2.705022096633911.\n",
            "Epoch 4173. Training loss: 0.7630065083503723. Validation loss: 2.705021619796753.\n",
            "Epoch 4174. Training loss: 0.7630025744438171. Validation loss: 2.7050209045410156.\n",
            "Epoch 4175. Training loss: 0.7629985213279724. Validation loss: 2.7050204277038574.\n",
            "Epoch 4176. Training loss: 0.7629944682121277. Validation loss: 2.7050201892852783.\n",
            "Epoch 4177. Training loss: 0.762990415096283. Validation loss: 2.70501971244812.\n",
            "Epoch 4178. Training loss: 0.7629863619804382. Validation loss: 2.705018997192383.\n",
            "Epoch 4179. Training loss: 0.7629823088645935. Validation loss: 2.7050182819366455.\n",
            "Epoch 4180. Training loss: 0.7629782557487488. Validation loss: 2.7050180435180664.\n",
            "Epoch 4181. Training loss: 0.762974202632904. Validation loss: 2.705017566680908.\n",
            "Epoch 4182. Training loss: 0.7629701495170593. Validation loss: 2.70501708984375.\n",
            "Epoch 4183. Training loss: 0.7629661560058594. Validation loss: 2.705016613006592.\n",
            "Epoch 4184. Training loss: 0.7629620432853699. Validation loss: 2.7050161361694336.\n",
            "Epoch 4185. Training loss: 0.7629580497741699. Validation loss: 2.7050156593322754.\n",
            "Epoch 4186. Training loss: 0.7629539370536804. Validation loss: 2.7050154209136963.\n",
            "Epoch 4187. Training loss: 0.7629498839378357. Validation loss: 2.705014705657959.\n",
            "Epoch 4188. Training loss: 0.762945830821991. Validation loss: 2.7050137519836426.\n",
            "Epoch 4189. Training loss: 0.7629416584968567. Validation loss: 2.7050137519836426.\n",
            "Epoch 4190. Training loss: 0.762937605381012. Validation loss: 2.7050132751464844.\n",
            "Epoch 4191. Training loss: 0.7629336714744568. Validation loss: 2.705012559890747.\n",
            "Epoch 4192. Training loss: 0.7629296183586121. Validation loss: 2.705012083053589.\n",
            "Epoch 4193. Training loss: 0.7629254460334778. Validation loss: 2.7050116062164307.\n",
            "Epoch 4194. Training loss: 0.7629213333129883. Validation loss: 2.7050108909606934.\n",
            "Epoch 4195. Training loss: 0.7629173398017883. Validation loss: 2.705010414123535.\n",
            "Epoch 4196. Training loss: 0.7629132270812988. Validation loss: 2.705010175704956.\n",
            "Epoch 4197. Training loss: 0.7629091739654541. Validation loss: 2.705009698867798.\n",
            "Epoch 4198. Training loss: 0.7629051208496094. Validation loss: 2.7050089836120605.\n",
            "Epoch 4199. Training loss: 0.7629010677337646. Validation loss: 2.7050087451934814.\n",
            "Epoch 4200. Training loss: 0.7628969550132751. Validation loss: 2.705007553100586.\n",
            "Epoch 4201. Training loss: 0.7628929615020752. Validation loss: 2.7050070762634277.\n",
            "Epoch 4202. Training loss: 0.7628888487815857. Validation loss: 2.7050068378448486.\n",
            "Epoch 4203. Training loss: 0.762884795665741. Validation loss: 2.7050063610076904.\n",
            "Epoch 4204. Training loss: 0.7628806233406067. Validation loss: 2.705005645751953.\n",
            "Epoch 4205. Training loss: 0.762876570224762. Validation loss: 2.705005168914795.\n",
            "Epoch 4206. Training loss: 0.7628725171089172. Validation loss: 2.7050046920776367.\n",
            "Epoch 4207. Training loss: 0.7628684043884277. Validation loss: 2.7050042152404785.\n",
            "Epoch 4208. Training loss: 0.7628644108772278. Validation loss: 2.7050037384033203.\n",
            "Epoch 4209. Training loss: 0.7628602981567383. Validation loss: 2.705003261566162.\n",
            "Epoch 4210. Training loss: 0.7628562450408936. Validation loss: 2.705002784729004.\n",
            "Epoch 4211. Training loss: 0.7628521919250488. Validation loss: 2.7050023078918457.\n",
            "Epoch 4212. Training loss: 0.7628480792045593. Validation loss: 2.7050015926361084.\n",
            "Epoch 4213. Training loss: 0.762843906879425. Validation loss: 2.70500111579895.\n",
            "Epoch 4214. Training loss: 0.7628398537635803. Validation loss: 2.705000638961792.\n",
            "Epoch 4215. Training loss: 0.7628357410430908. Validation loss: 2.705000400543213.\n",
            "Epoch 4216. Training loss: 0.7628316879272461. Validation loss: 2.7049999237060547.\n",
            "Epoch 4217. Training loss: 0.7628276944160461. Validation loss: 2.7049989700317383.\n",
            "Epoch 4218. Training loss: 0.7628235220909119. Validation loss: 2.704998731613159.\n",
            "Epoch 4219. Training loss: 0.7628194689750671. Validation loss: 2.704998254776001.\n",
            "Epoch 4220. Training loss: 0.7628154158592224. Validation loss: 2.7049977779388428.\n",
            "Epoch 4221. Training loss: 0.7628112435340881. Validation loss: 2.7049970626831055.\n",
            "Epoch 4222. Training loss: 0.7628071308135986. Validation loss: 2.7049965858459473.\n",
            "Epoch 4223. Training loss: 0.7628030180931091. Validation loss: 2.704996109008789.\n",
            "Epoch 4224. Training loss: 0.7627990245819092. Validation loss: 2.704995632171631.\n",
            "Epoch 4225. Training loss: 0.7627949118614197. Validation loss: 2.7049951553344727.\n",
            "Epoch 4226. Training loss: 0.762790858745575. Validation loss: 2.7049946784973145.\n",
            "Epoch 4227. Training loss: 0.7627866864204407. Validation loss: 2.704993724822998.\n",
            "Epoch 4228. Training loss: 0.762782633304596. Validation loss: 2.70499324798584.\n",
            "Epoch 4229. Training loss: 0.7627785205841064. Validation loss: 2.7049927711486816.\n",
            "Epoch 4230. Training loss: 0.7627744078636169. Validation loss: 2.7049922943115234.\n",
            "Epoch 4231. Training loss: 0.7627703547477722. Validation loss: 2.7049920558929443.\n",
            "Epoch 4232. Training loss: 0.7627661824226379. Validation loss: 2.704991340637207.\n",
            "Epoch 4233. Training loss: 0.7627621293067932. Validation loss: 2.704990863800049.\n",
            "Epoch 4234. Training loss: 0.7627580761909485. Validation loss: 2.7049901485443115.\n",
            "Epoch 4235. Training loss: 0.7627539038658142. Validation loss: 2.7049901485443115.\n",
            "Epoch 4236. Training loss: 0.7627498507499695. Validation loss: 2.704988956451416.\n",
            "Epoch 4237. Training loss: 0.7627456784248352. Validation loss: 2.704988479614258.\n",
            "Epoch 4238. Training loss: 0.7627415657043457. Validation loss: 2.7049882411956787.\n",
            "Epoch 4239. Training loss: 0.7627375721931458. Validation loss: 2.7049877643585205.\n",
            "Epoch 4240. Training loss: 0.7627334594726562. Validation loss: 2.7049872875213623.\n",
            "Epoch 4241. Training loss: 0.7627293467521667. Validation loss: 2.704986810684204.\n",
            "Epoch 4242. Training loss: 0.7627251744270325. Validation loss: 2.704986572265625.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4243. Training loss: 0.7627211213111877. Validation loss: 2.7049856185913086.\n",
            "Epoch 4244. Training loss: 0.7627169489860535. Validation loss: 2.7049849033355713.\n",
            "Epoch 4245. Training loss: 0.7627129554748535. Validation loss: 2.7049851417541504.\n",
            "Epoch 4246. Training loss: 0.762708842754364. Validation loss: 2.704984188079834.\n",
            "Epoch 4247. Training loss: 0.7627046704292297. Validation loss: 2.704983949661255.\n",
            "Epoch 4248. Training loss: 0.7627005577087402. Validation loss: 2.7049829959869385.\n",
            "Epoch 4249. Training loss: 0.7626964449882507. Validation loss: 2.7049825191497803.\n",
            "Epoch 4250. Training loss: 0.762692391872406. Validation loss: 2.704982280731201.\n",
            "Epoch 4251. Training loss: 0.7626882195472717. Validation loss: 2.704981803894043.\n",
            "Epoch 4252. Training loss: 0.762684166431427. Validation loss: 2.7049810886383057.\n",
            "Epoch 4253. Training loss: 0.7626799941062927. Validation loss: 2.7049808502197266.\n",
            "Epoch 4254. Training loss: 0.762675940990448. Validation loss: 2.7049803733825684.\n",
            "Epoch 4255. Training loss: 0.7626717686653137. Validation loss: 2.704979658126831.\n",
            "Epoch 4256. Training loss: 0.7626676559448242. Validation loss: 2.704979181289673.\n",
            "Epoch 4257. Training loss: 0.7626635432243347. Validation loss: 2.7049789428710938.\n",
            "Epoch 4258. Training loss: 0.7626593708992004. Validation loss: 2.7049779891967773.\n",
            "Epoch 4259. Training loss: 0.7626552581787109. Validation loss: 2.70497727394104.\n",
            "Epoch 4260. Training loss: 0.7626512050628662. Validation loss: 2.704977035522461.\n",
            "Epoch 4261. Training loss: 0.7626469731330872. Validation loss: 2.7049763202667236.\n",
            "Epoch 4262. Training loss: 0.7626428604125977. Validation loss: 2.7049758434295654.\n",
            "Epoch 4263. Training loss: 0.7626388072967529. Validation loss: 2.7049756050109863.\n",
            "Epoch 4264. Training loss: 0.7626346945762634. Validation loss: 2.704974889755249.\n",
            "Epoch 4265. Training loss: 0.7626306414604187. Validation loss: 2.704974412918091.\n",
            "Epoch 4266. Training loss: 0.7626264095306396. Validation loss: 2.7049736976623535.\n",
            "Epoch 4267. Training loss: 0.7626223564147949. Validation loss: 2.7049734592437744.\n",
            "Epoch 4268. Training loss: 0.7626181244850159. Validation loss: 2.704972982406616.\n",
            "Epoch 4269. Training loss: 0.7626140713691711. Validation loss: 2.704972267150879.\n",
            "Epoch 4270. Training loss: 0.7626099586486816. Validation loss: 2.7049717903137207.\n",
            "Epoch 4271. Training loss: 0.7626058459281921. Validation loss: 2.7049713134765625.\n",
            "Epoch 4272. Training loss: 0.7626016736030579. Validation loss: 2.7049708366394043.\n",
            "Epoch 4273. Training loss: 0.7625975012779236. Validation loss: 2.704970359802246.\n",
            "Epoch 4274. Training loss: 0.7625933289527893. Validation loss: 2.704969882965088.\n",
            "Epoch 4275. Training loss: 0.7625892758369446. Validation loss: 2.7049694061279297.\n",
            "Epoch 4276. Training loss: 0.7625851631164551. Validation loss: 2.7049686908721924.\n",
            "Epoch 4277. Training loss: 0.7625810503959656. Validation loss: 2.704967975616455.\n",
            "Epoch 4278. Training loss: 0.7625768780708313. Validation loss: 2.704967737197876.\n",
            "Epoch 4279. Training loss: 0.7625727653503418. Validation loss: 2.7049670219421387.\n",
            "Epoch 4280. Training loss: 0.7625686526298523. Validation loss: 2.7049670219421387.\n",
            "Epoch 4281. Training loss: 0.762564480304718. Validation loss: 2.7049663066864014.\n",
            "Epoch 4282. Training loss: 0.7625603079795837. Validation loss: 2.704965829849243.\n",
            "Epoch 4283. Training loss: 0.762556254863739. Validation loss: 2.704965353012085.\n",
            "Epoch 4284. Training loss: 0.7625520825386047. Validation loss: 2.7049648761749268.\n",
            "Epoch 4285. Training loss: 0.7625479698181152. Validation loss: 2.7049639225006104.\n",
            "Epoch 4286. Training loss: 0.7625437378883362. Validation loss: 2.704963445663452.\n",
            "Epoch 4287. Training loss: 0.7625396847724915. Validation loss: 2.704963207244873.\n",
            "Epoch 4288. Training loss: 0.7625355124473572. Validation loss: 2.704962730407715.\n",
            "Epoch 4289. Training loss: 0.7625313401222229. Validation loss: 2.7049617767333984.\n",
            "Epoch 4290. Training loss: 0.7625272274017334. Validation loss: 2.7049615383148193.\n",
            "Epoch 4291. Training loss: 0.7625229954719543. Validation loss: 2.704961061477661.\n",
            "Epoch 4292. Training loss: 0.7625188827514648. Validation loss: 2.704960584640503.\n",
            "Epoch 4293. Training loss: 0.7625148296356201. Validation loss: 2.7049598693847656.\n",
            "Epoch 4294. Training loss: 0.7625105977058411. Validation loss: 2.7049598693847656.\n",
            "Epoch 4295. Training loss: 0.7625064849853516. Validation loss: 2.7049593925476074.\n",
            "Epoch 4296. Training loss: 0.7625022530555725. Validation loss: 2.704958438873291.\n",
            "Epoch 4297. Training loss: 0.7624981999397278. Validation loss: 2.704957962036133.\n",
            "Epoch 4298. Training loss: 0.7624940276145935. Validation loss: 2.7049574851989746.\n",
            "Epoch 4299. Training loss: 0.7624898552894592. Validation loss: 2.7049570083618164.\n",
            "Epoch 4300. Training loss: 0.762485682964325. Validation loss: 2.704956531524658.\n",
            "Epoch 4301. Training loss: 0.7624815106391907. Validation loss: 2.7049560546875.\n",
            "Epoch 4302. Training loss: 0.762477457523346. Validation loss: 2.7049551010131836.\n",
            "Epoch 4303. Training loss: 0.7624732851982117. Validation loss: 2.7049546241760254.\n",
            "Epoch 4304. Training loss: 0.7624691128730774. Validation loss: 2.704954147338867.\n",
            "Epoch 4305. Training loss: 0.7624650001525879. Validation loss: 2.704953670501709.\n",
            "Epoch 4306. Training loss: 0.7624607086181641. Validation loss: 2.704953670501709.\n",
            "Epoch 4307. Training loss: 0.7624565958976746. Validation loss: 2.7049524784088135.\n",
            "Epoch 4308. Training loss: 0.7624524235725403. Validation loss: 2.7049522399902344.\n",
            "Epoch 4309. Training loss: 0.762448251247406. Validation loss: 2.704951763153076.\n",
            "Epoch 4310. Training loss: 0.7624441981315613. Validation loss: 2.704951286315918.\n",
            "Epoch 4311. Training loss: 0.7624399065971375. Validation loss: 2.7049505710601807.\n",
            "Epoch 4312. Training loss: 0.7624358534812927. Validation loss: 2.7049500942230225.\n",
            "Epoch 4313. Training loss: 0.7624316215515137. Validation loss: 2.7049496173858643.\n",
            "Epoch 4314. Training loss: 0.7624275088310242. Validation loss: 2.704948902130127.\n",
            "Epoch 4315. Training loss: 0.7624232769012451. Validation loss: 2.704948663711548.\n",
            "Epoch 4316. Training loss: 0.7624191641807556. Validation loss: 2.7049479484558105.\n",
            "Epoch 4317. Training loss: 0.7624149918556213. Validation loss: 2.7049474716186523.\n",
            "Epoch 4318. Training loss: 0.7624108195304871. Validation loss: 2.704946994781494.\n",
            "Epoch 4319. Training loss: 0.762406587600708. Validation loss: 2.704946756362915.\n",
            "Epoch 4320. Training loss: 0.7624025344848633. Validation loss: 2.704946279525757.\n",
            "Epoch 4321. Training loss: 0.7623983025550842. Validation loss: 2.7049450874328613.\n",
            "Epoch 4322. Training loss: 0.76239413022995. Validation loss: 2.704944610595703.\n",
            "Epoch 4323. Training loss: 0.7623899579048157. Validation loss: 2.704944610595703.\n",
            "Epoch 4324. Training loss: 0.7623858451843262. Validation loss: 2.704944133758545.\n",
            "Epoch 4325. Training loss: 0.7623817324638367. Validation loss: 2.7049436569213867.\n",
            "Epoch 4326. Training loss: 0.7623775005340576. Validation loss: 2.7049431800842285.\n",
            "Epoch 4327. Training loss: 0.7623732686042786. Validation loss: 2.7049427032470703.\n",
            "Epoch 4328. Training loss: 0.7623691558837891. Validation loss: 2.704941987991333.\n",
            "Epoch 4329. Training loss: 0.7623650431632996. Validation loss: 2.704941511154175.\n",
            "Epoch 4330. Training loss: 0.7623608112335205. Validation loss: 2.7049410343170166.\n",
            "Epoch 4331. Training loss: 0.7623565793037415. Validation loss: 2.704939842224121.\n",
            "Epoch 4332. Training loss: 0.7623524069786072. Validation loss: 2.704939365386963.\n",
            "Epoch 4333. Training loss: 0.7623482346534729. Validation loss: 2.704939126968384.\n",
            "Epoch 4334. Training loss: 0.7623440623283386. Validation loss: 2.7049386501312256.\n",
            "Epoch 4335. Training loss: 0.7623398303985596. Validation loss: 2.7049381732940674.\n",
            "Epoch 4336. Training loss: 0.7623357176780701. Validation loss: 2.7049379348754883.\n",
            "Epoch 4337. Training loss: 0.7623315453529358. Validation loss: 2.704936981201172.\n",
            "Epoch 4338. Training loss: 0.762327253818512. Validation loss: 2.7049365043640137.\n",
            "Epoch 4339. Training loss: 0.7623231410980225. Validation loss: 2.7049360275268555.\n",
            "Epoch 4340. Training loss: 0.762319028377533. Validation loss: 2.7049357891082764.\n",
            "Epoch 4341. Training loss: 0.7623147964477539. Validation loss: 2.704935312271118.\n",
            "Epoch 4342. Training loss: 0.7623105645179749. Validation loss: 2.70493483543396.\n",
            "Epoch 4343. Training loss: 0.7623063921928406. Validation loss: 2.7049338817596436.\n",
            "Epoch 4344. Training loss: 0.7623021006584167. Validation loss: 2.7049334049224854.\n",
            "Epoch 4345. Training loss: 0.762298047542572. Validation loss: 2.704932928085327.\n",
            "Epoch 4346. Training loss: 0.762293815612793. Validation loss: 2.704932451248169.\n",
            "Epoch 4347. Training loss: 0.7622895836830139. Validation loss: 2.7049317359924316.\n",
            "Epoch 4348. Training loss: 0.7622854709625244. Validation loss: 2.7049312591552734.\n",
            "Epoch 4349. Training loss: 0.7622811794281006. Validation loss: 2.704930543899536.\n",
            "Epoch 4350. Training loss: 0.7622771263122559. Validation loss: 2.704930305480957.\n",
            "Epoch 4351. Training loss: 0.762272834777832. Validation loss: 2.704930067062378.\n",
            "Epoch 4352. Training loss: 0.7622687220573425. Validation loss: 2.7049293518066406.\n",
            "Epoch 4353. Training loss: 0.7622645497322083. Validation loss: 2.7049286365509033.\n",
            "Epoch 4354. Training loss: 0.7622602581977844. Validation loss: 2.704928159713745.\n",
            "Epoch 4355. Training loss: 0.7622560858726501. Validation loss: 2.704927921295166.\n",
            "Epoch 4356. Training loss: 0.7622518539428711. Validation loss: 2.7049272060394287.\n",
            "Epoch 4357. Training loss: 0.7622477412223816. Validation loss: 2.7049267292022705.\n",
            "Epoch 4358. Training loss: 0.7622434496879578. Validation loss: 2.7049264907836914.\n",
            "Epoch 4359. Training loss: 0.7622392773628235. Validation loss: 2.704925775527954.\n",
            "Epoch 4360. Training loss: 0.762235164642334. Validation loss: 2.704925298690796.\n",
            "Epoch 4361. Training loss: 0.7622308731079102. Validation loss: 2.7049248218536377.\n",
            "Epoch 4362. Training loss: 0.7622265815734863. Validation loss: 2.7049243450164795.\n",
            "Epoch 4363. Training loss: 0.7622224688529968. Validation loss: 2.704923629760742.\n",
            "Epoch 4364. Training loss: 0.7622182369232178. Validation loss: 2.704923152923584.\n",
            "Epoch 4365. Training loss: 0.7622140049934387. Validation loss: 2.7049224376678467.\n",
            "Epoch 4366. Training loss: 0.7622098326683044. Validation loss: 2.7049221992492676.\n",
            "Epoch 4367. Training loss: 0.7622056603431702. Validation loss: 2.7049217224121094.\n",
            "Epoch 4368. Training loss: 0.7622013688087463. Validation loss: 2.7049214839935303.\n",
            "Epoch 4369. Training loss: 0.7621971964836121. Validation loss: 2.704920530319214.\n",
            "Epoch 4370. Training loss: 0.762192964553833. Validation loss: 2.7049200534820557.\n",
            "Epoch 4371. Training loss: 0.762188732624054. Validation loss: 2.7049193382263184.\n",
            "Epoch 4372. Training loss: 0.7621846199035645. Validation loss: 2.7049190998077393.\n",
            "Epoch 4373. Training loss: 0.7621803879737854. Validation loss: 2.704918622970581.\n",
            "Epoch 4374. Training loss: 0.7621760964393616. Validation loss: 2.7049179077148438.\n",
            "Epoch 4375. Training loss: 0.7621719241142273. Validation loss: 2.7049174308776855.\n",
            "Epoch 4376. Training loss: 0.7621676921844482. Validation loss: 2.7049167156219482.\n",
            "Epoch 4377. Training loss: 0.7621634602546692. Validation loss: 2.704916000366211.\n",
            "Epoch 4378. Training loss: 0.7621592879295349. Validation loss: 2.704915761947632.\n",
            "Epoch 4379. Training loss: 0.7621550559997559. Validation loss: 2.7049152851104736.\n",
            "Epoch 4380. Training loss: 0.7621508240699768. Validation loss: 2.7049145698547363.\n",
            "Epoch 4381. Training loss: 0.7621466517448425. Validation loss: 2.7049143314361572.\n",
            "Epoch 4382. Training loss: 0.7621424794197083. Validation loss: 2.70491361618042.\n",
            "Epoch 4383. Training loss: 0.7621381878852844. Validation loss: 2.704913377761841.\n",
            "Epoch 4384. Training loss: 0.7621340155601501. Validation loss: 2.7049124240875244.\n",
            "Epoch 4385. Training loss: 0.7621297836303711. Validation loss: 2.7049124240875244.\n",
            "Epoch 4386. Training loss: 0.762125551700592. Validation loss: 2.704911470413208.\n",
            "Epoch 4387. Training loss: 0.7621212601661682. Validation loss: 2.70491099357605.\n",
            "Epoch 4388. Training loss: 0.7621171474456787. Validation loss: 2.7049105167388916.\n",
            "Epoch 4389. Training loss: 0.7621129155158997. Validation loss: 2.7049098014831543.\n",
            "Epoch 4390. Training loss: 0.7621087431907654. Validation loss: 2.704909563064575.\n",
            "Epoch 4391. Training loss: 0.7621044516563416. Validation loss: 2.704908847808838.\n",
            "Epoch 4392. Training loss: 0.7621002197265625. Validation loss: 2.704908609390259.\n",
            "Epoch 4393. Training loss: 0.7620959877967834. Validation loss: 2.7049078941345215.\n",
            "Epoch 4394. Training loss: 0.7620916962623596. Validation loss: 2.7049074172973633.\n",
            "Epoch 4395. Training loss: 0.7620875239372253. Validation loss: 2.704907178878784.\n",
            "Epoch 4396. Training loss: 0.7620832920074463. Validation loss: 2.7049062252044678.\n",
            "Epoch 4397. Training loss: 0.7620790600776672. Validation loss: 2.7049057483673096.\n",
            "Epoch 4398. Training loss: 0.7620747685432434. Validation loss: 2.7049055099487305.\n",
            "Epoch 4399. Training loss: 0.7620705962181091. Validation loss: 2.704904794692993.\n",
            "Epoch 4400. Training loss: 0.7620663642883301. Validation loss: 2.704904556274414.\n",
            "Epoch 4401. Training loss: 0.762062132358551. Validation loss: 2.7049038410186768.\n",
            "Epoch 4402. Training loss: 0.7620579600334167. Validation loss: 2.7049028873443604.\n",
            "Epoch 4403. Training loss: 0.7620536684989929. Validation loss: 2.7049028873443604.\n",
            "Epoch 4404. Training loss: 0.7620494365692139. Validation loss: 2.704902410507202.\n",
            "Epoch 4405. Training loss: 0.76204514503479. Validation loss: 2.704901933670044.\n",
            "Epoch 4406. Training loss: 0.7620410323143005. Validation loss: 2.7049012184143066.\n",
            "Epoch 4407. Training loss: 0.7620367407798767. Validation loss: 2.7049005031585693.\n",
            "Epoch 4408. Training loss: 0.7620324492454529. Validation loss: 2.7049002647399902.\n",
            "Epoch 4409. Training loss: 0.7620282769203186. Validation loss: 2.704899549484253.\n",
            "Epoch 4410. Training loss: 0.7620239853858948. Validation loss: 2.7048990726470947.\n",
            "Epoch 4411. Training loss: 0.762019693851471. Validation loss: 2.7048988342285156.\n",
            "Epoch 4412. Training loss: 0.7620155215263367. Validation loss: 2.70489764213562.\n",
            "Epoch 4413. Training loss: 0.7620112299919128. Validation loss: 2.70489764213562.\n",
            "Epoch 4414. Training loss: 0.7620069980621338. Validation loss: 2.7048966884613037.\n",
            "Epoch 4415. Training loss: 0.76200270652771. Validation loss: 2.7048964500427246.\n",
            "Epoch 4416. Training loss: 0.7619984745979309. Validation loss: 2.7048957347869873.\n",
            "Epoch 4417. Training loss: 0.7619941830635071. Validation loss: 2.704895496368408.\n",
            "Epoch 4418. Training loss: 0.7619900107383728. Validation loss: 2.704894781112671.\n",
            "Epoch 4419. Training loss: 0.7619857788085938. Validation loss: 2.7048943042755127.\n",
            "Epoch 4420. Training loss: 0.7619814872741699. Validation loss: 2.7048935890197754.\n",
            "Epoch 4421. Training loss: 0.7619772553443909. Validation loss: 2.704893112182617.\n",
            "Epoch 4422. Training loss: 0.7619730830192566. Validation loss: 2.704892873764038.\n",
            "Epoch 4423. Training loss: 0.7619687914848328. Validation loss: 2.7048919200897217.\n",
            "Epoch 4424. Training loss: 0.7619645595550537. Validation loss: 2.7048916816711426.\n",
            "Epoch 4425. Training loss: 0.7619602680206299. Validation loss: 2.7048909664154053.\n",
            "Epoch 4426. Training loss: 0.761955976486206. Validation loss: 2.704890727996826.\n",
            "Epoch 4427. Training loss: 0.761951744556427. Validation loss: 2.704890251159668.\n",
            "Epoch 4428. Training loss: 0.7619474530220032. Validation loss: 2.7048895359039307.\n",
            "Epoch 4429. Training loss: 0.7619431614875793. Validation loss: 2.7048890590667725.\n",
            "Epoch 4430. Training loss: 0.7619388699531555. Validation loss: 2.7048888206481934.\n",
            "Epoch 4431. Training loss: 0.761934757232666. Validation loss: 2.704887866973877.\n",
            "Epoch 4432. Training loss: 0.7619304656982422. Validation loss: 2.704887866973877.\n",
            "Epoch 4433. Training loss: 0.7619261741638184. Validation loss: 2.7048873901367188.\n",
            "Epoch 4434. Training loss: 0.7619219422340393. Validation loss: 2.7048861980438232.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4435. Training loss: 0.7619176506996155. Validation loss: 2.704885959625244.\n",
            "Epoch 4436. Training loss: 0.7619133591651917. Validation loss: 2.704885482788086.\n",
            "Epoch 4437. Training loss: 0.761909008026123. Validation loss: 2.7048850059509277.\n",
            "Epoch 4438. Training loss: 0.7619049549102783. Validation loss: 2.7048845291137695.\n",
            "Epoch 4439. Training loss: 0.7619006037712097. Validation loss: 2.7048838138580322.\n",
            "Epoch 4440. Training loss: 0.7618964314460754. Validation loss: 2.704883575439453.\n",
            "Epoch 4441. Training loss: 0.7618920803070068. Validation loss: 2.704883098602295.\n",
            "Epoch 4442. Training loss: 0.761887788772583. Validation loss: 2.7048821449279785.\n",
            "Epoch 4443. Training loss: 0.761883556842804. Validation loss: 2.7048821449279785.\n",
            "Epoch 4444. Training loss: 0.7618792653083801. Validation loss: 2.704880952835083.\n",
            "Epoch 4445. Training loss: 0.7618749737739563. Validation loss: 2.704880475997925.\n",
            "Epoch 4446. Training loss: 0.7618706822395325. Validation loss: 2.7048802375793457.\n",
            "Epoch 4447. Training loss: 0.7618663907051086. Validation loss: 2.7048797607421875.\n",
            "Epoch 4448. Training loss: 0.7618620991706848. Validation loss: 2.70487904548645.\n",
            "Epoch 4449. Training loss: 0.7618579268455505. Validation loss: 2.704878807067871.\n",
            "Epoch 4450. Training loss: 0.7618535161018372. Validation loss: 2.704878091812134.\n",
            "Epoch 4451. Training loss: 0.7618494033813477. Validation loss: 2.7048776149749756.\n",
            "Epoch 4452. Training loss: 0.7618451118469238. Validation loss: 2.704876661300659.\n",
            "Epoch 4453. Training loss: 0.7618408203125. Validation loss: 2.70487642288208.\n",
            "Epoch 4454. Training loss: 0.7618365287780762. Validation loss: 2.704875946044922.\n",
            "Epoch 4455. Training loss: 0.7618322372436523. Validation loss: 2.7048754692077637.\n",
            "Epoch 4456. Training loss: 0.7618279457092285. Validation loss: 2.7048749923706055.\n",
            "Epoch 4457. Training loss: 0.7618236541748047. Validation loss: 2.7048745155334473.\n",
            "Epoch 4458. Training loss: 0.7618193626403809. Validation loss: 2.704874038696289.\n",
            "Epoch 4459. Training loss: 0.761815071105957. Validation loss: 2.7048730850219727.\n",
            "Epoch 4460. Training loss: 0.761810839176178. Validation loss: 2.7048730850219727.\n",
            "Epoch 4461. Training loss: 0.7618065476417542. Validation loss: 2.7048726081848145.\n",
            "Epoch 4462. Training loss: 0.7618021965026855. Validation loss: 2.704871654510498.\n",
            "Epoch 4463. Training loss: 0.7617979049682617. Validation loss: 2.70487117767334.\n",
            "Epoch 4464. Training loss: 0.7617936730384827. Validation loss: 2.7048704624176025.\n",
            "Epoch 4465. Training loss: 0.7617893815040588. Validation loss: 2.7048702239990234.\n",
            "Epoch 4466. Training loss: 0.761785089969635. Validation loss: 2.7048697471618652.\n",
            "Epoch 4467. Training loss: 0.7617807984352112. Validation loss: 2.704869508743286.\n",
            "Epoch 4468. Training loss: 0.7617764472961426. Validation loss: 2.7048683166503906.\n",
            "Epoch 4469. Training loss: 0.7617722153663635. Validation loss: 2.7048678398132324.\n",
            "Epoch 4470. Training loss: 0.7617679238319397. Validation loss: 2.704867362976074.\n",
            "Epoch 4471. Training loss: 0.7617637515068054. Validation loss: 2.704866886138916.\n",
            "Epoch 4472. Training loss: 0.761759340763092. Validation loss: 2.704866409301758.\n",
            "Epoch 4473. Training loss: 0.7617551684379578. Validation loss: 2.7048659324645996.\n",
            "Epoch 4474. Training loss: 0.7617506980895996. Validation loss: 2.7048654556274414.\n",
            "Epoch 4475. Training loss: 0.7617464661598206. Validation loss: 2.704864978790283.\n",
            "Epoch 4476. Training loss: 0.7617421746253967. Validation loss: 2.704864263534546.\n",
            "Epoch 4477. Training loss: 0.7617378830909729. Validation loss: 2.7048637866973877.\n",
            "Epoch 4478. Training loss: 0.7617335319519043. Validation loss: 2.7048630714416504.\n",
            "Epoch 4479. Training loss: 0.7617293000221252. Validation loss: 2.704862594604492.\n",
            "Epoch 4480. Training loss: 0.7617249488830566. Validation loss: 2.704862117767334.\n",
            "Epoch 4481. Training loss: 0.7617207169532776. Validation loss: 2.704861640930176.\n",
            "Epoch 4482. Training loss: 0.7617164254188538. Validation loss: 2.7048611640930176.\n",
            "Epoch 4483. Training loss: 0.7617120742797852. Validation loss: 2.7048604488372803.\n",
            "Epoch 4484. Training loss: 0.7617077827453613. Validation loss: 2.704860210418701.\n",
            "Epoch 4485. Training loss: 0.7617034912109375. Validation loss: 2.7048592567443848.\n",
            "Epoch 4486. Training loss: 0.7616991996765137. Validation loss: 2.7048590183258057.\n",
            "Epoch 4487. Training loss: 0.7616948485374451. Validation loss: 2.7048585414886475.\n",
            "Epoch 4488. Training loss: 0.7616905570030212. Validation loss: 2.7048580646514893.\n",
            "Epoch 4489. Training loss: 0.7616862654685974. Validation loss: 2.704857587814331.\n",
            "Epoch 4490. Training loss: 0.7616820335388184. Validation loss: 2.7048566341400146.\n",
            "Epoch 4491. Training loss: 0.7616776823997498. Validation loss: 2.7048563957214355.\n",
            "Epoch 4492. Training loss: 0.7616732716560364. Validation loss: 2.7048559188842773.\n",
            "Epoch 4493. Training loss: 0.7616689801216125. Validation loss: 2.70485520362854.\n",
            "Epoch 4494. Training loss: 0.7616646885871887. Validation loss: 2.704854965209961.\n",
            "Epoch 4495. Training loss: 0.7616603970527649. Validation loss: 2.7048542499542236.\n",
            "Epoch 4496. Training loss: 0.7616561055183411. Validation loss: 2.7048540115356445.\n",
            "Epoch 4497. Training loss: 0.7616517543792725. Validation loss: 2.7048532962799072.\n",
            "Epoch 4498. Training loss: 0.7616474628448486. Validation loss: 2.704852819442749.\n",
            "Epoch 4499. Training loss: 0.7616431713104248. Validation loss: 2.7048521041870117.\n",
            "Epoch 4500. Training loss: 0.7616388201713562. Validation loss: 2.7048516273498535.\n",
            "Epoch 4501. Training loss: 0.7616345286369324. Validation loss: 2.7048511505126953.\n",
            "Epoch 4502. Training loss: 0.761630117893219. Validation loss: 2.704850673675537.\n",
            "Epoch 4503. Training loss: 0.7616258263587952. Validation loss: 2.704850435256958.\n",
            "Epoch 4504. Training loss: 0.7616214752197266. Validation loss: 2.7048497200012207.\n",
            "Epoch 4505. Training loss: 0.7616171836853027. Validation loss: 2.7048490047454834.\n",
            "Epoch 4506. Training loss: 0.7616128325462341. Validation loss: 2.704848527908325.\n",
            "Epoch 4507. Training loss: 0.7616085410118103. Validation loss: 2.704848051071167.\n",
            "Epoch 4508. Training loss: 0.7616042494773865. Validation loss: 2.704847574234009.\n",
            "Epoch 4509. Training loss: 0.7615998387336731. Validation loss: 2.7048470973968506.\n",
            "Epoch 4510. Training loss: 0.7615955471992493. Validation loss: 2.7048463821411133.\n",
            "Epoch 4511. Training loss: 0.7615911960601807. Validation loss: 2.704845666885376.\n",
            "Epoch 4512. Training loss: 0.7615869045257568. Validation loss: 2.704845428466797.\n",
            "Epoch 4513. Training loss: 0.7615825533866882. Validation loss: 2.7048447132110596.\n",
            "Epoch 4514. Training loss: 0.7615782618522644. Validation loss: 2.7048444747924805.\n",
            "Epoch 4515. Training loss: 0.761573851108551. Validation loss: 2.7048439979553223.\n",
            "Epoch 4516. Training loss: 0.7615695595741272. Validation loss: 2.704843521118164.\n",
            "Epoch 4517. Training loss: 0.7615651488304138. Validation loss: 2.7048425674438477.\n",
            "Epoch 4518. Training loss: 0.76156085729599. Validation loss: 2.7048420906066895.\n",
            "Epoch 4519. Training loss: 0.7615565657615662. Validation loss: 2.7048416137695312.\n",
            "Epoch 4520. Training loss: 0.7615522742271423. Validation loss: 2.704841136932373.\n",
            "Epoch 4521. Training loss: 0.761547863483429. Validation loss: 2.7048404216766357.\n",
            "Epoch 4522. Training loss: 0.7615434527397156. Validation loss: 2.7048399448394775.\n",
            "Epoch 4523. Training loss: 0.7615392208099365. Validation loss: 2.7048394680023193.\n",
            "Epoch 4524. Training loss: 0.7615346908569336. Validation loss: 2.704838752746582.\n",
            "Epoch 4525. Training loss: 0.7615304589271545. Validation loss: 2.704838514328003.\n",
            "Epoch 4526. Training loss: 0.7615261077880859. Validation loss: 2.7048377990722656.\n",
            "Epoch 4527. Training loss: 0.7615217566490173. Validation loss: 2.7048370838165283.\n",
            "Epoch 4528. Training loss: 0.7615175247192383. Validation loss: 2.704836845397949.\n",
            "Epoch 4529. Training loss: 0.7615131735801697. Validation loss: 2.704836368560791.\n",
            "Epoch 4530. Training loss: 0.7615087628364563. Validation loss: 2.7048354148864746.\n",
            "Epoch 4531. Training loss: 0.7615044713020325. Validation loss: 2.7048349380493164.\n",
            "Epoch 4532. Training loss: 0.7615000605583191. Validation loss: 2.704834461212158.\n",
            "Epoch 4533. Training loss: 0.7614957690238953. Validation loss: 2.704834222793579.\n",
            "Epoch 4534. Training loss: 0.7614913582801819. Validation loss: 2.7048332691192627.\n",
            "Epoch 4535. Training loss: 0.7614870667457581. Validation loss: 2.7048327922821045.\n",
            "Epoch 4536. Training loss: 0.7614826560020447. Validation loss: 2.7048325538635254.\n",
            "Epoch 4537. Training loss: 0.7614782452583313. Validation loss: 2.704831838607788.\n",
            "Epoch 4538. Training loss: 0.7614739537239075. Validation loss: 2.704831600189209.\n",
            "Epoch 4539. Training loss: 0.7614696621894836. Validation loss: 2.7048308849334717.\n",
            "Epoch 4540. Training loss: 0.7614652514457703. Validation loss: 2.7048304080963135.\n",
            "Epoch 4541. Training loss: 0.7614609599113464. Validation loss: 2.704829692840576.\n",
            "Epoch 4542. Training loss: 0.7614564895629883. Validation loss: 2.704829216003418.\n",
            "Epoch 4543. Training loss: 0.7614521980285645. Validation loss: 2.7048287391662598.\n",
            "Epoch 4544. Training loss: 0.7614479064941406. Validation loss: 2.7048282623291016.\n",
            "Epoch 4545. Training loss: 0.7614434361457825. Validation loss: 2.7048277854919434.\n",
            "Epoch 4546. Training loss: 0.7614391446113586. Validation loss: 2.704827308654785.\n",
            "Epoch 4547. Training loss: 0.7614347338676453. Validation loss: 2.704826831817627.\n",
            "Epoch 4548. Training loss: 0.7614303231239319. Validation loss: 2.7048261165618896.\n",
            "Epoch 4549. Training loss: 0.7614260315895081. Validation loss: 2.7048258781433105.\n",
            "Epoch 4550. Training loss: 0.7614216804504395. Validation loss: 2.7048251628875732.\n",
            "Epoch 4551. Training loss: 0.7614173293113708. Validation loss: 2.704824924468994.\n",
            "Epoch 4552. Training loss: 0.761413037776947. Validation loss: 2.704824209213257.\n",
            "Epoch 4553. Training loss: 0.7614085674285889. Validation loss: 2.7048234939575195.\n",
            "Epoch 4554. Training loss: 0.7614042162895203. Validation loss: 2.7048227787017822.\n",
            "Epoch 4555. Training loss: 0.7613998055458069. Validation loss: 2.704822540283203.\n",
            "Epoch 4556. Training loss: 0.7613954544067383. Validation loss: 2.704822063446045.\n",
            "Epoch 4557. Training loss: 0.7613911032676697. Validation loss: 2.7048215866088867.\n",
            "Epoch 4558. Training loss: 0.7613866925239563. Validation loss: 2.7048211097717285.\n",
            "Epoch 4559. Training loss: 0.7613823413848877. Validation loss: 2.704820394515991.\n",
            "Epoch 4560. Training loss: 0.7613779902458191. Validation loss: 2.704820156097412.\n",
            "Epoch 4561. Training loss: 0.7613736987113953. Validation loss: 2.7048192024230957.\n",
            "Epoch 4562. Training loss: 0.7613692879676819. Validation loss: 2.7048187255859375.\n",
            "Epoch 4563. Training loss: 0.7613649368286133. Validation loss: 2.7048182487487793.\n",
            "Epoch 4564. Training loss: 0.7613604664802551. Validation loss: 2.704817533493042.\n",
            "Epoch 4565. Training loss: 0.7613560557365417. Validation loss: 2.704817295074463.\n",
            "Epoch 4566. Training loss: 0.7613518238067627. Validation loss: 2.7048168182373047.\n",
            "Epoch 4567. Training loss: 0.7613474726676941. Validation loss: 2.7048163414001465.\n",
            "Epoch 4568. Training loss: 0.7613430619239807. Validation loss: 2.704815626144409.\n",
            "Epoch 4569. Training loss: 0.7613385319709778. Validation loss: 2.704814910888672.\n",
            "Epoch 4570. Training loss: 0.761334240436554. Validation loss: 2.7048146724700928.\n",
            "Epoch 4571. Training loss: 0.7613298892974854. Validation loss: 2.7048141956329346.\n",
            "Epoch 4572. Training loss: 0.7613255381584167. Validation loss: 2.7048134803771973.\n",
            "Epoch 4573. Training loss: 0.7613210678100586. Validation loss: 2.70481276512146.\n",
            "Epoch 4574. Training loss: 0.7613167762756348. Validation loss: 2.704812526702881.\n",
            "Epoch 4575. Training loss: 0.7613124251365662. Validation loss: 2.7048120498657227.\n",
            "Epoch 4576. Training loss: 0.761307954788208. Validation loss: 2.7048115730285645.\n",
            "Epoch 4577. Training loss: 0.7613036036491394. Validation loss: 2.7048110961914062.\n",
            "Epoch 4578. Training loss: 0.7612991333007812. Validation loss: 2.70481014251709.\n",
            "Epoch 4579. Training loss: 0.7612948417663574. Validation loss: 2.7048094272613525.\n",
            "Epoch 4580. Training loss: 0.7612903714179993. Validation loss: 2.7048091888427734.\n",
            "Epoch 4581. Training loss: 0.7612860202789307. Validation loss: 2.704808473587036.\n",
            "Epoch 4582. Training loss: 0.7612816691398621. Validation loss: 2.704808235168457.\n",
            "Epoch 4583. Training loss: 0.7612772583961487. Validation loss: 2.704807758331299.\n",
            "Epoch 4584. Training loss: 0.7612729072570801. Validation loss: 2.7048070430755615.\n",
            "Epoch 4585. Training loss: 0.7612684369087219. Validation loss: 2.704806327819824.\n",
            "Epoch 4586. Training loss: 0.7612640261650085. Validation loss: 2.704806089401245.\n",
            "Epoch 4587. Training loss: 0.7612597346305847. Validation loss: 2.7048051357269287.\n",
            "Epoch 4588. Training loss: 0.7612552642822266. Validation loss: 2.7048048973083496.\n",
            "Epoch 4589. Training loss: 0.761250913143158. Validation loss: 2.7048044204711914.\n",
            "Epoch 4590. Training loss: 0.7612465023994446. Validation loss: 2.704803705215454.\n",
            "Epoch 4591. Training loss: 0.7612420916557312. Validation loss: 2.704803466796875.\n",
            "Epoch 4592. Training loss: 0.7612376809120178. Validation loss: 2.7048025131225586.\n",
            "Epoch 4593. Training loss: 0.7612333297729492. Validation loss: 2.7048025131225586.\n",
            "Epoch 4594. Training loss: 0.7612289786338806. Validation loss: 2.704801559448242.\n",
            "Epoch 4595. Training loss: 0.7612244486808777. Validation loss: 2.704801082611084.\n",
            "Epoch 4596. Training loss: 0.7612200379371643. Validation loss: 2.7048003673553467.\n",
            "Epoch 4597. Training loss: 0.7612156867980957. Validation loss: 2.7047998905181885.\n",
            "Epoch 4598. Training loss: 0.7612113356590271. Validation loss: 2.7047994136810303.\n",
            "Epoch 4599. Training loss: 0.761206865310669. Validation loss: 2.704799175262451.\n",
            "Epoch 4600. Training loss: 0.7612025141716003. Validation loss: 2.704798460006714.\n",
            "Epoch 4601. Training loss: 0.761198103427887. Validation loss: 2.7047979831695557.\n",
            "Epoch 4602. Training loss: 0.7611936926841736. Validation loss: 2.7047972679138184.\n",
            "Epoch 4603. Training loss: 0.7611892819404602. Validation loss: 2.70479679107666.\n",
            "Epoch 4604. Training loss: 0.7611848711967468. Validation loss: 2.704796552658081.\n",
            "Epoch 4605. Training loss: 0.7611804604530334. Validation loss: 2.7047958374023438.\n",
            "Epoch 4606. Training loss: 0.7611761093139648. Validation loss: 2.7047951221466064.\n",
            "Epoch 4607. Training loss: 0.7611716389656067. Validation loss: 2.7047948837280273.\n",
            "Epoch 4608. Training loss: 0.7611672282218933. Validation loss: 2.704793930053711.\n",
            "Epoch 4609. Training loss: 0.7611627578735352. Validation loss: 2.7047934532165527.\n",
            "Epoch 4610. Training loss: 0.7611584067344666. Validation loss: 2.7047929763793945.\n",
            "Epoch 4611. Training loss: 0.7611539959907532. Validation loss: 2.7047922611236572.\n",
            "Epoch 4612. Training loss: 0.7611495852470398. Validation loss: 2.704792022705078.\n",
            "Epoch 4613. Training loss: 0.7611451745033264. Validation loss: 2.704791307449341.\n",
            "Epoch 4614. Training loss: 0.761140763759613. Validation loss: 2.7047905921936035.\n",
            "Epoch 4615. Training loss: 0.7611362934112549. Validation loss: 2.7047903537750244.\n",
            "Epoch 4616. Training loss: 0.7611319422721863. Validation loss: 2.704789638519287.\n",
            "Epoch 4617. Training loss: 0.7611275315284729. Validation loss: 2.704789161682129.\n",
            "Epoch 4618. Training loss: 0.76112300157547. Validation loss: 2.70478892326355.\n",
            "Epoch 4619. Training loss: 0.7611186504364014. Validation loss: 2.7047882080078125.\n",
            "Epoch 4620. Training loss: 0.7611141800880432. Validation loss: 2.7047877311706543.\n",
            "Epoch 4621. Training loss: 0.7611098289489746. Validation loss: 2.704787015914917.\n",
            "Epoch 4622. Training loss: 0.7611053586006165. Validation loss: 2.704786777496338.\n",
            "Epoch 4623. Training loss: 0.7611009478569031. Validation loss: 2.7047860622406006.\n",
            "Epoch 4624. Training loss: 0.7610965371131897. Validation loss: 2.7047858238220215.\n",
            "Epoch 4625. Training loss: 0.7610921859741211. Validation loss: 2.704784870147705.\n",
            "Epoch 4626. Training loss: 0.7610877156257629. Validation loss: 2.704784393310547.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4627. Training loss: 0.7610833048820496. Validation loss: 2.7047841548919678.\n",
            "Epoch 4628. Training loss: 0.7610788345336914. Validation loss: 2.7047836780548096.\n",
            "Epoch 4629. Training loss: 0.7610744833946228. Validation loss: 2.7047829627990723.\n",
            "Epoch 4630. Training loss: 0.7610699534416199. Validation loss: 2.704782247543335.\n",
            "Epoch 4631. Training loss: 0.7610655426979065. Validation loss: 2.704782009124756.\n",
            "Epoch 4632. Training loss: 0.7610610127449036. Validation loss: 2.7047812938690186.\n",
            "Epoch 4633. Training loss: 0.761056661605835. Validation loss: 2.7047808170318604.\n",
            "Epoch 4634. Training loss: 0.7610523104667664. Validation loss: 2.704780101776123.\n",
            "Epoch 4635. Training loss: 0.7610478401184082. Validation loss: 2.704779624938965.\n",
            "Epoch 4636. Training loss: 0.76104336977005. Validation loss: 2.7047793865203857.\n",
            "Epoch 4637. Training loss: 0.7610389590263367. Validation loss: 2.7047786712646484.\n",
            "Epoch 4638. Training loss: 0.7610344886779785. Validation loss: 2.7047781944274902.\n",
            "Epoch 4639. Training loss: 0.7610300183296204. Validation loss: 2.704777717590332.\n",
            "Epoch 4640. Training loss: 0.7610257267951965. Validation loss: 2.704777240753174.\n",
            "Epoch 4641. Training loss: 0.7610211968421936. Validation loss: 2.7047765254974365.\n",
            "Epoch 4642. Training loss: 0.7610167860984802. Validation loss: 2.7047760486602783.\n",
            "Epoch 4643. Training loss: 0.7610123157501221. Validation loss: 2.70477557182312.\n",
            "Epoch 4644. Training loss: 0.7610078454017639. Validation loss: 2.704774856567383.\n",
            "Epoch 4645. Training loss: 0.7610034942626953. Validation loss: 2.7047743797302246.\n",
            "Epoch 4646. Training loss: 0.7609990239143372. Validation loss: 2.7047736644744873.\n",
            "Epoch 4647. Training loss: 0.7609946131706238. Validation loss: 2.704773187637329.\n",
            "Epoch 4648. Training loss: 0.7609901428222656. Validation loss: 2.70477294921875.\n",
            "Epoch 4649. Training loss: 0.7609856724739075. Validation loss: 2.7047719955444336.\n",
            "Epoch 4650. Training loss: 0.7609812617301941. Validation loss: 2.7047715187072754.\n",
            "Epoch 4651. Training loss: 0.7609767317771912. Validation loss: 2.704770803451538.\n",
            "Epoch 4652. Training loss: 0.7609723210334778. Validation loss: 2.704770803451538.\n",
            "Epoch 4653. Training loss: 0.7609679102897644. Validation loss: 2.704770088195801.\n",
            "Epoch 4654. Training loss: 0.7609634399414062. Validation loss: 2.7047696113586426.\n",
            "Epoch 4655. Training loss: 0.7609589695930481. Validation loss: 2.7047688961029053.\n",
            "Epoch 4656. Training loss: 0.7609545588493347. Validation loss: 2.704768419265747.\n",
            "Epoch 4657. Training loss: 0.7609500885009766. Validation loss: 2.7047677040100098.\n",
            "Epoch 4658. Training loss: 0.7609456181526184. Validation loss: 2.7047674655914307.\n",
            "Epoch 4659. Training loss: 0.7609410881996155. Validation loss: 2.7047667503356934.\n",
            "Epoch 4660. Training loss: 0.7609367370605469. Validation loss: 2.7047667503356934.\n",
            "Epoch 4661. Training loss: 0.760932207107544. Validation loss: 2.7047653198242188.\n",
            "Epoch 4662. Training loss: 0.7609277367591858. Validation loss: 2.7047650814056396.\n",
            "Epoch 4663. Training loss: 0.7609233260154724. Validation loss: 2.7047646045684814.\n",
            "Epoch 4664. Training loss: 0.760918915271759. Validation loss: 2.7047641277313232.\n",
            "Epoch 4665. Training loss: 0.7609143853187561. Validation loss: 2.704763412475586.\n",
            "Epoch 4666. Training loss: 0.7609099745750427. Validation loss: 2.7047629356384277.\n",
            "Epoch 4667. Training loss: 0.7609055042266846. Validation loss: 2.7047624588012695.\n",
            "Epoch 4668. Training loss: 0.7609010338783264. Validation loss: 2.7047619819641113.\n",
            "Epoch 4669. Training loss: 0.760896623134613. Validation loss: 2.704761505126953.\n",
            "Epoch 4670. Training loss: 0.7608920931816101. Validation loss: 2.7047605514526367.\n",
            "Epoch 4671. Training loss: 0.760887622833252. Validation loss: 2.7047600746154785.\n",
            "Epoch 4672. Training loss: 0.7608831524848938. Validation loss: 2.7047595977783203.\n",
            "Epoch 4673. Training loss: 0.7608787417411804. Validation loss: 2.704758882522583.\n",
            "Epoch 4674. Training loss: 0.7608740925788879. Validation loss: 2.704758644104004.\n",
            "Epoch 4675. Training loss: 0.7608697414398193. Validation loss: 2.7047576904296875.\n",
            "Epoch 4676. Training loss: 0.7608652114868164. Validation loss: 2.7047576904296875.\n",
            "Epoch 4677. Training loss: 0.7608609199523926. Validation loss: 2.7047572135925293.\n",
            "Epoch 4678. Training loss: 0.7608563303947449. Validation loss: 2.704756498336792.\n",
            "Epoch 4679. Training loss: 0.7608518600463867. Validation loss: 2.704756021499634.\n",
            "Epoch 4680. Training loss: 0.7608473300933838. Validation loss: 2.7047555446624756.\n",
            "Epoch 4681. Training loss: 0.7608428597450256. Validation loss: 2.7047548294067383.\n",
            "Epoch 4682. Training loss: 0.7608384490013123. Validation loss: 2.70475435256958.\n",
            "Epoch 4683. Training loss: 0.7608339190483093. Validation loss: 2.704753875732422.\n",
            "Epoch 4684. Training loss: 0.760829508304596. Validation loss: 2.7047531604766846.\n",
            "Epoch 4685. Training loss: 0.7608250975608826. Validation loss: 2.7047524452209473.\n",
            "Epoch 4686. Training loss: 0.7608205676078796. Validation loss: 2.704751968383789.\n",
            "Epoch 4687. Training loss: 0.7608160376548767. Validation loss: 2.7047512531280518.\n",
            "Epoch 4688. Training loss: 0.7608115077018738. Validation loss: 2.7047507762908936.\n",
            "Epoch 4689. Training loss: 0.7608070373535156. Validation loss: 2.7047505378723145.\n",
            "Epoch 4690. Training loss: 0.7608025670051575. Validation loss: 2.7047500610351562.\n",
            "Epoch 4691. Training loss: 0.7607980370521545. Validation loss: 2.704749584197998.\n",
            "Epoch 4692. Training loss: 0.7607936263084412. Validation loss: 2.7047488689422607.\n",
            "Epoch 4693. Training loss: 0.760789155960083. Validation loss: 2.7047483921051025.\n",
            "Epoch 4694. Training loss: 0.7607846260070801. Validation loss: 2.704747438430786.\n",
            "Epoch 4695. Training loss: 0.7607801556587219. Validation loss: 2.704747200012207.\n",
            "Epoch 4696. Training loss: 0.7607755661010742. Validation loss: 2.704746723175049.\n",
            "Epoch 4697. Training loss: 0.7607712149620056. Validation loss: 2.7047460079193115.\n",
            "Epoch 4698. Training loss: 0.7607666850090027. Validation loss: 2.7047455310821533.\n",
            "Epoch 4699. Training loss: 0.7607622146606445. Validation loss: 2.704745054244995.\n",
            "Epoch 4700. Training loss: 0.7607576847076416. Validation loss: 2.704744577407837.\n",
            "Epoch 4701. Training loss: 0.7607531547546387. Validation loss: 2.7047438621520996.\n",
            "Epoch 4702. Training loss: 0.7607486844062805. Validation loss: 2.7047431468963623.\n",
            "Epoch 4703. Training loss: 0.7607441544532776. Validation loss: 2.7047431468963623.\n",
            "Epoch 4704. Training loss: 0.7607397437095642. Validation loss: 2.704742193222046.\n",
            "Epoch 4705. Training loss: 0.7607352137565613. Validation loss: 2.7047417163848877.\n",
            "Epoch 4706. Training loss: 0.7607307434082031. Validation loss: 2.7047410011291504.\n",
            "Epoch 4707. Training loss: 0.760726273059845. Validation loss: 2.704740524291992.\n",
            "Epoch 4708. Training loss: 0.7607216835021973. Validation loss: 2.704740047454834.\n",
            "Epoch 4709. Training loss: 0.7607172131538391. Validation loss: 2.704739570617676.\n",
            "Epoch 4710. Training loss: 0.7607126832008362. Validation loss: 2.7047390937805176.\n",
            "Epoch 4711. Training loss: 0.7607081532478333. Validation loss: 2.7047386169433594.\n",
            "Epoch 4712. Training loss: 0.7607037425041199. Validation loss: 2.704738140106201.\n",
            "Epoch 4713. Training loss: 0.7606992125511169. Validation loss: 2.704737424850464.\n",
            "Epoch 4714. Training loss: 0.760694682598114. Validation loss: 2.7047369480133057.\n",
            "Epoch 4715. Training loss: 0.7606902122497559. Validation loss: 2.7047364711761475.\n",
            "Epoch 4716. Training loss: 0.7606856822967529. Validation loss: 2.70473575592041.\n",
            "Epoch 4717. Training loss: 0.7606810927391052. Validation loss: 2.704735279083252.\n",
            "Epoch 4718. Training loss: 0.7606766819953918. Validation loss: 2.7047348022460938.\n",
            "Epoch 4719. Training loss: 0.7606721520423889. Validation loss: 2.7047343254089355.\n",
            "Epoch 4720. Training loss: 0.760667622089386. Validation loss: 2.7047338485717773.\n",
            "Epoch 4721. Training loss: 0.7606630921363831. Validation loss: 2.704733371734619.\n",
            "Epoch 4722. Training loss: 0.7606586813926697. Validation loss: 2.704732894897461.\n",
            "Epoch 4723. Training loss: 0.7606540322303772. Validation loss: 2.7047321796417236.\n",
            "Epoch 4724. Training loss: 0.7606496214866638. Validation loss: 2.7047317028045654.\n",
            "Epoch 4725. Training loss: 0.7606450915336609. Validation loss: 2.7047312259674072.\n",
            "Epoch 4726. Training loss: 0.760640561580658. Validation loss: 2.704730749130249.\n",
            "Epoch 4727. Training loss: 0.7606360912322998. Validation loss: 2.7047300338745117.\n",
            "Epoch 4728. Training loss: 0.7606315612792969. Validation loss: 2.7047293186187744.\n",
            "Epoch 4729. Training loss: 0.7606269717216492. Validation loss: 2.7047290802001953.\n",
            "Epoch 4730. Training loss: 0.760622501373291. Validation loss: 2.704728126525879.\n",
            "Epoch 4731. Training loss: 0.7606179118156433. Validation loss: 2.7047276496887207.\n",
            "Epoch 4732. Training loss: 0.7606134414672852. Validation loss: 2.7047274112701416.\n",
            "Epoch 4733. Training loss: 0.7606089115142822. Validation loss: 2.704726457595825.\n",
            "Epoch 4734. Training loss: 0.7606043815612793. Validation loss: 2.704726219177246.\n",
            "Epoch 4735. Training loss: 0.7605999112129211. Validation loss: 2.704725503921509.\n",
            "Epoch 4736. Training loss: 0.7605953216552734. Validation loss: 2.7047250270843506.\n",
            "Epoch 4737. Training loss: 0.7605908513069153. Validation loss: 2.7047245502471924.\n",
            "Epoch 4738. Training loss: 0.7605863213539124. Validation loss: 2.704724073410034.\n",
            "Epoch 4739. Training loss: 0.7605816721916199. Validation loss: 2.704723596572876.\n",
            "Epoch 4740. Training loss: 0.7605772018432617. Validation loss: 2.7047228813171387.\n",
            "Epoch 4741. Training loss: 0.7605726718902588. Validation loss: 2.7047224044799805.\n",
            "Epoch 4742. Training loss: 0.7605682015419006. Validation loss: 2.704721689224243.\n",
            "Epoch 4743. Training loss: 0.7605636119842529. Validation loss: 2.704720973968506.\n",
            "Epoch 4744. Training loss: 0.76055908203125. Validation loss: 2.7047207355499268.\n",
            "Epoch 4745. Training loss: 0.7605546116828918. Validation loss: 2.7047202587127686.\n",
            "Epoch 4746. Training loss: 0.7605500221252441. Validation loss: 2.7047197818756104.\n",
            "Epoch 4747. Training loss: 0.760545551776886. Validation loss: 2.704719066619873.\n",
            "Epoch 4748. Training loss: 0.7605409026145935. Validation loss: 2.704718589782715.\n",
            "Epoch 4749. Training loss: 0.7605364322662354. Validation loss: 2.7047181129455566.\n",
            "Epoch 4750. Training loss: 0.7605319023132324. Validation loss: 2.7047173976898193.\n",
            "Epoch 4751. Training loss: 0.7605273127555847. Validation loss: 2.704716920852661.\n",
            "Epoch 4752. Training loss: 0.7605228424072266. Validation loss: 2.704716205596924.\n",
            "Epoch 4753. Training loss: 0.7605182528495789. Validation loss: 2.7047157287597656.\n",
            "Epoch 4754. Training loss: 0.7605137825012207. Validation loss: 2.7047152519226074.\n",
            "Epoch 4755. Training loss: 0.7605092525482178. Validation loss: 2.704714775085449.\n",
            "Epoch 4756. Training loss: 0.7605047225952148. Validation loss: 2.704714298248291.\n",
            "Epoch 4757. Training loss: 0.7605001330375671. Validation loss: 2.7047133445739746.\n",
            "Epoch 4758. Training loss: 0.760495662689209. Validation loss: 2.7047133445739746.\n",
            "Epoch 4759. Training loss: 0.7604910731315613. Validation loss: 2.704712390899658.\n",
            "Epoch 4760. Training loss: 0.7604865431785583. Validation loss: 2.704712152481079.\n",
            "Epoch 4761. Training loss: 0.7604820132255554. Validation loss: 2.704711437225342.\n",
            "Epoch 4762. Training loss: 0.7604773640632629. Validation loss: 2.7047109603881836.\n",
            "Epoch 4763. Training loss: 0.76047283411026. Validation loss: 2.7047104835510254.\n",
            "Epoch 4764. Training loss: 0.7604682445526123. Validation loss: 2.704709768295288.\n",
            "Epoch 4765. Training loss: 0.7604637742042542. Validation loss: 2.70470929145813.\n",
            "Epoch 4766. Training loss: 0.7604591846466064. Validation loss: 2.7047088146209717.\n",
            "Epoch 4767. Training loss: 0.7604547142982483. Validation loss: 2.7047080993652344.\n",
            "Epoch 4768. Training loss: 0.7604501247406006. Validation loss: 2.704707145690918.\n",
            "Epoch 4769. Training loss: 0.7604455351829529. Validation loss: 2.704706907272339.\n",
            "Epoch 4770. Training loss: 0.76044100522995. Validation loss: 2.7047064304351807.\n",
            "Epoch 4771. Training loss: 0.760436475276947. Validation loss: 2.7047061920166016.\n",
            "Epoch 4772. Training loss: 0.7604319453239441. Validation loss: 2.704705238342285.\n",
            "Epoch 4773. Training loss: 0.7604272961616516. Validation loss: 2.704704761505127.\n",
            "Epoch 4774. Training loss: 0.7604227662086487. Validation loss: 2.7047042846679688.\n",
            "Epoch 4775. Training loss: 0.7604183554649353. Validation loss: 2.7047035694122314.\n",
            "Epoch 4776. Training loss: 0.760413646697998. Validation loss: 2.7047030925750732.\n",
            "Epoch 4777. Training loss: 0.7604091167449951. Validation loss: 2.704702615737915.\n",
            "Epoch 4778. Training loss: 0.7604045271873474. Validation loss: 2.704702138900757.\n",
            "Epoch 4779. Training loss: 0.7603999972343445. Validation loss: 2.7047014236450195.\n",
            "Epoch 4780. Training loss: 0.760395348072052. Validation loss: 2.7047009468078613.\n",
            "Epoch 4781. Training loss: 0.7603909373283386. Validation loss: 2.7047007083892822.\n",
            "Epoch 4782. Training loss: 0.7603862285614014. Validation loss: 2.704699993133545.\n",
            "Epoch 4783. Training loss: 0.7603816986083984. Validation loss: 2.7046995162963867.\n",
            "Epoch 4784. Training loss: 0.7603771686553955. Validation loss: 2.7046988010406494.\n",
            "Epoch 4785. Training loss: 0.7603725790977478. Validation loss: 2.704698324203491.\n",
            "Epoch 4786. Training loss: 0.7603681087493896. Validation loss: 2.704697608947754.\n",
            "Epoch 4787. Training loss: 0.7603633999824524. Validation loss: 2.704697370529175.\n",
            "Epoch 4788. Training loss: 0.7603588104248047. Validation loss: 2.7046966552734375.\n",
            "Epoch 4789. Training loss: 0.7603543400764465. Validation loss: 2.7046961784362793.\n",
            "Epoch 4790. Training loss: 0.7603497505187988. Validation loss: 2.704695463180542.\n",
            "Epoch 4791. Training loss: 0.7603452205657959. Validation loss: 2.704695224761963.\n",
            "Epoch 4792. Training loss: 0.7603406310081482. Validation loss: 2.7046947479248047.\n",
            "Epoch 4793. Training loss: 0.7603361010551453. Validation loss: 2.7046937942504883.\n",
            "Epoch 4794. Training loss: 0.7603314518928528. Validation loss: 2.704693078994751.\n",
            "Epoch 4795. Training loss: 0.7603268623352051. Validation loss: 2.704693078994751.\n",
            "Epoch 4796. Training loss: 0.7603223323822021. Validation loss: 2.7046923637390137.\n",
            "Epoch 4797. Training loss: 0.7603177428245544. Validation loss: 2.7046916484832764.\n",
            "Epoch 4798. Training loss: 0.760313093662262. Validation loss: 2.704690933227539.\n",
            "Epoch 4799. Training loss: 0.760308563709259. Validation loss: 2.704690933227539.\n",
            "Epoch 4800. Training loss: 0.7603039741516113. Validation loss: 2.7046899795532227.\n",
            "Epoch 4801. Training loss: 0.7602993845939636. Validation loss: 2.7046892642974854.\n",
            "Epoch 4802. Training loss: 0.7602947354316711. Validation loss: 2.7046890258789062.\n",
            "Epoch 4803. Training loss: 0.7602901458740234. Validation loss: 2.70468807220459.\n",
            "Epoch 4804. Training loss: 0.7602856159210205. Validation loss: 2.7046878337860107.\n",
            "Epoch 4805. Training loss: 0.7602810859680176. Validation loss: 2.7046873569488525.\n",
            "Epoch 4806. Training loss: 0.7602764964103699. Validation loss: 2.7046866416931152.\n",
            "Epoch 4807. Training loss: 0.7602719664573669. Validation loss: 2.704686403274536.\n",
            "Epoch 4808. Training loss: 0.7602673172950745. Validation loss: 2.704685688018799.\n",
            "Epoch 4809. Training loss: 0.760262668132782. Validation loss: 2.7046847343444824.\n",
            "Epoch 4810. Training loss: 0.760258138179779. Validation loss: 2.704684257507324.\n",
            "Epoch 4811. Training loss: 0.7602536082267761. Validation loss: 2.704684019088745.\n",
            "Epoch 4812. Training loss: 0.7602489590644836. Validation loss: 2.704683303833008.\n",
            "Epoch 4813. Training loss: 0.7602443695068359. Validation loss: 2.7046828269958496.\n",
            "Epoch 4814. Training loss: 0.7602397799491882. Validation loss: 2.7046823501586914.\n",
            "Epoch 4815. Training loss: 0.7602351307868958. Validation loss: 2.704681634902954.\n",
            "Epoch 4816. Training loss: 0.760230541229248. Validation loss: 2.704681396484375.\n",
            "Epoch 4817. Training loss: 0.7602260112762451. Validation loss: 2.704680919647217.\n",
            "Epoch 4818. Training loss: 0.7602214217185974. Validation loss: 2.7046799659729004.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4819. Training loss: 0.7602168917655945. Validation loss: 2.704679489135742.\n",
            "Epoch 4820. Training loss: 0.7602121829986572. Validation loss: 2.704678773880005.\n",
            "Epoch 4821. Training loss: 0.7602075934410095. Validation loss: 2.7046782970428467.\n",
            "Epoch 4822. Training loss: 0.760202944278717. Validation loss: 2.7046782970428467.\n",
            "Epoch 4823. Training loss: 0.7601983547210693. Validation loss: 2.7046773433685303.\n",
            "Epoch 4824. Training loss: 0.7601938247680664. Validation loss: 2.704677104949951.\n",
            "Epoch 4825. Training loss: 0.7601892352104187. Validation loss: 2.704676389694214.\n",
            "Epoch 4826. Training loss: 0.7601845860481262. Validation loss: 2.7046754360198975.\n",
            "Epoch 4827. Training loss: 0.7601799964904785. Validation loss: 2.7046754360198975.\n",
            "Epoch 4828. Training loss: 0.7601754069328308. Validation loss: 2.7046749591827393.\n",
            "Epoch 4829. Training loss: 0.7601707577705383. Validation loss: 2.7046737670898438.\n",
            "Epoch 4830. Training loss: 0.7601659893989563. Validation loss: 2.7046737670898438.\n",
            "Epoch 4831. Training loss: 0.7601615786552429. Validation loss: 2.7046728134155273.\n",
            "Epoch 4832. Training loss: 0.7601569294929504. Validation loss: 2.7046725749969482.\n",
            "Epoch 4833. Training loss: 0.7601523399353027. Validation loss: 2.704671859741211.\n",
            "Epoch 4834. Training loss: 0.760147750377655. Validation loss: 2.7046713829040527.\n",
            "Epoch 4835. Training loss: 0.7601430416107178. Validation loss: 2.7046704292297363.\n",
            "Epoch 4836. Training loss: 0.7601385116577148. Validation loss: 2.7046706676483154.\n",
            "Epoch 4837. Training loss: 0.7601339221000671. Validation loss: 2.70466947555542.\n",
            "Epoch 4838. Training loss: 0.7601292133331299. Validation loss: 2.7046689987182617.\n",
            "Epoch 4839. Training loss: 0.760124683380127. Validation loss: 2.7046685218811035.\n",
            "Epoch 4840. Training loss: 0.7601200938224792. Validation loss: 2.704667806625366.\n",
            "Epoch 4841. Training loss: 0.760115385055542. Validation loss: 2.704667329788208.\n",
            "Epoch 4842. Training loss: 0.7601107954978943. Validation loss: 2.704667091369629.\n",
            "Epoch 4843. Training loss: 0.7601062655448914. Validation loss: 2.7046663761138916.\n",
            "Epoch 4844. Training loss: 0.7601014971733093. Validation loss: 2.7046656608581543.\n",
            "Epoch 4845. Training loss: 0.7600969672203064. Validation loss: 2.704664945602417.\n",
            "Epoch 4846. Training loss: 0.7600923180580139. Validation loss: 2.704664468765259.\n",
            "Epoch 4847. Training loss: 0.7600877285003662. Validation loss: 2.7046642303466797.\n",
            "Epoch 4848. Training loss: 0.760083019733429. Validation loss: 2.7046632766723633.\n",
            "Epoch 4849. Training loss: 0.7600784301757812. Validation loss: 2.7046632766723633.\n",
            "Epoch 4850. Training loss: 0.7600738406181335. Validation loss: 2.704662322998047.\n",
            "Epoch 4851. Training loss: 0.7600691914558411. Validation loss: 2.7046618461608887.\n",
            "Epoch 4852. Training loss: 0.7600646018981934. Validation loss: 2.7046613693237305.\n",
            "Epoch 4853. Training loss: 0.7600600123405457. Validation loss: 2.7046608924865723.\n",
            "Epoch 4854. Training loss: 0.7600553631782532. Validation loss: 2.704660177230835.\n",
            "Epoch 4855. Training loss: 0.7600507140159607. Validation loss: 2.7046597003936768.\n",
            "Epoch 4856. Training loss: 0.7600460648536682. Validation loss: 2.7046589851379395.\n",
            "Epoch 4857. Training loss: 0.7600414156913757. Validation loss: 2.7046587467193604.\n",
            "Epoch 4858. Training loss: 0.7600367665290833. Validation loss: 2.704658031463623.\n",
            "Epoch 4859. Training loss: 0.7600321769714355. Validation loss: 2.7046570777893066.\n",
            "Epoch 4860. Training loss: 0.7600275874137878. Validation loss: 2.7046570777893066.\n",
            "Epoch 4861. Training loss: 0.7600229382514954. Validation loss: 2.7046561241149902.\n",
            "Epoch 4862. Training loss: 0.7600181698799133. Validation loss: 2.7046561241149902.\n",
            "Epoch 4863. Training loss: 0.7600136399269104. Validation loss: 2.704655408859253.\n",
            "Epoch 4864. Training loss: 0.7600089907646179. Validation loss: 2.7046546936035156.\n",
            "Epoch 4865. Training loss: 0.7600043416023254. Validation loss: 2.7046542167663574.\n",
            "Epoch 4866. Training loss: 0.759999692440033. Validation loss: 2.704653739929199.\n",
            "Epoch 4867. Training loss: 0.7599950432777405. Validation loss: 2.704653024673462.\n",
            "Epoch 4868. Training loss: 0.7599904537200928. Validation loss: 2.7046525478363037.\n",
            "Epoch 4869. Training loss: 0.7599857449531555. Validation loss: 2.7046520709991455.\n",
            "Epoch 4870. Training loss: 0.7599812150001526. Validation loss: 2.704651355743408.\n",
            "Epoch 4871. Training loss: 0.7599764466285706. Validation loss: 2.704650640487671.\n",
            "Epoch 4872. Training loss: 0.7599719166755676. Validation loss: 2.7046501636505127.\n",
            "Epoch 4873. Training loss: 0.7599671483039856. Validation loss: 2.7046496868133545.\n",
            "Epoch 4874. Training loss: 0.7599625587463379. Validation loss: 2.704648971557617.\n",
            "Epoch 4875. Training loss: 0.7599579691886902. Validation loss: 2.704648494720459.\n",
            "Epoch 4876. Training loss: 0.7599533200263977. Validation loss: 2.704648017883301.\n",
            "Epoch 4877. Training loss: 0.7599485516548157. Validation loss: 2.7046477794647217.\n",
            "Epoch 4878. Training loss: 0.7599440217018127. Validation loss: 2.7046470642089844.\n",
            "Epoch 4879. Training loss: 0.7599393725395203. Validation loss: 2.704646110534668.\n",
            "Epoch 4880. Training loss: 0.759934663772583. Validation loss: 2.7046456336975098.\n",
            "Epoch 4881. Training loss: 0.7599300742149353. Validation loss: 2.7046451568603516.\n",
            "Epoch 4882. Training loss: 0.7599254250526428. Validation loss: 2.7046446800231934.\n",
            "Epoch 4883. Training loss: 0.7599207758903503. Validation loss: 2.704644203186035.\n",
            "Epoch 4884. Training loss: 0.7599160671234131. Validation loss: 2.704643726348877.\n",
            "Epoch 4885. Training loss: 0.7599113583564758. Validation loss: 2.7046432495117188.\n",
            "Epoch 4886. Training loss: 0.7599067687988281. Validation loss: 2.7046425342559814.\n",
            "Epoch 4887. Training loss: 0.7599021792411804. Validation loss: 2.7046420574188232.\n",
            "Epoch 4888. Training loss: 0.7598974704742432. Validation loss: 2.704641342163086.\n",
            "Epoch 4889. Training loss: 0.7598927617073059. Validation loss: 2.7046408653259277.\n",
            "Epoch 4890. Training loss: 0.7598881721496582. Validation loss: 2.7046403884887695.\n",
            "Epoch 4891. Training loss: 0.7598835825920105. Validation loss: 2.7046396732330322.\n",
            "Epoch 4892. Training loss: 0.7598788142204285. Validation loss: 2.704639196395874.\n",
            "Epoch 4893. Training loss: 0.7598741054534912. Validation loss: 2.704638719558716.\n",
            "Epoch 4894. Training loss: 0.7598695158958435. Validation loss: 2.7046380043029785.\n",
            "Epoch 4895. Training loss: 0.7598648071289062. Validation loss: 2.7046375274658203.\n",
            "Epoch 4896. Training loss: 0.7598602175712585. Validation loss: 2.704636812210083.\n",
            "Epoch 4897. Training loss: 0.7598554491996765. Validation loss: 2.704636335372925.\n",
            "Epoch 4898. Training loss: 0.759850800037384. Validation loss: 2.7046358585357666.\n",
            "Epoch 4899. Training loss: 0.7598462104797363. Validation loss: 2.7046353816986084.\n",
            "Epoch 4900. Training loss: 0.7598415017127991. Validation loss: 2.704634428024292.\n",
            "Epoch 4901. Training loss: 0.7598368525505066. Validation loss: 2.704634428024292.\n",
            "Epoch 4902. Training loss: 0.7598321437835693. Validation loss: 2.7046337127685547.\n",
            "Epoch 4903. Training loss: 0.7598274350166321. Validation loss: 2.7046332359313965.\n",
            "Epoch 4904. Training loss: 0.7598227858543396. Validation loss: 2.70463228225708.\n",
            "Epoch 4905. Training loss: 0.7598181366920471. Validation loss: 2.704631805419922.\n",
            "Epoch 4906. Training loss: 0.7598134875297546. Validation loss: 2.7046315670013428.\n",
            "Epoch 4907. Training loss: 0.7598087787628174. Validation loss: 2.7046308517456055.\n",
            "Epoch 4908. Training loss: 0.7598041892051697. Validation loss: 2.7046303749084473.\n",
            "Epoch 4909. Training loss: 0.7597994804382324. Validation loss: 2.704629898071289.\n",
            "Epoch 4910. Training loss: 0.7597947716712952. Validation loss: 2.704629421234131.\n",
            "Epoch 4911. Training loss: 0.7597901225090027. Validation loss: 2.7046284675598145.\n",
            "Epoch 4912. Training loss: 0.7597854137420654. Validation loss: 2.7046284675598145.\n",
            "Epoch 4913. Training loss: 0.7597807049751282. Validation loss: 2.704627752304077.\n",
            "Epoch 4914. Training loss: 0.7597760558128357. Validation loss: 2.70462703704834.\n",
            "Epoch 4915. Training loss: 0.7597713470458984. Validation loss: 2.7046263217926025.\n",
            "Epoch 4916. Training loss: 0.7597667574882507. Validation loss: 2.7046258449554443.\n",
            "Epoch 4917. Training loss: 0.7597619891166687. Validation loss: 2.704625368118286.\n",
            "Epoch 4918. Training loss: 0.7597573399543762. Validation loss: 2.704624891281128.\n",
            "Epoch 4919. Training loss: 0.7597526907920837. Validation loss: 2.7046239376068115.\n",
            "Epoch 4920. Training loss: 0.7597479820251465. Validation loss: 2.704623222351074.\n",
            "Epoch 4921. Training loss: 0.7597432732582092. Validation loss: 2.704623222351074.\n",
            "Epoch 4922. Training loss: 0.7597386240959167. Validation loss: 2.704622268676758.\n",
            "Epoch 4923. Training loss: 0.7597339153289795. Validation loss: 2.7046220302581787.\n",
            "Epoch 4924. Training loss: 0.7597292065620422. Validation loss: 2.7046215534210205.\n",
            "Epoch 4925. Training loss: 0.7597244381904602. Validation loss: 2.704620838165283.\n",
            "Epoch 4926. Training loss: 0.7597198486328125. Validation loss: 2.704620122909546.\n",
            "Epoch 4927. Training loss: 0.7597150802612305. Validation loss: 2.704619884490967.\n",
            "Epoch 4928. Training loss: 0.7597103714942932. Validation loss: 2.7046191692352295.\n",
            "Epoch 4929. Training loss: 0.7597057223320007. Validation loss: 2.7046189308166504.\n",
            "Epoch 4930. Training loss: 0.7597010731697083. Validation loss: 2.704618453979492.\n",
            "Epoch 4931. Training loss: 0.7596963047981262. Validation loss: 2.704617500305176.\n",
            "Epoch 4932. Training loss: 0.7596916556358337. Validation loss: 2.7046167850494385.\n",
            "Epoch 4933. Training loss: 0.7596869468688965. Validation loss: 2.7046163082122803.\n",
            "Epoch 4934. Training loss: 0.7596822381019592. Validation loss: 2.704615592956543.\n",
            "Epoch 4935. Training loss: 0.7596775889396667. Validation loss: 2.704615354537964.\n",
            "Epoch 4936. Training loss: 0.7596728801727295. Validation loss: 2.7046146392822266.\n",
            "Epoch 4937. Training loss: 0.7596681118011475. Validation loss: 2.7046144008636475.\n",
            "Epoch 4938. Training loss: 0.7596634030342102. Validation loss: 2.7046139240264893.\n",
            "Epoch 4939. Training loss: 0.7596587538719177. Validation loss: 2.7046127319335938.\n",
            "Epoch 4940. Training loss: 0.7596540451049805. Validation loss: 2.7046127319335938.\n",
            "Epoch 4941. Training loss: 0.7596492767333984. Validation loss: 2.7046120166778564.\n",
            "Epoch 4942. Training loss: 0.7596446871757507. Validation loss: 2.7046115398406982.\n",
            "Epoch 4943. Training loss: 0.7596399188041687. Validation loss: 2.704610824584961.\n",
            "Epoch 4944. Training loss: 0.7596352100372314. Validation loss: 2.7046103477478027.\n",
            "Epoch 4945. Training loss: 0.7596305012702942. Validation loss: 2.7046096324920654.\n",
            "Epoch 4946. Training loss: 0.7596257328987122. Validation loss: 2.7046093940734863.\n",
            "Epoch 4947. Training loss: 0.7596211433410645. Validation loss: 2.704608678817749.\n",
            "Epoch 4948. Training loss: 0.7596163153648376. Validation loss: 2.704608201980591.\n",
            "Epoch 4949. Training loss: 0.7596116065979004. Validation loss: 2.7046077251434326.\n",
            "Epoch 4950. Training loss: 0.7596070170402527. Validation loss: 2.704606771469116.\n",
            "Epoch 4951. Training loss: 0.7596022486686707. Validation loss: 2.704606056213379.\n",
            "Epoch 4952. Training loss: 0.7595975399017334. Validation loss: 2.7046058177948.\n",
            "Epoch 4953. Training loss: 0.7595927715301514. Validation loss: 2.7046053409576416.\n",
            "Epoch 4954. Training loss: 0.7595880031585693. Validation loss: 2.7046046257019043.\n",
            "Epoch 4955. Training loss: 0.7595834136009216. Validation loss: 2.704604148864746.\n",
            "Epoch 4956. Training loss: 0.7595786452293396. Validation loss: 2.704603433609009.\n",
            "Epoch 4957. Training loss: 0.7595739364624023. Validation loss: 2.7046031951904297.\n",
            "Epoch 4958. Training loss: 0.7595691680908203. Validation loss: 2.7046029567718506.\n",
            "Epoch 4959. Training loss: 0.7595645785331726. Validation loss: 2.7046022415161133.\n",
            "Epoch 4960. Training loss: 0.7595598101615906. Validation loss: 2.704601764678955.\n",
            "Epoch 4961. Training loss: 0.7595550417900085. Validation loss: 2.7046008110046387.\n",
            "Epoch 4962. Training loss: 0.7595503330230713. Validation loss: 2.7046003341674805.\n",
            "Epoch 4963. Training loss: 0.7595455646514893. Validation loss: 2.7045998573303223.\n",
            "Epoch 4964. Training loss: 0.759540855884552. Validation loss: 2.704599380493164.\n",
            "Epoch 4965. Training loss: 0.75953608751297. Validation loss: 2.7045984268188477.\n",
            "Epoch 4966. Training loss: 0.7595314979553223. Validation loss: 2.7045979499816895.\n",
            "Epoch 4967. Training loss: 0.7595267295837402. Validation loss: 2.7045974731445312.\n",
            "Epoch 4968. Training loss: 0.7595219612121582. Validation loss: 2.704596996307373.\n",
            "Epoch 4969. Training loss: 0.759517252445221. Validation loss: 2.7045960426330566.\n",
            "Epoch 4970. Training loss: 0.7595124840736389. Validation loss: 2.7045960426330566.\n",
            "Epoch 4971. Training loss: 0.7595078349113464. Validation loss: 2.7045953273773193.\n",
            "Epoch 4972. Training loss: 0.7595031261444092. Validation loss: 2.704594850540161.\n",
            "Epoch 4973. Training loss: 0.7594982981681824. Validation loss: 2.704594135284424.\n",
            "Epoch 4974. Training loss: 0.7594935894012451. Validation loss: 2.7045938968658447.\n",
            "Epoch 4975. Training loss: 0.7594888806343079. Validation loss: 2.7045931816101074.\n",
            "Epoch 4976. Training loss: 0.7594842314720154. Validation loss: 2.70459246635437.\n",
            "Epoch 4977. Training loss: 0.7594793438911438. Validation loss: 2.704591751098633.\n",
            "Epoch 4978. Training loss: 0.7594745755195618. Validation loss: 2.7045915126800537.\n",
            "Epoch 4979. Training loss: 0.7594699263572693. Validation loss: 2.7045907974243164.\n",
            "Epoch 4980. Training loss: 0.7594651579856873. Validation loss: 2.704590320587158.\n",
            "Epoch 4981. Training loss: 0.75946044921875. Validation loss: 2.70458984375.\n",
            "Epoch 4982. Training loss: 0.759455680847168. Validation loss: 2.7045888900756836.\n",
            "Epoch 4983. Training loss: 0.7594509720802307. Validation loss: 2.7045886516571045.\n",
            "Epoch 4984. Training loss: 0.7594462037086487. Validation loss: 2.704587936401367.\n",
            "Epoch 4985. Training loss: 0.7594414353370667. Validation loss: 2.704587936401367.\n",
            "Epoch 4986. Training loss: 0.7594367861747742. Validation loss: 2.7045867443084717.\n",
            "Epoch 4987. Training loss: 0.7594320178031921. Validation loss: 2.7045862674713135.\n",
            "Epoch 4988. Training loss: 0.7594272494316101. Validation loss: 2.7045860290527344.\n",
            "Epoch 4989. Training loss: 0.7594225406646729. Validation loss: 2.704585313796997.\n",
            "Epoch 4990. Training loss: 0.7594177722930908. Validation loss: 2.704584836959839.\n",
            "Epoch 4991. Training loss: 0.759412944316864. Validation loss: 2.7045841217041016.\n",
            "Epoch 4992. Training loss: 0.7594082355499268. Validation loss: 2.7045836448669434.\n",
            "Epoch 4993. Training loss: 0.7594034075737. Validation loss: 2.704583168029785.\n",
            "Epoch 4994. Training loss: 0.7593987584114075. Validation loss: 2.7045822143554688.\n",
            "Epoch 4995. Training loss: 0.7593939900398254. Validation loss: 2.7045817375183105.\n",
            "Epoch 4996. Training loss: 0.7593891620635986. Validation loss: 2.7045812606811523.\n",
            "Epoch 4997. Training loss: 0.7593844532966614. Validation loss: 2.704580783843994.\n",
            "Epoch 4998. Training loss: 0.7593796849250793. Validation loss: 2.704580307006836.\n",
            "Epoch 4999. Training loss: 0.7593749165534973. Validation loss: 2.7045795917510986.\n"
          ]
        }
      ],
      "source": [
        "train_model = True  # TODO hyperparameter\n",
        "load_pretrained_model = True  # TODO hyperparameter\n",
        "plot_distributions = True  # TODO hyperparameter\n",
        "standardize = False  # TODO hyperparameter\n",
        "\n",
        "input_dim = 1\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "apmlp = []\n",
        "apbm = []\n",
        "\n",
        "##% TRAINING\n",
        "if train_model:\n",
        "    model = MLPsumV2(input_dim=input_dim, latent_dim=16, output_dim=4,  # latent_dim=64\n",
        "                     k=8, dropout=0, cell_layers=1,  # k=4\n",
        "                     proj_layers=2, reduction='sum')\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=1e-6)\n",
        "    loss_func = losses.SupConLoss(distance=distances.CosineSimilarity())\n",
        "\n",
        "    epochs = 5000\n",
        "\n",
        "    for e in range(epochs):\n",
        "        model.train()\n",
        "        tr_loss = 0.0\n",
        "        for points, labels in train_loader:\n",
        "            feats, _ = model(points)\n",
        "\n",
        "            tr_loss_tmp = loss_func(feats, labels)\n",
        "            tr_loss += tr_loss_tmp.item()\n",
        "\n",
        "            tr_loss_tmp.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        tr_loss /= (idx + 1)\n",
        "\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        best_val = np.inf\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            points = torch.concatenate([torch.tensor(x['data'], dtype=torch.float32) for x in val_samples])\n",
        "            labels = torch.concatenate([torch.tensor(x['label'], dtype=torch.int16) for x in val_samples])\n",
        "            feats, _ = model(points)\n",
        "\n",
        "            val_loss_tmp = loss_func(feats, labels)\n",
        "            val_loss += val_loss_tmp.item()\n",
        "\n",
        "        if val_loss < best_val:\n",
        "            best_val = val_loss\n",
        "            best_model = copy.deepcopy(model)\n",
        "\n",
        "        print(f\"Epoch {e}. Training loss: {tr_loss}. Validation loss: {val_loss}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c93f98bc",
      "metadata": {
        "id": "c93f98bc",
        "outputId": "58014fd4-9ffa-41cd-d6a9-60e4a9f59677"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total model mAP: 0.3337187696562696 \n",
            "Total model precision at R: 0.41666666666666663\n",
            "|   compound |       AP |   precision at R |\n",
            "|-----------:|---------:|-----------------:|\n",
            "|          6 | 0.553788 |         0.583333 |\n",
            "|          7 | 0.380853 |         0.416667 |\n",
            "|          9 | 0.221032 |         0.416667 |\n",
            "|          8 | 0.179202 |         0.25     |\n",
            "Total baseline (mean) mAP: 0.2228174603174603 \n",
            "Total baseline (mean) precision at R: 0.29166666666666663\n",
            "|   compound |        AP |   precision at R |\n",
            "|-----------:|----------:|-----------------:|\n",
            "|          6 | 0.443254  |         0.5      |\n",
            "|          9 | 0.216667  |         0.5      |\n",
            "|          7 | 0.203571  |         0.166667 |\n",
            "|          8 | 0.0277778 |         0        |\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/rdijk/Downloads/utils.py:335: FutureWarning: DataFrame.set_axis 'inplace' keyword is deprecated and will be removed in a future version. Use `obj = obj.set_axis(..., copy=False)` instead\n",
            "  dist.set_axis(compound_names, axis=1, inplace=True)\n",
            "/Users/rdijk/Downloads/utils.py:336: FutureWarning: DataFrame.set_axis 'inplace' keyword is deprecated and will be removed in a future version. Use `obj = obj.set_axis(..., copy=False)` instead\n",
            "  dist.set_axis(compound_names, axis=0, inplace=True)\n",
            "/Users/rdijk/Downloads/utils.py:335: FutureWarning: DataFrame.set_axis 'inplace' keyword is deprecated and will be removed in a future version. Use `obj = obj.set_axis(..., copy=False)` instead\n",
            "  dist.set_axis(compound_names, axis=1, inplace=True)\n",
            "/Users/rdijk/Downloads/utils.py:336: FutureWarning: DataFrame.set_axis 'inplace' keyword is deprecated and will be removed in a future version. Use `obj = obj.set_axis(..., copy=False)` instead\n",
            "  dist.set_axis(compound_names, axis=0, inplace=True)\n"
          ]
        }
      ],
      "source": [
        "#%% Evaluate model performance\n",
        "model.eval()\n",
        "MLP_profiles = pd.DataFrame()\n",
        "BM_profiles = pd.DataFrame()\n",
        "with torch.no_grad():\n",
        "    for idx in range(len(val_samples)):\n",
        "        points = torch.tensor(val_samples[idx]['data'], dtype=torch.float32)\n",
        "        labels = val_samples[idx]['label']\n",
        "        feats, _ = model(points)\n",
        "\n",
        "        # Append everything to dataframes\n",
        "        c1 = pd.concat([pd.DataFrame(feats), pd.Series(labels)], axis=1)\n",
        "        MLP_profiles = pd.concat([MLP_profiles, c1])\n",
        "\n",
        "        c2 = pd.concat([pd.DataFrame(points.mean(dim=1)), pd.Series(labels)], axis=1)\n",
        "        BM_profiles = pd.concat([BM_profiles, c2])\n",
        "\n",
        "MLP_profiles.columns = [f\"f{x}\" for x in range(MLP_profiles.shape[1] - 1)] + ['Metadata_labels']\n",
        "BM_profiles.columns = [f\"f{x}\" for x in range(BM_profiles.shape[1] - 1)] + ['Metadata_labels']\n",
        "AP_MLP = utils.CalculateMAP(MLP_profiles, 'cosine_similarity', groupby='Metadata_labels')\n",
        "apmlp.append(AP_MLP.AP.mean())\n",
        "print('Total model mAP:', AP_MLP.AP.mean(), '\\nTotal model precision at R:', AP_MLP['precision at R'].mean())\n",
        "print(AP_MLP.groupby(by='compound').mean().sort_values(by='AP', ascending=False).to_markdown())\n",
        "\n",
        "AP_BM = utils.CalculateMAP(BM_profiles, 'cosine_similarity', groupby='Metadata_labels')\n",
        "apbm.append(AP_BM.AP.mean())\n",
        "print('Total baseline (mean) mAP:', AP_BM.AP.mean(), '\\nTotal baseline (mean) precision at R:', AP_BM['precision at R'].mean())\n",
        "print(AP_BM.groupby(by='compound').mean().sort_values(by='AP', ascending=False).to_markdown())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fcad9f7",
      "metadata": {
        "id": "4fcad9f7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
