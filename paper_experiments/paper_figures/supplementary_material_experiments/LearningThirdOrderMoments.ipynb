{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed801383-19da-4bd2-afce-a4b027af7cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "ROOT_DIR = Path(os.getcwd()).resolve().parent.parent\n",
    "if (ROOT_DIR / \"networks\").exists():\n",
    "    sys.path.append(str(ROOT_DIR))\n",
    "else:\n",
    "    raise FileNotFoundError(\"Root directory does not contain expected structure. Please adjust ROOT_DIR.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0e0135f",
   "metadata": {
    "id": "a0e0135f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/FeatureAggregation/lib/python3.8/site-packages/umap/plot.py:203: NumbaDeprecationWarning: The keyword argument 'nopython=False' was supplied. From Numba 0.59.0 the default is being changed to True and use of 'nopython=False' will raise a warning as the argument will have no effect. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit(nopython=False)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Training stuff\n",
    "from pytorch_metric_learning import losses, distances\n",
    "from networks.SimpleMLPs import MLPsumV2\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Evaluation stuff\n",
    "import pandas as pd\n",
    "import src.utils as utils\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cceeeff9",
   "metadata": {
    "id": "cceeeff9"
   },
   "outputs": [],
   "source": [
    "def get_correlated_dataset(n, dependency, mu, input_dim):\n",
    "    latent = np.random.randn(n, input_dim)\n",
    "    dependent = latent.dot(dependency)\n",
    "    scaled = dependent\n",
    "    scaled_with_offset = scaled + mu\n",
    "    # return x and y of the new, correlated dataset\n",
    "    return scaled_with_offset[:, 0], scaled_with_offset[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a746885",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "2a746885",
    "outputId": "c74dbfe4-7357-4c84-9027-fa295e0c838e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_batched_data(num_samples, skewness, nr_replicates):\n",
    "    data = []\n",
    "    for _ in range(nr_replicates):\n",
    "        samples = np.random.standard_normal(num_samples)\n",
    "        samples = samples + skewness * (samples ** 3)\n",
    "        data.append(samples)\n",
    "\n",
    "    # Convert data to a numpy array and reshape for batching\n",
    "    batch = np.array(data)\n",
    "\n",
    "    distribution_means = np.mean(batch, axis=1)\n",
    "    distribution_stds = np.std(batch, axis=1)\n",
    "\n",
    "    norm_batch = (batch - distribution_means[:, np.newaxis]) / distribution_stds[:, np.newaxis]\n",
    "\n",
    "    return norm_batch[:, :, np.newaxis]\n",
    "\n",
    "\n",
    "# Parameters\n",
    "num_samples = 800\n",
    "num_distributions = 10\n",
    "skewness_values = np.linspace(-1, 1, num_distributions)  # Varying skewness values\n",
    "nr_replicates = 4  # Number of samples per class or distribution\n",
    "\n",
    "# Create a 2x5 grid of plots\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, skewness in enumerate(skewness_values):\n",
    "    # Generate skewed data with varying skewness values and different points\n",
    "    skewed_data = generate_batched_data(num_samples, skewness, nr_replicates)\n",
    "\n",
    "    # Plot the distribution\n",
    "    axes[i].hist(skewed_data[0], bins=30, density=True, alpha=0.7)\n",
    "    axes[i].set_title(f\"Class label: {i}\\nSkewness: {scipy.stats.skew(skewed_data[0])[0]}\\nMean: {np.mean(skewed_data)}\\nStd: {np.std(skewed_data)}\")\n",
    "    axes[i].set_xlabel(\"Value\")\n",
    "    axes[i].set_ylabel(\"Density\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "qCkhzQG95LzW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "qCkhzQG95LzW",
    "outputId": "bf2566c8-d9a6-453b-9428-78acff24ec5a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Create a 2x5 grid of plots\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Create a table to display statistics\n",
    "stat_table_data = []\n",
    "\n",
    "for i, skewness in enumerate(skewness_values):\n",
    "    # Generate skewed data with varying skewness values and different points\n",
    "    skewed_data = generate_batched_data(num_samples, skewness, nr_replicates)\n",
    "\n",
    "    # Plot the distribution\n",
    "    axes[i].hist(skewed_data[0], bins=30, density=True, alpha=0.7, color='blue', edgecolor='black')\n",
    "    axes[i].set_title(f\"Class {i+1}\")\n",
    "    axes[i].grid(False)\n",
    "\n",
    "    # Collect statistics for the table\n",
    "    skewness_value = scipy.stats.skew(skewed_data[0]).round(2)\n",
    "    mean_value = np.mean(skewed_data).round(2)\n",
    "    std_value = np.std(skewed_data).round(2)\n",
    "    stat_table_data.append([skewness_value, mean_value, std_value])\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "DITNOJfwAUg3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 791
    },
    "id": "DITNOJfwAUg3",
    "outputId": "f361b3b0-26db-4e39-eb5a-f8315d728f25"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0, 0.0, 1.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "table_rows = []\n",
    "for i, (skewness, mean, std) in enumerate(stat_table_data):\n",
    "    table_rows.append([i+1, f'{skewness[0]}', f'{mean}', f'{std}'])\n",
    "\n",
    "# Create a table on the side for statistics\n",
    "stat_table = plt.table(cellText=table_rows,\n",
    "                       cellLoc='center',\n",
    "                       colLabels=['Class', 'Skewness', 'Mean', 'Std'],\n",
    "                       )\n",
    "stat_table.auto_set_font_size(False)\n",
    "stat_table.set_fontsize(12)\n",
    "stat_table.scale(1, 2)  # Adjust the scaling as needed\n",
    "\n",
    "plt.gca().add_table(stat_table)\n",
    "plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NvgNOvmX7rCf",
   "metadata": {
    "id": "NvgNOvmX7rCf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2cfab3e",
   "metadata": {
    "id": "c2cfab3e",
    "outputId": "ace76465-809e-42f1-a361-6949ad12ab9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nr. train samples: 6 nr. val samples: 4 data shape: (4, 800, 1)\n"
     ]
    }
   ],
   "source": [
    "nr_replicates = 4\n",
    "num_samples = 800\n",
    "num_distributions = 10\n",
    "skewness_values = np.linspace(-1, 1, num_distributions)  # Varying skewness values\n",
    "\n",
    "train_samples = []\n",
    "val_samples = []\n",
    "\n",
    "for i, skewness in enumerate(skewness_values):\n",
    "    data = generate_batched_data(num_samples, skewness, nr_replicates)\n",
    "\n",
    "    if i < 6:\n",
    "        train_samples.append({'data':data,\n",
    "         'label': [i]*nr_replicates})\n",
    "    else:\n",
    "        val_samples.append({'data': data,\n",
    "             'label': [i]*nr_replicates})\n",
    "\n",
    "print(\"nr. train samples:\", len(train_samples), \"nr. val samples:\", len(val_samples), \"data shape:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bdf6d16",
   "metadata": {
    "id": "1bdf6d16"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Shuffle the training samples\n",
    "import random\n",
    "random.shuffle(train_samples)\n",
    "\n",
    "# Batch size\n",
    "bs = 24\n",
    "\n",
    "# Lists to store data and labels for each batch\n",
    "batched_data = []\n",
    "batched_labels = []\n",
    "\n",
    "for idx in range(0, len(train_samples), bs):\n",
    "    batch_data = []\n",
    "    batch_label = []\n",
    "\n",
    "    # Collect data and labels for the current batch\n",
    "    for sample in train_samples[idx:idx + bs]:\n",
    "        batch_data.append(torch.tensor(sample['data'], dtype=torch.float32))\n",
    "        batch_label.append(torch.tensor(sample['label'], dtype=torch.int64))\n",
    "\n",
    "    batched_data.append(torch.cat(batch_data))\n",
    "    batched_labels.append(torch.cat(batch_label))\n",
    "\n",
    "# Concatenate batched data and labels\n",
    "batched_data = torch.cat(batched_data)\n",
    "batched_labels = torch.cat(batched_labels)\n",
    "\n",
    "# Create a DataLoader for batching\n",
    "train_dataset = TensorDataset(batched_data, batched_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ab5f0d",
   "metadata": {
    "id": "b2ab5f0d",
    "outputId": "377536f0-a58b-437c-85ed-ccd9065309b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Training loss: 3.1328341960906982. Validation loss: 2.708005666732788.\n",
      "Epoch 1. Training loss: 3.1328327655792236. Validation loss: 2.708005428314209.\n",
      "Epoch 2. Training loss: 3.132831335067749. Validation loss: 2.70800518989563.\n",
      "Epoch 3. Training loss: 3.1328299045562744. Validation loss: 2.708005666732788.\n",
      "Epoch 4. Training loss: 3.132828712463379. Validation loss: 2.708005428314209.\n",
      "Epoch 5. Training loss: 3.132827043533325. Validation loss: 2.708004951477051.\n",
      "Epoch 6. Training loss: 3.1328256130218506. Validation loss: 2.708004951477051.\n",
      "Epoch 7. Training loss: 3.132823944091797. Validation loss: 2.708004951477051.\n",
      "Epoch 8. Training loss: 3.1328232288360596. Validation loss: 2.708004951477051.\n",
      "Epoch 9. Training loss: 3.132821798324585. Validation loss: 2.708004951477051.\n",
      "Epoch 10. Training loss: 3.1328201293945312. Validation loss: 2.70800518989563.\n",
      "Epoch 11. Training loss: 3.1328189373016357. Validation loss: 2.70800518989563.\n",
      "Epoch 12. Training loss: 3.132817268371582. Validation loss: 2.70800518989563.\n",
      "Epoch 13. Training loss: 3.1328160762786865. Validation loss: 2.70800518989563.\n",
      "Epoch 14. Training loss: 3.132814407348633. Validation loss: 2.70800518989563.\n",
      "Epoch 15. Training loss: 3.1328132152557373. Validation loss: 2.70800518989563.\n",
      "Epoch 16. Training loss: 3.1328117847442627. Validation loss: 2.708004951477051.\n",
      "Epoch 17. Training loss: 3.132810354232788. Validation loss: 2.708004951477051.\n",
      "Epoch 18. Training loss: 3.1328089237213135. Validation loss: 2.70800518989563.\n",
      "Epoch 19. Training loss: 3.132807731628418. Validation loss: 2.708004951477051.\n",
      "Epoch 20. Training loss: 3.1328060626983643. Validation loss: 2.70800518989563.\n",
      "Epoch 21. Training loss: 3.1328048706054688. Validation loss: 2.708004951477051.\n",
      "Epoch 22. Training loss: 3.132803201675415. Validation loss: 2.7080047130584717.\n",
      "Epoch 23. Training loss: 3.1328020095825195. Validation loss: 2.70800518989563.\n",
      "Epoch 24. Training loss: 3.132800340652466. Validation loss: 2.70800518989563.\n",
      "Epoch 25. Training loss: 3.1327991485595703. Validation loss: 2.7080047130584717.\n",
      "Epoch 26. Training loss: 3.132797956466675. Validation loss: 2.7080047130584717.\n",
      "Epoch 27. Training loss: 3.132796287536621. Validation loss: 2.7080047130584717.\n",
      "Epoch 28. Training loss: 3.1327946186065674. Validation loss: 2.7080044746398926.\n",
      "Epoch 29. Training loss: 3.132793426513672. Validation loss: 2.7080047130584717.\n",
      "Epoch 30. Training loss: 3.132791757583618. Validation loss: 2.7080047130584717.\n",
      "Epoch 31. Training loss: 3.1327905654907227. Validation loss: 2.7080047130584717.\n",
      "Epoch 32. Training loss: 3.132788896560669. Validation loss: 2.7080047130584717.\n",
      "Epoch 33. Training loss: 3.1327877044677734. Validation loss: 2.70800518989563.\n",
      "Epoch 34. Training loss: 3.132786512374878. Validation loss: 2.7080047130584717.\n",
      "Epoch 35. Training loss: 3.1327850818634033. Validation loss: 2.7080044746398926.\n",
      "Epoch 36. Training loss: 3.1327836513519287. Validation loss: 2.7080047130584717.\n",
      "Epoch 37. Training loss: 3.132781982421875. Validation loss: 2.7080047130584717.\n",
      "Epoch 38. Training loss: 3.1327807903289795. Validation loss: 2.7080047130584717.\n",
      "Epoch 39. Training loss: 3.132779121398926. Validation loss: 2.7080047130584717.\n",
      "Epoch 40. Training loss: 3.1327779293060303. Validation loss: 2.7080047130584717.\n",
      "Epoch 41. Training loss: 3.1327764987945557. Validation loss: 2.7080047130584717.\n",
      "Epoch 42. Training loss: 3.132774591445923. Validation loss: 2.7080044746398926.\n",
      "Epoch 43. Training loss: 3.1327733993530273. Validation loss: 2.7080047130584717.\n",
      "Epoch 44. Training loss: 3.132772207260132. Validation loss: 2.7080047130584717.\n",
      "Epoch 45. Training loss: 3.132770538330078. Validation loss: 2.7080044746398926.\n",
      "Epoch 46. Training loss: 3.1327693462371826. Validation loss: 2.7080042362213135.\n",
      "Epoch 47. Training loss: 3.132767677307129. Validation loss: 2.7080039978027344.\n",
      "Epoch 48. Training loss: 3.1327664852142334. Validation loss: 2.7080039978027344.\n",
      "Epoch 49. Training loss: 3.132765054702759. Validation loss: 2.7080042362213135.\n",
      "Epoch 50. Training loss: 3.132763624191284. Validation loss: 2.7080039978027344.\n",
      "Epoch 51. Training loss: 3.1327619552612305. Validation loss: 2.7080039978027344.\n",
      "Epoch 52. Training loss: 3.132760763168335. Validation loss: 2.7080039978027344.\n",
      "Epoch 53. Training loss: 3.1327590942382812. Validation loss: 2.7080039978027344.\n",
      "Epoch 54. Training loss: 3.1327579021453857. Validation loss: 2.7080042362213135.\n",
      "Epoch 55. Training loss: 3.132756233215332. Validation loss: 2.7080039978027344.\n",
      "Epoch 56. Training loss: 3.1327545642852783. Validation loss: 2.708003520965576.\n",
      "Epoch 57. Training loss: 3.132753372192383. Validation loss: 2.7080037593841553.\n",
      "Epoch 58. Training loss: 3.1327521800994873. Validation loss: 2.7080042362213135.\n",
      "Epoch 59. Training loss: 3.1327507495880127. Validation loss: 2.7080039978027344.\n",
      "Epoch 60. Training loss: 3.132749319076538. Validation loss: 2.7080039978027344.\n",
      "Epoch 61. Training loss: 3.1327474117279053. Validation loss: 2.7080037593841553.\n",
      "Epoch 62. Training loss: 3.1327459812164307. Validation loss: 2.7080039978027344.\n",
      "Epoch 63. Training loss: 3.132744789123535. Validation loss: 2.7080039978027344.\n",
      "Epoch 64. Training loss: 3.1327428817749023. Validation loss: 2.708003520965576.\n",
      "Epoch 65. Training loss: 3.132741689682007. Validation loss: 2.708003520965576.\n",
      "Epoch 66. Training loss: 3.132740020751953. Validation loss: 2.708003520965576.\n",
      "Epoch 67. Training loss: 3.1327388286590576. Validation loss: 2.7080039978027344.\n",
      "Epoch 68. Training loss: 3.132737398147583. Validation loss: 2.708003520965576.\n",
      "Epoch 69. Training loss: 3.13273549079895. Validation loss: 2.708003520965576.\n",
      "Epoch 70. Training loss: 3.132734537124634. Validation loss: 2.708003520965576.\n",
      "Epoch 71. Training loss: 3.1327333450317383. Validation loss: 2.708003520965576.\n",
      "Epoch 72. Training loss: 3.1327314376831055. Validation loss: 2.708003520965576.\n",
      "Epoch 73. Training loss: 3.13273024559021. Validation loss: 2.708003520965576.\n",
      "Epoch 74. Training loss: 3.1327288150787354. Validation loss: 2.7080039978027344.\n",
      "Epoch 75. Training loss: 3.1327273845672607. Validation loss: 2.7080037593841553.\n",
      "Epoch 76. Training loss: 3.132725715637207. Validation loss: 2.7080042362213135.\n",
      "Epoch 77. Training loss: 3.1327240467071533. Validation loss: 2.7080037593841553.\n",
      "Epoch 78. Training loss: 3.132723093032837. Validation loss: 2.708003520965576.\n",
      "Epoch 79. Training loss: 3.132721185684204. Validation loss: 2.708003520965576.\n",
      "Epoch 80. Training loss: 3.1327199935913086. Validation loss: 2.708003520965576.\n",
      "Epoch 81. Training loss: 3.132718324661255. Validation loss: 2.7080037593841553.\n",
      "Epoch 82. Training loss: 3.1327171325683594. Validation loss: 2.708003520965576.\n",
      "Epoch 83. Training loss: 3.1327154636383057. Validation loss: 2.7080037593841553.\n",
      "Epoch 84. Training loss: 3.13271427154541. Validation loss: 2.708003520965576.\n",
      "Epoch 85. Training loss: 3.1327126026153564. Validation loss: 2.708003520965576.\n",
      "Epoch 86. Training loss: 3.132711172103882. Validation loss: 2.7080037593841553.\n",
      "Epoch 87. Training loss: 3.1327097415924072. Validation loss: 2.708003520965576.\n",
      "Epoch 88. Training loss: 3.1327078342437744. Validation loss: 2.708003520965576.\n",
      "Epoch 89. Training loss: 3.132706880569458. Validation loss: 2.708003520965576.\n",
      "Epoch 90. Training loss: 3.1327056884765625. Validation loss: 2.708003044128418.\n",
      "Epoch 91. Training loss: 3.1327037811279297. Validation loss: 2.708003282546997.\n",
      "Epoch 92. Training loss: 3.132702589035034. Validation loss: 2.708003044128418.\n",
      "Epoch 93. Training loss: 3.1327011585235596. Validation loss: 2.708003282546997.\n",
      "Epoch 94. Training loss: 3.1326992511749268. Validation loss: 2.708003044128418.\n",
      "Epoch 95. Training loss: 3.1326980590820312. Validation loss: 2.708003044128418.\n",
      "Epoch 96. Training loss: 3.1326963901519775. Validation loss: 2.708003044128418.\n",
      "Epoch 97. Training loss: 3.132694959640503. Validation loss: 2.708003044128418.\n",
      "Epoch 98. Training loss: 3.132693290710449. Validation loss: 2.708003044128418.\n",
      "Epoch 99. Training loss: 3.1326920986175537. Validation loss: 2.708003044128418.\n",
      "Epoch 100. Training loss: 3.1326904296875. Validation loss: 2.708003044128418.\n",
      "Epoch 101. Training loss: 3.1326887607574463. Validation loss: 2.708002805709839.\n",
      "Epoch 102. Training loss: 3.132687568664551. Validation loss: 2.708003044128418.\n",
      "Epoch 103. Training loss: 3.132685899734497. Validation loss: 2.708003044128418.\n",
      "Epoch 104. Training loss: 3.1326847076416016. Validation loss: 2.708002805709839.\n",
      "Epoch 105. Training loss: 3.1326828002929688. Validation loss: 2.708003044128418.\n",
      "Epoch 106. Training loss: 3.1326816082000732. Validation loss: 2.7080025672912598.\n",
      "Epoch 107. Training loss: 3.1326801776885986. Validation loss: 2.7080025672912598.\n",
      "Epoch 108. Training loss: 3.132678747177124. Validation loss: 2.7080025672912598.\n",
      "Epoch 109. Training loss: 3.1326770782470703. Validation loss: 2.708003044128418.\n",
      "Epoch 110. Training loss: 3.1326754093170166. Validation loss: 2.7080025672912598.\n",
      "Epoch 111. Training loss: 3.132673978805542. Validation loss: 2.7080025672912598.\n",
      "Epoch 112. Training loss: 3.1326725482940674. Validation loss: 2.7080023288726807.\n",
      "Epoch 113. Training loss: 3.1326711177825928. Validation loss: 2.7080025672912598.\n",
      "Epoch 114. Training loss: 3.132669448852539. Validation loss: 2.7080025672912598.\n",
      "Epoch 115. Training loss: 3.1326682567596436. Validation loss: 2.7080025672912598.\n",
      "Epoch 116. Training loss: 3.13266658782959. Validation loss: 2.7080025672912598.\n",
      "Epoch 117. Training loss: 3.132664918899536. Validation loss: 2.7080023288726807.\n",
      "Epoch 118. Training loss: 3.1326634883880615. Validation loss: 2.7080023288726807.\n",
      "Epoch 119. Training loss: 3.132661819458008. Validation loss: 2.7080025672912598.\n",
      "Epoch 120. Training loss: 3.1326606273651123. Validation loss: 2.7080023288726807.\n",
      "Epoch 121. Training loss: 3.1326591968536377. Validation loss: 2.7080023288726807.\n",
      "Epoch 122. Training loss: 3.132658004760742. Validation loss: 2.7080023288726807.\n",
      "Epoch 123. Training loss: 3.1326563358306885. Validation loss: 2.7080025672912598.\n",
      "Epoch 124. Training loss: 3.1326544284820557. Validation loss: 2.7080020904541016.\n",
      "Epoch 125. Training loss: 3.1326534748077393. Validation loss: 2.7080023288726807.\n",
      "Epoch 126. Training loss: 3.1326515674591064. Validation loss: 2.7080023288726807.\n",
      "Epoch 127. Training loss: 3.132650375366211. Validation loss: 2.7080023288726807.\n",
      "Epoch 128. Training loss: 3.132648468017578. Validation loss: 2.7080020904541016.\n",
      "Epoch 129. Training loss: 3.1326472759246826. Validation loss: 2.7080020904541016.\n",
      "Epoch 130. Training loss: 3.132645845413208. Validation loss: 2.7080020904541016.\n",
      "Epoch 131. Training loss: 3.132643938064575. Validation loss: 2.7080020904541016.\n",
      "Epoch 132. Training loss: 3.132642984390259. Validation loss: 2.7080020904541016.\n",
      "Epoch 133. Training loss: 3.132641077041626. Validation loss: 2.7080020904541016.\n",
      "Epoch 134. Training loss: 3.1326396465301514. Validation loss: 2.7080018520355225.\n",
      "Epoch 135. Training loss: 3.1326379776000977. Validation loss: 2.7080023288726807.\n",
      "Epoch 136. Training loss: 3.132636785507202. Validation loss: 2.7080020904541016.\n",
      "Epoch 137. Training loss: 3.132634401321411. Validation loss: 2.7080020904541016.\n",
      "Epoch 138. Training loss: 3.1326332092285156. Validation loss: 2.7080018520355225.\n",
      "Epoch 139. Training loss: 3.13263201713562. Validation loss: 2.7080018520355225.\n",
      "Epoch 140. Training loss: 3.1326303482055664. Validation loss: 2.7080018520355225.\n",
      "Epoch 141. Training loss: 3.1326286792755127. Validation loss: 2.7080018520355225.\n",
      "Epoch 142. Training loss: 3.132627487182617. Validation loss: 2.7080018520355225.\n",
      "Epoch 143. Training loss: 3.1326258182525635. Validation loss: 2.7080018520355225.\n",
      "Epoch 144. Training loss: 3.132624387741089. Validation loss: 2.7080018520355225.\n",
      "Epoch 145. Training loss: 3.1326229572296143. Validation loss: 2.7080018520355225.\n",
      "Epoch 146. Training loss: 3.1326215267181396. Validation loss: 2.7080018520355225.\n",
      "Epoch 147. Training loss: 3.132619857788086. Validation loss: 2.7080020904541016.\n",
      "Epoch 148. Training loss: 3.1326181888580322. Validation loss: 2.7080020904541016.\n",
      "Epoch 149. Training loss: 3.1326167583465576. Validation loss: 2.7080020904541016.\n",
      "Epoch 150. Training loss: 3.132615089416504. Validation loss: 2.7080016136169434.\n",
      "Epoch 151. Training loss: 3.1326138973236084. Validation loss: 2.7080020904541016.\n",
      "Epoch 152. Training loss: 3.1326122283935547. Validation loss: 2.7080020904541016.\n",
      "Epoch 153. Training loss: 3.132610321044922. Validation loss: 2.7080016136169434.\n",
      "Epoch 154. Training loss: 3.1326091289520264. Validation loss: 2.7080016136169434.\n",
      "Epoch 155. Training loss: 3.1326076984405518. Validation loss: 2.7080018520355225.\n",
      "Epoch 156. Training loss: 3.132605791091919. Validation loss: 2.7080016136169434.\n",
      "Epoch 157. Training loss: 3.1326045989990234. Validation loss: 2.7080013751983643.\n",
      "Epoch 158. Training loss: 3.1326026916503906. Validation loss: 2.7080013751983643.\n",
      "Epoch 159. Training loss: 3.132601499557495. Validation loss: 2.708001136779785.\n",
      "Epoch 160. Training loss: 3.1326000690460205. Validation loss: 2.7080013751983643.\n",
      "Epoch 161. Training loss: 3.132598638534546. Validation loss: 2.7080013751983643.\n",
      "Epoch 162. Training loss: 3.132596969604492. Validation loss: 2.7080013751983643.\n",
      "Epoch 163. Training loss: 3.1325953006744385. Validation loss: 2.7080013751983643.\n",
      "Epoch 164. Training loss: 3.132593870162964. Validation loss: 2.7080013751983643.\n",
      "Epoch 165. Training loss: 3.13259220123291. Validation loss: 2.7080013751983643.\n",
      "Epoch 166. Training loss: 3.1325905323028564. Validation loss: 2.708001136779785.\n",
      "Epoch 167. Training loss: 3.132589340209961. Validation loss: 2.708001136779785.\n",
      "Epoch 168. Training loss: 3.132587432861328. Validation loss: 2.708001136779785.\n",
      "Epoch 169. Training loss: 3.1325857639312744. Validation loss: 2.708001136779785.\n",
      "Epoch 170. Training loss: 3.132584571838379. Validation loss: 2.708000898361206.\n",
      "Epoch 171. Training loss: 3.132582902908325. Validation loss: 2.708001136779785.\n",
      "Epoch 172. Training loss: 3.1325809955596924. Validation loss: 2.708001136779785.\n",
      "Epoch 173. Training loss: 3.132579803466797. Validation loss: 2.708001136779785.\n",
      "Epoch 174. Training loss: 3.1325786113739014. Validation loss: 2.708000898361206.\n",
      "Epoch 175. Training loss: 3.1325767040252686. Validation loss: 2.708000898361206.\n",
      "Epoch 176. Training loss: 3.132575273513794. Validation loss: 2.708000898361206.\n",
      "Epoch 177. Training loss: 3.1325738430023193. Validation loss: 2.708000898361206.\n",
      "Epoch 178. Training loss: 3.1325719356536865. Validation loss: 2.708000898361206.\n",
      "Epoch 179. Training loss: 3.132570505142212. Validation loss: 2.708000898361206.\n",
      "Epoch 180. Training loss: 3.1325690746307373. Validation loss: 2.708000898361206.\n",
      "Epoch 181. Training loss: 3.1325674057006836. Validation loss: 2.708000659942627.\n",
      "Epoch 182. Training loss: 3.132566213607788. Validation loss: 2.708000898361206.\n",
      "Epoch 183. Training loss: 3.1325645446777344. Validation loss: 2.708000659942627.\n",
      "Epoch 184. Training loss: 3.1325626373291016. Validation loss: 2.708000898361206.\n",
      "Epoch 185. Training loss: 3.132560968399048. Validation loss: 2.708000659942627.\n",
      "Epoch 186. Training loss: 3.1325597763061523. Validation loss: 2.708000659942627.\n",
      "Epoch 187. Training loss: 3.1325581073760986. Validation loss: 2.708000898361206.\n",
      "Epoch 188. Training loss: 3.132556915283203. Validation loss: 2.708000659942627.\n",
      "Epoch 189. Training loss: 3.1325552463531494. Validation loss: 2.708000659942627.\n",
      "Epoch 190. Training loss: 3.1325533390045166. Validation loss: 2.708000421524048.\n",
      "Epoch 191. Training loss: 3.132551908493042. Validation loss: 2.708000421524048.\n",
      "Epoch 192. Training loss: 3.1325502395629883. Validation loss: 2.708000659942627.\n",
      "Epoch 193. Training loss: 3.1325485706329346. Validation loss: 2.708000659942627.\n",
      "Epoch 194. Training loss: 3.13254714012146. Validation loss: 2.708000659942627.\n",
      "Epoch 195. Training loss: 3.1325454711914062. Validation loss: 2.708000421524048.\n",
      "Epoch 196. Training loss: 3.1325438022613525. Validation loss: 2.708000659942627.\n",
      "Epoch 197. Training loss: 3.132542610168457. Validation loss: 2.708000421524048.\n",
      "Epoch 198. Training loss: 3.1325409412384033. Validation loss: 2.708000421524048.\n",
      "Epoch 199. Training loss: 3.1325395107269287. Validation loss: 2.708000659942627.\n",
      "Epoch 200. Training loss: 3.132537603378296. Validation loss: 2.708000421524048.\n",
      "Epoch 201. Training loss: 3.1325361728668213. Validation loss: 2.708000659942627.\n",
      "Epoch 202. Training loss: 3.1325342655181885. Validation loss: 2.7080001831054688.\n",
      "Epoch 203. Training loss: 3.132532835006714. Validation loss: 2.708000421524048.\n",
      "Epoch 204. Training loss: 3.1325314044952393. Validation loss: 2.708000421524048.\n",
      "Epoch 205. Training loss: 3.1325294971466064. Validation loss: 2.7080001831054688.\n",
      "Epoch 206. Training loss: 3.132528066635132. Validation loss: 2.708000421524048.\n",
      "Epoch 207. Training loss: 3.1325266361236572. Validation loss: 2.708000421524048.\n",
      "Epoch 208. Training loss: 3.1325244903564453. Validation loss: 2.7080001831054688.\n",
      "Epoch 209. Training loss: 3.132523536682129. Validation loss: 2.7080001831054688.\n",
      "Epoch 210. Training loss: 3.132521390914917. Validation loss: 2.7080001831054688.\n",
      "Epoch 211. Training loss: 3.1325204372406006. Validation loss: 2.7080001831054688.\n",
      "Epoch 212. Training loss: 3.1325185298919678. Validation loss: 2.708000421524048.\n",
      "Epoch 213. Training loss: 3.132516860961914. Validation loss: 2.7080001831054688.\n",
      "Epoch 214. Training loss: 3.1325151920318604. Validation loss: 2.7080001831054688.\n",
      "Epoch 215. Training loss: 3.1325137615203857. Validation loss: 2.7080001831054688.\n",
      "Epoch 216. Training loss: 3.132512331008911. Validation loss: 2.7080001831054688.\n",
      "Epoch 217. Training loss: 3.1325109004974365. Validation loss: 2.7079999446868896.\n",
      "Epoch 218. Training loss: 3.132509231567383. Validation loss: 2.7080001831054688.\n",
      "Epoch 219. Training loss: 3.13250732421875. Validation loss: 2.7079999446868896.\n",
      "Epoch 220. Training loss: 3.1325061321258545. Validation loss: 2.7079999446868896.\n",
      "Epoch 221. Training loss: 3.132504463195801. Validation loss: 2.7079997062683105.\n",
      "Epoch 222. Training loss: 3.132502794265747. Validation loss: 2.7079999446868896.\n",
      "Epoch 223. Training loss: 3.1325013637542725. Validation loss: 2.7079997062683105.\n",
      "Epoch 224. Training loss: 3.1324996948242188. Validation loss: 2.7079999446868896.\n",
      "Epoch 225. Training loss: 3.132497787475586. Validation loss: 2.7079997062683105.\n",
      "Epoch 226. Training loss: 3.1324961185455322. Validation loss: 2.7079994678497314.\n",
      "Epoch 227. Training loss: 3.1324946880340576. Validation loss: 2.7079997062683105.\n",
      "Epoch 228. Training loss: 3.132493019104004. Validation loss: 2.7079997062683105.\n",
      "Epoch 229. Training loss: 3.13249135017395. Validation loss: 2.7079994678497314.\n",
      "Epoch 230. Training loss: 3.1324901580810547. Validation loss: 2.7079997062683105.\n",
      "Epoch 231. Training loss: 3.132488250732422. Validation loss: 2.7079994678497314.\n",
      "Epoch 232. Training loss: 3.1324870586395264. Validation loss: 2.7079997062683105.\n",
      "Epoch 233. Training loss: 3.1324851512908936. Validation loss: 2.7079992294311523.\n",
      "Epoch 234. Training loss: 3.13248348236084. Validation loss: 2.7079994678497314.\n",
      "Epoch 235. Training loss: 3.132481813430786. Validation loss: 2.7079994678497314.\n",
      "Epoch 236. Training loss: 3.1324806213378906. Validation loss: 2.7079994678497314.\n",
      "Epoch 237. Training loss: 3.1324784755706787. Validation loss: 2.7079994678497314.\n",
      "Epoch 238. Training loss: 3.132477045059204. Validation loss: 2.7079994678497314.\n",
      "Epoch 239. Training loss: 3.1324751377105713. Validation loss: 2.7079992294311523.\n",
      "Epoch 240. Training loss: 3.132473945617676. Validation loss: 2.7079994678497314.\n",
      "Epoch 241. Training loss: 3.132472038269043. Validation loss: 2.7079992294311523.\n",
      "Epoch 242. Training loss: 3.1324703693389893. Validation loss: 2.7079992294311523.\n",
      "Epoch 243. Training loss: 3.1324691772460938. Validation loss: 2.7079992294311523.\n",
      "Epoch 244. Training loss: 3.132467269897461. Validation loss: 2.7079992294311523.\n",
      "Epoch 245. Training loss: 3.1324656009674072. Validation loss: 2.7079992294311523.\n",
      "Epoch 246. Training loss: 3.1324641704559326. Validation loss: 2.7079992294311523.\n",
      "Epoch 247. Training loss: 3.132462739944458. Validation loss: 2.7079992294311523.\n",
      "Epoch 248. Training loss: 3.132460832595825. Validation loss: 2.707998752593994.\n",
      "Epoch 249. Training loss: 3.1324589252471924. Validation loss: 2.7079992294311523.\n",
      "Epoch 250. Training loss: 3.1324574947357178. Validation loss: 2.707998752593994.\n",
      "Epoch 251. Training loss: 3.132456064224243. Validation loss: 2.7079989910125732.\n",
      "Epoch 252. Training loss: 3.1324541568756104. Validation loss: 2.7079992294311523.\n",
      "Epoch 253. Training loss: 3.132452964782715. Validation loss: 2.7079992294311523.\n",
      "Epoch 254. Training loss: 3.132451295852661. Validation loss: 2.7079989910125732.\n",
      "Epoch 255. Training loss: 3.1324493885040283. Validation loss: 2.7079989910125732.\n",
      "Epoch 256. Training loss: 3.1324474811553955. Validation loss: 2.7079989910125732.\n",
      "Epoch 257. Training loss: 3.132446050643921. Validation loss: 2.7079989910125732.\n",
      "Epoch 258. Training loss: 3.1324446201324463. Validation loss: 2.7079989910125732.\n",
      "Epoch 259. Training loss: 3.1324427127838135. Validation loss: 2.7079989910125732.\n",
      "Epoch 260. Training loss: 3.132441282272339. Validation loss: 2.7079989910125732.\n",
      "Epoch 261. Training loss: 3.132439374923706. Validation loss: 2.707998752593994.\n",
      "Epoch 262. Training loss: 3.1324379444122314. Validation loss: 2.707998752593994.\n",
      "Epoch 263. Training loss: 3.1324360370635986. Validation loss: 2.707998752593994.\n",
      "Epoch 264. Training loss: 3.132434844970703. Validation loss: 2.707998752593994.\n",
      "Epoch 265. Training loss: 3.1324329376220703. Validation loss: 2.707998275756836.\n",
      "Epoch 266. Training loss: 3.1324312686920166. Validation loss: 2.707998514175415.\n",
      "Epoch 267. Training loss: 3.132429838180542. Validation loss: 2.707998752593994.\n",
      "Epoch 268. Training loss: 3.132427930831909. Validation loss: 2.707998752593994.\n",
      "Epoch 269. Training loss: 3.1324265003204346. Validation loss: 2.707998752593994.\n",
      "Epoch 270. Training loss: 3.1324245929718018. Validation loss: 2.707998275756836.\n",
      "Epoch 271. Training loss: 3.132423162460327. Validation loss: 2.707998514175415.\n",
      "Epoch 272. Training loss: 3.1324212551116943. Validation loss: 2.707998275756836.\n",
      "Epoch 273. Training loss: 3.1324195861816406. Validation loss: 2.707998275756836.\n",
      "Epoch 274. Training loss: 3.132417917251587. Validation loss: 2.707998275756836.\n",
      "Epoch 275. Training loss: 3.1324164867401123. Validation loss: 2.707998514175415.\n",
      "Epoch 276. Training loss: 3.1324150562286377. Validation loss: 2.707998514175415.\n",
      "Epoch 277. Training loss: 3.132412910461426. Validation loss: 2.707998275756836.\n",
      "Epoch 278. Training loss: 3.132411241531372. Validation loss: 2.707998514175415.\n",
      "Epoch 279. Training loss: 3.1324100494384766. Validation loss: 2.707998275756836.\n",
      "Epoch 280. Training loss: 3.132408380508423. Validation loss: 2.707998275756836.\n",
      "Epoch 281. Training loss: 3.132406234741211. Validation loss: 2.707998275756836.\n",
      "Epoch 282. Training loss: 3.1324050426483154. Validation loss: 2.707998037338257.\n",
      "Epoch 283. Training loss: 3.1324031352996826. Validation loss: 2.7079977989196777.\n",
      "Epoch 284. Training loss: 3.132401466369629. Validation loss: 2.707998275756836.\n",
      "Epoch 285. Training loss: 3.132399797439575. Validation loss: 2.707998275756836.\n",
      "Epoch 286. Training loss: 3.1323983669281006. Validation loss: 2.707998275756836.\n",
      "Epoch 287. Training loss: 3.1323964595794678. Validation loss: 2.707998275756836.\n",
      "Epoch 288. Training loss: 3.132395029067993. Validation loss: 2.7079977989196777.\n",
      "Epoch 289. Training loss: 3.1323931217193604. Validation loss: 2.707998275756836.\n",
      "Epoch 290. Training loss: 3.132391929626465. Validation loss: 2.7079977989196777.\n",
      "Epoch 291. Training loss: 3.132390260696411. Validation loss: 2.707998275756836.\n",
      "Epoch 292. Training loss: 3.1323883533477783. Validation loss: 2.707998037338257.\n",
      "Epoch 293. Training loss: 3.1323869228363037. Validation loss: 2.707998037338257.\n",
      "Epoch 294. Training loss: 3.132385015487671. Validation loss: 2.7079977989196777.\n",
      "Epoch 295. Training loss: 3.132383346557617. Validation loss: 2.7079977989196777.\n",
      "Epoch 296. Training loss: 3.1323812007904053. Validation loss: 2.7079977989196777.\n",
      "Epoch 297. Training loss: 3.1323797702789307. Validation loss: 2.7079977989196777.\n",
      "Epoch 298. Training loss: 3.132378339767456. Validation loss: 2.7079975605010986.\n",
      "Epoch 299. Training loss: 3.1323764324188232. Validation loss: 2.7079977989196777.\n",
      "Epoch 300. Training loss: 3.1323750019073486. Validation loss: 2.7079975605010986.\n",
      "Epoch 301. Training loss: 3.132373094558716. Validation loss: 2.7079973220825195.\n",
      "Epoch 302. Training loss: 3.1323719024658203. Validation loss: 2.7079977989196777.\n",
      "Epoch 303. Training loss: 3.1323699951171875. Validation loss: 2.7079977989196777.\n",
      "Epoch 304. Training loss: 3.1323680877685547. Validation loss: 2.7079977989196777.\n",
      "Epoch 305. Training loss: 3.132366180419922. Validation loss: 2.7079973220825195.\n",
      "Epoch 306. Training loss: 3.132364511489868. Validation loss: 2.7079973220825195.\n",
      "Epoch 307. Training loss: 3.1323630809783936. Validation loss: 2.7079973220825195.\n",
      "Epoch 308. Training loss: 3.13236141204834. Validation loss: 2.7079973220825195.\n",
      "Epoch 309. Training loss: 3.132359504699707. Validation loss: 2.7079973220825195.\n",
      "Epoch 310. Training loss: 3.1323578357696533. Validation loss: 2.7079973220825195.\n",
      "Epoch 311. Training loss: 3.132356643676758. Validation loss: 2.7079973220825195.\n",
      "Epoch 312. Training loss: 3.132354736328125. Validation loss: 2.7079973220825195.\n",
      "Epoch 313. Training loss: 3.132352828979492. Validation loss: 2.7079973220825195.\n",
      "Epoch 314. Training loss: 3.1323511600494385. Validation loss: 2.7079973220825195.\n",
      "Epoch 315. Training loss: 3.132349729537964. Validation loss: 2.7079973220825195.\n",
      "Epoch 316. Training loss: 3.132347822189331. Validation loss: 2.7079973220825195.\n",
      "Epoch 317. Training loss: 3.1323461532592773. Validation loss: 2.7079973220825195.\n",
      "Epoch 318. Training loss: 3.1323444843292236. Validation loss: 2.7079970836639404.\n",
      "Epoch 319. Training loss: 3.132343053817749. Validation loss: 2.7079973220825195.\n",
      "Epoch 320. Training loss: 3.132341146469116. Validation loss: 2.7079970836639404.\n",
      "Epoch 321. Training loss: 3.1323394775390625. Validation loss: 2.7079968452453613.\n",
      "Epoch 322. Training loss: 3.1323375701904297. Validation loss: 2.7079968452453613.\n",
      "Epoch 323. Training loss: 3.132335662841797. Validation loss: 2.7079970836639404.\n",
      "Epoch 324. Training loss: 3.1323344707489014. Validation loss: 2.7079968452453613.\n",
      "Epoch 325. Training loss: 3.1323325634002686. Validation loss: 2.7079973220825195.\n",
      "Epoch 326. Training loss: 3.132330894470215. Validation loss: 2.7079968452453613.\n",
      "Epoch 327. Training loss: 3.132328987121582. Validation loss: 2.7079968452453613.\n",
      "Epoch 328. Training loss: 3.1323273181915283. Validation loss: 2.7079968452453613.\n",
      "Epoch 329. Training loss: 3.1323254108428955. Validation loss: 2.7079968452453613.\n",
      "Epoch 330. Training loss: 3.13232421875. Validation loss: 2.7079968452453613.\n",
      "Epoch 331. Training loss: 3.132322311401367. Validation loss: 2.7079966068267822.\n",
      "Epoch 332. Training loss: 3.1323204040527344. Validation loss: 2.7079966068267822.\n",
      "Epoch 333. Training loss: 3.1323187351226807. Validation loss: 2.7079966068267822.\n",
      "Epoch 334. Training loss: 3.132317304611206. Validation loss: 2.7079966068267822.\n",
      "Epoch 335. Training loss: 3.1323158740997314. Validation loss: 2.7079966068267822.\n",
      "Epoch 336. Training loss: 3.1323134899139404. Validation loss: 2.7079966068267822.\n",
      "Epoch 337. Training loss: 3.1323118209838867. Validation loss: 2.7079966068267822.\n",
      "Epoch 338. Training loss: 3.132310152053833. Validation loss: 2.707996368408203.\n",
      "Epoch 339. Training loss: 3.1323087215423584. Validation loss: 2.7079966068267822.\n",
      "Epoch 340. Training loss: 3.1323070526123047. Validation loss: 2.7079966068267822.\n",
      "Epoch 341. Training loss: 3.132305145263672. Validation loss: 2.7079966068267822.\n",
      "Epoch 342. Training loss: 3.132303237915039. Validation loss: 2.707996368408203.\n",
      "Epoch 343. Training loss: 3.1323015689849854. Validation loss: 2.707996368408203.\n",
      "Epoch 344. Training loss: 3.1322996616363525. Validation loss: 2.707996368408203.\n",
      "Epoch 345. Training loss: 3.132298469543457. Validation loss: 2.707996368408203.\n",
      "Epoch 346. Training loss: 3.132296562194824. Validation loss: 2.707996368408203.\n",
      "Epoch 347. Training loss: 3.1322946548461914. Validation loss: 2.707996129989624.\n",
      "Epoch 348. Training loss: 3.1322929859161377. Validation loss: 2.707996129989624.\n",
      "Epoch 349. Training loss: 3.132291555404663. Validation loss: 2.707996368408203.\n",
      "Epoch 350. Training loss: 3.132289171218872. Validation loss: 2.707996368408203.\n",
      "Epoch 351. Training loss: 3.1322879791259766. Validation loss: 2.707995891571045.\n",
      "Epoch 352. Training loss: 3.1322860717773438. Validation loss: 2.707995891571045.\n",
      "Epoch 353. Training loss: 3.132284164428711. Validation loss: 2.707996129989624.\n",
      "Epoch 354. Training loss: 3.1322824954986572. Validation loss: 2.707995891571045.\n",
      "Epoch 355. Training loss: 3.1322805881500244. Validation loss: 2.707995891571045.\n",
      "Epoch 356. Training loss: 3.13227915763855. Validation loss: 2.707995891571045.\n",
      "Epoch 357. Training loss: 3.132277488708496. Validation loss: 2.707996129989624.\n",
      "Epoch 358. Training loss: 3.1322755813598633. Validation loss: 2.707995891571045.\n",
      "Epoch 359. Training loss: 3.1322736740112305. Validation loss: 2.707995891571045.\n",
      "Epoch 360. Training loss: 3.1322720050811768. Validation loss: 2.707995891571045.\n",
      "Epoch 361. Training loss: 3.132270574569702. Validation loss: 2.707995891571045.\n",
      "Epoch 362. Training loss: 3.1322686672210693. Validation loss: 2.707995891571045.\n",
      "Epoch 363. Training loss: 3.1322667598724365. Validation loss: 2.707995891571045.\n",
      "Epoch 364. Training loss: 3.132265329360962. Validation loss: 2.707995891571045.\n",
      "Epoch 365. Training loss: 3.132263422012329. Validation loss: 2.707995891571045.\n",
      "Epoch 366. Training loss: 3.1322619915008545. Validation loss: 2.7079954147338867.\n",
      "Epoch 367. Training loss: 3.1322596073150635. Validation loss: 2.707995653152466.\n",
      "Epoch 368. Training loss: 3.132258176803589. Validation loss: 2.707995891571045.\n",
      "Epoch 369. Training loss: 3.132256269454956. Validation loss: 2.7079954147338867.\n",
      "Epoch 370. Training loss: 3.1322546005249023. Validation loss: 2.7079954147338867.\n",
      "Epoch 371. Training loss: 3.1322529315948486. Validation loss: 2.7079954147338867.\n",
      "Epoch 372. Training loss: 3.132251024246216. Validation loss: 2.7079954147338867.\n",
      "Epoch 373. Training loss: 3.132249116897583. Validation loss: 2.7079954147338867.\n",
      "Epoch 374. Training loss: 3.1322479248046875. Validation loss: 2.7079954147338867.\n",
      "Epoch 375. Training loss: 3.1322460174560547. Validation loss: 2.7079954147338867.\n",
      "Epoch 376. Training loss: 3.132244348526001. Validation loss: 2.7079954147338867.\n",
      "Epoch 377. Training loss: 3.132242441177368. Validation loss: 2.7079949378967285.\n",
      "Epoch 378. Training loss: 3.1322405338287354. Validation loss: 2.7079949378967285.\n",
      "Epoch 379. Training loss: 3.1322391033172607. Validation loss: 2.7079954147338867.\n",
      "Epoch 380. Training loss: 3.132237195968628. Validation loss: 2.7079954147338867.\n",
      "Epoch 381. Training loss: 3.132235288619995. Validation loss: 2.7079949378967285.\n",
      "Epoch 382. Training loss: 3.1322336196899414. Validation loss: 2.7079949378967285.\n",
      "Epoch 383. Training loss: 3.1322319507598877. Validation loss: 2.7079949378967285.\n",
      "Epoch 384. Training loss: 3.132230043411255. Validation loss: 2.7079949378967285.\n",
      "Epoch 385. Training loss: 3.132228136062622. Validation loss: 2.7079949378967285.\n",
      "Epoch 386. Training loss: 3.1322267055511475. Validation loss: 2.7079949378967285.\n",
      "Epoch 387. Training loss: 3.1322247982025146. Validation loss: 2.7079949378967285.\n",
      "Epoch 388. Training loss: 3.13222336769104. Validation loss: 2.7079949378967285.\n",
      "Epoch 389. Training loss: 3.132221221923828. Validation loss: 2.7079946994781494.\n",
      "Epoch 390. Training loss: 3.1322193145751953. Validation loss: 2.7079949378967285.\n",
      "Epoch 391. Training loss: 3.1322174072265625. Validation loss: 2.7079949378967285.\n",
      "Epoch 392. Training loss: 3.132215738296509. Validation loss: 2.7079949378967285.\n",
      "Epoch 393. Training loss: 3.132213830947876. Validation loss: 2.7079949378967285.\n",
      "Epoch 394. Training loss: 3.1322126388549805. Validation loss: 2.7079949378967285.\n",
      "Epoch 395. Training loss: 3.1322104930877686. Validation loss: 2.7079949378967285.\n",
      "Epoch 396. Training loss: 3.132208824157715. Validation loss: 2.7079949378967285.\n",
      "Epoch 397. Training loss: 3.132206916809082. Validation loss: 2.7079949378967285.\n",
      "Epoch 398. Training loss: 3.1322052478790283. Validation loss: 2.7079949378967285.\n",
      "Epoch 399. Training loss: 3.1322038173675537. Validation loss: 2.7079946994781494.\n",
      "Epoch 400. Training loss: 3.132201910018921. Validation loss: 2.7079944610595703.\n",
      "Epoch 401. Training loss: 3.13219952583313. Validation loss: 2.7079944610595703.\n",
      "Epoch 402. Training loss: 3.1321980953216553. Validation loss: 2.7079944610595703.\n",
      "Epoch 403. Training loss: 3.1321961879730225. Validation loss: 2.7079949378967285.\n",
      "Epoch 404. Training loss: 3.1321942806243896. Validation loss: 2.7079944610595703.\n",
      "Epoch 405. Training loss: 3.132192611694336. Validation loss: 2.7079944610595703.\n",
      "Epoch 406. Training loss: 3.132190704345703. Validation loss: 2.7079944610595703.\n",
      "Epoch 407. Training loss: 3.1321887969970703. Validation loss: 2.707994222640991.\n",
      "Epoch 408. Training loss: 3.1321871280670166. Validation loss: 2.7079944610595703.\n",
      "Epoch 409. Training loss: 3.132185697555542. Validation loss: 2.707994222640991.\n",
      "Epoch 410. Training loss: 3.132183313369751. Validation loss: 2.7079944610595703.\n",
      "Epoch 411. Training loss: 3.1321818828582764. Validation loss: 2.707994222640991.\n",
      "Epoch 412. Training loss: 3.1321799755096436. Validation loss: 2.7079944610595703.\n",
      "Epoch 413. Training loss: 3.1321780681610107. Validation loss: 2.7079944610595703.\n",
      "Epoch 414. Training loss: 3.132176637649536. Validation loss: 2.7079944610595703.\n",
      "Epoch 415. Training loss: 3.132174491882324. Validation loss: 2.707994222640991.\n",
      "Epoch 416. Training loss: 3.1321728229522705. Validation loss: 2.707994222640991.\n",
      "Epoch 417. Training loss: 3.1321709156036377. Validation loss: 2.707993984222412.\n",
      "Epoch 418. Training loss: 3.132169485092163. Validation loss: 2.7079944610595703.\n",
      "Epoch 419. Training loss: 3.1321675777435303. Validation loss: 2.707993745803833.\n",
      "Epoch 420. Training loss: 3.1321659088134766. Validation loss: 2.707993984222412.\n",
      "Epoch 421. Training loss: 3.1321637630462646. Validation loss: 2.7079944610595703.\n",
      "Epoch 422. Training loss: 3.132162094116211. Validation loss: 2.707994222640991.\n",
      "Epoch 423. Training loss: 3.132160186767578. Validation loss: 2.707993745803833.\n",
      "Epoch 424. Training loss: 3.1321582794189453. Validation loss: 2.707993745803833.\n",
      "Epoch 425. Training loss: 3.1321566104888916. Validation loss: 2.707993745803833.\n",
      "Epoch 426. Training loss: 3.132154703140259. Validation loss: 2.707993745803833.\n",
      "Epoch 427. Training loss: 3.132152795791626. Validation loss: 2.707993745803833.\n",
      "Epoch 428. Training loss: 3.1321513652801514. Validation loss: 2.707993745803833.\n",
      "Epoch 429. Training loss: 3.1321494579315186. Validation loss: 2.707993507385254.\n",
      "Epoch 430. Training loss: 3.1321475505828857. Validation loss: 2.707993745803833.\n",
      "Epoch 431. Training loss: 3.132145881652832. Validation loss: 2.707993507385254.\n",
      "Epoch 432. Training loss: 3.13214373588562. Validation loss: 2.707993507385254.\n",
      "Epoch 433. Training loss: 3.1321420669555664. Validation loss: 2.707993507385254.\n",
      "Epoch 434. Training loss: 3.1321401596069336. Validation loss: 2.707993507385254.\n",
      "Epoch 435. Training loss: 3.13213849067688. Validation loss: 2.707993507385254.\n",
      "Epoch 436. Training loss: 3.132136583328247. Validation loss: 2.707993507385254.\n",
      "Epoch 437. Training loss: 3.1321351528167725. Validation loss: 2.707993268966675.\n",
      "Epoch 438. Training loss: 3.1321327686309814. Validation loss: 2.707993268966675.\n",
      "Epoch 439. Training loss: 3.1321308612823486. Validation loss: 2.707993268966675.\n",
      "Epoch 440. Training loss: 3.132129430770874. Validation loss: 2.707993268966675.\n",
      "Epoch 441. Training loss: 3.132127523422241. Validation loss: 2.707993268966675.\n",
      "Epoch 442. Training loss: 3.1321258544921875. Validation loss: 2.707993268966675.\n",
      "Epoch 443. Training loss: 3.1321237087249756. Validation loss: 2.707993268966675.\n",
      "Epoch 444. Training loss: 3.132122039794922. Validation loss: 2.707993268966675.\n",
      "Epoch 445. Training loss: 3.132120132446289. Validation loss: 2.7079930305480957.\n",
      "Epoch 446. Training loss: 3.1321182250976562. Validation loss: 2.7079927921295166.\n",
      "Epoch 447. Training loss: 3.1321163177490234. Validation loss: 2.707993268966675.\n",
      "Epoch 448. Training loss: 3.1321144104003906. Validation loss: 2.707993268966675.\n",
      "Epoch 449. Training loss: 3.132112503051758. Validation loss: 2.707993268966675.\n",
      "Epoch 450. Training loss: 3.1321113109588623. Validation loss: 2.7079930305480957.\n",
      "Epoch 451. Training loss: 3.1321089267730713. Validation loss: 2.7079930305480957.\n",
      "Epoch 452. Training loss: 3.1321070194244385. Validation loss: 2.707993268966675.\n",
      "Epoch 453. Training loss: 3.132105588912964. Validation loss: 2.7079927921295166.\n",
      "Epoch 454. Training loss: 3.132103204727173. Validation loss: 2.7079927921295166.\n",
      "Epoch 455. Training loss: 3.1321017742156982. Validation loss: 2.7079927921295166.\n",
      "Epoch 456. Training loss: 3.1320998668670654. Validation loss: 2.7079930305480957.\n",
      "Epoch 457. Training loss: 3.1320979595184326. Validation loss: 2.7079927921295166.\n",
      "Epoch 458. Training loss: 3.132096290588379. Validation loss: 2.7079927921295166.\n",
      "Epoch 459. Training loss: 3.132094621658325. Validation loss: 2.7079927921295166.\n",
      "Epoch 460. Training loss: 3.132092237472534. Validation loss: 2.7079927921295166.\n",
      "Epoch 461. Training loss: 3.1320903301239014. Validation loss: 2.7079925537109375.\n",
      "Epoch 462. Training loss: 3.1320886611938477. Validation loss: 2.7079927921295166.\n",
      "Epoch 463. Training loss: 3.132086753845215. Validation loss: 2.7079927921295166.\n",
      "Epoch 464. Training loss: 3.132084846496582. Validation loss: 2.7079927921295166.\n",
      "Epoch 465. Training loss: 3.1320831775665283. Validation loss: 2.7079927921295166.\n",
      "Epoch 466. Training loss: 3.1320812702178955. Validation loss: 2.7079925537109375.\n",
      "Epoch 467. Training loss: 3.132079839706421. Validation loss: 2.7079923152923584.\n",
      "Epoch 468. Training loss: 3.132077932357788. Validation loss: 2.7079923152923584.\n",
      "Epoch 469. Training loss: 3.132075548171997. Validation loss: 2.7079923152923584.\n",
      "Epoch 470. Training loss: 3.1320736408233643. Validation loss: 2.7079918384552.\n",
      "Epoch 471. Training loss: 3.1320722103118896. Validation loss: 2.7079923152923584.\n",
      "Epoch 472. Training loss: 3.132070302963257. Validation loss: 2.7079923152923584.\n",
      "Epoch 473. Training loss: 3.132068395614624. Validation loss: 2.7079923152923584.\n",
      "Epoch 474. Training loss: 3.1320667266845703. Validation loss: 2.7079923152923584.\n",
      "Epoch 475. Training loss: 3.1320648193359375. Validation loss: 2.7079923152923584.\n",
      "Epoch 476. Training loss: 3.1320629119873047. Validation loss: 2.7079920768737793.\n",
      "Epoch 477. Training loss: 3.132061004638672. Validation loss: 2.7079918384552.\n",
      "Epoch 478. Training loss: 3.13205885887146. Validation loss: 2.7079918384552.\n",
      "Epoch 479. Training loss: 3.132056951522827. Validation loss: 2.7079923152923584.\n",
      "Epoch 480. Training loss: 3.1320552825927734. Validation loss: 2.7079918384552.\n",
      "Epoch 481. Training loss: 3.1320531368255615. Validation loss: 2.7079918384552.\n",
      "Epoch 482. Training loss: 3.132051467895508. Validation loss: 2.7079918384552.\n",
      "Epoch 483. Training loss: 3.132049560546875. Validation loss: 2.7079918384552.\n",
      "Epoch 484. Training loss: 3.132047653198242. Validation loss: 2.7079918384552.\n",
      "Epoch 485. Training loss: 3.132045030593872. Validation loss: 2.7079918384552.\n",
      "Epoch 486. Training loss: 3.1320436000823975. Validation loss: 2.7079918384552.\n",
      "Epoch 487. Training loss: 3.1320419311523438. Validation loss: 2.7079918384552.\n",
      "Epoch 488. Training loss: 3.132039785385132. Validation loss: 2.7079918384552.\n",
      "Epoch 489. Training loss: 3.1320383548736572. Validation loss: 2.7079918384552.\n",
      "Epoch 490. Training loss: 3.1320362091064453. Validation loss: 2.7079918384552.\n",
      "Epoch 491. Training loss: 3.1320343017578125. Validation loss: 2.7079918384552.\n",
      "Epoch 492. Training loss: 3.132032632827759. Validation loss: 2.707991600036621.\n",
      "Epoch 493. Training loss: 3.132030487060547. Validation loss: 2.7079918384552.\n",
      "Epoch 494. Training loss: 3.132028818130493. Validation loss: 2.707991600036621.\n",
      "Epoch 495. Training loss: 3.1320266723632812. Validation loss: 2.707991361618042.\n",
      "Epoch 496. Training loss: 3.1320247650146484. Validation loss: 2.707991361618042.\n",
      "Epoch 497. Training loss: 3.1320228576660156. Validation loss: 2.707991361618042.\n",
      "Epoch 498. Training loss: 3.132021188735962. Validation loss: 2.707991361618042.\n",
      "Epoch 499. Training loss: 3.13201904296875. Validation loss: 2.707991361618042.\n",
      "Epoch 500. Training loss: 3.132017135620117. Validation loss: 2.707991361618042.\n",
      "Epoch 501. Training loss: 3.1320152282714844. Validation loss: 2.707991361618042.\n",
      "Epoch 502. Training loss: 3.1320133209228516. Validation loss: 2.707991361618042.\n",
      "Epoch 503. Training loss: 3.1320114135742188. Validation loss: 2.707991361618042.\n",
      "Epoch 504. Training loss: 3.132009744644165. Validation loss: 2.707991123199463.\n",
      "Epoch 505. Training loss: 3.132007598876953. Validation loss: 2.707991361618042.\n",
      "Epoch 506. Training loss: 3.1320059299468994. Validation loss: 2.707991361618042.\n",
      "Epoch 507. Training loss: 3.1320040225982666. Validation loss: 2.707991361618042.\n",
      "Epoch 508. Training loss: 3.132002115249634. Validation loss: 2.707991361618042.\n",
      "Epoch 509. Training loss: 3.132000207901001. Validation loss: 2.707990884780884.\n",
      "Epoch 510. Training loss: 3.131998300552368. Validation loss: 2.707990884780884.\n",
      "Epoch 511. Training loss: 3.1319963932037354. Validation loss: 2.707990884780884.\n",
      "Epoch 512. Training loss: 3.1319942474365234. Validation loss: 2.707990884780884.\n",
      "Epoch 513. Training loss: 3.1319925785064697. Validation loss: 2.7079906463623047.\n",
      "Epoch 514. Training loss: 3.131990671157837. Validation loss: 2.707990884780884.\n",
      "Epoch 515. Training loss: 3.1319892406463623. Validation loss: 2.7079906463623047.\n",
      "Epoch 516. Training loss: 3.1319868564605713. Validation loss: 2.7079906463623047.\n",
      "Epoch 517. Training loss: 3.1319849491119385. Validation loss: 2.7079906463623047.\n",
      "Epoch 518. Training loss: 3.1319830417633057. Validation loss: 2.7079906463623047.\n",
      "Epoch 519. Training loss: 3.131981134414673. Validation loss: 2.7079906463623047.\n",
      "Epoch 520. Training loss: 3.13197922706604. Validation loss: 2.7079904079437256.\n",
      "Epoch 521. Training loss: 3.131977081298828. Validation loss: 2.7079904079437256.\n",
      "Epoch 522. Training loss: 3.1319751739501953. Validation loss: 2.7079904079437256.\n",
      "Epoch 523. Training loss: 3.1319735050201416. Validation loss: 2.7079906463623047.\n",
      "Epoch 524. Training loss: 3.1319713592529297. Validation loss: 2.7079906463623047.\n",
      "Epoch 525. Training loss: 3.131969690322876. Validation loss: 2.7079906463623047.\n",
      "Epoch 526. Training loss: 3.131967782974243. Validation loss: 2.7079906463623047.\n",
      "Epoch 527. Training loss: 3.1319656372070312. Validation loss: 2.7079904079437256.\n",
      "Epoch 528. Training loss: 3.1319637298583984. Validation loss: 2.7079906463623047.\n",
      "Epoch 529. Training loss: 3.1319615840911865. Validation loss: 2.7079904079437256.\n",
      "Epoch 530. Training loss: 3.1319596767425537. Validation loss: 2.7079904079437256.\n",
      "Epoch 531. Training loss: 3.131957769393921. Validation loss: 2.7079901695251465.\n",
      "Epoch 532. Training loss: 3.131955862045288. Validation loss: 2.7079904079437256.\n",
      "Epoch 533. Training loss: 3.131953477859497. Validation loss: 2.7079904079437256.\n",
      "Epoch 534. Training loss: 3.1319515705108643. Validation loss: 2.7079904079437256.\n",
      "Epoch 535. Training loss: 3.1319496631622314. Validation loss: 2.7079901695251465.\n",
      "Epoch 536. Training loss: 3.131948471069336. Validation loss: 2.7079901695251465.\n",
      "Epoch 537. Training loss: 3.131946325302124. Validation loss: 2.7079899311065674.\n",
      "Epoch 538. Training loss: 3.131944417953491. Validation loss: 2.7079901695251465.\n",
      "Epoch 539. Training loss: 3.1319425106048584. Validation loss: 2.7079901695251465.\n",
      "Epoch 540. Training loss: 3.1319401264190674. Validation loss: 2.7079896926879883.\n",
      "Epoch 541. Training loss: 3.131938934326172. Validation loss: 2.7079901695251465.\n",
      "Epoch 542. Training loss: 3.13193678855896. Validation loss: 2.7079901695251465.\n",
      "Epoch 543. Training loss: 3.131934404373169. Validation loss: 2.7079896926879883.\n",
      "Epoch 544. Training loss: 3.1319329738616943. Validation loss: 2.7079896926879883.\n",
      "Epoch 545. Training loss: 3.1319305896759033. Validation loss: 2.7079896926879883.\n",
      "Epoch 546. Training loss: 3.1319286823272705. Validation loss: 2.7079899311065674.\n",
      "Epoch 547. Training loss: 3.1319267749786377. Validation loss: 2.7079896926879883.\n",
      "Epoch 548. Training loss: 3.131924867630005. Validation loss: 2.7079896926879883.\n",
      "Epoch 549. Training loss: 3.131922960281372. Validation loss: 2.7079896926879883.\n",
      "Epoch 550. Training loss: 3.13192081451416. Validation loss: 2.7079896926879883.\n",
      "Epoch 551. Training loss: 3.1319189071655273. Validation loss: 2.7079896926879883.\n",
      "Epoch 552. Training loss: 3.1319169998168945. Validation loss: 2.7079896926879883.\n",
      "Epoch 553. Training loss: 3.1319148540496826. Validation loss: 2.707989454269409.\n",
      "Epoch 554. Training loss: 3.131913185119629. Validation loss: 2.7079896926879883.\n",
      "Epoch 555. Training loss: 3.131910562515259. Validation loss: 2.707989454269409.\n",
      "Epoch 556. Training loss: 3.1319093704223633. Validation loss: 2.7079896926879883.\n",
      "Epoch 557. Training loss: 3.131906747817993. Validation loss: 2.70798921585083.\n",
      "Epoch 558. Training loss: 3.1319048404693604. Validation loss: 2.70798921585083.\n",
      "Epoch 559. Training loss: 3.1319034099578857. Validation loss: 2.707989454269409.\n",
      "Epoch 560. Training loss: 3.131901502609253. Validation loss: 2.70798921585083.\n",
      "Epoch 561. Training loss: 3.131898880004883. Validation loss: 2.70798921585083.\n",
      "Epoch 562. Training loss: 3.13189697265625. Validation loss: 2.70798921585083.\n",
      "Epoch 563. Training loss: 3.1318953037261963. Validation loss: 2.70798921585083.\n",
      "Epoch 564. Training loss: 3.1318933963775635. Validation loss: 2.70798921585083.\n",
      "Epoch 565. Training loss: 3.1318912506103516. Validation loss: 2.707989454269409.\n",
      "Epoch 566. Training loss: 3.1318893432617188. Validation loss: 2.70798921585083.\n",
      "Epoch 567. Training loss: 3.131887435913086. Validation loss: 2.70798921585083.\n",
      "Epoch 568. Training loss: 3.131885528564453. Validation loss: 2.70798921585083.\n",
      "Epoch 569. Training loss: 3.1318836212158203. Validation loss: 2.70798921585083.\n",
      "Epoch 570. Training loss: 3.1318817138671875. Validation loss: 2.707988977432251.\n",
      "Epoch 571. Training loss: 3.1318795680999756. Validation loss: 2.707988977432251.\n",
      "Epoch 572. Training loss: 3.1318776607513428. Validation loss: 2.707988977432251.\n",
      "Epoch 573. Training loss: 3.13187575340271. Validation loss: 2.707988739013672.\n",
      "Epoch 574. Training loss: 3.131873369216919. Validation loss: 2.707988977432251.\n",
      "Epoch 575. Training loss: 3.131871461868286. Validation loss: 2.707988977432251.\n",
      "Epoch 576. Training loss: 3.1318695545196533. Validation loss: 2.707988739013672.\n",
      "Epoch 577. Training loss: 3.1318674087524414. Validation loss: 2.707988977432251.\n",
      "Epoch 578. Training loss: 3.1318657398223877. Validation loss: 2.707988739013672.\n",
      "Epoch 579. Training loss: 3.131863594055176. Validation loss: 2.707988739013672.\n",
      "Epoch 580. Training loss: 3.131861448287964. Validation loss: 2.707988739013672.\n",
      "Epoch 581. Training loss: 3.13185977935791. Validation loss: 2.707988739013672.\n",
      "Epoch 582. Training loss: 3.1318576335906982. Validation loss: 2.707988739013672.\n",
      "Epoch 583. Training loss: 3.1318557262420654. Validation loss: 2.707988739013672.\n",
      "Epoch 584. Training loss: 3.1318533420562744. Validation loss: 2.7079882621765137.\n",
      "Epoch 585. Training loss: 3.1318514347076416. Validation loss: 2.7079885005950928.\n",
      "Epoch 586. Training loss: 3.131850004196167. Validation loss: 2.7079882621765137.\n",
      "Epoch 587. Training loss: 3.131847620010376. Validation loss: 2.7079882621765137.\n",
      "Epoch 588. Training loss: 3.131845235824585. Validation loss: 2.7079885005950928.\n",
      "Epoch 589. Training loss: 3.1318438053131104. Validation loss: 2.7079882621765137.\n",
      "Epoch 590. Training loss: 3.1318416595458984. Validation loss: 2.707988739013672.\n",
      "Epoch 591. Training loss: 3.1318395137786865. Validation loss: 2.7079882621765137.\n",
      "Epoch 592. Training loss: 3.1318376064300537. Validation loss: 2.7079882621765137.\n",
      "Epoch 593. Training loss: 3.131835699081421. Validation loss: 2.7079880237579346.\n",
      "Epoch 594. Training loss: 3.131833791732788. Validation loss: 2.7079882621765137.\n",
      "Epoch 595. Training loss: 3.131831407546997. Validation loss: 2.7079882621765137.\n",
      "Epoch 596. Training loss: 3.131829261779785. Validation loss: 2.7079882621765137.\n",
      "Epoch 597. Training loss: 3.1318273544311523. Validation loss: 2.7079882621765137.\n",
      "Epoch 598. Training loss: 3.1318254470825195. Validation loss: 2.7079880237579346.\n",
      "Epoch 599. Training loss: 3.1318233013153076. Validation loss: 2.7079880237579346.\n",
      "Epoch 600. Training loss: 3.131821393966675. Validation loss: 2.7079877853393555.\n",
      "Epoch 601. Training loss: 3.131819486618042. Validation loss: 2.7079877853393555.\n",
      "Epoch 602. Training loss: 3.131817102432251. Validation loss: 2.7079877853393555.\n",
      "Epoch 603. Training loss: 3.131815195083618. Validation loss: 2.7079877853393555.\n",
      "Epoch 604. Training loss: 3.1318132877349854. Validation loss: 2.7079877853393555.\n",
      "Epoch 605. Training loss: 3.1318111419677734. Validation loss: 2.7079877853393555.\n",
      "Epoch 606. Training loss: 3.1318092346191406. Validation loss: 2.7079877853393555.\n",
      "Epoch 607. Training loss: 3.131807327270508. Validation loss: 2.7079877853393555.\n",
      "Epoch 608. Training loss: 3.131805181503296. Validation loss: 2.7079877853393555.\n",
      "Epoch 609. Training loss: 3.131803512573242. Validation loss: 2.7079875469207764.\n",
      "Epoch 610. Training loss: 3.1318016052246094. Validation loss: 2.7079875469207764.\n",
      "Epoch 611. Training loss: 3.1317994594573975. Validation loss: 2.7079877853393555.\n",
      "Epoch 612. Training loss: 3.1317970752716064. Validation loss: 2.7079875469207764.\n",
      "Epoch 613. Training loss: 3.1317951679229736. Validation loss: 2.7079873085021973.\n",
      "Epoch 614. Training loss: 3.131793260574341. Validation loss: 2.7079875469207764.\n",
      "Epoch 615. Training loss: 3.13179087638855. Validation loss: 2.7079873085021973.\n",
      "Epoch 616. Training loss: 3.131789207458496. Validation loss: 2.7079873085021973.\n",
      "Epoch 617. Training loss: 3.131787061691284. Validation loss: 2.7079873085021973.\n",
      "Epoch 618. Training loss: 3.1317851543426514. Validation loss: 2.7079873085021973.\n",
      "Epoch 619. Training loss: 3.1317827701568604. Validation loss: 2.7079873085021973.\n",
      "Epoch 620. Training loss: 3.1317806243896484. Validation loss: 2.7079873085021973.\n",
      "Epoch 621. Training loss: 3.1317787170410156. Validation loss: 2.7079873085021973.\n",
      "Epoch 622. Training loss: 3.131776809692383. Validation loss: 2.707986831665039.\n",
      "Epoch 623. Training loss: 3.131774663925171. Validation loss: 2.7079873085021973.\n",
      "Epoch 624. Training loss: 3.131772756576538. Validation loss: 2.7079873085021973.\n",
      "Epoch 625. Training loss: 3.131770372390747. Validation loss: 2.707987070083618.\n",
      "Epoch 626. Training loss: 3.1317689418792725. Validation loss: 2.7079873085021973.\n",
      "Epoch 627. Training loss: 3.1317663192749023. Validation loss: 2.707986831665039.\n",
      "Epoch 628. Training loss: 3.1317644119262695. Validation loss: 2.707986831665039.\n",
      "Epoch 629. Training loss: 3.1317625045776367. Validation loss: 2.707986831665039.\n",
      "Epoch 630. Training loss: 3.131760358810425. Validation loss: 2.707987070083618.\n",
      "Epoch 631. Training loss: 3.131758451461792. Validation loss: 2.707986831665039.\n",
      "Epoch 632. Training loss: 3.131756067276001. Validation loss: 2.707986354827881.\n",
      "Epoch 633. Training loss: 3.131754159927368. Validation loss: 2.707986831665039.\n",
      "Epoch 634. Training loss: 3.1317520141601562. Validation loss: 2.707986831665039.\n",
      "Epoch 635. Training loss: 3.1317501068115234. Validation loss: 2.707986831665039.\n",
      "Epoch 636. Training loss: 3.1317481994628906. Validation loss: 2.707986354827881.\n",
      "Epoch 637. Training loss: 3.1317455768585205. Validation loss: 2.707986354827881.\n",
      "Epoch 638. Training loss: 3.1317436695098877. Validation loss: 2.707986831665039.\n",
      "Epoch 639. Training loss: 3.131741762161255. Validation loss: 2.707986354827881.\n",
      "Epoch 640. Training loss: 3.131739854812622. Validation loss: 2.707986354827881.\n",
      "Epoch 641. Training loss: 3.13173770904541. Validation loss: 2.707986354827881.\n",
      "Epoch 642. Training loss: 3.1317355632781982. Validation loss: 2.707986354827881.\n",
      "Epoch 643. Training loss: 3.1317336559295654. Validation loss: 2.707986354827881.\n",
      "Epoch 644. Training loss: 3.1317312717437744. Validation loss: 2.7079858779907227.\n",
      "Epoch 645. Training loss: 3.1317293643951416. Validation loss: 2.707986354827881.\n",
      "Epoch 646. Training loss: 3.1317269802093506. Validation loss: 2.7079861164093018.\n",
      "Epoch 647. Training loss: 3.131725311279297. Validation loss: 2.7079858779907227.\n",
      "Epoch 648. Training loss: 3.131723165512085. Validation loss: 2.707986354827881.\n",
      "Epoch 649. Training loss: 3.131720781326294. Validation loss: 2.7079861164093018.\n",
      "Epoch 650. Training loss: 3.131718873977661. Validation loss: 2.707986354827881.\n",
      "Epoch 651. Training loss: 3.1317169666290283. Validation loss: 2.7079858779907227.\n",
      "Epoch 652. Training loss: 3.1317148208618164. Validation loss: 2.7079858779907227.\n",
      "Epoch 653. Training loss: 3.1317126750946045. Validation loss: 2.7079858779907227.\n",
      "Epoch 654. Training loss: 3.1317107677459717. Validation loss: 2.7079856395721436.\n",
      "Epoch 655. Training loss: 3.131708860397339. Validation loss: 2.7079861164093018.\n",
      "Epoch 656. Training loss: 3.131706476211548. Validation loss: 2.7079858779907227.\n",
      "Epoch 657. Training loss: 3.131704330444336. Validation loss: 2.7079858779907227.\n",
      "Epoch 658. Training loss: 3.1317026615142822. Validation loss: 2.7079858779907227.\n",
      "Epoch 659. Training loss: 3.131700277328491. Validation loss: 2.7079858779907227.\n",
      "Epoch 660. Training loss: 3.1316978931427. Validation loss: 2.7079856395721436.\n",
      "Epoch 661. Training loss: 3.1316959857940674. Validation loss: 2.7079858779907227.\n",
      "Epoch 662. Training loss: 3.1316938400268555. Validation loss: 2.7079858779907227.\n",
      "Epoch 663. Training loss: 3.1316919326782227. Validation loss: 2.7079858779907227.\n",
      "Epoch 664. Training loss: 3.1316897869110107. Validation loss: 2.7079851627349854.\n",
      "Epoch 665. Training loss: 3.1316874027252197. Validation loss: 2.7079854011535645.\n",
      "Epoch 666. Training loss: 3.131685495376587. Validation loss: 2.7079854011535645.\n",
      "Epoch 667. Training loss: 3.131683349609375. Validation loss: 2.7079854011535645.\n",
      "Epoch 668. Training loss: 3.131681442260742. Validation loss: 2.7079854011535645.\n",
      "Epoch 669. Training loss: 3.1316792964935303. Validation loss: 2.7079854011535645.\n",
      "Epoch 670. Training loss: 3.1316769123077393. Validation loss: 2.7079849243164062.\n",
      "Epoch 671. Training loss: 3.1316747665405273. Validation loss: 2.7079854011535645.\n",
      "Epoch 672. Training loss: 3.1316728591918945. Validation loss: 2.7079849243164062.\n",
      "Epoch 673. Training loss: 3.1316709518432617. Validation loss: 2.7079849243164062.\n",
      "Epoch 674. Training loss: 3.131669044494629. Validation loss: 2.7079849243164062.\n",
      "Epoch 675. Training loss: 3.131666898727417. Validation loss: 2.7079851627349854.\n",
      "Epoch 676. Training loss: 3.131664276123047. Validation loss: 2.7079849243164062.\n",
      "Epoch 677. Training loss: 3.131662607192993. Validation loss: 2.7079849243164062.\n",
      "Epoch 678. Training loss: 3.1316604614257812. Validation loss: 2.7079849243164062.\n",
      "Epoch 679. Training loss: 3.1316583156585693. Validation loss: 2.7079849243164062.\n",
      "Epoch 680. Training loss: 3.1316564083099365. Validation loss: 2.7079849243164062.\n",
      "Epoch 681. Training loss: 3.1316537857055664. Validation loss: 2.7079849243164062.\n",
      "Epoch 682. Training loss: 3.1316518783569336. Validation loss: 2.7079849243164062.\n",
      "Epoch 683. Training loss: 3.1316497325897217. Validation loss: 2.7079849243164062.\n",
      "Epoch 684. Training loss: 3.1316473484039307. Validation loss: 2.7079849243164062.\n",
      "Epoch 685. Training loss: 3.1316452026367188. Validation loss: 2.7079849243164062.\n",
      "Epoch 686. Training loss: 3.131643295288086. Validation loss: 2.707984685897827.\n",
      "Epoch 687. Training loss: 3.131641387939453. Validation loss: 2.707984685897827.\n",
      "Epoch 688. Training loss: 3.131639242172241. Validation loss: 2.7079849243164062.\n",
      "Epoch 689. Training loss: 3.13163685798645. Validation loss: 2.707984447479248.\n",
      "Epoch 690. Training loss: 3.1316349506378174. Validation loss: 2.707984447479248.\n",
      "Epoch 691. Training loss: 3.1316328048706055. Validation loss: 2.707984685897827.\n",
      "Epoch 692. Training loss: 3.1316306591033936. Validation loss: 2.707984447479248.\n",
      "Epoch 693. Training loss: 3.1316280364990234. Validation loss: 2.707984447479248.\n",
      "Epoch 694. Training loss: 3.1316261291503906. Validation loss: 2.707984447479248.\n",
      "Epoch 695. Training loss: 3.131624221801758. Validation loss: 2.707984447479248.\n",
      "Epoch 696. Training loss: 3.131622076034546. Validation loss: 2.707984209060669.\n",
      "Epoch 697. Training loss: 3.131619691848755. Validation loss: 2.707984209060669.\n",
      "Epoch 698. Training loss: 3.131617546081543. Validation loss: 2.70798397064209.\n",
      "Epoch 699. Training loss: 3.131615400314331. Validation loss: 2.70798397064209.\n",
      "Epoch 700. Training loss: 3.1316134929656982. Validation loss: 2.70798397064209.\n",
      "Epoch 701. Training loss: 3.1316111087799072. Validation loss: 2.70798397064209.\n",
      "Epoch 702. Training loss: 3.1316092014312744. Validation loss: 2.707984209060669.\n",
      "Epoch 703. Training loss: 3.1316068172454834. Validation loss: 2.70798397064209.\n",
      "Epoch 704. Training loss: 3.1316049098968506. Validation loss: 2.70798397064209.\n",
      "Epoch 705. Training loss: 3.1316025257110596. Validation loss: 2.70798397064209.\n",
      "Epoch 706. Training loss: 3.1316006183624268. Validation loss: 2.707984209060669.\n",
      "Epoch 707. Training loss: 3.131598711013794. Validation loss: 2.7079837322235107.\n",
      "Epoch 708. Training loss: 3.131596565246582. Validation loss: 2.70798397064209.\n",
      "Epoch 709. Training loss: 3.13159441947937. Validation loss: 2.70798397064209.\n",
      "Epoch 710. Training loss: 3.131591796875. Validation loss: 2.7079837322235107.\n",
      "Epoch 711. Training loss: 3.131589889526367. Validation loss: 2.7079837322235107.\n",
      "Epoch 712. Training loss: 3.131587266921997. Validation loss: 2.7079837322235107.\n",
      "Epoch 713. Training loss: 3.1315853595733643. Validation loss: 2.7079837322235107.\n",
      "Epoch 714. Training loss: 3.1315832138061523. Validation loss: 2.7079837322235107.\n",
      "Epoch 715. Training loss: 3.1315813064575195. Validation loss: 2.7079837322235107.\n",
      "Epoch 716. Training loss: 3.1315786838531494. Validation loss: 2.7079834938049316.\n",
      "Epoch 717. Training loss: 3.1315767765045166. Validation loss: 2.7079834938049316.\n",
      "Epoch 718. Training loss: 3.1315746307373047. Validation loss: 2.7079834938049316.\n",
      "Epoch 719. Training loss: 3.131572723388672. Validation loss: 2.7079834938049316.\n",
      "Epoch 720. Training loss: 3.1315701007843018. Validation loss: 2.7079834938049316.\n",
      "Epoch 721. Training loss: 3.131568193435669. Validation loss: 2.7079834938049316.\n",
      "Epoch 722. Training loss: 3.131565809249878. Validation loss: 2.7079834938049316.\n",
      "Epoch 723. Training loss: 3.131563425064087. Validation loss: 2.7079834938049316.\n",
      "Epoch 724. Training loss: 3.131561279296875. Validation loss: 2.7079834938049316.\n",
      "Epoch 725. Training loss: 3.131559371948242. Validation loss: 2.7079832553863525.\n",
      "Epoch 726. Training loss: 3.1315572261810303. Validation loss: 2.7079834938049316.\n",
      "Epoch 727. Training loss: 3.13155460357666. Validation loss: 2.7079830169677734.\n",
      "Epoch 728. Training loss: 3.1315526962280273. Validation loss: 2.7079832553863525.\n",
      "Epoch 729. Training loss: 3.1315505504608154. Validation loss: 2.7079830169677734.\n",
      "Epoch 730. Training loss: 3.1315486431121826. Validation loss: 2.7079830169677734.\n",
      "Epoch 731. Training loss: 3.1315462589263916. Validation loss: 2.7079830169677734.\n",
      "Epoch 732. Training loss: 3.1315441131591797. Validation loss: 2.7079830169677734.\n",
      "Epoch 733. Training loss: 3.1315414905548096. Validation loss: 2.7079830169677734.\n",
      "Epoch 734. Training loss: 3.1315395832061768. Validation loss: 2.7079830169677734.\n",
      "Epoch 735. Training loss: 3.131537675857544. Validation loss: 2.7079830169677734.\n",
      "Epoch 736. Training loss: 3.131535291671753. Validation loss: 2.7079830169677734.\n",
      "Epoch 737. Training loss: 3.131533622741699. Validation loss: 2.7079830169677734.\n",
      "Epoch 738. Training loss: 3.13153076171875. Validation loss: 2.7079827785491943.\n",
      "Epoch 739. Training loss: 3.131528854370117. Validation loss: 2.7079827785491943.\n",
      "Epoch 740. Training loss: 3.1315267086029053. Validation loss: 2.7079827785491943.\n",
      "Epoch 741. Training loss: 3.1315243244171143. Validation loss: 2.7079825401306152.\n",
      "Epoch 742. Training loss: 3.1315219402313232. Validation loss: 2.7079825401306152.\n",
      "Epoch 743. Training loss: 3.1315202713012695. Validation loss: 2.7079825401306152.\n",
      "Epoch 744. Training loss: 3.1315176486968994. Validation loss: 2.7079825401306152.\n",
      "Epoch 745. Training loss: 3.1315155029296875. Validation loss: 2.7079825401306152.\n",
      "Epoch 746. Training loss: 3.1315133571624756. Validation loss: 2.7079825401306152.\n",
      "Epoch 747. Training loss: 3.1315114498138428. Validation loss: 2.7079825401306152.\n",
      "Epoch 748. Training loss: 3.1315088272094727. Validation loss: 2.7079825401306152.\n",
      "Epoch 749. Training loss: 3.1315066814422607. Validation loss: 2.7079825401306152.\n",
      "Epoch 750. Training loss: 3.131504774093628. Validation loss: 2.707982301712036.\n",
      "Epoch 751. Training loss: 3.131502151489258. Validation loss: 2.7079825401306152.\n",
      "Epoch 752. Training loss: 3.131500244140625. Validation loss: 2.707982301712036.\n",
      "Epoch 753. Training loss: 3.131498336791992. Validation loss: 2.7079825401306152.\n",
      "Epoch 754. Training loss: 3.131495237350464. Validation loss: 2.707982063293457.\n",
      "Epoch 755. Training loss: 3.13149356842041. Validation loss: 2.7079825401306152.\n",
      "Epoch 756. Training loss: 3.1314914226531982. Validation loss: 2.707982063293457.\n",
      "Epoch 757. Training loss: 3.1314890384674072. Validation loss: 2.707982063293457.\n",
      "Epoch 758. Training loss: 3.131486654281616. Validation loss: 2.707982063293457.\n",
      "Epoch 759. Training loss: 3.1314847469329834. Validation loss: 2.707982063293457.\n",
      "Epoch 760. Training loss: 3.1314823627471924. Validation loss: 2.707982063293457.\n",
      "Epoch 761. Training loss: 3.1314802169799805. Validation loss: 2.707981824874878.\n",
      "Epoch 762. Training loss: 3.1314775943756104. Validation loss: 2.707981586456299.\n",
      "Epoch 763. Training loss: 3.1314754486083984. Validation loss: 2.707981824874878.\n",
      "Epoch 764. Training loss: 3.1314735412597656. Validation loss: 2.707981586456299.\n",
      "Epoch 765. Training loss: 3.1314713954925537. Validation loss: 2.707982063293457.\n",
      "Epoch 766. Training loss: 3.1314690113067627. Validation loss: 2.707981586456299.\n",
      "Epoch 767. Training loss: 3.1314666271209717. Validation loss: 2.707981586456299.\n",
      "Epoch 768. Training loss: 3.1314642429351807. Validation loss: 2.707981586456299.\n",
      "Epoch 769. Training loss: 3.1314620971679688. Validation loss: 2.707981586456299.\n",
      "Epoch 770. Training loss: 3.131460189819336. Validation loss: 2.707981586456299.\n",
      "Epoch 771. Training loss: 3.131457567214966. Validation loss: 2.707981586456299.\n",
      "Epoch 772. Training loss: 3.131455659866333. Validation loss: 2.707981586456299.\n",
      "Epoch 773. Training loss: 3.131453514099121. Validation loss: 2.7079813480377197.\n",
      "Epoch 774. Training loss: 3.131451368331909. Validation loss: 2.707981586456299.\n",
      "Epoch 775. Training loss: 3.131448745727539. Validation loss: 2.7079811096191406.\n",
      "Epoch 776. Training loss: 3.1314468383789062. Validation loss: 2.7079813480377197.\n",
      "Epoch 777. Training loss: 3.131444215774536. Validation loss: 2.7079811096191406.\n",
      "Epoch 778. Training loss: 3.131442070007324. Validation loss: 2.7079811096191406.\n",
      "Epoch 779. Training loss: 3.1314399242401123. Validation loss: 2.7079811096191406.\n",
      "Epoch 780. Training loss: 3.1314375400543213. Validation loss: 2.7079811096191406.\n",
      "Epoch 781. Training loss: 3.1314353942871094. Validation loss: 2.7079811096191406.\n",
      "Epoch 782. Training loss: 3.1314327716827393. Validation loss: 2.7079811096191406.\n",
      "Epoch 783. Training loss: 3.1314308643341064. Validation loss: 2.7079811096191406.\n",
      "Epoch 784. Training loss: 3.1314287185668945. Validation loss: 2.7079811096191406.\n",
      "Epoch 785. Training loss: 3.1314260959625244. Validation loss: 2.7079811096191406.\n",
      "Epoch 786. Training loss: 3.1314239501953125. Validation loss: 2.7079808712005615.\n",
      "Epoch 787. Training loss: 3.1314218044281006. Validation loss: 2.7079808712005615.\n",
      "Epoch 788. Training loss: 3.1314198970794678. Validation loss: 2.7079811096191406.\n",
      "Epoch 789. Training loss: 3.1314175128936768. Validation loss: 2.7079806327819824.\n",
      "Epoch 790. Training loss: 3.1314146518707275. Validation loss: 2.7079806327819824.\n",
      "Epoch 791. Training loss: 3.1314127445220947. Validation loss: 2.7079811096191406.\n",
      "Epoch 792. Training loss: 3.131410598754883. Validation loss: 2.7079803943634033.\n",
      "Epoch 793. Training loss: 3.1314079761505127. Validation loss: 2.7079803943634033.\n",
      "Epoch 794. Training loss: 3.13140606880188. Validation loss: 2.7079806327819824.\n",
      "Epoch 795. Training loss: 3.131403684616089. Validation loss: 2.7079806327819824.\n",
      "Epoch 796. Training loss: 3.1314010620117188. Validation loss: 2.7079806327819824.\n",
      "Epoch 797. Training loss: 3.131399154663086. Validation loss: 2.707979917526245.\n",
      "Epoch 798. Training loss: 3.131396532058716. Validation loss: 2.7079803943634033.\n",
      "Epoch 799. Training loss: 3.131394386291504. Validation loss: 2.7079803943634033.\n",
      "Epoch 800. Training loss: 3.131392478942871. Validation loss: 2.7079803943634033.\n",
      "Epoch 801. Training loss: 3.131389856338501. Validation loss: 2.707980155944824.\n",
      "Epoch 802. Training loss: 3.131387710571289. Validation loss: 2.7079806327819824.\n",
      "Epoch 803. Training loss: 3.131385564804077. Validation loss: 2.7079803943634033.\n",
      "Epoch 804. Training loss: 3.1313836574554443. Validation loss: 2.707980155944824.\n",
      "Epoch 805. Training loss: 3.131381034851074. Validation loss: 2.707979917526245.\n",
      "Epoch 806. Training loss: 3.131378173828125. Validation loss: 2.707979679107666.\n",
      "Epoch 807. Training loss: 3.131376266479492. Validation loss: 2.707980155944824.\n",
      "Epoch 808. Training loss: 3.1313741207122803. Validation loss: 2.707980155944824.\n",
      "Epoch 809. Training loss: 3.1313717365264893. Validation loss: 2.707979917526245.\n",
      "Epoch 810. Training loss: 3.1313695907592773. Validation loss: 2.707979917526245.\n",
      "Epoch 811. Training loss: 3.1313669681549072. Validation loss: 2.707979679107666.\n",
      "Epoch 812. Training loss: 3.1313650608062744. Validation loss: 2.707979917526245.\n",
      "Epoch 813. Training loss: 3.1313626766204834. Validation loss: 2.707979679107666.\n",
      "Epoch 814. Training loss: 3.1313602924346924. Validation loss: 2.707979917526245.\n",
      "Epoch 815. Training loss: 3.1313581466674805. Validation loss: 2.707979679107666.\n",
      "Epoch 816. Training loss: 3.1313560009002686. Validation loss: 2.707979679107666.\n",
      "Epoch 817. Training loss: 3.1313533782958984. Validation loss: 2.707979679107666.\n",
      "Epoch 818. Training loss: 3.1313507556915283. Validation loss: 2.707979202270508.\n",
      "Epoch 819. Training loss: 3.1313488483428955. Validation loss: 2.707979440689087.\n",
      "Epoch 820. Training loss: 3.1313464641571045. Validation loss: 2.707979202270508.\n",
      "Epoch 821. Training loss: 3.1313440799713135. Validation loss: 2.707979202270508.\n",
      "Epoch 822. Training loss: 3.1313419342041016. Validation loss: 2.707979440689087.\n",
      "Epoch 823. Training loss: 3.1313393115997314. Validation loss: 2.707979440689087.\n",
      "Epoch 824. Training loss: 3.1313374042510986. Validation loss: 2.707979440689087.\n",
      "Epoch 825. Training loss: 3.1313350200653076. Validation loss: 2.707979202270508.\n",
      "Epoch 826. Training loss: 3.1313326358795166. Validation loss: 2.707979202270508.\n",
      "Epoch 827. Training loss: 3.1313302516937256. Validation loss: 2.707979202270508.\n",
      "Epoch 828. Training loss: 3.1313283443450928. Validation loss: 2.707979202270508.\n",
      "Epoch 829. Training loss: 3.1313254833221436. Validation loss: 2.7079789638519287.\n",
      "Epoch 830. Training loss: 3.1313230991363525. Validation loss: 2.7079789638519287.\n",
      "Epoch 831. Training loss: 3.1313209533691406. Validation loss: 2.7079787254333496.\n",
      "Epoch 832. Training loss: 3.1313188076019287. Validation loss: 2.7079787254333496.\n",
      "Epoch 833. Training loss: 3.1313164234161377. Validation loss: 2.7079787254333496.\n",
      "Epoch 834. Training loss: 3.1313140392303467. Validation loss: 2.7079787254333496.\n",
      "Epoch 835. Training loss: 3.1313116550445557. Validation loss: 2.7079787254333496.\n",
      "Epoch 836. Training loss: 3.131309747695923. Validation loss: 2.7079787254333496.\n",
      "Epoch 837. Training loss: 3.131307363510132. Validation loss: 2.7079787254333496.\n",
      "Epoch 838. Training loss: 3.1313047409057617. Validation loss: 2.7079787254333496.\n",
      "Epoch 839. Training loss: 3.13130259513855. Validation loss: 2.7079789638519287.\n",
      "Epoch 840. Training loss: 3.131300210952759. Validation loss: 2.7079784870147705.\n",
      "Epoch 841. Training loss: 3.131298065185547. Validation loss: 2.7079787254333496.\n",
      "Epoch 842. Training loss: 3.1312954425811768. Validation loss: 2.7079787254333496.\n",
      "Epoch 843. Training loss: 3.1312930583953857. Validation loss: 2.7079784870147705.\n",
      "Epoch 844. Training loss: 3.131291151046753. Validation loss: 2.7079782485961914.\n",
      "Epoch 845. Training loss: 3.131288528442383. Validation loss: 2.7079784870147705.\n",
      "Epoch 846. Training loss: 3.131286382675171. Validation loss: 2.7079782485961914.\n",
      "Epoch 847. Training loss: 3.13128399848938. Validation loss: 2.7079782485961914.\n",
      "Epoch 848. Training loss: 3.131281614303589. Validation loss: 2.7079782485961914.\n",
      "Epoch 849. Training loss: 3.1312789916992188. Validation loss: 2.7079782485961914.\n",
      "Epoch 850. Training loss: 3.131277084350586. Validation loss: 2.7079782485961914.\n",
      "Epoch 851. Training loss: 3.131274461746216. Validation loss: 2.7079780101776123.\n",
      "Epoch 852. Training loss: 3.131272077560425. Validation loss: 2.7079782485961914.\n",
      "Epoch 853. Training loss: 3.1312694549560547. Validation loss: 2.7079780101776123.\n",
      "Epoch 854. Training loss: 3.1312673091888428. Validation loss: 2.7079780101776123.\n",
      "Epoch 855. Training loss: 3.1312649250030518. Validation loss: 2.707977771759033.\n",
      "Epoch 856. Training loss: 3.1312625408172607. Validation loss: 2.707977771759033.\n",
      "Epoch 857. Training loss: 3.1312599182128906. Validation loss: 2.707977771759033.\n",
      "Epoch 858. Training loss: 3.131258249282837. Validation loss: 2.7079782485961914.\n",
      "Epoch 859. Training loss: 3.1312553882598877. Validation loss: 2.707977771759033.\n",
      "Epoch 860. Training loss: 3.131253242492676. Validation loss: 2.707977533340454.\n",
      "Epoch 861. Training loss: 3.131251096725464. Validation loss: 2.707977771759033.\n",
      "Epoch 862. Training loss: 3.1312482357025146. Validation loss: 2.707977771759033.\n",
      "Epoch 863. Training loss: 3.131246566772461. Validation loss: 2.707977533340454.\n",
      "Epoch 864. Training loss: 3.1312434673309326. Validation loss: 2.707977533340454.\n",
      "Epoch 865. Training loss: 3.1312415599823. Validation loss: 2.707977533340454.\n",
      "Epoch 866. Training loss: 3.1312389373779297. Validation loss: 2.707977533340454.\n",
      "Epoch 867. Training loss: 3.1312367916107178. Validation loss: 2.707977533340454.\n",
      "Epoch 868. Training loss: 3.1312341690063477. Validation loss: 2.707977533340454.\n",
      "Epoch 869. Training loss: 3.1312320232391357. Validation loss: 2.707977771759033.\n",
      "Epoch 870. Training loss: 3.131230115890503. Validation loss: 2.707977294921875.\n",
      "Epoch 871. Training loss: 3.1312267780303955. Validation loss: 2.707977294921875.\n",
      "Epoch 872. Training loss: 3.1312248706817627. Validation loss: 2.707977294921875.\n",
      "Epoch 873. Training loss: 3.1312224864959717. Validation loss: 2.707977294921875.\n",
      "Epoch 874. Training loss: 3.1312198638916016. Validation loss: 2.707977533340454.\n",
      "Epoch 875. Training loss: 3.1312177181243896. Validation loss: 2.707977056503296.\n",
      "Epoch 876. Training loss: 3.1312153339385986. Validation loss: 2.707977294921875.\n",
      "Epoch 877. Training loss: 3.1312129497528076. Validation loss: 2.707977294921875.\n",
      "Epoch 878. Training loss: 3.1312103271484375. Validation loss: 2.707977056503296.\n",
      "Epoch 879. Training loss: 3.1312081813812256. Validation loss: 2.707977056503296.\n",
      "Epoch 880. Training loss: 3.1312055587768555. Validation loss: 2.707977056503296.\n",
      "Epoch 881. Training loss: 3.1312029361724854. Validation loss: 2.707977056503296.\n",
      "Epoch 882. Training loss: 3.1312007904052734. Validation loss: 2.707977056503296.\n",
      "Epoch 883. Training loss: 3.1311988830566406. Validation loss: 2.707976818084717.\n",
      "Epoch 884. Training loss: 3.1311960220336914. Validation loss: 2.707976818084717.\n",
      "Epoch 885. Training loss: 3.1311933994293213. Validation loss: 2.7079765796661377.\n",
      "Epoch 886. Training loss: 3.1311914920806885. Validation loss: 2.7079765796661377.\n",
      "Epoch 887. Training loss: 3.1311891078948975. Validation loss: 2.7079765796661377.\n",
      "Epoch 888. Training loss: 3.1311867237091064. Validation loss: 2.7079765796661377.\n",
      "Epoch 889. Training loss: 3.1311838626861572. Validation loss: 2.707976818084717.\n",
      "Epoch 890. Training loss: 3.1311817169189453. Validation loss: 2.7079765796661377.\n",
      "Epoch 891. Training loss: 3.131179094314575. Validation loss: 2.7079765796661377.\n",
      "Epoch 892. Training loss: 3.1311769485473633. Validation loss: 2.7079761028289795.\n",
      "Epoch 893. Training loss: 3.1311748027801514. Validation loss: 2.7079763412475586.\n",
      "Epoch 894. Training loss: 3.1311724185943604. Validation loss: 2.7079761028289795.\n",
      "Epoch 895. Training loss: 3.1311700344085693. Validation loss: 2.7079763412475586.\n",
      "Epoch 896. Training loss: 3.131167411804199. Validation loss: 2.7079761028289795.\n",
      "Epoch 897. Training loss: 3.1311652660369873. Validation loss: 2.7079761028289795.\n",
      "Epoch 898. Training loss: 3.1311628818511963. Validation loss: 2.7079761028289795.\n",
      "Epoch 899. Training loss: 3.131160020828247. Validation loss: 2.7079761028289795.\n",
      "Epoch 900. Training loss: 3.131157875061035. Validation loss: 2.7079761028289795.\n",
      "Epoch 901. Training loss: 3.131155252456665. Validation loss: 2.7079761028289795.\n",
      "Epoch 902. Training loss: 3.131152868270874. Validation loss: 2.7079761028289795.\n",
      "Epoch 903. Training loss: 3.131150960922241. Validation loss: 2.7079761028289795.\n",
      "Epoch 904. Training loss: 3.131148099899292. Validation loss: 2.7079761028289795.\n",
      "Epoch 905. Training loss: 3.131145715713501. Validation loss: 2.7079756259918213.\n",
      "Epoch 906. Training loss: 3.13114333152771. Validation loss: 2.7079756259918213.\n",
      "Epoch 907. Training loss: 3.13114070892334. Validation loss: 2.7079756259918213.\n",
      "Epoch 908. Training loss: 3.1311380863189697. Validation loss: 2.7079756259918213.\n",
      "Epoch 909. Training loss: 3.131135940551758. Validation loss: 2.7079756259918213.\n",
      "Epoch 910. Training loss: 3.131133794784546. Validation loss: 2.7079756259918213.\n",
      "Epoch 911. Training loss: 3.131131172180176. Validation loss: 2.7079756259918213.\n",
      "Epoch 912. Training loss: 3.1311285495758057. Validation loss: 2.707975387573242.\n",
      "Epoch 913. Training loss: 3.1311261653900146. Validation loss: 2.7079756259918213.\n",
      "Epoch 914. Training loss: 3.1311235427856445. Validation loss: 2.7079756259918213.\n",
      "Epoch 915. Training loss: 3.1311213970184326. Validation loss: 2.707975387573242.\n",
      "Epoch 916. Training loss: 3.1311190128326416. Validation loss: 2.707975387573242.\n",
      "Epoch 917. Training loss: 3.1311166286468506. Validation loss: 2.707975149154663.\n",
      "Epoch 918. Training loss: 3.1311137676239014. Validation loss: 2.707975149154663.\n",
      "Epoch 919. Training loss: 3.1311118602752686. Validation loss: 2.7079756259918213.\n",
      "Epoch 920. Training loss: 3.1311092376708984. Validation loss: 2.707975149154663.\n",
      "Epoch 921. Training loss: 3.1311066150665283. Validation loss: 2.707974910736084.\n",
      "Epoch 922. Training loss: 3.1311042308807373. Validation loss: 2.707975149154663.\n",
      "Epoch 923. Training loss: 3.1311018466949463. Validation loss: 2.707975149154663.\n",
      "Epoch 924. Training loss: 3.131098985671997. Validation loss: 2.707974910736084.\n",
      "Epoch 925. Training loss: 3.131096839904785. Validation loss: 2.707975149154663.\n",
      "Epoch 926. Training loss: 3.1310946941375732. Validation loss: 2.707974672317505.\n",
      "Epoch 927. Training loss: 3.131092071533203. Validation loss: 2.707974672317505.\n",
      "Epoch 928. Training loss: 3.131089925765991. Validation loss: 2.707974672317505.\n",
      "Epoch 929. Training loss: 3.131087064743042. Validation loss: 2.707974672317505.\n",
      "Epoch 930. Training loss: 3.131084442138672. Validation loss: 2.707974672317505.\n",
      "Epoch 931. Training loss: 3.13108229637146. Validation loss: 2.707974672317505.\n",
      "Epoch 932. Training loss: 3.13107967376709. Validation loss: 2.707974672317505.\n",
      "Epoch 933. Training loss: 3.131077527999878. Validation loss: 2.707974672317505.\n",
      "Epoch 934. Training loss: 3.1310746669769287. Validation loss: 2.707974672317505.\n",
      "Epoch 935. Training loss: 3.1310722827911377. Validation loss: 2.707974672317505.\n",
      "Epoch 936. Training loss: 3.1310698986053467. Validation loss: 2.707974433898926.\n",
      "Epoch 937. Training loss: 3.1310675144195557. Validation loss: 2.707974672317505.\n",
      "Epoch 938. Training loss: 3.1310651302337646. Validation loss: 2.7079741954803467.\n",
      "Epoch 939. Training loss: 3.1310625076293945. Validation loss: 2.7079741954803467.\n",
      "Epoch 940. Training loss: 3.1310596466064453. Validation loss: 2.7079741954803467.\n",
      "Epoch 941. Training loss: 3.1310577392578125. Validation loss: 2.707974433898926.\n",
      "Epoch 942. Training loss: 3.1310548782348633. Validation loss: 2.7079741954803467.\n",
      "Epoch 943. Training loss: 3.1310527324676514. Validation loss: 2.7079739570617676.\n",
      "Epoch 944. Training loss: 3.1310501098632812. Validation loss: 2.7079741954803467.\n",
      "Epoch 945. Training loss: 3.1310479640960693. Validation loss: 2.7079739570617676.\n",
      "Epoch 946. Training loss: 3.131045341491699. Validation loss: 2.7079739570617676.\n",
      "Epoch 947. Training loss: 3.1310431957244873. Validation loss: 2.7079739570617676.\n",
      "Epoch 948. Training loss: 3.13103985786438. Validation loss: 2.7079739570617676.\n",
      "Epoch 949. Training loss: 3.131037950515747. Validation loss: 2.7079739570617676.\n",
      "Epoch 950. Training loss: 3.131035566329956. Validation loss: 2.7079739570617676.\n",
      "Epoch 951. Training loss: 3.131032705307007. Validation loss: 2.7079737186431885.\n",
      "Epoch 952. Training loss: 3.1310300827026367. Validation loss: 2.7079737186431885.\n",
      "Epoch 953. Training loss: 3.131027936935425. Validation loss: 2.7079737186431885.\n",
      "Epoch 954. Training loss: 3.131025552749634. Validation loss: 2.7079737186431885.\n",
      "Epoch 955. Training loss: 3.1310226917266846. Validation loss: 2.7079737186431885.\n",
      "Epoch 956. Training loss: 3.1310203075408936. Validation loss: 2.7079737186431885.\n",
      "Epoch 957. Training loss: 3.1310176849365234. Validation loss: 2.7079734802246094.\n",
      "Epoch 958. Training loss: 3.1310150623321533. Validation loss: 2.7079734802246094.\n",
      "Epoch 959. Training loss: 3.1310126781463623. Validation loss: 2.7079734802246094.\n",
      "Epoch 960. Training loss: 3.1310102939605713. Validation loss: 2.7079734802246094.\n",
      "Epoch 961. Training loss: 3.131007432937622. Validation loss: 2.7079732418060303.\n",
      "Epoch 962. Training loss: 3.1310055255889893. Validation loss: 2.7079734802246094.\n",
      "Epoch 963. Training loss: 3.13100266456604. Validation loss: 2.7079734802246094.\n",
      "Epoch 964. Training loss: 3.131000280380249. Validation loss: 2.7079732418060303.\n",
      "Epoch 965. Training loss: 3.130997896194458. Validation loss: 2.7079734802246094.\n",
      "Epoch 966. Training loss: 3.130995512008667. Validation loss: 2.7079734802246094.\n",
      "Epoch 967. Training loss: 3.130992889404297. Validation loss: 2.7079732418060303.\n",
      "Epoch 968. Training loss: 3.1309902667999268. Validation loss: 2.7079732418060303.\n",
      "Epoch 969. Training loss: 3.1309878826141357. Validation loss: 2.707973003387451.\n",
      "Epoch 970. Training loss: 3.1309852600097656. Validation loss: 2.707972764968872.\n",
      "Epoch 971. Training loss: 3.1309826374053955. Validation loss: 2.707973003387451.\n",
      "Epoch 972. Training loss: 3.1309802532196045. Validation loss: 2.707972764968872.\n",
      "Epoch 973. Training loss: 3.1309776306152344. Validation loss: 2.707973003387451.\n",
      "Epoch 974. Training loss: 3.1309750080108643. Validation loss: 2.707972764968872.\n",
      "Epoch 975. Training loss: 3.1309728622436523. Validation loss: 2.707972764968872.\n",
      "Epoch 976. Training loss: 3.130970001220703. Validation loss: 2.707972764968872.\n",
      "Epoch 977. Training loss: 3.130967855453491. Validation loss: 2.707972764968872.\n",
      "Epoch 978. Training loss: 3.130965232849121. Validation loss: 2.707972526550293.\n",
      "Epoch 979. Training loss: 3.130962371826172. Validation loss: 2.707972764968872.\n",
      "Epoch 980. Training loss: 3.1309595108032227. Validation loss: 2.707972764968872.\n",
      "Epoch 981. Training loss: 3.13095760345459. Validation loss: 2.707972526550293.\n",
      "Epoch 982. Training loss: 3.1309547424316406. Validation loss: 2.707972288131714.\n",
      "Epoch 983. Training loss: 3.1309525966644287. Validation loss: 2.707972764968872.\n",
      "Epoch 984. Training loss: 3.1309502124786377. Validation loss: 2.707972288131714.\n",
      "Epoch 985. Training loss: 3.1309471130371094. Validation loss: 2.7079720497131348.\n",
      "Epoch 986. Training loss: 3.1309444904327393. Validation loss: 2.707972526550293.\n",
      "Epoch 987. Training loss: 3.1309423446655273. Validation loss: 2.707972288131714.\n",
      "Epoch 988. Training loss: 3.1309397220611572. Validation loss: 2.7079720497131348.\n",
      "Epoch 989. Training loss: 3.130936861038208. Validation loss: 2.7079720497131348.\n",
      "Epoch 990. Training loss: 3.130934476852417. Validation loss: 2.707972288131714.\n",
      "Epoch 991. Training loss: 3.130931854248047. Validation loss: 2.7079720497131348.\n",
      "Epoch 992. Training loss: 3.1309292316436768. Validation loss: 2.7079720497131348.\n",
      "Epoch 993. Training loss: 3.1309268474578857. Validation loss: 2.7079720497131348.\n",
      "Epoch 994. Training loss: 3.1309242248535156. Validation loss: 2.7079720497131348.\n",
      "Epoch 995. Training loss: 3.1309216022491455. Validation loss: 2.7079720497131348.\n",
      "Epoch 996. Training loss: 3.1309194564819336. Validation loss: 2.7079720497131348.\n",
      "Epoch 997. Training loss: 3.1309165954589844. Validation loss: 2.7079715728759766.\n",
      "Epoch 998. Training loss: 3.1309144496917725. Validation loss: 2.7079720497131348.\n",
      "Epoch 999. Training loss: 3.1309115886688232. Validation loss: 2.7079715728759766.\n",
      "Epoch 1000. Training loss: 3.1309092044830322. Validation loss: 2.7079715728759766.\n",
      "Epoch 1001. Training loss: 3.130906343460083. Validation loss: 2.7079715728759766.\n",
      "Epoch 1002. Training loss: 3.130904197692871. Validation loss: 2.7079718112945557.\n",
      "Epoch 1003. Training loss: 3.130901336669922. Validation loss: 2.7079715728759766.\n",
      "Epoch 1004. Training loss: 3.1308987140655518. Validation loss: 2.7079713344573975.\n",
      "Epoch 1005. Training loss: 3.1308963298797607. Validation loss: 2.7079713344573975.\n",
      "Epoch 1006. Training loss: 3.1308937072753906. Validation loss: 2.7079710960388184.\n",
      "Epoch 1007. Training loss: 3.1308910846710205. Validation loss: 2.7079713344573975.\n",
      "Epoch 1008. Training loss: 3.1308889389038086. Validation loss: 2.7079718112945557.\n",
      "Epoch 1009. Training loss: 3.1308860778808594. Validation loss: 2.7079710960388184.\n",
      "Epoch 1010. Training loss: 3.13088321685791. Validation loss: 2.7079713344573975.\n",
      "Epoch 1011. Training loss: 3.1308810710906982. Validation loss: 2.7079713344573975.\n",
      "Epoch 1012. Training loss: 3.130878210067749. Validation loss: 2.7079710960388184.\n",
      "Epoch 1013. Training loss: 3.130875825881958. Validation loss: 2.7079710960388184.\n",
      "Epoch 1014. Training loss: 3.130873680114746. Validation loss: 2.7079708576202393.\n",
      "Epoch 1015. Training loss: 3.1308705806732178. Validation loss: 2.70797061920166.\n",
      "Epoch 1016. Training loss: 3.1308681964874268. Validation loss: 2.70797061920166.\n",
      "Epoch 1017. Training loss: 3.1308653354644775. Validation loss: 2.7079710960388184.\n",
      "Epoch 1018. Training loss: 3.1308629512786865. Validation loss: 2.70797061920166.\n",
      "Epoch 1019. Training loss: 3.1308603286743164. Validation loss: 2.7079708576202393.\n",
      "Epoch 1020. Training loss: 3.130857467651367. Validation loss: 2.7079708576202393.\n",
      "Epoch 1021. Training loss: 3.130854845046997. Validation loss: 2.70797061920166.\n",
      "Epoch 1022. Training loss: 3.130852699279785. Validation loss: 2.707970380783081.\n",
      "Epoch 1023. Training loss: 3.130849838256836. Validation loss: 2.707970142364502.\n",
      "Epoch 1024. Training loss: 3.130847215652466. Validation loss: 2.70797061920166.\n",
      "Epoch 1025. Training loss: 3.130845069885254. Validation loss: 2.707970380783081.\n",
      "Epoch 1026. Training loss: 3.1308414936065674. Validation loss: 2.707970142364502.\n",
      "Epoch 1027. Training loss: 3.1308395862579346. Validation loss: 2.707970142364502.\n",
      "Epoch 1028. Training loss: 3.1308367252349854. Validation loss: 2.707970142364502.\n",
      "Epoch 1029. Training loss: 3.1308343410491943. Validation loss: 2.707970142364502.\n",
      "Epoch 1030. Training loss: 3.130831718444824. Validation loss: 2.707969903945923.\n",
      "Epoch 1031. Training loss: 3.130829095840454. Validation loss: 2.707970142364502.\n",
      "Epoch 1032. Training loss: 3.130826234817505. Validation loss: 2.707970142364502.\n",
      "Epoch 1033. Training loss: 3.130824089050293. Validation loss: 2.707970142364502.\n",
      "Epoch 1034. Training loss: 3.1308212280273438. Validation loss: 2.707969903945923.\n",
      "Epoch 1035. Training loss: 3.1308186054229736. Validation loss: 2.707969903945923.\n",
      "Epoch 1036. Training loss: 3.1308157444000244. Validation loss: 2.707970142364502.\n",
      "Epoch 1037. Training loss: 3.1308135986328125. Validation loss: 2.707969903945923.\n",
      "Epoch 1038. Training loss: 3.1308107376098633. Validation loss: 2.707969903945923.\n",
      "Epoch 1039. Training loss: 3.130808115005493. Validation loss: 2.707969903945923.\n",
      "Epoch 1040. Training loss: 3.130805253982544. Validation loss: 2.7079696655273438.\n",
      "Epoch 1041. Training loss: 3.130802869796753. Validation loss: 2.7079694271087646.\n",
      "Epoch 1042. Training loss: 3.130800247192383. Validation loss: 2.7079694271087646.\n",
      "Epoch 1043. Training loss: 3.1307976245880127. Validation loss: 2.7079696655273438.\n",
      "Epoch 1044. Training loss: 3.1307952404022217. Validation loss: 2.7079694271087646.\n",
      "Epoch 1045. Training loss: 3.1307919025421143. Validation loss: 2.7079694271087646.\n",
      "Epoch 1046. Training loss: 3.1307899951934814. Validation loss: 2.7079691886901855.\n",
      "Epoch 1047. Training loss: 3.130786895751953. Validation loss: 2.7079696655273438.\n",
      "Epoch 1048. Training loss: 3.130784034729004. Validation loss: 2.7079696655273438.\n",
      "Epoch 1049. Training loss: 3.130781412124634. Validation loss: 2.7079694271087646.\n",
      "Epoch 1050. Training loss: 3.130779266357422. Validation loss: 2.7079689502716064.\n",
      "Epoch 1051. Training loss: 3.1307764053344727. Validation loss: 2.7079691886901855.\n",
      "Epoch 1052. Training loss: 3.1307737827301025. Validation loss: 2.7079691886901855.\n",
      "Epoch 1053. Training loss: 3.1307716369628906. Validation loss: 2.7079689502716064.\n",
      "Epoch 1054. Training loss: 3.1307685375213623. Validation loss: 2.7079689502716064.\n",
      "Epoch 1055. Training loss: 3.130765914916992. Validation loss: 2.7079689502716064.\n",
      "Epoch 1056. Training loss: 3.130763292312622. Validation loss: 2.7079687118530273.\n",
      "Epoch 1057. Training loss: 3.130760908126831. Validation loss: 2.7079689502716064.\n",
      "Epoch 1058. Training loss: 3.130758285522461. Validation loss: 2.7079684734344482.\n",
      "Epoch 1059. Training loss: 3.130755662918091. Validation loss: 2.7079687118530273.\n",
      "Epoch 1060. Training loss: 3.1307528018951416. Validation loss: 2.7079687118530273.\n",
      "Epoch 1061. Training loss: 3.1307499408721924. Validation loss: 2.7079687118530273.\n",
      "Epoch 1062. Training loss: 3.1307475566864014. Validation loss: 2.7079687118530273.\n",
      "Epoch 1063. Training loss: 3.1307449340820312. Validation loss: 2.7079684734344482.\n",
      "Epoch 1064. Training loss: 3.130742073059082. Validation loss: 2.7079684734344482.\n",
      "Epoch 1065. Training loss: 3.130739450454712. Validation loss: 2.7079684734344482.\n",
      "Epoch 1066. Training loss: 3.1307365894317627. Validation loss: 2.707968235015869.\n",
      "Epoch 1067. Training loss: 3.1307342052459717. Validation loss: 2.7079684734344482.\n",
      "Epoch 1068. Training loss: 3.1307318210601807. Validation loss: 2.7079684734344482.\n",
      "Epoch 1069. Training loss: 3.1307289600372314. Validation loss: 2.707968235015869.\n",
      "Epoch 1070. Training loss: 3.1307260990142822. Validation loss: 2.707968235015869.\n",
      "Epoch 1071. Training loss: 3.130723237991333. Validation loss: 2.7079684734344482.\n",
      "Epoch 1072. Training loss: 3.130721092224121. Validation loss: 2.70796799659729.\n",
      "Epoch 1073. Training loss: 3.130718231201172. Validation loss: 2.707968235015869.\n",
      "Epoch 1074. Training loss: 3.1307156085968018. Validation loss: 2.70796799659729.\n",
      "Epoch 1075. Training loss: 3.1307127475738525. Validation loss: 2.70796799659729.\n",
      "Epoch 1076. Training loss: 3.1307098865509033. Validation loss: 2.70796799659729.\n",
      "Epoch 1077. Training loss: 3.130707025527954. Validation loss: 2.707967758178711.\n",
      "Epoch 1078. Training loss: 3.130704641342163. Validation loss: 2.707967758178711.\n",
      "Epoch 1079. Training loss: 3.130702018737793. Validation loss: 2.707967519760132.\n",
      "Epoch 1080. Training loss: 3.130699396133423. Validation loss: 2.707967758178711.\n",
      "Epoch 1081. Training loss: 3.130697011947632. Validation loss: 2.707967758178711.\n",
      "Epoch 1082. Training loss: 3.1306941509246826. Validation loss: 2.707967758178711.\n",
      "Epoch 1083. Training loss: 3.1306912899017334. Validation loss: 2.707967758178711.\n",
      "Epoch 1084. Training loss: 3.1306886672973633. Validation loss: 2.707967519760132.\n",
      "Epoch 1085. Training loss: 3.130685806274414. Validation loss: 2.707967519760132.\n",
      "Epoch 1086. Training loss: 3.130683183670044. Validation loss: 2.707967519760132.\n",
      "Epoch 1087. Training loss: 3.130680799484253. Validation loss: 2.707967519760132.\n",
      "Epoch 1088. Training loss: 3.130678176879883. Validation loss: 2.7079672813415527.\n",
      "Epoch 1089. Training loss: 3.1306753158569336. Validation loss: 2.7079672813415527.\n",
      "Epoch 1090. Training loss: 3.1306724548339844. Validation loss: 2.7079672813415527.\n",
      "Epoch 1091. Training loss: 3.1306698322296143. Validation loss: 2.7079672813415527.\n",
      "Epoch 1092. Training loss: 3.130666971206665. Validation loss: 2.7079672813415527.\n",
      "Epoch 1093. Training loss: 3.130664825439453. Validation loss: 2.707967519760132.\n",
      "Epoch 1094. Training loss: 3.130661725997925. Validation loss: 2.7079672813415527.\n",
      "Epoch 1095. Training loss: 3.1306591033935547. Validation loss: 2.7079672813415527.\n",
      "Epoch 1096. Training loss: 3.1306560039520264. Validation loss: 2.7079672813415527.\n",
      "Epoch 1097. Training loss: 3.1306533813476562. Validation loss: 2.7079672813415527.\n",
      "Epoch 1098. Training loss: 3.130650758743286. Validation loss: 2.7079668045043945.\n",
      "Epoch 1099. Training loss: 3.130648374557495. Validation loss: 2.7079670429229736.\n",
      "Epoch 1100. Training loss: 3.130645513534546. Validation loss: 2.7079670429229736.\n",
      "Epoch 1101. Training loss: 3.130643129348755. Validation loss: 2.7079670429229736.\n",
      "Epoch 1102. Training loss: 3.1306400299072266. Validation loss: 2.7079670429229736.\n",
      "Epoch 1103. Training loss: 3.1306371688842773. Validation loss: 2.7079668045043945.\n",
      "Epoch 1104. Training loss: 3.1306345462799072. Validation loss: 2.7079663276672363.\n",
      "Epoch 1105. Training loss: 3.130631685256958. Validation loss: 2.7079663276672363.\n",
      "Epoch 1106. Training loss: 3.130629301071167. Validation loss: 2.7079663276672363.\n",
      "Epoch 1107. Training loss: 3.1306264400482178. Validation loss: 2.7079663276672363.\n",
      "Epoch 1108. Training loss: 3.1306238174438477. Validation loss: 2.7079663276672363.\n",
      "Epoch 1109. Training loss: 3.1306207180023193. Validation loss: 2.7079663276672363.\n",
      "Epoch 1110. Training loss: 3.130618095397949. Validation loss: 2.7079660892486572.\n",
      "Epoch 1111. Training loss: 3.130615472793579. Validation loss: 2.7079660892486572.\n",
      "Epoch 1112. Training loss: 3.13061261177063. Validation loss: 2.7079660892486572.\n",
      "Epoch 1113. Training loss: 3.1306097507476807. Validation loss: 2.7079660892486572.\n",
      "Epoch 1114. Training loss: 3.1306068897247314. Validation loss: 2.7079660892486572.\n",
      "Epoch 1115. Training loss: 3.1306047439575195. Validation loss: 2.7079660892486572.\n",
      "Epoch 1116. Training loss: 3.1306018829345703. Validation loss: 2.707965850830078.\n",
      "Epoch 1117. Training loss: 3.1305992603302. Validation loss: 2.707965850830078.\n",
      "Epoch 1118. Training loss: 3.130596160888672. Validation loss: 2.707965612411499.\n",
      "Epoch 1119. Training loss: 3.13059401512146. Validation loss: 2.707965850830078.\n",
      "Epoch 1120. Training loss: 3.1305906772613525. Validation loss: 2.707965850830078.\n",
      "Epoch 1121. Training loss: 3.1305882930755615. Validation loss: 2.707965850830078.\n",
      "Epoch 1122. Training loss: 3.1305854320526123. Validation loss: 2.707965850830078.\n",
      "Epoch 1123. Training loss: 3.130582571029663. Validation loss: 2.707965850830078.\n",
      "Epoch 1124. Training loss: 3.130579710006714. Validation loss: 2.707965612411499.\n",
      "Epoch 1125. Training loss: 3.1305768489837646. Validation loss: 2.707965850830078.\n",
      "Epoch 1126. Training loss: 3.1305742263793945. Validation loss: 2.70796537399292.\n",
      "Epoch 1127. Training loss: 3.1305716037750244. Validation loss: 2.707965612411499.\n",
      "Epoch 1128. Training loss: 3.130568742752075. Validation loss: 2.707965612411499.\n",
      "Epoch 1129. Training loss: 3.130566358566284. Validation loss: 2.70796537399292.\n",
      "Epoch 1130. Training loss: 3.130563497543335. Validation loss: 2.70796537399292.\n",
      "Epoch 1131. Training loss: 3.130560874938965. Validation loss: 2.70796537399292.\n",
      "Epoch 1132. Training loss: 3.1305577754974365. Validation loss: 2.70796537399292.\n",
      "Epoch 1133. Training loss: 3.1305549144744873. Validation loss: 2.707965135574341.\n",
      "Epoch 1134. Training loss: 3.130552291870117. Validation loss: 2.707965135574341.\n",
      "Epoch 1135. Training loss: 3.130549430847168. Validation loss: 2.7079648971557617.\n",
      "Epoch 1136. Training loss: 3.1305465698242188. Validation loss: 2.7079648971557617.\n",
      "Epoch 1137. Training loss: 3.1305437088012695. Validation loss: 2.7079648971557617.\n",
      "Epoch 1138. Training loss: 3.1305408477783203. Validation loss: 2.7079648971557617.\n",
      "Epoch 1139. Training loss: 3.13053822517395. Validation loss: 2.7079648971557617.\n",
      "Epoch 1140. Training loss: 3.130535364151001. Validation loss: 2.7079648971557617.\n",
      "Epoch 1141. Training loss: 3.13053297996521. Validation loss: 2.7079648971557617.\n",
      "Epoch 1142. Training loss: 3.1305301189422607. Validation loss: 2.7079648971557617.\n",
      "Epoch 1143. Training loss: 3.1305274963378906. Validation loss: 2.7079646587371826.\n",
      "Epoch 1144. Training loss: 3.1305243968963623. Validation loss: 2.7079644203186035.\n",
      "Epoch 1145. Training loss: 3.130521535873413. Validation loss: 2.7079644203186035.\n",
      "Epoch 1146. Training loss: 3.130518913269043. Validation loss: 2.7079644203186035.\n",
      "Epoch 1147. Training loss: 3.1305160522460938. Validation loss: 2.7079644203186035.\n",
      "Epoch 1148. Training loss: 3.1305131912231445. Validation loss: 2.7079644203186035.\n",
      "Epoch 1149. Training loss: 3.1305105686187744. Validation loss: 2.7079644203186035.\n",
      "Epoch 1150. Training loss: 3.130507707595825. Validation loss: 2.7079641819000244.\n",
      "Epoch 1151. Training loss: 3.130504846572876. Validation loss: 2.7079641819000244.\n",
      "Epoch 1152. Training loss: 3.130502462387085. Validation loss: 2.7079641819000244.\n",
      "Epoch 1153. Training loss: 3.1304991245269775. Validation loss: 2.7079641819000244.\n",
      "Epoch 1154. Training loss: 3.1304962635040283. Validation loss: 2.7079641819000244.\n",
      "Epoch 1155. Training loss: 3.1304938793182373. Validation loss: 2.7079641819000244.\n",
      "Epoch 1156. Training loss: 3.130491018295288. Validation loss: 2.7079639434814453.\n",
      "Epoch 1157. Training loss: 3.130488157272339. Validation loss: 2.7079639434814453.\n",
      "Epoch 1158. Training loss: 3.1304852962493896. Validation loss: 2.7079639434814453.\n",
      "Epoch 1159. Training loss: 3.1304826736450195. Validation loss: 2.7079639434814453.\n",
      "Epoch 1160. Training loss: 3.130479574203491. Validation loss: 2.7079639434814453.\n",
      "Epoch 1161. Training loss: 3.130476713180542. Validation loss: 2.707963705062866.\n",
      "Epoch 1162. Training loss: 3.130474090576172. Validation loss: 2.7079639434814453.\n",
      "Epoch 1163. Training loss: 3.1304712295532227. Validation loss: 2.707963466644287.\n",
      "Epoch 1164. Training loss: 3.1304683685302734. Validation loss: 2.7079639434814453.\n",
      "Epoch 1165. Training loss: 3.1304657459259033. Validation loss: 2.707963705062866.\n",
      "Epoch 1166. Training loss: 3.130462884902954. Validation loss: 2.707963466644287.\n",
      "Epoch 1167. Training loss: 3.130459785461426. Validation loss: 2.707963466644287.\n",
      "Epoch 1168. Training loss: 3.1304571628570557. Validation loss: 2.707963466644287.\n",
      "Epoch 1169. Training loss: 3.1304543018341064. Validation loss: 2.707963466644287.\n",
      "Epoch 1170. Training loss: 3.1304519176483154. Validation loss: 2.707963466644287.\n",
      "Epoch 1171. Training loss: 3.130448579788208. Validation loss: 2.707963466644287.\n",
      "Epoch 1172. Training loss: 3.130445718765259. Validation loss: 2.707963466644287.\n",
      "Epoch 1173. Training loss: 3.1304428577423096. Validation loss: 2.707963466644287.\n",
      "Epoch 1174. Training loss: 3.1304399967193604. Validation loss: 2.707962989807129.\n",
      "Epoch 1175. Training loss: 3.1304376125335693. Validation loss: 2.707962989807129.\n",
      "Epoch 1176. Training loss: 3.130434274673462. Validation loss: 2.707963228225708.\n",
      "Epoch 1177. Training loss: 3.130431890487671. Validation loss: 2.707962989807129.\n",
      "Epoch 1178. Training loss: 3.1304290294647217. Validation loss: 2.707962989807129.\n",
      "Epoch 1179. Training loss: 3.1304261684417725. Validation loss: 2.707962989807129.\n",
      "Epoch 1180. Training loss: 3.130422830581665. Validation loss: 2.7079625129699707.\n",
      "Epoch 1181. Training loss: 3.130420446395874. Validation loss: 2.707962989807129.\n",
      "Epoch 1182. Training loss: 3.130417585372925. Validation loss: 2.707962989807129.\n",
      "Epoch 1183. Training loss: 3.1304142475128174. Validation loss: 2.707962989807129.\n",
      "Epoch 1184. Training loss: 3.1304118633270264. Validation loss: 2.707962989807129.\n",
      "Epoch 1185. Training loss: 3.130408525466919. Validation loss: 2.7079625129699707.\n",
      "Epoch 1186. Training loss: 3.130406141281128. Validation loss: 2.7079625129699707.\n",
      "Epoch 1187. Training loss: 3.1304032802581787. Validation loss: 2.7079622745513916.\n",
      "Epoch 1188. Training loss: 3.1304006576538086. Validation loss: 2.7079625129699707.\n",
      "Epoch 1189. Training loss: 3.1303975582122803. Validation loss: 2.7079622745513916.\n",
      "Epoch 1190. Training loss: 3.130394697189331. Validation loss: 2.7079620361328125.\n",
      "Epoch 1191. Training loss: 3.130391836166382. Validation loss: 2.7079625129699707.\n",
      "Epoch 1192. Training loss: 3.1303889751434326. Validation loss: 2.7079620361328125.\n",
      "Epoch 1193. Training loss: 3.1303861141204834. Validation loss: 2.7079620361328125.\n",
      "Epoch 1194. Training loss: 3.130383253097534. Validation loss: 2.7079620361328125.\n",
      "Epoch 1195. Training loss: 3.130380392074585. Validation loss: 2.7079617977142334.\n",
      "Epoch 1196. Training loss: 3.1303775310516357. Validation loss: 2.7079617977142334.\n",
      "Epoch 1197. Training loss: 3.1303741931915283. Validation loss: 2.7079617977142334.\n",
      "Epoch 1198. Training loss: 3.1303718090057373. Validation loss: 2.7079620361328125.\n",
      "Epoch 1199. Training loss: 3.13036847114563. Validation loss: 2.7079620361328125.\n",
      "Epoch 1200. Training loss: 3.130366086959839. Validation loss: 2.7079615592956543.\n",
      "Epoch 1201. Training loss: 3.1303634643554688. Validation loss: 2.7079615592956543.\n",
      "Epoch 1202. Training loss: 3.1303598880767822. Validation loss: 2.7079615592956543.\n",
      "Epoch 1203. Training loss: 3.130357027053833. Validation loss: 2.7079620361328125.\n",
      "Epoch 1204. Training loss: 3.1303539276123047. Validation loss: 2.7079620361328125.\n",
      "Epoch 1205. Training loss: 3.1303513050079346. Validation loss: 2.707961320877075.\n",
      "Epoch 1206. Training loss: 3.1303489208221436. Validation loss: 2.7079615592956543.\n",
      "Epoch 1207. Training loss: 3.130345582962036. Validation loss: 2.707961320877075.\n",
      "Epoch 1208. Training loss: 3.130343198776245. Validation loss: 2.707961320877075.\n",
      "Epoch 1209. Training loss: 3.1303398609161377. Validation loss: 2.707961082458496.\n",
      "Epoch 1210. Training loss: 3.1303369998931885. Validation loss: 2.707961082458496.\n",
      "Epoch 1211. Training loss: 3.1303341388702393. Validation loss: 2.707960844039917.\n",
      "Epoch 1212. Training loss: 3.13033127784729. Validation loss: 2.707961082458496.\n",
      "Epoch 1213. Training loss: 3.130328416824341. Validation loss: 2.707960844039917.\n",
      "Epoch 1214. Training loss: 3.1303255558013916. Validation loss: 2.707960605621338.\n",
      "Epoch 1215. Training loss: 3.1303224563598633. Validation loss: 2.707960605621338.\n",
      "Epoch 1216. Training loss: 3.130319833755493. Validation loss: 2.707960844039917.\n",
      "Epoch 1217. Training loss: 3.130316734313965. Validation loss: 2.707960605621338.\n",
      "Epoch 1218. Training loss: 3.1303141117095947. Validation loss: 2.707960605621338.\n",
      "Epoch 1219. Training loss: 3.1303110122680664. Validation loss: 2.707960367202759.\n",
      "Epoch 1220. Training loss: 3.130307912826538. Validation loss: 2.707960605621338.\n",
      "Epoch 1221. Training loss: 3.130305051803589. Validation loss: 2.707960367202759.\n",
      "Epoch 1222. Training loss: 3.1303021907806396. Validation loss: 2.707960367202759.\n",
      "Epoch 1223. Training loss: 3.1302995681762695. Validation loss: 2.707960367202759.\n",
      "Epoch 1224. Training loss: 3.1302967071533203. Validation loss: 2.7079601287841797.\n",
      "Epoch 1225. Training loss: 3.130293607711792. Validation loss: 2.7079601287841797.\n",
      "Epoch 1226. Training loss: 3.1302907466888428. Validation loss: 2.7079601287841797.\n",
      "Epoch 1227. Training loss: 3.1302878856658936. Validation loss: 2.7079601287841797.\n",
      "Epoch 1228. Training loss: 3.130284547805786. Validation loss: 2.7079601287841797.\n",
      "Epoch 1229. Training loss: 3.130282163619995. Validation loss: 2.7079601287841797.\n",
      "Epoch 1230. Training loss: 3.1302788257598877. Validation loss: 2.7079601287841797.\n",
      "Epoch 1231. Training loss: 3.1302759647369385. Validation loss: 2.7079598903656006.\n",
      "Epoch 1232. Training loss: 3.1302731037139893. Validation loss: 2.7079598903656006.\n",
      "Epoch 1233. Training loss: 3.130270004272461. Validation loss: 2.7079598903656006.\n",
      "Epoch 1234. Training loss: 3.1302671432495117. Validation loss: 2.7079598903656006.\n",
      "Epoch 1235. Training loss: 3.1302642822265625. Validation loss: 2.7079601287841797.\n",
      "Epoch 1236. Training loss: 3.1302614212036133. Validation loss: 2.7079601287841797.\n",
      "Epoch 1237. Training loss: 3.130258321762085. Validation loss: 2.7079598903656006.\n",
      "Epoch 1238. Training loss: 3.1302554607391357. Validation loss: 2.7079596519470215.\n",
      "Epoch 1239. Training loss: 3.1302521228790283. Validation loss: 2.7079596519470215.\n",
      "Epoch 1240. Training loss: 3.1302497386932373. Validation loss: 2.7079596519470215.\n",
      "Epoch 1241. Training loss: 3.130246877670288. Validation loss: 2.7079596519470215.\n",
      "Epoch 1242. Training loss: 3.130244016647339. Validation loss: 2.7079594135284424.\n",
      "Epoch 1243. Training loss: 3.1302411556243896. Validation loss: 2.7079594135284424.\n",
      "Epoch 1244. Training loss: 3.1302378177642822. Validation loss: 2.7079591751098633.\n",
      "Epoch 1245. Training loss: 3.130234718322754. Validation loss: 2.7079591751098633.\n",
      "Epoch 1246. Training loss: 3.130232095718384. Validation loss: 2.7079591751098633.\n",
      "Epoch 1247. Training loss: 3.1302287578582764. Validation loss: 2.7079591751098633.\n",
      "Epoch 1248. Training loss: 3.130225896835327. Validation loss: 2.7079591751098633.\n",
      "Epoch 1249. Training loss: 3.130223274230957. Validation loss: 2.7079591751098633.\n",
      "Epoch 1250. Training loss: 3.1302201747894287. Validation loss: 2.707958936691284.\n",
      "Epoch 1251. Training loss: 3.1302173137664795. Validation loss: 2.707958936691284.\n",
      "Epoch 1252. Training loss: 3.1302144527435303. Validation loss: 2.707958936691284.\n",
      "Epoch 1253. Training loss: 3.130211114883423. Validation loss: 2.7079591751098633.\n",
      "Epoch 1254. Training loss: 3.1302082538604736. Validation loss: 2.707958698272705.\n",
      "Epoch 1255. Training loss: 3.1302053928375244. Validation loss: 2.707958698272705.\n",
      "Epoch 1256. Training loss: 3.130202054977417. Validation loss: 2.707958698272705.\n",
      "Epoch 1257. Training loss: 3.1301991939544678. Validation loss: 2.707958459854126.\n",
      "Epoch 1258. Training loss: 3.1301963329315186. Validation loss: 2.707958698272705.\n",
      "Epoch 1259. Training loss: 3.1301934719085693. Validation loss: 2.707958698272705.\n",
      "Epoch 1260. Training loss: 3.13019061088562. Validation loss: 2.707958698272705.\n",
      "Epoch 1261. Training loss: 3.1301872730255127. Validation loss: 2.707958459854126.\n",
      "Epoch 1262. Training loss: 3.1301841735839844. Validation loss: 2.707958698272705.\n",
      "Epoch 1263. Training loss: 3.130181312561035. Validation loss: 2.707958221435547.\n",
      "Epoch 1264. Training loss: 3.130178451538086. Validation loss: 2.707958221435547.\n",
      "Epoch 1265. Training loss: 3.1301753520965576. Validation loss: 2.7079579830169678.\n",
      "Epoch 1266. Training loss: 3.1301724910736084. Validation loss: 2.707958221435547.\n",
      "Epoch 1267. Training loss: 3.130169630050659. Validation loss: 2.707958221435547.\n",
      "Epoch 1268. Training loss: 3.1301662921905518. Validation loss: 2.707958221435547.\n",
      "Epoch 1269. Training loss: 3.1301631927490234. Validation loss: 2.707958221435547.\n",
      "Epoch 1270. Training loss: 3.1301605701446533. Validation loss: 2.7079579830169678.\n",
      "Epoch 1271. Training loss: 3.130157709121704. Validation loss: 2.7079579830169678.\n",
      "Epoch 1272. Training loss: 3.130154609680176. Validation loss: 2.7079577445983887.\n",
      "Epoch 1273. Training loss: 3.1301510334014893. Validation loss: 2.7079579830169678.\n",
      "Epoch 1274. Training loss: 3.1301486492156982. Validation loss: 2.7079579830169678.\n",
      "Epoch 1275. Training loss: 3.130145311355591. Validation loss: 2.7079579830169678.\n",
      "Epoch 1276. Training loss: 3.1301424503326416. Validation loss: 2.7079577445983887.\n",
      "Epoch 1277. Training loss: 3.1301395893096924. Validation loss: 2.7079575061798096.\n",
      "Epoch 1278. Training loss: 3.130136489868164. Validation loss: 2.7079572677612305.\n",
      "Epoch 1279. Training loss: 3.1301333904266357. Validation loss: 2.7079572677612305.\n",
      "Epoch 1280. Training loss: 3.1301305294036865. Validation loss: 2.7079572677612305.\n",
      "Epoch 1281. Training loss: 3.130127191543579. Validation loss: 2.7079572677612305.\n",
      "Epoch 1282. Training loss: 3.130124092102051. Validation loss: 2.7079575061798096.\n",
      "Epoch 1283. Training loss: 3.1301214694976807. Validation loss: 2.7079572677612305.\n",
      "Epoch 1284. Training loss: 3.1301186084747314. Validation loss: 2.7079575061798096.\n",
      "Epoch 1285. Training loss: 3.130115270614624. Validation loss: 2.7079570293426514.\n",
      "Epoch 1286. Training loss: 3.130112409591675. Validation loss: 2.7079570293426514.\n",
      "Epoch 1287. Training loss: 3.1301095485687256. Validation loss: 2.7079570293426514.\n",
      "Epoch 1288. Training loss: 3.1301066875457764. Validation loss: 2.7079570293426514.\n",
      "Epoch 1289. Training loss: 3.130103349685669. Validation loss: 2.7079567909240723.\n",
      "Epoch 1290. Training loss: 3.1301004886627197. Validation loss: 2.7079570293426514.\n",
      "Epoch 1291. Training loss: 3.1300973892211914. Validation loss: 2.7079570293426514.\n",
      "Epoch 1292. Training loss: 3.130094289779663. Validation loss: 2.7079567909240723.\n",
      "Epoch 1293. Training loss: 3.130091428756714. Validation loss: 2.7079570293426514.\n",
      "Epoch 1294. Training loss: 3.1300885677337646. Validation loss: 2.707956552505493.\n",
      "Epoch 1295. Training loss: 3.130084991455078. Validation loss: 2.707956552505493.\n",
      "Epoch 1296. Training loss: 3.130082130432129. Validation loss: 2.707956314086914.\n",
      "Epoch 1297. Training loss: 3.1300792694091797. Validation loss: 2.707956314086914.\n",
      "Epoch 1298. Training loss: 3.1300761699676514. Validation loss: 2.707956314086914.\n",
      "Epoch 1299. Training loss: 3.130073308944702. Validation loss: 2.707956314086914.\n",
      "Epoch 1300. Training loss: 3.1300697326660156. Validation loss: 2.707956314086914.\n",
      "Epoch 1301. Training loss: 3.1300668716430664. Validation loss: 2.707956552505493.\n",
      "Epoch 1302. Training loss: 3.130063772201538. Validation loss: 2.707956314086914.\n",
      "Epoch 1303. Training loss: 3.1300604343414307. Validation loss: 2.707956314086914.\n",
      "Epoch 1304. Training loss: 3.1300575733184814. Validation loss: 2.707956075668335.\n",
      "Epoch 1305. Training loss: 3.1300547122955322. Validation loss: 2.707956075668335.\n",
      "Epoch 1306. Training loss: 3.130051612854004. Validation loss: 2.707956075668335.\n",
      "Epoch 1307. Training loss: 3.1300485134124756. Validation loss: 2.707955837249756.\n",
      "Epoch 1308. Training loss: 3.1300458908081055. Validation loss: 2.707955837249756.\n",
      "Epoch 1309. Training loss: 3.130042791366577. Validation loss: 2.7079555988311768.\n",
      "Epoch 1310. Training loss: 3.1300392150878906. Validation loss: 2.707955837249756.\n",
      "Epoch 1311. Training loss: 3.1300365924835205. Validation loss: 2.707955837249756.\n",
      "Epoch 1312. Training loss: 3.130033254623413. Validation loss: 2.7079555988311768.\n",
      "Epoch 1313. Training loss: 3.130030393600464. Validation loss: 2.7079555988311768.\n",
      "Epoch 1314. Training loss: 3.1300270557403564. Validation loss: 2.7079555988311768.\n",
      "Epoch 1315. Training loss: 3.130023956298828. Validation loss: 2.7079555988311768.\n",
      "Epoch 1316. Training loss: 3.130021095275879. Validation loss: 2.7079555988311768.\n",
      "Epoch 1317. Training loss: 3.1300179958343506. Validation loss: 2.7079553604125977.\n",
      "Epoch 1318. Training loss: 3.130014657974243. Validation loss: 2.7079553604125977.\n",
      "Epoch 1319. Training loss: 3.130011796951294. Validation loss: 2.7079553604125977.\n",
      "Epoch 1320. Training loss: 3.1300086975097656. Validation loss: 2.7079551219940186.\n",
      "Epoch 1321. Training loss: 3.1300055980682373. Validation loss: 2.7079553604125977.\n",
      "Epoch 1322. Training loss: 3.13000226020813. Validation loss: 2.7079546451568604.\n",
      "Epoch 1323. Training loss: 3.1299991607666016. Validation loss: 2.7079551219940186.\n",
      "Epoch 1324. Training loss: 3.1299962997436523. Validation loss: 2.7079548835754395.\n",
      "Epoch 1325. Training loss: 3.129993200302124. Validation loss: 2.7079551219940186.\n",
      "Epoch 1326. Training loss: 3.1299898624420166. Validation loss: 2.7079546451568604.\n",
      "Epoch 1327. Training loss: 3.1299870014190674. Validation loss: 2.7079548835754395.\n",
      "Epoch 1328. Training loss: 3.129983901977539. Validation loss: 2.7079544067382812.\n",
      "Epoch 1329. Training loss: 3.12998104095459. Validation loss: 2.7079544067382812.\n",
      "Epoch 1330. Training loss: 3.1299779415130615. Validation loss: 2.7079544067382812.\n",
      "Epoch 1331. Training loss: 3.129974603652954. Validation loss: 2.7079544067382812.\n",
      "Epoch 1332. Training loss: 3.129971504211426. Validation loss: 2.7079544067382812.\n",
      "Epoch 1333. Training loss: 3.1299684047698975. Validation loss: 2.7079544067382812.\n",
      "Epoch 1334. Training loss: 3.1299655437469482. Validation loss: 2.7079544067382812.\n",
      "Epoch 1335. Training loss: 3.129962205886841. Validation loss: 2.7079544067382812.\n",
      "Epoch 1336. Training loss: 3.1299591064453125. Validation loss: 2.7079544067382812.\n",
      "Epoch 1337. Training loss: 3.129956007003784. Validation loss: 2.7079544067382812.\n",
      "Epoch 1338. Training loss: 3.1299526691436768. Validation loss: 2.7079544067382812.\n",
      "Epoch 1339. Training loss: 3.1299498081207275. Validation loss: 2.707953929901123.\n",
      "Epoch 1340. Training loss: 3.129946708679199. Validation loss: 2.707953929901123.\n",
      "Epoch 1341. Training loss: 3.129943609237671. Validation loss: 2.707953929901123.\n",
      "Epoch 1342. Training loss: 3.1299407482147217. Validation loss: 2.707953929901123.\n",
      "Epoch 1343. Training loss: 3.1299374103546143. Validation loss: 2.707953929901123.\n",
      "Epoch 1344. Training loss: 3.129934310913086. Validation loss: 2.707953691482544.\n",
      "Epoch 1345. Training loss: 3.1299314498901367. Validation loss: 2.707953929901123.\n",
      "Epoch 1346. Training loss: 3.12992787361145. Validation loss: 2.707953453063965.\n",
      "Epoch 1347. Training loss: 3.129925012588501. Validation loss: 2.707953453063965.\n",
      "Epoch 1348. Training loss: 3.1299216747283936. Validation loss: 2.707953453063965.\n",
      "Epoch 1349. Training loss: 3.1299188137054443. Validation loss: 2.707953453063965.\n",
      "Epoch 1350. Training loss: 3.129915237426758. Validation loss: 2.707953453063965.\n",
      "Epoch 1351. Training loss: 3.1299123764038086. Validation loss: 2.707953453063965.\n",
      "Epoch 1352. Training loss: 3.1299092769622803. Validation loss: 2.707953453063965.\n",
      "Epoch 1353. Training loss: 3.129906415939331. Validation loss: 2.7079529762268066.\n",
      "Epoch 1354. Training loss: 3.1299028396606445. Validation loss: 2.707953453063965.\n",
      "Epoch 1355. Training loss: 3.1298999786376953. Validation loss: 2.7079529762268066.\n",
      "Epoch 1356. Training loss: 3.129896402359009. Validation loss: 2.7079529762268066.\n",
      "Epoch 1357. Training loss: 3.1298935413360596. Validation loss: 2.7079529762268066.\n",
      "Epoch 1358. Training loss: 3.129890203475952. Validation loss: 2.7079527378082275.\n",
      "Epoch 1359. Training loss: 3.1298868656158447. Validation loss: 2.7079529762268066.\n",
      "Epoch 1360. Training loss: 3.1298840045928955. Validation loss: 2.7079524993896484.\n",
      "Epoch 1361. Training loss: 3.129880905151367. Validation loss: 2.7079524993896484.\n",
      "Epoch 1362. Training loss: 3.129877805709839. Validation loss: 2.7079524993896484.\n",
      "Epoch 1363. Training loss: 3.1298744678497314. Validation loss: 2.7079527378082275.\n",
      "Epoch 1364. Training loss: 3.129871368408203. Validation loss: 2.7079527378082275.\n",
      "Epoch 1365. Training loss: 3.129868268966675. Validation loss: 2.7079524993896484.\n",
      "Epoch 1366. Training loss: 3.1298649311065674. Validation loss: 2.7079522609710693.\n",
      "Epoch 1367. Training loss: 3.129861831665039. Validation loss: 2.7079524993896484.\n",
      "Epoch 1368. Training loss: 3.12985897064209. Validation loss: 2.7079522609710693.\n",
      "Epoch 1369. Training loss: 3.1298558712005615. Validation loss: 2.7079522609710693.\n",
      "Epoch 1370. Training loss: 3.129852294921875. Validation loss: 2.7079524993896484.\n",
      "Epoch 1371. Training loss: 3.1298491954803467. Validation loss: 2.7079524993896484.\n",
      "Epoch 1372. Training loss: 3.1298463344573975. Validation loss: 2.7079520225524902.\n",
      "Epoch 1373. Training loss: 3.1298434734344482. Validation loss: 2.7079520225524902.\n",
      "Epoch 1374. Training loss: 3.1298398971557617. Validation loss: 2.7079520225524902.\n",
      "Epoch 1375. Training loss: 3.129836320877075. Validation loss: 2.7079520225524902.\n",
      "Epoch 1376. Training loss: 3.129833459854126. Validation loss: 2.707951784133911.\n",
      "Epoch 1377. Training loss: 3.1298301219940186. Validation loss: 2.707951784133911.\n",
      "Epoch 1378. Training loss: 3.1298274993896484. Validation loss: 2.707951545715332.\n",
      "Epoch 1379. Training loss: 3.129823684692383. Validation loss: 2.707951784133911.\n",
      "Epoch 1380. Training loss: 3.1298208236694336. Validation loss: 2.707951545715332.\n",
      "Epoch 1381. Training loss: 3.1298177242279053. Validation loss: 2.707951545715332.\n",
      "Epoch 1382. Training loss: 3.129814386367798. Validation loss: 2.707951545715332.\n",
      "Epoch 1383. Training loss: 3.1298110485076904. Validation loss: 2.707951545715332.\n",
      "Epoch 1384. Training loss: 3.129807710647583. Validation loss: 2.707951545715332.\n",
      "Epoch 1385. Training loss: 3.129804849624634. Validation loss: 2.707951545715332.\n",
      "Epoch 1386. Training loss: 3.1298015117645264. Validation loss: 2.707951307296753.\n",
      "Epoch 1387. Training loss: 3.129798650741577. Validation loss: 2.707951307296753.\n",
      "Epoch 1388. Training loss: 3.1297953128814697. Validation loss: 2.707951307296753.\n",
      "Epoch 1389. Training loss: 3.1297922134399414. Validation loss: 2.707951307296753.\n",
      "Epoch 1390. Training loss: 3.129788637161255. Validation loss: 2.707951068878174.\n",
      "Epoch 1391. Training loss: 3.1297857761383057. Validation loss: 2.707951068878174.\n",
      "Epoch 1392. Training loss: 3.1297824382781982. Validation loss: 2.707951307296753.\n",
      "Epoch 1393. Training loss: 3.129779100418091. Validation loss: 2.7079508304595947.\n",
      "Epoch 1394. Training loss: 3.1297760009765625. Validation loss: 2.707951068878174.\n",
      "Epoch 1395. Training loss: 3.129772901535034. Validation loss: 2.7079508304595947.\n",
      "Epoch 1396. Training loss: 3.1297693252563477. Validation loss: 2.7079508304595947.\n",
      "Epoch 1397. Training loss: 3.1297664642333984. Validation loss: 2.7079508304595947.\n",
      "Epoch 1398. Training loss: 3.12976336479187. Validation loss: 2.7079508304595947.\n",
      "Epoch 1399. Training loss: 3.1297597885131836. Validation loss: 2.7079508304595947.\n",
      "Epoch 1400. Training loss: 3.1297569274902344. Validation loss: 2.7079508304595947.\n",
      "Epoch 1401. Training loss: 3.129753351211548. Validation loss: 2.7079505920410156.\n",
      "Epoch 1402. Training loss: 3.1297500133514404. Validation loss: 2.7079505920410156.\n",
      "Epoch 1403. Training loss: 3.129746675491333. Validation loss: 2.7079503536224365.\n",
      "Epoch 1404. Training loss: 3.1297435760498047. Validation loss: 2.7079503536224365.\n",
      "Epoch 1405. Training loss: 3.1297404766082764. Validation loss: 2.7079503536224365.\n",
      "Epoch 1406. Training loss: 3.129737138748169. Validation loss: 2.7079501152038574.\n",
      "Epoch 1407. Training loss: 3.1297340393066406. Validation loss: 2.7079501152038574.\n",
      "Epoch 1408. Training loss: 3.1297309398651123. Validation loss: 2.7079501152038574.\n",
      "Epoch 1409. Training loss: 3.129727602005005. Validation loss: 2.7079501152038574.\n",
      "Epoch 1410. Training loss: 3.1297242641448975. Validation loss: 2.7079503536224365.\n",
      "Epoch 1411. Training loss: 3.1297214031219482. Validation loss: 2.7079498767852783.\n",
      "Epoch 1412. Training loss: 3.129718065261841. Validation loss: 2.7079498767852783.\n",
      "Epoch 1413. Training loss: 3.1297147274017334. Validation loss: 2.7079498767852783.\n",
      "Epoch 1414. Training loss: 3.129711151123047. Validation loss: 2.707949638366699.\n",
      "Epoch 1415. Training loss: 3.1297082901000977. Validation loss: 2.707949638366699.\n",
      "Epoch 1416. Training loss: 3.1297054290771484. Validation loss: 2.707949638366699.\n",
      "Epoch 1417. Training loss: 3.129701852798462. Validation loss: 2.707949638366699.\n",
      "Epoch 1418. Training loss: 3.1296980381011963. Validation loss: 2.707949638366699.\n",
      "Epoch 1419. Training loss: 3.129695177078247. Validation loss: 2.707949161529541.\n",
      "Epoch 1420. Training loss: 3.1296920776367188. Validation loss: 2.707949161529541.\n",
      "Epoch 1421. Training loss: 3.1296885013580322. Validation loss: 2.707949161529541.\n",
      "Epoch 1422. Training loss: 3.129685640335083. Validation loss: 2.707949161529541.\n",
      "Epoch 1423. Training loss: 3.1296818256378174. Validation loss: 2.707949161529541.\n",
      "Epoch 1424. Training loss: 3.129678726196289. Validation loss: 2.707949161529541.\n",
      "Epoch 1425. Training loss: 3.12967586517334. Validation loss: 2.707949161529541.\n",
      "Epoch 1426. Training loss: 3.1296722888946533. Validation loss: 2.707949161529541.\n",
      "Epoch 1427. Training loss: 3.129668951034546. Validation loss: 2.707948923110962.\n",
      "Epoch 1428. Training loss: 3.1296656131744385. Validation loss: 2.707948684692383.\n",
      "Epoch 1429. Training loss: 3.12966251373291. Validation loss: 2.707948684692383.\n",
      "Epoch 1430. Training loss: 3.129659414291382. Validation loss: 2.707948923110962.\n",
      "Epoch 1431. Training loss: 3.1296560764312744. Validation loss: 2.707948923110962.\n",
      "Epoch 1432. Training loss: 3.129652261734009. Validation loss: 2.707948684692383.\n",
      "Epoch 1433. Training loss: 3.1296491622924805. Validation loss: 2.7079484462738037.\n",
      "Epoch 1434. Training loss: 3.1296463012695312. Validation loss: 2.707948684692383.\n",
      "Epoch 1435. Training loss: 3.1296427249908447. Validation loss: 2.7079484462738037.\n",
      "Epoch 1436. Training loss: 3.1296398639678955. Validation loss: 2.707948684692383.\n",
      "Epoch 1437. Training loss: 3.12963604927063. Validation loss: 2.7079484462738037.\n",
      "Epoch 1438. Training loss: 3.1296327114105225. Validation loss: 2.7079477310180664.\n",
      "Epoch 1439. Training loss: 3.1296298503875732. Validation loss: 2.7079482078552246.\n",
      "Epoch 1440. Training loss: 3.1296260356903076. Validation loss: 2.7079479694366455.\n",
      "Epoch 1441. Training loss: 3.1296226978302. Validation loss: 2.7079482078552246.\n",
      "Epoch 1442. Training loss: 3.129619598388672. Validation loss: 2.7079479694366455.\n",
      "Epoch 1443. Training loss: 3.1296164989471436. Validation loss: 2.7079479694366455.\n",
      "Epoch 1444. Training loss: 3.129612922668457. Validation loss: 2.7079477310180664.\n",
      "Epoch 1445. Training loss: 3.1296098232269287. Validation loss: 2.7079477310180664.\n",
      "Epoch 1446. Training loss: 3.129606246948242. Validation loss: 2.7079477310180664.\n",
      "Epoch 1447. Training loss: 3.129603385925293. Validation loss: 2.7079477310180664.\n",
      "Epoch 1448. Training loss: 3.1295998096466064. Validation loss: 2.7079477310180664.\n",
      "Epoch 1449. Training loss: 3.1295969486236572. Validation loss: 2.7079477310180664.\n",
      "Epoch 1450. Training loss: 3.12959361076355. Validation loss: 2.7079477310180664.\n",
      "Epoch 1451. Training loss: 3.1295900344848633. Validation loss: 2.7079474925994873.\n",
      "Epoch 1452. Training loss: 3.129586935043335. Validation loss: 2.707947254180908.\n",
      "Epoch 1453. Training loss: 3.1295833587646484. Validation loss: 2.707947254180908.\n",
      "Epoch 1454. Training loss: 3.129579782485962. Validation loss: 2.707947254180908.\n",
      "Epoch 1455. Training loss: 3.1295766830444336. Validation loss: 2.707947254180908.\n",
      "Epoch 1456. Training loss: 3.1295735836029053. Validation loss: 2.707947254180908.\n",
      "Epoch 1457. Training loss: 3.1295700073242188. Validation loss: 2.707947254180908.\n",
      "Epoch 1458. Training loss: 3.1295669078826904. Validation loss: 2.7079474925994873.\n",
      "Epoch 1459. Training loss: 3.129563570022583. Validation loss: 2.707947254180908.\n",
      "Epoch 1460. Training loss: 3.1295604705810547. Validation loss: 2.707947254180908.\n",
      "Epoch 1461. Training loss: 3.129556655883789. Validation loss: 2.70794677734375.\n",
      "Epoch 1462. Training loss: 3.1295535564422607. Validation loss: 2.707947015762329.\n",
      "Epoch 1463. Training loss: 3.129549980163574. Validation loss: 2.70794677734375.\n",
      "Epoch 1464. Training loss: 3.129546880722046. Validation loss: 2.70794677734375.\n",
      "Epoch 1465. Training loss: 3.1295433044433594. Validation loss: 2.70794677734375.\n",
      "Epoch 1466. Training loss: 3.129540205001831. Validation loss: 2.70794677734375.\n",
      "Epoch 1467. Training loss: 3.1295366287231445. Validation loss: 2.70794677734375.\n",
      "Epoch 1468. Training loss: 3.129533052444458. Validation loss: 2.70794677734375.\n",
      "Epoch 1469. Training loss: 3.1295299530029297. Validation loss: 2.707946300506592.\n",
      "Epoch 1470. Training loss: 3.1295268535614014. Validation loss: 2.70794677734375.\n",
      "Epoch 1471. Training loss: 3.129523277282715. Validation loss: 2.707946538925171.\n",
      "Epoch 1472. Training loss: 3.1295197010040283. Validation loss: 2.707946300506592.\n",
      "Epoch 1473. Training loss: 3.1295166015625. Validation loss: 2.707946300506592.\n",
      "Epoch 1474. Training loss: 3.1295130252838135. Validation loss: 2.7079460620880127.\n",
      "Epoch 1475. Training loss: 3.129509687423706. Validation loss: 2.7079458236694336.\n",
      "Epoch 1476. Training loss: 3.1295063495635986. Validation loss: 2.7079455852508545.\n",
      "Epoch 1477. Training loss: 3.129503011703491. Validation loss: 2.7079455852508545.\n",
      "Epoch 1478. Training loss: 3.129500150680542. Validation loss: 2.7079458236694336.\n",
      "Epoch 1479. Training loss: 3.1294963359832764. Validation loss: 2.7079458236694336.\n",
      "Epoch 1480. Training loss: 3.129492998123169. Validation loss: 2.7079460620880127.\n",
      "Epoch 1481. Training loss: 3.1294896602630615. Validation loss: 2.7079458236694336.\n",
      "Epoch 1482. Training loss: 3.129486322402954. Validation loss: 2.7079458236694336.\n",
      "Epoch 1483. Training loss: 3.1294829845428467. Validation loss: 2.7079453468322754.\n",
      "Epoch 1484. Training loss: 3.1294796466827393. Validation loss: 2.7079451084136963.\n",
      "Epoch 1485. Training loss: 3.129476308822632. Validation loss: 2.7079453468322754.\n",
      "Epoch 1486. Training loss: 3.1294727325439453. Validation loss: 2.7079453468322754.\n",
      "Epoch 1487. Training loss: 3.129469156265259. Validation loss: 2.7079453468322754.\n",
      "Epoch 1488. Training loss: 3.1294660568237305. Validation loss: 2.7079451084136963.\n",
      "Epoch 1489. Training loss: 3.129462480545044. Validation loss: 2.7079453468322754.\n",
      "Epoch 1490. Training loss: 3.1294591426849365. Validation loss: 2.7079453468322754.\n",
      "Epoch 1491. Training loss: 3.129455804824829. Validation loss: 2.707944869995117.\n",
      "Epoch 1492. Training loss: 3.1294524669647217. Validation loss: 2.7079451084136963.\n",
      "Epoch 1493. Training loss: 3.1294491291046143. Validation loss: 2.7079453468322754.\n",
      "Epoch 1494. Training loss: 3.129445791244507. Validation loss: 2.707944869995117.\n",
      "Epoch 1495. Training loss: 3.1294422149658203. Validation loss: 2.7079453468322754.\n",
      "Epoch 1496. Training loss: 3.129438638687134. Validation loss: 2.707944869995117.\n",
      "Epoch 1497. Training loss: 3.1294353008270264. Validation loss: 2.707944393157959.\n",
      "Epoch 1498. Training loss: 3.12943172454834. Validation loss: 2.707944393157959.\n",
      "Epoch 1499. Training loss: 3.1294288635253906. Validation loss: 2.707944393157959.\n",
      "Epoch 1500. Training loss: 3.129425287246704. Validation loss: 2.707944393157959.\n",
      "Epoch 1501. Training loss: 3.1294219493865967. Validation loss: 2.707944393157959.\n",
      "Epoch 1502. Training loss: 3.12941837310791. Validation loss: 2.707943916320801.\n",
      "Epoch 1503. Training loss: 3.1294147968292236. Validation loss: 2.707943916320801.\n",
      "Epoch 1504. Training loss: 3.1294116973876953. Validation loss: 2.707944393157959.\n",
      "Epoch 1505. Training loss: 3.129408121109009. Validation loss: 2.707943916320801.\n",
      "Epoch 1506. Training loss: 3.1294047832489014. Validation loss: 2.707943916320801.\n",
      "Epoch 1507. Training loss: 3.129401206970215. Validation loss: 2.7079436779022217.\n",
      "Epoch 1508. Training loss: 3.1293981075286865. Validation loss: 2.7079436779022217.\n",
      "Epoch 1509. Training loss: 3.12939453125. Validation loss: 2.707943916320801.\n",
      "Epoch 1510. Training loss: 3.1293909549713135. Validation loss: 2.7079436779022217.\n",
      "Epoch 1511. Training loss: 3.129387617111206. Validation loss: 2.7079436779022217.\n",
      "Epoch 1512. Training loss: 3.1293840408325195. Validation loss: 2.7079436779022217.\n",
      "Epoch 1513. Training loss: 3.129380464553833. Validation loss: 2.7079434394836426.\n",
      "Epoch 1514. Training loss: 3.1293773651123047. Validation loss: 2.7079434394836426.\n",
      "Epoch 1515. Training loss: 3.129373788833618. Validation loss: 2.7079434394836426.\n",
      "Epoch 1516. Training loss: 3.1293704509735107. Validation loss: 2.7079434394836426.\n",
      "Epoch 1517. Training loss: 3.129366874694824. Validation loss: 2.7079432010650635.\n",
      "Epoch 1518. Training loss: 3.1293632984161377. Validation loss: 2.7079434394836426.\n",
      "Epoch 1519. Training loss: 3.1293604373931885. Validation loss: 2.7079432010650635.\n",
      "Epoch 1520. Training loss: 3.129356622695923. Validation loss: 2.7079432010650635.\n",
      "Epoch 1521. Training loss: 3.1293532848358154. Validation loss: 2.7079432010650635.\n",
      "Epoch 1522. Training loss: 3.129349946975708. Validation loss: 2.7079429626464844.\n",
      "Epoch 1523. Training loss: 3.1293461322784424. Validation loss: 2.7079429626464844.\n",
      "Epoch 1524. Training loss: 3.129343032836914. Validation loss: 2.7079429626464844.\n",
      "Epoch 1525. Training loss: 3.1293392181396484. Validation loss: 2.7079427242279053.\n",
      "Epoch 1526. Training loss: 3.12933611869812. Validation loss: 2.7079429626464844.\n",
      "Epoch 1527. Training loss: 3.1293325424194336. Validation loss: 2.7079429626464844.\n",
      "Epoch 1528. Training loss: 3.129328966140747. Validation loss: 2.7079429626464844.\n",
      "Epoch 1529. Training loss: 3.1293258666992188. Validation loss: 2.7079427242279053.\n",
      "Epoch 1530. Training loss: 3.129322052001953. Validation loss: 2.707942485809326.\n",
      "Epoch 1531. Training loss: 3.129318952560425. Validation loss: 2.707942247390747.\n",
      "Epoch 1532. Training loss: 3.129315137863159. Validation loss: 2.707942485809326.\n",
      "Epoch 1533. Training loss: 3.1293115615844727. Validation loss: 2.707942247390747.\n",
      "Epoch 1534. Training loss: 3.129307985305786. Validation loss: 2.707942485809326.\n",
      "Epoch 1535. Training loss: 3.1293046474456787. Validation loss: 2.707942247390747.\n",
      "Epoch 1536. Training loss: 3.129301071166992. Validation loss: 2.707942247390747.\n",
      "Epoch 1537. Training loss: 3.129298210144043. Validation loss: 2.707942008972168.\n",
      "Epoch 1538. Training loss: 3.1292943954467773. Validation loss: 2.707942008972168.\n",
      "Epoch 1539. Training loss: 3.129291296005249. Validation loss: 2.707942008972168.\n",
      "Epoch 1540. Training loss: 3.1292874813079834. Validation loss: 2.707942008972168.\n",
      "Epoch 1541. Training loss: 3.129283905029297. Validation loss: 2.707941770553589.\n",
      "Epoch 1542. Training loss: 3.1292808055877686. Validation loss: 2.707941770553589.\n",
      "Epoch 1543. Training loss: 3.129276990890503. Validation loss: 2.707942008972168.\n",
      "Epoch 1544. Training loss: 3.1292734146118164. Validation loss: 2.707941770553589.\n",
      "Epoch 1545. Training loss: 3.12926983833313. Validation loss: 2.7079415321350098.\n",
      "Epoch 1546. Training loss: 3.1292667388916016. Validation loss: 2.7079415321350098.\n",
      "Epoch 1547. Training loss: 3.129262924194336. Validation loss: 2.707941770553589.\n",
      "Epoch 1548. Training loss: 3.1292598247528076. Validation loss: 2.7079415321350098.\n",
      "Epoch 1549. Training loss: 3.129256010055542. Validation loss: 2.7079415321350098.\n",
      "Epoch 1550. Training loss: 3.1292524337768555. Validation loss: 2.7079415321350098.\n",
      "Epoch 1551. Training loss: 3.129248857498169. Validation loss: 2.7079408168792725.\n",
      "Epoch 1552. Training loss: 3.1292450428009033. Validation loss: 2.7079412937164307.\n",
      "Epoch 1553. Training loss: 3.129242181777954. Validation loss: 2.7079412937164307.\n",
      "Epoch 1554. Training loss: 3.1292381286621094. Validation loss: 2.7079410552978516.\n",
      "Epoch 1555. Training loss: 3.129235029220581. Validation loss: 2.7079408168792725.\n",
      "Epoch 1556. Training loss: 3.1292314529418945. Validation loss: 2.7079408168792725.\n",
      "Epoch 1557. Training loss: 3.129227638244629. Validation loss: 2.7079408168792725.\n",
      "Epoch 1558. Training loss: 3.1292245388031006. Validation loss: 2.7079408168792725.\n",
      "Epoch 1559. Training loss: 3.129220962524414. Validation loss: 2.7079408168792725.\n",
      "Epoch 1560. Training loss: 3.1292173862457275. Validation loss: 2.7079408168792725.\n",
      "Epoch 1561. Training loss: 3.12921404838562. Validation loss: 2.7079403400421143.\n",
      "Epoch 1562. Training loss: 3.1292102336883545. Validation loss: 2.7079403400421143.\n",
      "Epoch 1563. Training loss: 3.129206895828247. Validation loss: 2.7079403400421143.\n",
      "Epoch 1564. Training loss: 3.1292030811309814. Validation loss: 2.7079403400421143.\n",
      "Epoch 1565. Training loss: 3.129199743270874. Validation loss: 2.7079403400421143.\n",
      "Epoch 1566. Training loss: 3.1291961669921875. Validation loss: 2.707940101623535.\n",
      "Epoch 1567. Training loss: 3.129192352294922. Validation loss: 2.707939863204956.\n",
      "Epoch 1568. Training loss: 3.1291887760162354. Validation loss: 2.707939863204956.\n",
      "Epoch 1569. Training loss: 3.129185438156128. Validation loss: 2.707939863204956.\n",
      "Epoch 1570. Training loss: 3.1291821002960205. Validation loss: 2.707939863204956.\n",
      "Epoch 1571. Training loss: 3.129178285598755. Validation loss: 2.707939863204956.\n",
      "Epoch 1572. Training loss: 3.1291749477386475. Validation loss: 2.707939863204956.\n",
      "Epoch 1573. Training loss: 3.129171133041382. Validation loss: 2.707939624786377.\n",
      "Epoch 1574. Training loss: 3.1291677951812744. Validation loss: 2.707939624786377.\n",
      "Epoch 1575. Training loss: 3.129164457321167. Validation loss: 2.707939624786377.\n",
      "Epoch 1576. Training loss: 3.1291608810424805. Validation loss: 2.707939624786377.\n",
      "Epoch 1577. Training loss: 3.129157066345215. Validation loss: 2.707939386367798.\n",
      "Epoch 1578. Training loss: 3.1291534900665283. Validation loss: 2.707939386367798.\n",
      "Epoch 1579. Training loss: 3.1291496753692627. Validation loss: 2.707939386367798.\n",
      "Epoch 1580. Training loss: 3.1291465759277344. Validation loss: 2.707939386367798.\n",
      "Epoch 1581. Training loss: 3.1291427612304688. Validation loss: 2.7079391479492188.\n",
      "Epoch 1582. Training loss: 3.1291391849517822. Validation loss: 2.7079391479492188.\n",
      "Epoch 1583. Training loss: 3.129135847091675. Validation loss: 2.7079391479492188.\n",
      "Epoch 1584. Training loss: 3.129132032394409. Validation loss: 2.7079391479492188.\n",
      "Epoch 1585. Training loss: 3.1291284561157227. Validation loss: 2.7079389095306396.\n",
      "Epoch 1586. Training loss: 3.129124879837036. Validation loss: 2.7079389095306396.\n",
      "Epoch 1587. Training loss: 3.1291210651397705. Validation loss: 2.7079389095306396.\n",
      "Epoch 1588. Training loss: 3.1291182041168213. Validation loss: 2.7079389095306396.\n",
      "Epoch 1589. Training loss: 3.1291143894195557. Validation loss: 2.7079389095306396.\n",
      "Epoch 1590. Training loss: 3.1291110515594482. Validation loss: 2.7079381942749023.\n",
      "Epoch 1591. Training loss: 3.1291072368621826. Validation loss: 2.7079389095306396.\n",
      "Epoch 1592. Training loss: 3.129103660583496. Validation loss: 2.7079381942749023.\n",
      "Epoch 1593. Training loss: 3.1291000843048096. Validation loss: 2.7079386711120605.\n",
      "Epoch 1594. Training loss: 3.129096269607544. Validation loss: 2.7079386711120605.\n",
      "Epoch 1595. Training loss: 3.1290924549102783. Validation loss: 2.7079384326934814.\n",
      "Epoch 1596. Training loss: 3.12908935546875. Validation loss: 2.7079384326934814.\n",
      "Epoch 1597. Training loss: 3.1290855407714844. Validation loss: 2.7079381942749023.\n",
      "Epoch 1598. Training loss: 3.129081964492798. Validation loss: 2.7079381942749023.\n",
      "Epoch 1599. Training loss: 3.1290786266326904. Validation loss: 2.707937717437744.\n",
      "Epoch 1600. Training loss: 3.129075050354004. Validation loss: 2.7079381942749023.\n",
      "Epoch 1601. Training loss: 3.1290712356567383. Validation loss: 2.707937717437744.\n",
      "Epoch 1602. Training loss: 3.1290676593780518. Validation loss: 2.707937717437744.\n",
      "Epoch 1603. Training loss: 3.1290643215179443. Validation loss: 2.707937717437744.\n",
      "Epoch 1604. Training loss: 3.129060745239258. Validation loss: 2.707937717437744.\n",
      "Epoch 1605. Training loss: 3.129056692123413. Validation loss: 2.707937717437744.\n",
      "Epoch 1606. Training loss: 3.1290533542633057. Validation loss: 2.707937717437744.\n",
      "Epoch 1607. Training loss: 3.12904953956604. Validation loss: 2.707937479019165.\n",
      "Epoch 1608. Training loss: 3.1290462017059326. Validation loss: 2.707937240600586.\n",
      "Epoch 1609. Training loss: 3.129042387008667. Validation loss: 2.707937240600586.\n",
      "Epoch 1610. Training loss: 3.1290388107299805. Validation loss: 2.707937240600586.\n",
      "Epoch 1611. Training loss: 3.129034996032715. Validation loss: 2.707937479019165.\n",
      "Epoch 1612. Training loss: 3.1290314197540283. Validation loss: 2.707937240600586.\n",
      "Epoch 1613. Training loss: 3.129028081893921. Validation loss: 2.707937240600586.\n",
      "Epoch 1614. Training loss: 3.1290245056152344. Validation loss: 2.7079367637634277.\n",
      "Epoch 1615. Training loss: 3.1290204524993896. Validation loss: 2.7079367637634277.\n",
      "Epoch 1616. Training loss: 3.1290171146392822. Validation loss: 2.7079367637634277.\n",
      "Epoch 1617. Training loss: 3.1290132999420166. Validation loss: 2.7079367637634277.\n",
      "Epoch 1618. Training loss: 3.129009485244751. Validation loss: 2.7079367637634277.\n",
      "Epoch 1619. Training loss: 3.1290056705474854. Validation loss: 2.7079367637634277.\n",
      "Epoch 1620. Training loss: 3.129002571105957. Validation loss: 2.7079367637634277.\n",
      "Epoch 1621. Training loss: 3.1289987564086914. Validation loss: 2.7079367637634277.\n",
      "Epoch 1622. Training loss: 3.128995180130005. Validation loss: 2.7079367637634277.\n",
      "Epoch 1623. Training loss: 3.1289918422698975. Validation loss: 2.7079362869262695.\n",
      "Epoch 1624. Training loss: 3.128988027572632. Validation loss: 2.7079362869262695.\n",
      "Epoch 1625. Training loss: 3.1289844512939453. Validation loss: 2.7079362869262695.\n",
      "Epoch 1626. Training loss: 3.1289803981781006. Validation loss: 2.7079362869262695.\n",
      "Epoch 1627. Training loss: 3.128976821899414. Validation loss: 2.7079362869262695.\n",
      "Epoch 1628. Training loss: 3.1289730072021484. Validation loss: 2.7079362869262695.\n",
      "Epoch 1629. Training loss: 3.128969430923462. Validation loss: 2.7079358100891113.\n",
      "Epoch 1630. Training loss: 3.1289656162261963. Validation loss: 2.7079362869262695.\n",
      "Epoch 1631. Training loss: 3.128962278366089. Validation loss: 2.7079358100891113.\n",
      "Epoch 1632. Training loss: 3.1289587020874023. Validation loss: 2.7079355716705322.\n",
      "Epoch 1633. Training loss: 3.1289548873901367. Validation loss: 2.7079358100891113.\n",
      "Epoch 1634. Training loss: 3.12895131111145. Validation loss: 2.707935333251953.\n",
      "Epoch 1635. Training loss: 3.1289474964141846. Validation loss: 2.707935333251953.\n",
      "Epoch 1636. Training loss: 3.128944158554077. Validation loss: 2.707935333251953.\n",
      "Epoch 1637. Training loss: 3.1289405822753906. Validation loss: 2.707935333251953.\n",
      "Epoch 1638. Training loss: 3.128936767578125. Validation loss: 2.707935333251953.\n",
      "Epoch 1639. Training loss: 3.1289329528808594. Validation loss: 2.707935094833374.\n",
      "Epoch 1640. Training loss: 3.1289291381835938. Validation loss: 2.707935094833374.\n",
      "Epoch 1641. Training loss: 3.128925323486328. Validation loss: 2.707935094833374.\n",
      "Epoch 1642. Training loss: 3.1289215087890625. Validation loss: 2.707934856414795.\n",
      "Epoch 1643. Training loss: 3.128917932510376. Validation loss: 2.707935094833374.\n",
      "Epoch 1644. Training loss: 3.1289148330688477. Validation loss: 2.707934856414795.\n",
      "Epoch 1645. Training loss: 3.128910779953003. Validation loss: 2.707934856414795.\n",
      "Epoch 1646. Training loss: 3.1289072036743164. Validation loss: 2.707934617996216.\n",
      "Epoch 1647. Training loss: 3.12890362739563. Validation loss: 2.707934856414795.\n",
      "Epoch 1648. Training loss: 3.1288998126983643. Validation loss: 2.707934617996216.\n",
      "Epoch 1649. Training loss: 3.1288959980010986. Validation loss: 2.707934617996216.\n",
      "Epoch 1650. Training loss: 3.128892183303833. Validation loss: 2.707934617996216.\n",
      "Epoch 1651. Training loss: 3.1288888454437256. Validation loss: 2.707934617996216.\n",
      "Epoch 1652. Training loss: 3.12888503074646. Validation loss: 2.7079343795776367.\n",
      "Epoch 1653. Training loss: 3.1288814544677734. Validation loss: 2.707934617996216.\n",
      "Epoch 1654. Training loss: 3.128877639770508. Validation loss: 2.7079343795776367.\n",
      "Epoch 1655. Training loss: 3.128873825073242. Validation loss: 2.7079341411590576.\n",
      "Epoch 1656. Training loss: 3.1288700103759766. Validation loss: 2.7079341411590576.\n",
      "Epoch 1657. Training loss: 3.128866195678711. Validation loss: 2.7079341411590576.\n",
      "Epoch 1658. Training loss: 3.1288626194000244. Validation loss: 2.7079341411590576.\n",
      "Epoch 1659. Training loss: 3.128858804702759. Validation loss: 2.7079341411590576.\n",
      "Epoch 1660. Training loss: 3.128854990005493. Validation loss: 2.7079339027404785.\n",
      "Epoch 1661. Training loss: 3.1288516521453857. Validation loss: 2.7079339027404785.\n",
      "Epoch 1662. Training loss: 3.128847360610962. Validation loss: 2.7079339027404785.\n",
      "Epoch 1663. Training loss: 3.1288440227508545. Validation loss: 2.7079334259033203.\n",
      "Epoch 1664. Training loss: 3.128840208053589. Validation loss: 2.7079336643218994.\n",
      "Epoch 1665. Training loss: 3.1288366317749023. Validation loss: 2.7079334259033203.\n",
      "Epoch 1666. Training loss: 3.1288328170776367. Validation loss: 2.7079336643218994.\n",
      "Epoch 1667. Training loss: 3.128829002380371. Validation loss: 2.707933187484741.\n",
      "Epoch 1668. Training loss: 3.1288251876831055. Validation loss: 2.7079334259033203.\n",
      "Epoch 1669. Training loss: 3.128821611404419. Validation loss: 2.7079334259033203.\n",
      "Epoch 1670. Training loss: 3.1288177967071533. Validation loss: 2.707932949066162.\n",
      "Epoch 1671. Training loss: 3.1288139820098877. Validation loss: 2.7079334259033203.\n",
      "Epoch 1672. Training loss: 3.1288106441497803. Validation loss: 2.707933187484741.\n",
      "Epoch 1673. Training loss: 3.1288068294525146. Validation loss: 2.707932949066162.\n",
      "Epoch 1674. Training loss: 3.128803014755249. Validation loss: 2.707932949066162.\n",
      "Epoch 1675. Training loss: 3.128798723220825. Validation loss: 2.707932949066162.\n",
      "Epoch 1676. Training loss: 3.1287949085235596. Validation loss: 2.707932710647583.\n",
      "Epoch 1677. Training loss: 3.1287918090820312. Validation loss: 2.707932949066162.\n",
      "Epoch 1678. Training loss: 3.1287877559661865. Validation loss: 2.707932949066162.\n",
      "Epoch 1679. Training loss: 3.128783941268921. Validation loss: 2.707932472229004.\n",
      "Epoch 1680. Training loss: 3.1287803649902344. Validation loss: 2.707932233810425.\n",
      "Epoch 1681. Training loss: 3.1287765502929688. Validation loss: 2.707932472229004.\n",
      "Epoch 1682. Training loss: 3.128772735595703. Validation loss: 2.707932233810425.\n",
      "Epoch 1683. Training loss: 3.1287691593170166. Validation loss: 2.707932233810425.\n",
      "Epoch 1684. Training loss: 3.128765344619751. Validation loss: 2.707932472229004.\n",
      "Epoch 1685. Training loss: 3.1287615299224854. Validation loss: 2.7079319953918457.\n",
      "Epoch 1686. Training loss: 3.128758192062378. Validation loss: 2.7079319953918457.\n",
      "Epoch 1687. Training loss: 3.128753900527954. Validation loss: 2.7079319953918457.\n",
      "Epoch 1688. Training loss: 3.1287500858306885. Validation loss: 2.7079317569732666.\n",
      "Epoch 1689. Training loss: 3.128746747970581. Validation loss: 2.7079319953918457.\n",
      "Epoch 1690. Training loss: 3.1287429332733154. Validation loss: 2.7079319953918457.\n",
      "Epoch 1691. Training loss: 3.12873911857605. Validation loss: 2.7079317569732666.\n",
      "Epoch 1692. Training loss: 3.128734827041626. Validation loss: 2.7079315185546875.\n",
      "Epoch 1693. Training loss: 3.1287314891815186. Validation loss: 2.7079315185546875.\n",
      "Epoch 1694. Training loss: 3.128727674484253. Validation loss: 2.7079315185546875.\n",
      "Epoch 1695. Training loss: 3.1287238597869873. Validation loss: 2.7079315185546875.\n",
      "Epoch 1696. Training loss: 3.1287200450897217. Validation loss: 2.7079312801361084.\n",
      "Epoch 1697. Training loss: 3.128715753555298. Validation loss: 2.7079315185546875.\n",
      "Epoch 1698. Training loss: 3.1287124156951904. Validation loss: 2.7079312801361084.\n",
      "Epoch 1699. Training loss: 3.1287081241607666. Validation loss: 2.7079315185546875.\n",
      "Epoch 1700. Training loss: 3.1287050247192383. Validation loss: 2.7079312801361084.\n",
      "Epoch 1701. Training loss: 3.1287009716033936. Validation loss: 2.7079310417175293.\n",
      "Epoch 1702. Training loss: 3.128697156906128. Validation loss: 2.7079310417175293.\n",
      "Epoch 1703. Training loss: 3.1286933422088623. Validation loss: 2.7079310417175293.\n",
      "Epoch 1704. Training loss: 3.1286890506744385. Validation loss: 2.707930564880371.\n",
      "Epoch 1705. Training loss: 3.128685712814331. Validation loss: 2.707930564880371.\n",
      "Epoch 1706. Training loss: 3.1286818981170654. Validation loss: 2.70793080329895.\n",
      "Epoch 1707. Training loss: 3.1286780834198. Validation loss: 2.707930564880371.\n",
      "Epoch 1708. Training loss: 3.128674268722534. Validation loss: 2.707930564880371.\n",
      "Epoch 1709. Training loss: 3.1286704540252686. Validation loss: 2.707930564880371.\n",
      "Epoch 1710. Training loss: 3.128666639328003. Validation loss: 2.707930564880371.\n",
      "Epoch 1711. Training loss: 3.1286630630493164. Validation loss: 2.707930564880371.\n",
      "Epoch 1712. Training loss: 3.128659248352051. Validation loss: 2.707930564880371.\n",
      "Epoch 1713. Training loss: 3.128655433654785. Validation loss: 2.707930088043213.\n",
      "Epoch 1714. Training loss: 3.1286513805389404. Validation loss: 2.707930088043213.\n",
      "Epoch 1715. Training loss: 3.1286470890045166. Validation loss: 2.707930088043213.\n",
      "Epoch 1716. Training loss: 3.128643274307251. Validation loss: 2.707930088043213.\n",
      "Epoch 1717. Training loss: 3.1286399364471436. Validation loss: 2.707930088043213.\n",
      "Epoch 1718. Training loss: 3.128636121749878. Validation loss: 2.707930088043213.\n",
      "Epoch 1719. Training loss: 3.128631830215454. Validation loss: 2.707930088043213.\n",
      "Epoch 1720. Training loss: 3.1286284923553467. Validation loss: 2.7079296112060547.\n",
      "Epoch 1721. Training loss: 3.128624677658081. Validation loss: 2.7079296112060547.\n",
      "Epoch 1722. Training loss: 3.1286208629608154. Validation loss: 2.7079296112060547.\n",
      "Epoch 1723. Training loss: 3.1286165714263916. Validation loss: 2.7079296112060547.\n",
      "Epoch 1724. Training loss: 3.128613233566284. Validation loss: 2.7079296112060547.\n",
      "Epoch 1725. Training loss: 3.1286087036132812. Validation loss: 2.7079291343688965.\n",
      "Epoch 1726. Training loss: 3.1286051273345947. Validation loss: 2.7079291343688965.\n",
      "Epoch 1727. Training loss: 3.12860107421875. Validation loss: 2.7079291343688965.\n",
      "Epoch 1728. Training loss: 3.1285974979400635. Validation loss: 2.7079291343688965.\n",
      "Epoch 1729. Training loss: 3.128593683242798. Validation loss: 2.7079291343688965.\n",
      "Epoch 1730. Training loss: 3.128589630126953. Validation loss: 2.7079291343688965.\n",
      "Epoch 1731. Training loss: 3.1285858154296875. Validation loss: 2.7079291343688965.\n",
      "Epoch 1732. Training loss: 3.128582000732422. Validation loss: 2.7079291343688965.\n",
      "Epoch 1733. Training loss: 3.1285781860351562. Validation loss: 2.7079286575317383.\n",
      "Epoch 1734. Training loss: 3.1285743713378906. Validation loss: 2.7079286575317383.\n",
      "Epoch 1735. Training loss: 3.128570556640625. Validation loss: 2.7079286575317383.\n",
      "Epoch 1736. Training loss: 3.1285665035247803. Validation loss: 2.7079286575317383.\n",
      "Epoch 1737. Training loss: 3.1285629272460938. Validation loss: 2.7079286575317383.\n",
      "Epoch 1738. Training loss: 3.128558874130249. Validation loss: 2.70792818069458.\n",
      "Epoch 1739. Training loss: 3.1285550594329834. Validation loss: 2.70792818069458.\n",
      "Epoch 1740. Training loss: 3.1285507678985596. Validation loss: 2.707927942276001.\n",
      "Epoch 1741. Training loss: 3.128547430038452. Validation loss: 2.70792818069458.\n",
      "Epoch 1742. Training loss: 3.128542900085449. Validation loss: 2.70792818069458.\n",
      "Epoch 1743. Training loss: 3.1285390853881836. Validation loss: 2.707927703857422.\n",
      "Epoch 1744. Training loss: 3.128535509109497. Validation loss: 2.707927703857422.\n",
      "Epoch 1745. Training loss: 3.1285314559936523. Validation loss: 2.707927703857422.\n",
      "Epoch 1746. Training loss: 3.1285274028778076. Validation loss: 2.707927703857422.\n",
      "Epoch 1747. Training loss: 3.128523826599121. Validation loss: 2.7079274654388428.\n",
      "Epoch 1748. Training loss: 3.1285197734832764. Validation loss: 2.707927703857422.\n",
      "Epoch 1749. Training loss: 3.1285159587860107. Validation loss: 2.707927703857422.\n",
      "Epoch 1750. Training loss: 3.128512144088745. Validation loss: 2.7079274654388428.\n",
      "Epoch 1751. Training loss: 3.1285083293914795. Validation loss: 2.7079274654388428.\n",
      "Epoch 1752. Training loss: 3.128504514694214. Validation loss: 2.7079274654388428.\n",
      "Epoch 1753. Training loss: 3.12850022315979. Validation loss: 2.7079272270202637.\n",
      "Epoch 1754. Training loss: 3.1284964084625244. Validation loss: 2.7079269886016846.\n",
      "Epoch 1755. Training loss: 3.1284923553466797. Validation loss: 2.7079269886016846.\n",
      "Epoch 1756. Training loss: 3.128488540649414. Validation loss: 2.7079267501831055.\n",
      "Epoch 1757. Training loss: 3.1284847259521484. Validation loss: 2.7079267501831055.\n",
      "Epoch 1758. Training loss: 3.1284806728363037. Validation loss: 2.7079267501831055.\n",
      "Epoch 1759. Training loss: 3.128476858139038. Validation loss: 2.7079267501831055.\n",
      "Epoch 1760. Training loss: 3.1284730434417725. Validation loss: 2.7079267501831055.\n",
      "Epoch 1761. Training loss: 3.128469228744507. Validation loss: 2.7079267501831055.\n",
      "Epoch 1762. Training loss: 3.128464937210083. Validation loss: 2.7079265117645264.\n",
      "Epoch 1763. Training loss: 3.1284608840942383. Validation loss: 2.7079267501831055.\n",
      "Epoch 1764. Training loss: 3.1284573078155518. Validation loss: 2.7079267501831055.\n",
      "Epoch 1765. Training loss: 3.128453254699707. Validation loss: 2.7079262733459473.\n",
      "Epoch 1766. Training loss: 3.1284492015838623. Validation loss: 2.7079262733459473.\n",
      "Epoch 1767. Training loss: 3.1284453868865967. Validation loss: 2.7079262733459473.\n",
      "Epoch 1768. Training loss: 3.128441572189331. Validation loss: 2.7079262733459473.\n",
      "Epoch 1769. Training loss: 3.1284372806549072. Validation loss: 2.707926034927368.\n",
      "Epoch 1770. Training loss: 3.1284334659576416. Validation loss: 2.7079262733459473.\n",
      "Epoch 1771. Training loss: 3.128429651260376. Validation loss: 2.707925796508789.\n",
      "Epoch 1772. Training loss: 3.1284255981445312. Validation loss: 2.70792555809021.\n",
      "Epoch 1773. Training loss: 3.1284217834472656. Validation loss: 2.70792555809021.\n",
      "Epoch 1774. Training loss: 3.128417730331421. Validation loss: 2.707925796508789.\n",
      "Epoch 1775. Training loss: 3.128413438796997. Validation loss: 2.70792555809021.\n",
      "Epoch 1776. Training loss: 3.1284096240997314. Validation loss: 2.70792555809021.\n",
      "Epoch 1777. Training loss: 3.128406286239624. Validation loss: 2.70792555809021.\n",
      "Epoch 1778. Training loss: 3.128401756286621. Validation loss: 2.70792555809021.\n",
      "Epoch 1779. Training loss: 3.1283979415893555. Validation loss: 2.7079250812530518.\n",
      "Epoch 1780. Training loss: 3.12839412689209. Validation loss: 2.7079250812530518.\n",
      "Epoch 1781. Training loss: 3.128390073776245. Validation loss: 2.7079250812530518.\n",
      "Epoch 1782. Training loss: 3.1283862590789795. Validation loss: 2.7079250812530518.\n",
      "Epoch 1783. Training loss: 3.1283819675445557. Validation loss: 2.7079250812530518.\n",
      "Epoch 1784. Training loss: 3.128377914428711. Validation loss: 2.7079248428344727.\n",
      "Epoch 1785. Training loss: 3.1283740997314453. Validation loss: 2.7079250812530518.\n",
      "Epoch 1786. Training loss: 3.1283702850341797. Validation loss: 2.7079248428344727.\n",
      "Epoch 1787. Training loss: 3.128366231918335. Validation loss: 2.7079246044158936.\n",
      "Epoch 1788. Training loss: 3.128361940383911. Validation loss: 2.7079246044158936.\n",
      "Epoch 1789. Training loss: 3.1283578872680664. Validation loss: 2.7079246044158936.\n",
      "Epoch 1790. Training loss: 3.128354072570801. Validation loss: 2.7079246044158936.\n",
      "Epoch 1791. Training loss: 3.128350257873535. Validation loss: 2.7079243659973145.\n",
      "Epoch 1792. Training loss: 3.1283462047576904. Validation loss: 2.7079246044158936.\n",
      "Epoch 1793. Training loss: 3.1283419132232666. Validation loss: 2.7079241275787354.\n",
      "Epoch 1794. Training loss: 3.128338098526001. Validation loss: 2.7079241275787354.\n",
      "Epoch 1795. Training loss: 3.1283340454101562. Validation loss: 2.7079241275787354.\n",
      "Epoch 1796. Training loss: 3.1283302307128906. Validation loss: 2.7079241275787354.\n",
      "Epoch 1797. Training loss: 3.1283257007598877. Validation loss: 2.7079241275787354.\n",
      "Epoch 1798. Training loss: 3.128321886062622. Validation loss: 2.7079238891601562.\n",
      "Epoch 1799. Training loss: 3.1283178329467773. Validation loss: 2.7079238891601562.\n",
      "Epoch 1800. Training loss: 3.1283140182495117. Validation loss: 2.707923650741577.\n",
      "Epoch 1801. Training loss: 3.128310203552246. Validation loss: 2.707923650741577.\n",
      "Epoch 1802. Training loss: 3.1283063888549805. Validation loss: 2.707923650741577.\n",
      "Epoch 1803. Training loss: 3.1283018589019775. Validation loss: 2.707923650741577.\n",
      "Epoch 1804. Training loss: 3.128298044204712. Validation loss: 2.707923412322998.\n",
      "Epoch 1805. Training loss: 3.128293991088867. Validation loss: 2.707923412322998.\n",
      "Epoch 1806. Training loss: 3.1282901763916016. Validation loss: 2.707923412322998.\n",
      "Epoch 1807. Training loss: 3.128286361694336. Validation loss: 2.707923173904419.\n",
      "Epoch 1808. Training loss: 3.128281831741333. Validation loss: 2.707923412322998.\n",
      "Epoch 1809. Training loss: 3.1282780170440674. Validation loss: 2.707923173904419.\n",
      "Epoch 1810. Training loss: 3.1282739639282227. Validation loss: 2.707923173904419.\n",
      "Epoch 1811. Training loss: 3.128269910812378. Validation loss: 2.707923173904419.\n",
      "Epoch 1812. Training loss: 3.1282660961151123. Validation loss: 2.70792293548584.\n",
      "Epoch 1813. Training loss: 3.1282622814178467. Validation loss: 2.70792293548584.\n",
      "Epoch 1814. Training loss: 3.1282577514648438. Validation loss: 2.70792293548584.\n",
      "Epoch 1815. Training loss: 3.128253698348999. Validation loss: 2.7079226970672607.\n",
      "Epoch 1816. Training loss: 3.1282501220703125. Validation loss: 2.7079226970672607.\n",
      "Epoch 1817. Training loss: 3.1282455921173096. Validation loss: 2.7079226970672607.\n",
      "Epoch 1818. Training loss: 3.128241777420044. Validation loss: 2.7079224586486816.\n",
      "Epoch 1819. Training loss: 3.12823748588562. Validation loss: 2.7079224586486816.\n",
      "Epoch 1820. Training loss: 3.1282336711883545. Validation loss: 2.7079226970672607.\n",
      "Epoch 1821. Training loss: 3.1282293796539307. Validation loss: 2.7079222202301025.\n",
      "Epoch 1822. Training loss: 3.128225326538086. Validation loss: 2.7079222202301025.\n",
      "Epoch 1823. Training loss: 3.1282215118408203. Validation loss: 2.7079222202301025.\n",
      "Epoch 1824. Training loss: 3.1282174587249756. Validation loss: 2.7079219818115234.\n",
      "Epoch 1825. Training loss: 3.1282129287719727. Validation loss: 2.7079219818115234.\n",
      "Epoch 1826. Training loss: 3.128209352493286. Validation loss: 2.7079219818115234.\n",
      "Epoch 1827. Training loss: 3.1282052993774414. Validation loss: 2.7079217433929443.\n",
      "Epoch 1828. Training loss: 3.1282007694244385. Validation loss: 2.7079215049743652.\n",
      "Epoch 1829. Training loss: 3.128196954727173. Validation loss: 2.7079217433929443.\n",
      "Epoch 1830. Training loss: 3.1281931400299072. Validation loss: 2.7079217433929443.\n",
      "Epoch 1831. Training loss: 3.1281888484954834. Validation loss: 2.707921266555786.\n",
      "Epoch 1832. Training loss: 3.1281843185424805. Validation loss: 2.707921266555786.\n",
      "Epoch 1833. Training loss: 3.128180503845215. Validation loss: 2.707921266555786.\n",
      "Epoch 1834. Training loss: 3.12817645072937. Validation loss: 2.707921266555786.\n",
      "Epoch 1835. Training loss: 3.1281726360321045. Validation loss: 2.707921028137207.\n",
      "Epoch 1836. Training loss: 3.1281681060791016. Validation loss: 2.707921028137207.\n",
      "Epoch 1837. Training loss: 3.128164291381836. Validation loss: 2.707921028137207.\n",
      "Epoch 1838. Training loss: 3.128160238265991. Validation loss: 2.707920551300049.\n",
      "Epoch 1839. Training loss: 3.1281564235687256. Validation loss: 2.707920551300049.\n",
      "Epoch 1840. Training loss: 3.1281518936157227. Validation loss: 2.707920551300049.\n",
      "Epoch 1841. Training loss: 3.128147840499878. Validation loss: 2.707920551300049.\n",
      "Epoch 1842. Training loss: 3.1281440258026123. Validation loss: 2.7079203128814697.\n",
      "Epoch 1843. Training loss: 3.1281397342681885. Validation loss: 2.707920551300049.\n",
      "Epoch 1844. Training loss: 3.1281356811523438. Validation loss: 2.707920551300049.\n",
      "Epoch 1845. Training loss: 3.128131151199341. Validation loss: 2.7079200744628906.\n",
      "Epoch 1846. Training loss: 3.128127336502075. Validation loss: 2.7079200744628906.\n",
      "Epoch 1847. Training loss: 3.1281232833862305. Validation loss: 2.7079200744628906.\n",
      "Epoch 1848. Training loss: 3.1281192302703857. Validation loss: 2.7079203128814697.\n",
      "Epoch 1849. Training loss: 3.128114938735962. Validation loss: 2.7079203128814697.\n",
      "Epoch 1850. Training loss: 3.128110885620117. Validation loss: 2.7079200744628906.\n",
      "Epoch 1851. Training loss: 3.1281068325042725. Validation loss: 2.7079195976257324.\n",
      "Epoch 1852. Training loss: 3.1281025409698486. Validation loss: 2.7079195976257324.\n",
      "Epoch 1853. Training loss: 3.128098726272583. Validation loss: 2.7079195976257324.\n",
      "Epoch 1854. Training loss: 3.1280946731567383. Validation loss: 2.7079195976257324.\n",
      "Epoch 1855. Training loss: 3.1280901432037354. Validation loss: 2.7079195976257324.\n",
      "Epoch 1856. Training loss: 3.1280860900878906. Validation loss: 2.7079195976257324.\n",
      "Epoch 1857. Training loss: 3.128082275390625. Validation loss: 2.7079193592071533.\n",
      "Epoch 1858. Training loss: 3.128077745437622. Validation loss: 2.7079195976257324.\n",
      "Epoch 1859. Training loss: 3.1280736923217773. Validation loss: 2.707919120788574.\n",
      "Epoch 1860. Training loss: 3.1280696392059326. Validation loss: 2.7079195976257324.\n",
      "Epoch 1861. Training loss: 3.128065347671509. Validation loss: 2.7079193592071533.\n",
      "Epoch 1862. Training loss: 3.128061532974243. Validation loss: 2.707919120788574.\n",
      "Epoch 1863. Training loss: 3.128056764602661. Validation loss: 2.707918882369995.\n",
      "Epoch 1864. Training loss: 3.1280534267425537. Validation loss: 2.707918643951416.\n",
      "Epoch 1865. Training loss: 3.128048896789551. Validation loss: 2.707918643951416.\n",
      "Epoch 1866. Training loss: 3.1280453205108643. Validation loss: 2.707918882369995.\n",
      "Epoch 1867. Training loss: 3.1280405521392822. Validation loss: 2.707918643951416.\n",
      "Epoch 1868. Training loss: 3.1280364990234375. Validation loss: 2.707918405532837.\n",
      "Epoch 1869. Training loss: 3.1280324459075928. Validation loss: 2.707918405532837.\n",
      "Epoch 1870. Training loss: 3.128028154373169. Validation loss: 2.707918167114258.\n",
      "Epoch 1871. Training loss: 3.128024101257324. Validation loss: 2.707918405532837.\n",
      "Epoch 1872. Training loss: 3.1280200481414795. Validation loss: 2.707918167114258.\n",
      "Epoch 1873. Training loss: 3.1280157566070557. Validation loss: 2.707918167114258.\n",
      "Epoch 1874. Training loss: 3.128011703491211. Validation loss: 2.707918167114258.\n",
      "Epoch 1875. Training loss: 3.128007173538208. Validation loss: 2.707918167114258.\n",
      "Epoch 1876. Training loss: 3.1280031204223633. Validation loss: 2.7079176902770996.\n",
      "Epoch 1877. Training loss: 3.1279993057250977. Validation loss: 2.7079176902770996.\n",
      "Epoch 1878. Training loss: 3.1279945373535156. Validation loss: 2.7079179286956787.\n",
      "Epoch 1879. Training loss: 3.127990484237671. Validation loss: 2.7079176902770996.\n",
      "Epoch 1880. Training loss: 3.127986192703247. Validation loss: 2.7079176902770996.\n",
      "Epoch 1881. Training loss: 3.1279823780059814. Validation loss: 2.7079174518585205.\n",
      "Epoch 1882. Training loss: 3.1279780864715576. Validation loss: 2.7079172134399414.\n",
      "Epoch 1883. Training loss: 3.127973794937134. Validation loss: 2.7079172134399414.\n",
      "Epoch 1884. Training loss: 3.127969741821289. Validation loss: 2.7079172134399414.\n",
      "Epoch 1885. Training loss: 3.1279656887054443. Validation loss: 2.7079169750213623.\n",
      "Epoch 1886. Training loss: 3.1279613971710205. Validation loss: 2.7079172134399414.\n",
      "Epoch 1887. Training loss: 3.1279571056365967. Validation loss: 2.7079169750213623.\n",
      "Epoch 1888. Training loss: 3.127953290939331. Validation loss: 2.7079169750213623.\n",
      "Epoch 1889. Training loss: 3.127948760986328. Validation loss: 2.707916736602783.\n",
      "Epoch 1890. Training loss: 3.1279447078704834. Validation loss: 2.707916498184204.\n",
      "Epoch 1891. Training loss: 3.1279404163360596. Validation loss: 2.707916498184204.\n",
      "Epoch 1892. Training loss: 3.1279361248016357. Validation loss: 2.707916736602783.\n",
      "Epoch 1893. Training loss: 3.127931833267212. Validation loss: 2.707916736602783.\n",
      "Epoch 1894. Training loss: 3.127927780151367. Validation loss: 2.707916498184204.\n",
      "Epoch 1895. Training loss: 3.1279237270355225. Validation loss: 2.707916259765625.\n",
      "Epoch 1896. Training loss: 3.1279189586639404. Validation loss: 2.707916259765625.\n",
      "Epoch 1897. Training loss: 3.127915143966675. Validation loss: 2.707916259765625.\n",
      "Epoch 1898. Training loss: 3.127910614013672. Validation loss: 2.707916021347046.\n",
      "Epoch 1899. Training loss: 3.1279067993164062. Validation loss: 2.707916021347046.\n",
      "Epoch 1900. Training loss: 3.1279027462005615. Validation loss: 2.707916021347046.\n",
      "Epoch 1901. Training loss: 3.1278982162475586. Validation loss: 2.707916021347046.\n",
      "Epoch 1902. Training loss: 3.127894163131714. Validation loss: 2.707916021347046.\n",
      "Epoch 1903. Training loss: 3.12788987159729. Validation loss: 2.707916021347046.\n",
      "Epoch 1904. Training loss: 3.127885580062866. Validation loss: 2.707915782928467.\n",
      "Epoch 1905. Training loss: 3.1278810501098633. Validation loss: 2.7079155445098877.\n",
      "Epoch 1906. Training loss: 3.1278769969940186. Validation loss: 2.7079155445098877.\n",
      "Epoch 1907. Training loss: 3.1278727054595947. Validation loss: 2.7079155445098877.\n",
      "Epoch 1908. Training loss: 3.12786865234375. Validation loss: 2.7079153060913086.\n",
      "Epoch 1909. Training loss: 3.1278645992279053. Validation loss: 2.7079155445098877.\n",
      "Epoch 1910. Training loss: 3.1278600692749023. Validation loss: 2.7079153060913086.\n",
      "Epoch 1911. Training loss: 3.1278560161590576. Validation loss: 2.7079153060913086.\n",
      "Epoch 1912. Training loss: 3.1278514862060547. Validation loss: 2.7079153060913086.\n",
      "Epoch 1913. Training loss: 3.12784743309021. Validation loss: 2.7079150676727295.\n",
      "Epoch 1914. Training loss: 3.127842903137207. Validation loss: 2.7079150676727295.\n",
      "Epoch 1915. Training loss: 3.1278388500213623. Validation loss: 2.7079148292541504.\n",
      "Epoch 1916. Training loss: 3.1278345584869385. Validation loss: 2.7079148292541504.\n",
      "Epoch 1917. Training loss: 3.1278302669525146. Validation loss: 2.7079145908355713.\n",
      "Epoch 1918. Training loss: 3.127825975418091. Validation loss: 2.7079145908355713.\n",
      "Epoch 1919. Training loss: 3.127821683883667. Validation loss: 2.7079145908355713.\n",
      "Epoch 1920. Training loss: 3.1278178691864014. Validation loss: 2.707914352416992.\n",
      "Epoch 1921. Training loss: 3.1278133392333984. Validation loss: 2.7079145908355713.\n",
      "Epoch 1922. Training loss: 3.1278088092803955. Validation loss: 2.707914352416992.\n",
      "Epoch 1923. Training loss: 3.127804756164551. Validation loss: 2.707914352416992.\n",
      "Epoch 1924. Training loss: 3.127800703048706. Validation loss: 2.707914352416992.\n",
      "Epoch 1925. Training loss: 3.127796173095703. Validation loss: 2.707913875579834.\n",
      "Epoch 1926. Training loss: 3.1277916431427. Validation loss: 2.707913637161255.\n",
      "Epoch 1927. Training loss: 3.1277875900268555. Validation loss: 2.707913875579834.\n",
      "Epoch 1928. Training loss: 3.1277830600738525. Validation loss: 2.707913875579834.\n",
      "Epoch 1929. Training loss: 3.127779006958008. Validation loss: 2.707913875579834.\n",
      "Epoch 1930. Training loss: 3.127774477005005. Validation loss: 2.707913398742676.\n",
      "Epoch 1931. Training loss: 3.12777042388916. Validation loss: 2.707913875579834.\n",
      "Epoch 1932. Training loss: 3.1277658939361572. Validation loss: 2.707913875579834.\n",
      "Epoch 1933. Training loss: 3.1277618408203125. Validation loss: 2.707913398742676.\n",
      "Epoch 1934. Training loss: 3.1277573108673096. Validation loss: 2.7079129219055176.\n",
      "Epoch 1935. Training loss: 3.1277530193328857. Validation loss: 2.7079131603240967.\n",
      "Epoch 1936. Training loss: 3.12774920463562. Validation loss: 2.707913398742676.\n",
      "Epoch 1937. Training loss: 3.12774395942688. Validation loss: 2.7079129219055176.\n",
      "Epoch 1938. Training loss: 3.127739906311035. Validation loss: 2.7079129219055176.\n",
      "Epoch 1939. Training loss: 3.1277353763580322. Validation loss: 2.7079129219055176.\n",
      "Epoch 1940. Training loss: 3.1277315616607666. Validation loss: 2.7079129219055176.\n",
      "Epoch 1941. Training loss: 3.127727508544922. Validation loss: 2.7079129219055176.\n",
      "Epoch 1942. Training loss: 3.12772274017334. Validation loss: 2.7079126834869385.\n",
      "Epoch 1943. Training loss: 3.127718687057495. Validation loss: 2.7079124450683594.\n",
      "Epoch 1944. Training loss: 3.127714157104492. Validation loss: 2.7079126834869385.\n",
      "Epoch 1945. Training loss: 3.1277101039886475. Validation loss: 2.7079124450683594.\n",
      "Epoch 1946. Training loss: 3.1277055740356445. Validation loss: 2.7079124450683594.\n",
      "Epoch 1947. Training loss: 3.1277010440826416. Validation loss: 2.707911968231201.\n",
      "Epoch 1948. Training loss: 3.127697229385376. Validation loss: 2.7079122066497803.\n",
      "Epoch 1949. Training loss: 3.127692937850952. Validation loss: 2.7079124450683594.\n",
      "Epoch 1950. Training loss: 3.1276886463165283. Validation loss: 2.707911968231201.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 355. Training loss: 0.7765085697174072. Validation loss: 2.706979274749756.\n",
      "Epoch 356. Training loss: 0.7765057682991028. Validation loss: 2.7069787979125977.\n",
      "Epoch 357. Training loss: 0.7765028476715088. Validation loss: 2.7069783210754395.\n",
      "Epoch 358. Training loss: 0.7764999270439148. Validation loss: 2.706977605819702.\n",
      "Epoch 359. Training loss: 0.7764971256256104. Validation loss: 2.706977605819702.\n",
      "Epoch 360. Training loss: 0.7764942646026611. Validation loss: 2.706977367401123.\n",
      "Epoch 361. Training loss: 0.7764912247657776. Validation loss: 2.706976890563965.\n",
      "Epoch 362. Training loss: 0.7764884829521179. Validation loss: 2.7069766521453857.\n",
      "Epoch 363. Training loss: 0.7764855027198792. Validation loss: 2.7069761753082275.\n",
      "Epoch 364. Training loss: 0.7764825820922852. Validation loss: 2.7069759368896484.\n",
      "Epoch 365. Training loss: 0.7764796614646912. Validation loss: 2.7069754600524902.\n",
      "Epoch 366. Training loss: 0.7764768600463867. Validation loss: 2.706974983215332.\n",
      "Epoch 367. Training loss: 0.7764738202095032. Validation loss: 2.706974506378174.\n",
      "Epoch 368. Training loss: 0.7764710783958435. Validation loss: 2.706974506378174.\n",
      "Epoch 369. Training loss: 0.7764680981636047. Validation loss: 2.7069737911224365.\n",
      "Epoch 370. Training loss: 0.7764651775360107. Validation loss: 2.7069735527038574.\n",
      "Epoch 371. Training loss: 0.7764623165130615. Validation loss: 2.7069733142852783.\n",
      "Epoch 372. Training loss: 0.7764594554901123. Validation loss: 2.706973075866699.\n",
      "Epoch 373. Training loss: 0.7764565348625183. Validation loss: 2.706972122192383.\n",
      "Epoch 374. Training loss: 0.7764536738395691. Validation loss: 2.706972122192383.\n",
      "Epoch 375. Training loss: 0.7764506936073303. Validation loss: 2.7069718837738037.\n",
      "Epoch 376. Training loss: 0.7764477729797363. Validation loss: 2.7069711685180664.\n",
      "Epoch 377. Training loss: 0.7764448523521423. Validation loss: 2.7069711685180664.\n",
      "Epoch 378. Training loss: 0.7764420509338379. Validation loss: 2.706970691680908.\n",
      "Epoch 379. Training loss: 0.7764391303062439. Validation loss: 2.706970453262329.\n",
      "Epoch 380. Training loss: 0.7764361500740051. Validation loss: 2.706969738006592.\n",
      "Epoch 381. Training loss: 0.7764332890510559. Validation loss: 2.706969738006592.\n",
      "Epoch 382. Training loss: 0.7764303684234619. Validation loss: 2.7069692611694336.\n",
      "Epoch 383. Training loss: 0.7764274477958679. Validation loss: 2.7069687843322754.\n",
      "Epoch 384. Training loss: 0.7764246463775635. Validation loss: 2.7069685459136963.\n",
      "Epoch 385. Training loss: 0.7764216065406799. Validation loss: 2.706968069076538.\n",
      "Epoch 386. Training loss: 0.7764187455177307. Validation loss: 2.706968069076538.\n",
      "Epoch 387. Training loss: 0.7764158844947815. Validation loss: 2.70696759223938.\n",
      "Epoch 388. Training loss: 0.7764129638671875. Validation loss: 2.7069668769836426.\n",
      "Epoch 389. Training loss: 0.776409924030304. Validation loss: 2.7069668769836426.\n",
      "Epoch 390. Training loss: 0.7764070630073547. Validation loss: 2.7069664001464844.\n",
      "Epoch 391. Training loss: 0.7764042019844055. Validation loss: 2.706965923309326.\n",
      "Epoch 392. Training loss: 0.7764013409614563. Validation loss: 2.706965684890747.\n",
      "Epoch 393. Training loss: 0.7763984203338623. Validation loss: 2.706965208053589.\n",
      "Epoch 394. Training loss: 0.7763954997062683. Validation loss: 2.7069649696350098.\n",
      "Epoch 395. Training loss: 0.7763925194740295. Validation loss: 2.7069644927978516.\n",
      "Epoch 396. Training loss: 0.7763895988464355. Validation loss: 2.7069642543792725.\n",
      "Epoch 397. Training loss: 0.7763867378234863. Validation loss: 2.706963539123535.\n",
      "Epoch 398. Training loss: 0.7763838171958923. Validation loss: 2.706963539123535.\n",
      "Epoch 399. Training loss: 0.7763808369636536. Validation loss: 2.706963062286377.\n",
      "Epoch 400. Training loss: 0.7763779759407043. Validation loss: 2.7069625854492188.\n",
      "Epoch 401. Training loss: 0.7763750553131104. Validation loss: 2.7069625854492188.\n",
      "Epoch 402. Training loss: 0.7763721346855164. Validation loss: 2.7069616317749023.\n",
      "Epoch 403. Training loss: 0.7763692736625671. Validation loss: 2.7069616317749023.\n",
      "Epoch 404. Training loss: 0.7763664126396179. Validation loss: 2.706960916519165.\n",
      "Epoch 405. Training loss: 0.7763635516166687. Validation loss: 2.706960916519165.\n",
      "Epoch 406. Training loss: 0.7763605117797852. Validation loss: 2.706960439682007.\n",
      "Epoch 407. Training loss: 0.7763574719429016. Validation loss: 2.7069599628448486.\n",
      "Epoch 408. Training loss: 0.7763546109199524. Validation loss: 2.7069597244262695.\n",
      "Epoch 409. Training loss: 0.7763516902923584. Validation loss: 2.7069592475891113.\n",
      "Epoch 410. Training loss: 0.7763487696647644. Validation loss: 2.7069590091705322.\n",
      "Epoch 411. Training loss: 0.7763459086418152. Validation loss: 2.706958293914795.\n",
      "Epoch 412. Training loss: 0.776343047618866. Validation loss: 2.706958055496216.\n",
      "Epoch 413. Training loss: 0.7763400673866272. Validation loss: 2.7069578170776367.\n",
      "Epoch 414. Training loss: 0.7763371467590332. Validation loss: 2.7069573402404785.\n",
      "Epoch 415. Training loss: 0.7763342261314392. Validation loss: 2.7069573402404785.\n",
      "Epoch 416. Training loss: 0.7763312458992004. Validation loss: 2.7069568634033203.\n",
      "Epoch 417. Training loss: 0.7763283848762512. Validation loss: 2.706956386566162.\n",
      "Epoch 418. Training loss: 0.7763254046440125. Validation loss: 2.706955909729004.\n",
      "Epoch 419. Training loss: 0.7763225436210632. Validation loss: 2.706955671310425.\n",
      "Epoch 420. Training loss: 0.7763195037841797. Validation loss: 2.7069554328918457.\n",
      "Epoch 421. Training loss: 0.7763166427612305. Validation loss: 2.7069549560546875.\n",
      "Epoch 422. Training loss: 0.7763137221336365. Validation loss: 2.7069549560546875.\n",
      "Epoch 423. Training loss: 0.7763107419013977. Validation loss: 2.706954002380371.\n",
      "Epoch 424. Training loss: 0.7763078808784485. Validation loss: 2.706954002380371.\n",
      "Epoch 425. Training loss: 0.7763049006462097. Validation loss: 2.706953525543213.\n",
      "Epoch 426. Training loss: 0.7763020396232605. Validation loss: 2.7069530487060547.\n",
      "Epoch 427. Training loss: 0.7762991786003113. Validation loss: 2.7069528102874756.\n",
      "Epoch 428. Training loss: 0.7762961387634277. Validation loss: 2.7069520950317383.\n",
      "Epoch 429. Training loss: 0.7762932181358337. Validation loss: 2.7069520950317383.\n",
      "Epoch 430. Training loss: 0.776290237903595. Validation loss: 2.70695161819458.\n",
      "Epoch 431. Training loss: 0.7762873768806458. Validation loss: 2.706951141357422.\n",
      "Epoch 432. Training loss: 0.7762844562530518. Validation loss: 2.706951141357422.\n",
      "Epoch 433. Training loss: 0.7762814164161682. Validation loss: 2.7069504261016846.\n",
      "Epoch 434. Training loss: 0.7762784957885742. Validation loss: 2.7069499492645264.\n",
      "Epoch 435. Training loss: 0.7762755751609802. Validation loss: 2.7069499492645264.\n",
      "Epoch 436. Training loss: 0.776272714138031. Validation loss: 2.706949234008789.\n",
      "Epoch 437. Training loss: 0.7762697339057922. Validation loss: 2.70694899559021.\n",
      "Epoch 438. Training loss: 0.7762668132781982. Validation loss: 2.7069482803344727.\n",
      "Epoch 439. Training loss: 0.7762638926506042. Validation loss: 2.7069480419158936.\n",
      "Epoch 440. Training loss: 0.7762608528137207. Validation loss: 2.7069475650787354.\n",
      "Epoch 441. Training loss: 0.7762579321861267. Validation loss: 2.7069473266601562.\n",
      "Epoch 442. Training loss: 0.7762550711631775. Validation loss: 2.706947088241577.\n",
      "Epoch 443. Training loss: 0.776252031326294. Validation loss: 2.706947088241577.\n",
      "Epoch 444. Training loss: 0.7762491703033447. Validation loss: 2.70694637298584.\n",
      "Epoch 445. Training loss: 0.7762461304664612. Validation loss: 2.7069458961486816.\n",
      "Epoch 446. Training loss: 0.7762432098388672. Validation loss: 2.7069458961486816.\n",
      "Epoch 447. Training loss: 0.7762402892112732. Validation loss: 2.7069451808929443.\n",
      "Epoch 448. Training loss: 0.776237428188324. Validation loss: 2.7069449424743652.\n",
      "Epoch 449. Training loss: 0.7762344479560852. Validation loss: 2.706944465637207.\n",
      "Epoch 450. Training loss: 0.7762314677238464. Validation loss: 2.706944227218628.\n",
      "Epoch 451. Training loss: 0.7762284874916077. Validation loss: 2.706943988800049.\n",
      "Epoch 452. Training loss: 0.7762255668640137. Validation loss: 2.7069435119628906.\n",
      "Epoch 453. Training loss: 0.7762226462364197. Validation loss: 2.7069430351257324.\n",
      "Epoch 454. Training loss: 0.7762197852134705. Validation loss: 2.706942558288574.\n",
      "Epoch 455. Training loss: 0.7762168049812317. Validation loss: 2.706942081451416.\n",
      "Epoch 456. Training loss: 0.7762138247489929. Validation loss: 2.706942081451416.\n",
      "Epoch 457. Training loss: 0.7762107849121094. Validation loss: 2.706941604614258.\n",
      "Epoch 458. Training loss: 0.7762079238891602. Validation loss: 2.7069413661956787.\n",
      "Epoch 459. Training loss: 0.7762048840522766. Validation loss: 2.7069406509399414.\n",
      "Epoch 460. Training loss: 0.7762019634246826. Validation loss: 2.7069406509399414.\n",
      "Epoch 461. Training loss: 0.7761989235877991. Validation loss: 2.706939935684204.\n",
      "Epoch 462. Training loss: 0.7761960625648499. Validation loss: 2.706939458847046.\n",
      "Epoch 463. Training loss: 0.7761931419372559. Validation loss: 2.706939458847046.\n",
      "Epoch 464. Training loss: 0.7761901021003723. Validation loss: 2.7069389820098877.\n",
      "Epoch 465. Training loss: 0.7761872410774231. Validation loss: 2.7069385051727295.\n",
      "Epoch 466. Training loss: 0.7761842608451843. Validation loss: 2.706937789916992.\n",
      "Epoch 467. Training loss: 0.7761813998222351. Validation loss: 2.7069380283355713.\n",
      "Epoch 468. Training loss: 0.7761783599853516. Validation loss: 2.706937313079834.\n",
      "Epoch 469. Training loss: 0.7761754393577576. Validation loss: 2.706937313079834.\n",
      "Epoch 470. Training loss: 0.7761724591255188. Validation loss: 2.7069365978240967.\n",
      "Epoch 471. Training loss: 0.77616947889328. Validation loss: 2.7069363594055176.\n",
      "Epoch 472. Training loss: 0.7761664986610413. Validation loss: 2.7069358825683594.\n",
      "Epoch 473. Training loss: 0.7761635780334473. Validation loss: 2.7069356441497803.\n",
      "Epoch 474. Training loss: 0.7761606574058533. Validation loss: 2.706935405731201.\n",
      "Epoch 475. Training loss: 0.7761576771736145. Validation loss: 2.706934928894043.\n",
      "Epoch 476. Training loss: 0.7761546969413757. Validation loss: 2.706934690475464.\n",
      "Epoch 477. Training loss: 0.7761518359184265. Validation loss: 2.7069342136383057.\n",
      "Epoch 478. Training loss: 0.776148796081543. Validation loss: 2.7069337368011475.\n",
      "Epoch 479. Training loss: 0.7761457562446594. Validation loss: 2.7069334983825684.\n",
      "Epoch 480. Training loss: 0.7761428356170654. Validation loss: 2.70693302154541.\n",
      "Epoch 481. Training loss: 0.7761399149894714. Validation loss: 2.706932544708252.\n",
      "Epoch 482. Training loss: 0.7761368155479431. Validation loss: 2.7069320678710938.\n",
      "Epoch 483. Training loss: 0.7761339545249939. Validation loss: 2.7069318294525146.\n",
      "Epoch 484. Training loss: 0.7761309742927551. Validation loss: 2.7069315910339355.\n",
      "Epoch 485. Training loss: 0.7761280536651611. Validation loss: 2.7069311141967773.\n",
      "Epoch 486. Training loss: 0.7761251330375671. Validation loss: 2.7069308757781982.\n",
      "Epoch 487. Training loss: 0.7761220932006836. Validation loss: 2.70693039894104.\n",
      "Epoch 488. Training loss: 0.7761190533638. Validation loss: 2.706930160522461.\n",
      "Epoch 489. Training loss: 0.7761160731315613. Validation loss: 2.706930160522461.\n",
      "Epoch 490. Training loss: 0.7761130332946777. Validation loss: 2.7069292068481445.\n",
      "Epoch 491. Training loss: 0.7761101722717285. Validation loss: 2.7069289684295654.\n",
      "Epoch 492. Training loss: 0.776107132434845. Validation loss: 2.7069284915924072.\n",
      "Epoch 493. Training loss: 0.7761042714118958. Validation loss: 2.706928014755249.\n",
      "Epoch 494. Training loss: 0.776101291179657. Validation loss: 2.706928014755249.\n",
      "Epoch 495. Training loss: 0.7760982513427734. Validation loss: 2.7069272994995117.\n",
      "Epoch 496. Training loss: 0.7760952115058899. Validation loss: 2.706927537918091.\n",
      "Epoch 497. Training loss: 0.7760922908782959. Validation loss: 2.7069263458251953.\n",
      "Epoch 498. Training loss: 0.7760893702507019. Validation loss: 2.7069263458251953.\n",
      "Epoch 499. Training loss: 0.7760863304138184. Validation loss: 2.706925868988037.\n",
      "Epoch 500. Training loss: 0.7760834097862244. Validation loss: 2.706925392150879.\n",
      "Epoch 501. Training loss: 0.7760803699493408. Validation loss: 2.7069249153137207.\n",
      "Epoch 502. Training loss: 0.7760773301124573. Validation loss: 2.7069249153137207.\n",
      "Epoch 503. Training loss: 0.7760744690895081. Validation loss: 2.7069244384765625.\n",
      "Epoch 504. Training loss: 0.7760714888572693. Validation loss: 2.7069239616394043.\n",
      "Epoch 505. Training loss: 0.7760685086250305. Validation loss: 2.7069239616394043.\n",
      "Epoch 506. Training loss: 0.7760655283927917. Validation loss: 2.706923484802246.\n",
      "Epoch 507. Training loss: 0.7760624885559082. Validation loss: 2.706923007965088.\n",
      "Epoch 508. Training loss: 0.7760595679283142. Validation loss: 2.7069225311279297.\n",
      "Epoch 509. Training loss: 0.7760565280914307. Validation loss: 2.7069222927093506.\n",
      "Epoch 510. Training loss: 0.7760536074638367. Validation loss: 2.7069218158721924.\n",
      "Epoch 511. Training loss: 0.7760505676269531. Validation loss: 2.7069215774536133.\n",
      "Epoch 512. Training loss: 0.7760475277900696. Validation loss: 2.706920623779297.\n",
      "Epoch 513. Training loss: 0.7760446071624756. Validation loss: 2.706920862197876.\n",
      "Epoch 514. Training loss: 0.776041567325592. Validation loss: 2.7069203853607178.\n",
      "Epoch 515. Training loss: 0.7760385870933533. Validation loss: 2.7069201469421387.\n",
      "Epoch 516. Training loss: 0.776035726070404. Validation loss: 2.7069194316864014.\n",
      "Epoch 517. Training loss: 0.7760326862335205. Validation loss: 2.7069191932678223.\n",
      "Epoch 518. Training loss: 0.776029646396637. Validation loss: 2.706918954849243.\n",
      "Epoch 519. Training loss: 0.776026725769043. Validation loss: 2.706918239593506.\n",
      "Epoch 520. Training loss: 0.776023805141449. Validation loss: 2.706918239593506.\n",
      "Epoch 521. Training loss: 0.7760207056999207. Validation loss: 2.7069177627563477.\n",
      "Epoch 522. Training loss: 0.7760177254676819. Validation loss: 2.7069170475006104.\n",
      "Epoch 523. Training loss: 0.7760147452354431. Validation loss: 2.7069168090820312.\n",
      "Epoch 524. Training loss: 0.7760117650032043. Validation loss: 2.706916332244873.\n",
      "Epoch 525. Training loss: 0.7760087847709656. Validation loss: 2.706916093826294.\n",
      "Epoch 526. Training loss: 0.776005744934082. Validation loss: 2.706916093826294.\n",
      "Epoch 527. Training loss: 0.776002824306488. Validation loss: 2.7069149017333984.\n",
      "Epoch 528. Training loss: 0.7759997844696045. Validation loss: 2.7069149017333984.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 529. Training loss: 0.775996744632721. Validation loss: 2.7069144248962402.\n",
      "Epoch 530. Training loss: 0.7759937644004822. Validation loss: 2.7069144248962402.\n",
      "Epoch 531. Training loss: 0.7759907245635986. Validation loss: 2.706913948059082.\n",
      "Epoch 532. Training loss: 0.7759878039360046. Validation loss: 2.706913709640503.\n",
      "Epoch 533. Training loss: 0.7759847640991211. Validation loss: 2.7069129943847656.\n",
      "Epoch 534. Training loss: 0.7759817242622375. Validation loss: 2.7069127559661865.\n",
      "Epoch 535. Training loss: 0.7759787440299988. Validation loss: 2.7069125175476074.\n",
      "Epoch 536. Training loss: 0.7759758830070496. Validation loss: 2.70691180229187.\n",
      "Epoch 537. Training loss: 0.775972843170166. Validation loss: 2.706911563873291.\n",
      "Epoch 538. Training loss: 0.7759697437286377. Validation loss: 2.706911325454712.\n",
      "Epoch 539. Training loss: 0.7759668231010437. Validation loss: 2.706911087036133.\n",
      "Epoch 540. Training loss: 0.7759637832641602. Validation loss: 2.7069101333618164.\n",
      "Epoch 541. Training loss: 0.7759607434272766. Validation loss: 2.7069098949432373.\n",
      "Epoch 542. Training loss: 0.7759578227996826. Validation loss: 2.706909656524658.\n",
      "Epoch 543. Training loss: 0.7759547829627991. Validation loss: 2.706909418106079.\n",
      "Epoch 544. Training loss: 0.7759518623352051. Validation loss: 2.706908941268921.\n",
      "Epoch 545. Training loss: 0.7759487628936768. Validation loss: 2.706908702850342.\n",
      "Epoch 546. Training loss: 0.7759457230567932. Validation loss: 2.7069084644317627.\n",
      "Epoch 547. Training loss: 0.7759428024291992. Validation loss: 2.7069082260131836.\n",
      "Epoch 548. Training loss: 0.7759397625923157. Validation loss: 2.7069077491760254.\n",
      "Epoch 549. Training loss: 0.7759367823600769. Validation loss: 2.706907272338867.\n",
      "Epoch 550. Training loss: 0.7759337425231934. Validation loss: 2.70690655708313.\n",
      "Epoch 551. Training loss: 0.7759307026863098. Validation loss: 2.706906318664551.\n",
      "Epoch 552. Training loss: 0.775927722454071. Validation loss: 2.7069060802459717.\n",
      "Epoch 553. Training loss: 0.7759246826171875. Validation loss: 2.7069056034088135.\n",
      "Epoch 554. Training loss: 0.7759218215942383. Validation loss: 2.7069051265716553.\n",
      "Epoch 555. Training loss: 0.77591872215271. Validation loss: 2.706904411315918.\n",
      "Epoch 556. Training loss: 0.775915801525116. Validation loss: 2.706904411315918.\n",
      "Epoch 557. Training loss: 0.7759127616882324. Validation loss: 2.7069039344787598.\n",
      "Epoch 558. Training loss: 0.7759097218513489. Validation loss: 2.7069036960601807.\n",
      "Epoch 559. Training loss: 0.7759066224098206. Validation loss: 2.7069034576416016.\n",
      "Epoch 560. Training loss: 0.7759037017822266. Validation loss: 2.7069029808044434.\n",
      "Epoch 561. Training loss: 0.775900661945343. Validation loss: 2.7069027423858643.\n",
      "Epoch 562. Training loss: 0.7758976817131042. Validation loss: 2.706902027130127.\n",
      "Epoch 563. Training loss: 0.7758947014808655. Validation loss: 2.706901788711548.\n",
      "Epoch 564. Training loss: 0.7758916020393372. Validation loss: 2.7069013118743896.\n",
      "Epoch 565. Training loss: 0.7758886218070984. Validation loss: 2.7069010734558105.\n",
      "Epoch 566. Training loss: 0.7758856415748596. Validation loss: 2.7069005966186523.\n",
      "Epoch 567. Training loss: 0.7758825421333313. Validation loss: 2.7069003582000732.\n",
      "Epoch 568. Training loss: 0.7758796215057373. Validation loss: 2.706899881362915.\n",
      "Epoch 569. Training loss: 0.7758765816688538. Validation loss: 2.706899642944336.\n",
      "Epoch 570. Training loss: 0.7758734822273254. Validation loss: 2.7068991661071777.\n",
      "Epoch 571. Training loss: 0.7758705615997314. Validation loss: 2.7068986892700195.\n",
      "Epoch 572. Training loss: 0.7758674621582031. Validation loss: 2.7068984508514404.\n",
      "Epoch 573. Training loss: 0.7758645415306091. Validation loss: 2.706897735595703.\n",
      "Epoch 574. Training loss: 0.7758615016937256. Validation loss: 2.706897735595703.\n",
      "Epoch 575. Training loss: 0.7758584022521973. Validation loss: 2.706897258758545.\n",
      "Epoch 576. Training loss: 0.7758553624153137. Validation loss: 2.706897258758545.\n",
      "Epoch 577. Training loss: 0.775852382183075. Validation loss: 2.7068963050842285.\n",
      "Epoch 578. Training loss: 0.7758493423461914. Validation loss: 2.7068963050842285.\n",
      "Epoch 579. Training loss: 0.7758463025093079. Validation loss: 2.7068958282470703.\n",
      "Epoch 580. Training loss: 0.7758433222770691. Validation loss: 2.706895112991333.\n",
      "Epoch 581. Training loss: 0.7758403420448303. Validation loss: 2.706895112991333.\n",
      "Epoch 582. Training loss: 0.7758373618125916. Validation loss: 2.7068941593170166.\n",
      "Epoch 583. Training loss: 0.7758342623710632. Validation loss: 2.7068941593170166.\n",
      "Epoch 584. Training loss: 0.7758312821388245. Validation loss: 2.7068934440612793.\n",
      "Epoch 585. Training loss: 0.7758281826972961. Validation loss: 2.706892967224121.\n",
      "Epoch 586. Training loss: 0.7758252620697021. Validation loss: 2.706892967224121.\n",
      "Epoch 587. Training loss: 0.775822103023529. Validation loss: 2.706892490386963.\n",
      "Epoch 588. Training loss: 0.7758191227912903. Validation loss: 2.706892490386963.\n",
      "Epoch 589. Training loss: 0.775816023349762. Validation loss: 2.7068917751312256.\n",
      "Epoch 590. Training loss: 0.7758130431175232. Validation loss: 2.7068920135498047.\n",
      "Epoch 591. Training loss: 0.7758100032806396. Validation loss: 2.7068910598754883.\n",
      "Epoch 592. Training loss: 0.7758069634437561. Validation loss: 2.706890821456909.\n",
      "Epoch 593. Training loss: 0.7758039832115173. Validation loss: 2.706890106201172.\n",
      "Epoch 594. Training loss: 0.7758009433746338. Validation loss: 2.7068898677825928.\n",
      "Epoch 595. Training loss: 0.7757978439331055. Validation loss: 2.7068896293640137.\n",
      "Epoch 596. Training loss: 0.7757949233055115. Validation loss: 2.7068891525268555.\n",
      "Epoch 597. Training loss: 0.7757918238639832. Validation loss: 2.7068889141082764.\n",
      "Epoch 598. Training loss: 0.7757887840270996. Validation loss: 2.7068886756896973.\n",
      "Epoch 599. Training loss: 0.7757856845855713. Validation loss: 2.706888198852539.\n",
      "Epoch 600. Training loss: 0.7757826447486877. Validation loss: 2.706887722015381.\n",
      "Epoch 601. Training loss: 0.7757795453071594. Validation loss: 2.7068874835968018.\n",
      "Epoch 602. Training loss: 0.7757766246795654. Validation loss: 2.7068870067596436.\n",
      "Epoch 603. Training loss: 0.7757734656333923. Validation loss: 2.7068865299224854.\n",
      "Epoch 604. Training loss: 0.7757704854011536. Validation loss: 2.7068862915039062.\n",
      "Epoch 605. Training loss: 0.7757675647735596. Validation loss: 2.706886053085327.\n",
      "Epoch 606. Training loss: 0.7757644653320312. Validation loss: 2.70688533782959.\n",
      "Epoch 607. Training loss: 0.7757614254951477. Validation loss: 2.7068848609924316.\n",
      "Epoch 608. Training loss: 0.7757584452629089. Validation loss: 2.7068848609924316.\n",
      "Epoch 609. Training loss: 0.7757553458213806. Validation loss: 2.7068841457366943.\n",
      "Epoch 610. Training loss: 0.7757523059844971. Validation loss: 2.7068841457366943.\n",
      "Epoch 611. Training loss: 0.7757492661476135. Validation loss: 2.706883668899536.\n",
      "Epoch 612. Training loss: 0.7757462859153748. Validation loss: 2.706882953643799.\n",
      "Epoch 613. Training loss: 0.7757430672645569. Validation loss: 2.7068824768066406.\n",
      "Epoch 614. Training loss: 0.7757401466369629. Validation loss: 2.7068824768066406.\n",
      "Epoch 615. Training loss: 0.7757370471954346. Validation loss: 2.7068819999694824.\n",
      "Epoch 616. Training loss: 0.775734007358551. Validation loss: 2.706881523132324.\n",
      "Epoch 617. Training loss: 0.7757309079170227. Validation loss: 2.706881046295166.\n",
      "Epoch 618. Training loss: 0.7757279276847839. Validation loss: 2.706880807876587.\n",
      "Epoch 619. Training loss: 0.7757248878479004. Validation loss: 2.706880569458008.\n",
      "Epoch 620. Training loss: 0.7757217288017273. Validation loss: 2.7068796157836914.\n",
      "Epoch 621. Training loss: 0.7757186889648438. Validation loss: 2.7068796157836914.\n",
      "Epoch 622. Training loss: 0.7757156491279602. Validation loss: 2.706879138946533.\n",
      "Epoch 623. Training loss: 0.7757126688957214. Validation loss: 2.706878900527954.\n",
      "Epoch 624. Training loss: 0.7757095694541931. Validation loss: 2.706878662109375.\n",
      "Epoch 625. Training loss: 0.7757065296173096. Validation loss: 2.7068779468536377.\n",
      "Epoch 626. Training loss: 0.7757034301757812. Validation loss: 2.7068777084350586.\n",
      "Epoch 627. Training loss: 0.7757003903388977. Validation loss: 2.7068772315979004.\n",
      "Epoch 628. Training loss: 0.7756972908973694. Validation loss: 2.7068769931793213.\n",
      "Epoch 629. Training loss: 0.7756943702697754. Validation loss: 2.706876754760742.\n",
      "Epoch 630. Training loss: 0.7756912112236023. Validation loss: 2.706876039505005.\n",
      "Epoch 631. Training loss: 0.7756882309913635. Validation loss: 2.706876039505005.\n",
      "Epoch 632. Training loss: 0.7756850719451904. Validation loss: 2.7068755626678467.\n",
      "Epoch 633. Training loss: 0.7756820321083069. Validation loss: 2.7068750858306885.\n",
      "Epoch 634. Training loss: 0.7756790518760681. Validation loss: 2.7068750858306885.\n",
      "Epoch 635. Training loss: 0.7756759524345398. Validation loss: 2.706874132156372.\n",
      "Epoch 636. Training loss: 0.7756728529930115. Validation loss: 2.706873893737793.\n",
      "Epoch 637. Training loss: 0.7756698727607727. Validation loss: 2.7068734169006348.\n",
      "Epoch 638. Training loss: 0.7756667733192444. Validation loss: 2.7068731784820557.\n",
      "Epoch 639. Training loss: 0.7756636738777161. Validation loss: 2.7068729400634766.\n",
      "Epoch 640. Training loss: 0.7756606936454773. Validation loss: 2.7068727016448975.\n",
      "Epoch 641. Training loss: 0.7756576538085938. Validation loss: 2.706871747970581.\n",
      "Epoch 642. Training loss: 0.7756545543670654. Validation loss: 2.706871509552002.\n",
      "Epoch 643. Training loss: 0.7756514549255371. Validation loss: 2.7068710327148438.\n",
      "Epoch 644. Training loss: 0.7756483554840088. Validation loss: 2.7068705558776855.\n",
      "Epoch 645. Training loss: 0.7756454348564148. Validation loss: 2.7068705558776855.\n",
      "Epoch 646. Training loss: 0.7756422162055969. Validation loss: 2.7068698406219482.\n",
      "Epoch 647. Training loss: 0.7756392359733582. Validation loss: 2.706869602203369.\n",
      "Epoch 648. Training loss: 0.7756361365318298. Validation loss: 2.70686936378479.\n",
      "Epoch 649. Training loss: 0.775632917881012. Validation loss: 2.706868886947632.\n",
      "Epoch 650. Training loss: 0.775629997253418. Validation loss: 2.7068686485290527.\n",
      "Epoch 651. Training loss: 0.7756268382072449. Validation loss: 2.7068681716918945.\n",
      "Epoch 652. Training loss: 0.7756238579750061. Validation loss: 2.7068676948547363.\n",
      "Epoch 653. Training loss: 0.7756207585334778. Validation loss: 2.7068674564361572.\n",
      "Epoch 654. Training loss: 0.7756176590919495. Validation loss: 2.706866979598999.\n",
      "Epoch 655. Training loss: 0.7756145596504211. Validation loss: 2.7068662643432617.\n",
      "Epoch 656. Training loss: 0.7756115794181824. Validation loss: 2.7068660259246826.\n",
      "Epoch 657. Training loss: 0.7756083607673645. Validation loss: 2.7068660259246826.\n",
      "Epoch 658. Training loss: 0.7756053805351257. Validation loss: 2.7068653106689453.\n",
      "Epoch 659. Training loss: 0.7756023406982422. Validation loss: 2.706865072250366.\n",
      "Epoch 660. Training loss: 0.7755992412567139. Validation loss: 2.706864833831787.\n",
      "Epoch 661. Training loss: 0.7755962014198303. Validation loss: 2.706864356994629.\n",
      "Epoch 662. Training loss: 0.775593101978302. Validation loss: 2.70686411857605.\n",
      "Epoch 663. Training loss: 0.7755901217460632. Validation loss: 2.7068636417388916.\n",
      "Epoch 664. Training loss: 0.7755870223045349. Validation loss: 2.7068634033203125.\n",
      "Epoch 665. Training loss: 0.7755839228630066. Validation loss: 2.706862688064575.\n",
      "Epoch 666. Training loss: 0.7755808234214783. Validation loss: 2.706862211227417.\n",
      "Epoch 667. Training loss: 0.7755776047706604. Validation loss: 2.706861734390259.\n",
      "Epoch 668. Training loss: 0.7755746245384216. Validation loss: 2.7068614959716797.\n",
      "Epoch 669. Training loss: 0.7755715250968933. Validation loss: 2.7068610191345215.\n",
      "Epoch 670. Training loss: 0.775568425655365. Validation loss: 2.7068605422973633.\n",
      "Epoch 671. Training loss: 0.7755653858184814. Validation loss: 2.7068605422973633.\n",
      "Epoch 672. Training loss: 0.7755622863769531. Validation loss: 2.706860065460205.\n",
      "Epoch 673. Training loss: 0.7755591869354248. Validation loss: 2.7068593502044678.\n",
      "Epoch 674. Training loss: 0.7755560874938965. Validation loss: 2.7068591117858887.\n",
      "Epoch 675. Training loss: 0.7755530476570129. Validation loss: 2.7068588733673096.\n",
      "Epoch 676. Training loss: 0.7755498290061951. Validation loss: 2.7068583965301514.\n",
      "Epoch 677. Training loss: 0.7755467891693115. Validation loss: 2.7068581581115723.\n",
      "Epoch 678. Training loss: 0.7755436897277832. Validation loss: 2.706857681274414.\n",
      "Epoch 679. Training loss: 0.7755406498908997. Validation loss: 2.706857204437256.\n",
      "Epoch 680. Training loss: 0.7755375504493713. Validation loss: 2.7068567276000977.\n",
      "Epoch 681. Training loss: 0.775534451007843. Validation loss: 2.7068564891815186.\n",
      "Epoch 682. Training loss: 0.7755313515663147. Validation loss: 2.7068560123443604.\n",
      "Epoch 683. Training loss: 0.7755283713340759. Validation loss: 2.7068557739257812.\n",
      "Epoch 684. Training loss: 0.7755252718925476. Validation loss: 2.706855297088623.\n",
      "Epoch 685. Training loss: 0.7755220532417297. Validation loss: 2.706854820251465.\n",
      "Epoch 686. Training loss: 0.7755189538002014. Validation loss: 2.7068545818328857.\n",
      "Epoch 687. Training loss: 0.7755159735679626. Validation loss: 2.7068541049957275.\n",
      "Epoch 688. Training loss: 0.7755127549171448. Validation loss: 2.7068536281585693.\n",
      "Epoch 689. Training loss: 0.7755096554756165. Validation loss: 2.7068536281585693.\n",
      "Epoch 690. Training loss: 0.7755066752433777. Validation loss: 2.706853151321411.\n",
      "Epoch 691. Training loss: 0.7755035758018494. Validation loss: 2.706852674484253.\n",
      "Epoch 692. Training loss: 0.775500476360321. Validation loss: 2.7068519592285156.\n",
      "Epoch 693. Training loss: 0.7754972577095032. Validation loss: 2.7068519592285156.\n",
      "Epoch 694. Training loss: 0.7754942774772644. Validation loss: 2.7068514823913574.\n",
      "Epoch 695. Training loss: 0.7754910588264465. Validation loss: 2.706851005554199.\n",
      "Epoch 696. Training loss: 0.7754880785942078. Validation loss: 2.706850290298462.\n",
      "Epoch 697. Training loss: 0.7754848599433899. Validation loss: 2.706850290298462.\n",
      "Epoch 698. Training loss: 0.7754818797111511. Validation loss: 2.7068498134613037.\n",
      "Epoch 699. Training loss: 0.7754787802696228. Validation loss: 2.7068498134613037.\n",
      "Epoch 700. Training loss: 0.7754756808280945. Validation loss: 2.7068490982055664.\n",
      "Epoch 701. Training loss: 0.7754725813865662. Validation loss: 2.706848621368408.\n",
      "Epoch 702. Training loss: 0.7754694819450378. Validation loss: 2.70684814453125.\n",
      "Epoch 703. Training loss: 0.77546626329422. Validation loss: 2.706847667694092.\n",
      "Epoch 704. Training loss: 0.7754631638526917. Validation loss: 2.706847667694092.\n",
      "Epoch 705. Training loss: 0.7754600644111633. Validation loss: 2.7068471908569336.\n",
      "Epoch 706. Training loss: 0.775456964969635. Validation loss: 2.7068467140197754.\n",
      "Epoch 707. Training loss: 0.7754539847373962. Validation loss: 2.706846237182617.\n",
      "Epoch 708. Training loss: 0.7754507660865784. Validation loss: 2.706845760345459.\n",
      "Epoch 709. Training loss: 0.7754477858543396. Validation loss: 2.70684552192688.\n",
      "Epoch 710. Training loss: 0.7754446864128113. Validation loss: 2.7068450450897217.\n",
      "Epoch 711. Training loss: 0.7754414677619934. Validation loss: 2.7068448066711426.\n",
      "Epoch 712. Training loss: 0.7754383087158203. Validation loss: 2.7068443298339844.\n",
      "Epoch 713. Training loss: 0.7754352688789368. Validation loss: 2.7068440914154053.\n",
      "Epoch 714. Training loss: 0.7754321098327637. Validation loss: 2.706843376159668.\n",
      "Epoch 715. Training loss: 0.7754290699958801. Validation loss: 2.706843376159668.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 716. Training loss: 0.775425910949707. Validation loss: 2.7068428993225098.\n",
      "Epoch 717. Training loss: 0.7754228115081787. Validation loss: 2.7068424224853516.\n",
      "Epoch 718. Training loss: 0.7754197120666504. Validation loss: 2.7068419456481934.\n",
      "Epoch 719. Training loss: 0.7754166126251221. Validation loss: 2.7068417072296143.\n",
      "Epoch 720. Training loss: 0.7754133343696594. Validation loss: 2.706841468811035.\n",
      "Epoch 721. Training loss: 0.7754104137420654. Validation loss: 2.706840991973877.\n",
      "Epoch 722. Training loss: 0.7754073143005371. Validation loss: 2.7068402767181396.\n",
      "Epoch 723. Training loss: 0.7754040360450745. Validation loss: 2.7068402767181396.\n",
      "Epoch 724. Training loss: 0.7754010558128357. Validation loss: 2.7068397998809814.\n",
      "Epoch 725. Training loss: 0.7753979563713074. Validation loss: 2.7068395614624023.\n",
      "Epoch 726. Training loss: 0.7753947377204895. Validation loss: 2.706838846206665.\n",
      "Epoch 727. Training loss: 0.7753916382789612. Validation loss: 2.706838846206665.\n",
      "Epoch 728. Training loss: 0.7753884792327881. Validation loss: 2.706838369369507.\n",
      "Epoch 729. Training loss: 0.775385320186615. Validation loss: 2.7068376541137695.\n",
      "Epoch 730. Training loss: 0.7753822803497314. Validation loss: 2.7068374156951904.\n",
      "Epoch 731. Training loss: 0.7753791213035583. Validation loss: 2.706836700439453.\n",
      "Epoch 732. Training loss: 0.77537602186203. Validation loss: 2.706836462020874.\n",
      "Epoch 733. Training loss: 0.7753729224205017. Validation loss: 2.706836223602295.\n",
      "Epoch 734. Training loss: 0.7753698229789734. Validation loss: 2.7068357467651367.\n",
      "Epoch 735. Training loss: 0.7753667235374451. Validation loss: 2.7068355083465576.\n",
      "Epoch 736. Training loss: 0.7753635048866272. Validation loss: 2.7068352699279785.\n",
      "Epoch 737. Training loss: 0.7753605246543884. Validation loss: 2.706834316253662.\n",
      "Epoch 738. Training loss: 0.7753573060035706. Validation loss: 2.706834077835083.\n",
      "Epoch 739. Training loss: 0.7753542065620422. Validation loss: 2.706833600997925.\n",
      "Epoch 740. Training loss: 0.7753511071205139. Validation loss: 2.7068333625793457.\n",
      "Epoch 741. Training loss: 0.7753479480743408. Validation loss: 2.7068328857421875.\n",
      "Epoch 742. Training loss: 0.7753448486328125. Validation loss: 2.7068326473236084.\n",
      "Epoch 743. Training loss: 0.7753416895866394. Validation loss: 2.7068324089050293.\n",
      "Epoch 744. Training loss: 0.7753385901451111. Validation loss: 2.706831932067871.\n",
      "Epoch 745. Training loss: 0.7753354907035828. Validation loss: 2.706831216812134.\n",
      "Epoch 746. Training loss: 0.7753322720527649. Validation loss: 2.706831216812134.\n",
      "Epoch 747. Training loss: 0.7753291726112366. Validation loss: 2.7068307399749756.\n",
      "Epoch 748. Training loss: 0.7753261923789978. Validation loss: 2.7068305015563965.\n",
      "Epoch 749. Training loss: 0.7753229737281799. Validation loss: 2.7068300247192383.\n",
      "Epoch 750. Training loss: 0.7753197550773621. Validation loss: 2.70682954788208.\n",
      "Epoch 751. Training loss: 0.7753166556358337. Validation loss: 2.706829071044922.\n",
      "Epoch 752. Training loss: 0.7753135561943054. Validation loss: 2.7068285942077637.\n",
      "Epoch 753. Training loss: 0.7753103375434875. Validation loss: 2.7068283557891846.\n",
      "Epoch 754. Training loss: 0.7753072381019592. Validation loss: 2.7068281173706055.\n",
      "Epoch 755. Training loss: 0.7753041386604309. Validation loss: 2.706827402114868.\n",
      "Epoch 756. Training loss: 0.7753009796142578. Validation loss: 2.706827163696289.\n",
      "Epoch 757. Training loss: 0.7752978801727295. Validation loss: 2.70682692527771.\n",
      "Epoch 758. Training loss: 0.7752947807312012. Validation loss: 2.7068262100219727.\n",
      "Epoch 759. Training loss: 0.7752916216850281. Validation loss: 2.7068262100219727.\n",
      "Epoch 760. Training loss: 0.7752884030342102. Validation loss: 2.7068254947662354.\n",
      "Epoch 761. Training loss: 0.7752852439880371. Validation loss: 2.706825017929077.\n",
      "Epoch 762. Training loss: 0.7752821445465088. Validation loss: 2.706825017929077.\n",
      "Epoch 763. Training loss: 0.7752790451049805. Validation loss: 2.70682430267334.\n",
      "Epoch 764. Training loss: 0.7752758860588074. Validation loss: 2.7068240642547607.\n",
      "Epoch 765. Training loss: 0.775272786617279. Validation loss: 2.7068231105804443.\n",
      "Epoch 766. Training loss: 0.7752695083618164. Validation loss: 2.7068228721618652.\n",
      "Epoch 767. Training loss: 0.7752664089202881. Validation loss: 2.706822633743286.\n",
      "Epoch 768. Training loss: 0.775263249874115. Validation loss: 2.706822156906128.\n",
      "Epoch 769. Training loss: 0.7752601504325867. Validation loss: 2.706821918487549.\n",
      "Epoch 770. Training loss: 0.7752571105957031. Validation loss: 2.7068216800689697.\n",
      "Epoch 771. Training loss: 0.7752537727355957. Validation loss: 2.7068214416503906.\n",
      "Epoch 772. Training loss: 0.7752506732940674. Validation loss: 2.7068209648132324.\n",
      "Epoch 773. Training loss: 0.7752475738525391. Validation loss: 2.706820487976074.\n",
      "Epoch 774. Training loss: 0.775244414806366. Validation loss: 2.706820011138916.\n",
      "Epoch 775. Training loss: 0.7752411961555481. Validation loss: 2.706819534301758.\n",
      "Epoch 776. Training loss: 0.7752380967140198. Validation loss: 2.7068192958831787.\n",
      "Epoch 777. Training loss: 0.7752349376678467. Validation loss: 2.7068185806274414.\n",
      "Epoch 778. Training loss: 0.775231659412384. Validation loss: 2.706818103790283.\n",
      "Epoch 779. Training loss: 0.7752285599708557. Validation loss: 2.706818103790283.\n",
      "Epoch 780. Training loss: 0.7752255797386169. Validation loss: 2.706817626953125.\n",
      "Epoch 781. Training loss: 0.7752223014831543. Validation loss: 2.706817150115967.\n",
      "Epoch 782. Training loss: 0.7752192616462708. Validation loss: 2.7068169116973877.\n",
      "Epoch 783. Training loss: 0.7752160429954529. Validation loss: 2.7068164348602295.\n",
      "Epoch 784. Training loss: 0.7752129435539246. Validation loss: 2.706815719604492.\n",
      "Epoch 785. Training loss: 0.7752097249031067. Validation loss: 2.7068159580230713.\n",
      "Epoch 786. Training loss: 0.7752066254615784. Validation loss: 2.706815242767334.\n",
      "Epoch 787. Training loss: 0.7752034068107605. Validation loss: 2.706814765930176.\n",
      "Epoch 788. Training loss: 0.7752001881599426. Validation loss: 2.7068142890930176.\n",
      "Epoch 789. Training loss: 0.7751970887184143. Validation loss: 2.7068140506744385.\n",
      "Epoch 790. Training loss: 0.7751939296722412. Validation loss: 2.7068138122558594.\n",
      "Epoch 791. Training loss: 0.7751907706260681. Validation loss: 2.706813335418701.\n",
      "Epoch 792. Training loss: 0.7751876711845398. Validation loss: 2.706812858581543.\n",
      "Epoch 793. Training loss: 0.7751844525337219. Validation loss: 2.7068123817443848.\n",
      "Epoch 794. Training loss: 0.7751812934875488. Validation loss: 2.7068119049072266.\n",
      "Epoch 795. Training loss: 0.7751781344413757. Validation loss: 2.7068114280700684.\n",
      "Epoch 796. Training loss: 0.7751749157905579. Validation loss: 2.70681095123291.\n",
      "Epoch 797. Training loss: 0.7751719355583191. Validation loss: 2.706810712814331.\n",
      "Epoch 798. Training loss: 0.7751687169075012. Validation loss: 2.706810474395752.\n",
      "Epoch 799. Training loss: 0.7751654982566833. Validation loss: 2.7068099975585938.\n",
      "Epoch 800. Training loss: 0.7751622796058655. Validation loss: 2.7068097591400146.\n",
      "Epoch 801. Training loss: 0.7751591801643372. Validation loss: 2.7068090438842773.\n",
      "Epoch 802. Training loss: 0.7751560211181641. Validation loss: 2.7068088054656982.\n",
      "Epoch 803. Training loss: 0.775152862071991. Validation loss: 2.706808567047119.\n",
      "Epoch 804. Training loss: 0.7751496434211731. Validation loss: 2.706807851791382.\n",
      "Epoch 805. Training loss: 0.7751465439796448. Validation loss: 2.7068076133728027.\n",
      "Epoch 806. Training loss: 0.7751433253288269. Validation loss: 2.7068073749542236.\n",
      "Epoch 807. Training loss: 0.7751402258872986. Validation loss: 2.7068071365356445.\n",
      "Epoch 808. Training loss: 0.7751370072364807. Validation loss: 2.7068068981170654.\n",
      "Epoch 809. Training loss: 0.7751338481903076. Validation loss: 2.706806182861328.\n",
      "Epoch 810. Training loss: 0.7751306891441345. Validation loss: 2.70680570602417.\n",
      "Epoch 811. Training loss: 0.7751274108886719. Validation loss: 2.706805467605591.\n",
      "Epoch 812. Training loss: 0.7751243114471436. Validation loss: 2.7068049907684326.\n",
      "Epoch 813. Training loss: 0.7751212120056152. Validation loss: 2.7068045139312744.\n",
      "Epoch 814. Training loss: 0.7751179337501526. Validation loss: 2.706804037094116.\n",
      "Epoch 815. Training loss: 0.7751148343086243. Validation loss: 2.706803321838379.\n",
      "Epoch 816. Training loss: 0.7751116752624512. Validation loss: 2.706803321838379.\n",
      "Epoch 817. Training loss: 0.7751085162162781. Validation loss: 2.7068028450012207.\n",
      "Epoch 818. Training loss: 0.7751052975654602. Validation loss: 2.7068023681640625.\n",
      "Epoch 819. Training loss: 0.7751020789146423. Validation loss: 2.7068023681640625.\n",
      "Epoch 820. Training loss: 0.775098979473114. Validation loss: 2.706801414489746.\n",
      "Epoch 821. Training loss: 0.7750957608222961. Validation loss: 2.706801414489746.\n",
      "Epoch 822. Training loss: 0.775092601776123. Validation loss: 2.706800937652588.\n",
      "Epoch 823. Training loss: 0.7750893235206604. Validation loss: 2.7068004608154297.\n",
      "Epoch 824. Training loss: 0.7750863432884216. Validation loss: 2.7068002223968506.\n",
      "Epoch 825. Training loss: 0.775083065032959. Validation loss: 2.7067997455596924.\n",
      "Epoch 826. Training loss: 0.7750799059867859. Validation loss: 2.706799268722534.\n",
      "Epoch 827. Training loss: 0.775076687335968. Validation loss: 2.706798791885376.\n",
      "Epoch 828. Training loss: 0.7750734686851501. Validation loss: 2.706798791885376.\n",
      "Epoch 829. Training loss: 0.7750703692436218. Validation loss: 2.7067983150482178.\n",
      "Epoch 830. Training loss: 0.7750670909881592. Validation loss: 2.7067975997924805.\n",
      "Epoch 831. Training loss: 0.7750639319419861. Validation loss: 2.7067971229553223.\n",
      "Epoch 832. Training loss: 0.7750607132911682. Validation loss: 2.706796646118164.\n",
      "Epoch 833. Training loss: 0.7750576138496399. Validation loss: 2.706796407699585.\n",
      "Epoch 834. Training loss: 0.775054395198822. Validation loss: 2.706796169281006.\n",
      "Epoch 835. Training loss: 0.7750511169433594. Validation loss: 2.7067956924438477.\n",
      "Epoch 836. Training loss: 0.7750479578971863. Validation loss: 2.7067956924438477.\n",
      "Epoch 837. Training loss: 0.7750447392463684. Validation loss: 2.7067952156066895.\n",
      "Epoch 838. Training loss: 0.7750415802001953. Validation loss: 2.7067947387695312.\n",
      "Epoch 839. Training loss: 0.7750384211540222. Validation loss: 2.706794023513794.\n",
      "Epoch 840. Training loss: 0.7750352025032043. Validation loss: 2.7067935466766357.\n",
      "Epoch 841. Training loss: 0.7750320434570312. Validation loss: 2.7067933082580566.\n",
      "Epoch 842. Training loss: 0.7750288844108582. Validation loss: 2.7067930698394775.\n",
      "Epoch 843. Training loss: 0.7750256061553955. Validation loss: 2.7067925930023193.\n",
      "Epoch 844. Training loss: 0.7750225067138672. Validation loss: 2.706791877746582.\n",
      "Epoch 845. Training loss: 0.7750192284584045. Validation loss: 2.706791877746582.\n",
      "Epoch 846. Training loss: 0.7750160098075867. Validation loss: 2.706791400909424.\n",
      "Epoch 847. Training loss: 0.7750127911567688. Validation loss: 2.7067906856536865.\n",
      "Epoch 848. Training loss: 0.7750096321105957. Validation loss: 2.7067904472351074.\n",
      "Epoch 849. Training loss: 0.7750064730644226. Validation loss: 2.7067902088165283.\n",
      "Epoch 850. Training loss: 0.77500319480896. Validation loss: 2.706789493560791.\n",
      "Epoch 851. Training loss: 0.7750000357627869. Validation loss: 2.706789493560791.\n",
      "Epoch 852. Training loss: 0.774996817111969. Validation loss: 2.706789016723633.\n",
      "Epoch 853. Training loss: 0.7749936580657959. Validation loss: 2.7067883014678955.\n",
      "Epoch 854. Training loss: 0.7749905586242676. Validation loss: 2.7067880630493164.\n",
      "Epoch 855. Training loss: 0.7749872803688049. Validation loss: 2.706787586212158.\n",
      "Epoch 856. Training loss: 0.7749840617179871. Validation loss: 2.706787109375.\n",
      "Epoch 857. Training loss: 0.7749808430671692. Validation loss: 2.706787109375.\n",
      "Epoch 858. Training loss: 0.7749776244163513. Validation loss: 2.706786632537842.\n",
      "Epoch 859. Training loss: 0.7749744057655334. Validation loss: 2.7067861557006836.\n",
      "Epoch 860. Training loss: 0.7749712467193604. Validation loss: 2.7067856788635254.\n",
      "Epoch 861. Training loss: 0.7749679684638977. Validation loss: 2.706785202026367.\n",
      "Epoch 862. Training loss: 0.7749648094177246. Validation loss: 2.706784725189209.\n",
      "Epoch 863. Training loss: 0.7749616503715515. Validation loss: 2.70678448677063.\n",
      "Epoch 864. Training loss: 0.7749583721160889. Validation loss: 2.706784248352051.\n",
      "Epoch 865. Training loss: 0.7749552130699158. Validation loss: 2.7067837715148926.\n",
      "Epoch 866. Training loss: 0.7749519348144531. Validation loss: 2.7067832946777344.\n",
      "Epoch 867. Training loss: 0.77494877576828. Validation loss: 2.706782817840576.\n",
      "Epoch 868. Training loss: 0.7749455571174622. Validation loss: 2.706782341003418.\n",
      "Epoch 869. Training loss: 0.7749423980712891. Validation loss: 2.706782102584839.\n",
      "Epoch 870. Training loss: 0.774939239025116. Validation loss: 2.7067818641662598.\n",
      "Epoch 871. Training loss: 0.7749359607696533. Validation loss: 2.7067813873291016.\n",
      "Epoch 872. Training loss: 0.7749326825141907. Validation loss: 2.7067809104919434.\n",
      "Epoch 873. Training loss: 0.7749295234680176. Validation loss: 2.706780433654785.\n",
      "Epoch 874. Training loss: 0.7749262452125549. Validation loss: 2.706780195236206.\n",
      "Epoch 875. Training loss: 0.7749230861663818. Validation loss: 2.7067794799804688.\n",
      "Epoch 876. Training loss: 0.7749199271202087. Validation loss: 2.7067790031433105.\n",
      "Epoch 877. Training loss: 0.7749166488647461. Validation loss: 2.7067787647247314.\n",
      "Epoch 878. Training loss: 0.774913489818573. Validation loss: 2.7067785263061523.\n",
      "Epoch 879. Training loss: 0.7749102711677551. Validation loss: 2.706777572631836.\n",
      "Epoch 880. Training loss: 0.7749070525169373. Validation loss: 2.706777334213257.\n",
      "Epoch 881. Training loss: 0.7749037742614746. Validation loss: 2.7067766189575195.\n",
      "Epoch 882. Training loss: 0.7749004364013672. Validation loss: 2.7067763805389404.\n",
      "Epoch 883. Training loss: 0.7748973369598389. Validation loss: 2.7067759037017822.\n",
      "Epoch 884. Training loss: 0.7748940587043762. Validation loss: 2.706775188446045.\n",
      "Epoch 885. Training loss: 0.7748908400535583. Validation loss: 2.706774950027466.\n",
      "Epoch 886. Training loss: 0.77488774061203. Validation loss: 2.7067742347717285.\n",
      "Epoch 887. Training loss: 0.7748845219612122. Validation loss: 2.7067742347717285.\n",
      "Epoch 888. Training loss: 0.7748811841011047. Validation loss: 2.706773281097412.\n",
      "Epoch 889. Training loss: 0.7748780250549316. Validation loss: 2.706773042678833.\n",
      "Epoch 890. Training loss: 0.7748746871948242. Validation loss: 2.7067720890045166.\n",
      "Epoch 891. Training loss: 0.7748715281486511. Validation loss: 2.7067718505859375.\n",
      "Epoch 892. Training loss: 0.7748683094978333. Validation loss: 2.7067713737487793.\n",
      "Epoch 893. Training loss: 0.7748650908470154. Validation loss: 2.706770896911621.\n",
      "Epoch 894. Training loss: 0.7748618721961975. Validation loss: 2.706770420074463.\n",
      "Epoch 895. Training loss: 0.7748586535453796. Validation loss: 2.7067699432373047.\n",
      "Epoch 896. Training loss: 0.7748554348945618. Validation loss: 2.7067694664001465.\n",
      "Epoch 897. Training loss: 0.7748522162437439. Validation loss: 2.7067689895629883.\n",
      "Epoch 898. Training loss: 0.774848997592926. Validation loss: 2.70676851272583.\n",
      "Epoch 899. Training loss: 0.7748458385467529. Validation loss: 2.7067677974700928.\n",
      "Epoch 900. Training loss: 0.7748425006866455. Validation loss: 2.7067673206329346.\n",
      "Epoch 901. Training loss: 0.7748392224311829. Validation loss: 2.7067666053771973.\n",
      "Epoch 902. Training loss: 0.774836003780365. Validation loss: 2.706766366958618.\n",
      "Epoch 903. Training loss: 0.7748329043388367. Validation loss: 2.70676589012146.\n",
      "Epoch 904. Training loss: 0.774829626083374. Validation loss: 2.7067651748657227.\n",
      "Epoch 905. Training loss: 0.7748263478279114. Validation loss: 2.7067646980285645.\n",
      "Epoch 906. Training loss: 0.7748231887817383. Validation loss: 2.7067642211914062.\n",
      "Epoch 907. Training loss: 0.7748198509216309. Validation loss: 2.706763744354248.\n",
      "Epoch 908. Training loss: 0.7748166918754578. Validation loss: 2.70676326751709.\n",
      "Epoch 909. Training loss: 0.7748134732246399. Validation loss: 2.7067627906799316.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 910. Training loss: 0.774810254573822. Validation loss: 2.7067620754241943.\n",
      "Epoch 911. Training loss: 0.7748069763183594. Validation loss: 2.7067618370056152.\n",
      "Epoch 912. Training loss: 0.7748036980628967. Validation loss: 2.706761360168457.\n",
      "Epoch 913. Training loss: 0.7748003602027893. Validation loss: 2.706760883331299.\n",
      "Epoch 914. Training loss: 0.774797260761261. Validation loss: 2.7067604064941406.\n",
      "Epoch 915. Training loss: 0.7747940421104431. Validation loss: 2.7067599296569824.\n",
      "Epoch 916. Training loss: 0.7747907042503357. Validation loss: 2.706759214401245.\n",
      "Epoch 917. Training loss: 0.7747876048088074. Validation loss: 2.706758975982666.\n",
      "Epoch 918. Training loss: 0.7747843265533447. Validation loss: 2.7067582607269287.\n",
      "Epoch 919. Training loss: 0.7747810482978821. Validation loss: 2.7067575454711914.\n",
      "Epoch 920. Training loss: 0.7747777104377747. Validation loss: 2.7067573070526123.\n",
      "Epoch 921. Training loss: 0.7747745513916016. Validation loss: 2.706756591796875.\n",
      "Epoch 922. Training loss: 0.7747712731361389. Validation loss: 2.706756353378296.\n",
      "Epoch 923. Training loss: 0.7747681140899658. Validation loss: 2.7067556381225586.\n",
      "Epoch 924. Training loss: 0.7747648358345032. Validation loss: 2.7067549228668213.\n",
      "Epoch 925. Training loss: 0.7747616767883301. Validation loss: 2.706754684448242.\n",
      "Epoch 926. Training loss: 0.7747583985328674. Validation loss: 2.706753969192505.\n",
      "Epoch 927. Training loss: 0.7747551798820496. Validation loss: 2.706753730773926.\n",
      "Epoch 928. Training loss: 0.7747518420219421. Validation loss: 2.7067532539367676.\n",
      "Epoch 929. Training loss: 0.7747485637664795. Validation loss: 2.7067525386810303.\n",
      "Epoch 930. Training loss: 0.7747454643249512. Validation loss: 2.706752061843872.\n",
      "Epoch 931. Training loss: 0.7747421264648438. Validation loss: 2.706751585006714.\n",
      "Epoch 932. Training loss: 0.7747388482093811. Validation loss: 2.7067511081695557.\n",
      "Epoch 933. Training loss: 0.7747356295585632. Validation loss: 2.7067506313323975.\n",
      "Epoch 934. Training loss: 0.7747324109077454. Validation loss: 2.70674991607666.\n",
      "Epoch 935. Training loss: 0.7747290730476379. Validation loss: 2.706749677658081.\n",
      "Epoch 936. Training loss: 0.7747259140014648. Validation loss: 2.706749200820923.\n",
      "Epoch 937. Training loss: 0.7747226357460022. Validation loss: 2.7067487239837646.\n",
      "Epoch 938. Training loss: 0.7747194170951843. Validation loss: 2.7067480087280273.\n",
      "Epoch 939. Training loss: 0.7747160792350769. Validation loss: 2.7067480087280273.\n",
      "Epoch 940. Training loss: 0.774712860584259. Validation loss: 2.706747055053711.\n",
      "Epoch 941. Training loss: 0.7747096419334412. Validation loss: 2.7067465782165527.\n",
      "Epoch 942. Training loss: 0.7747063636779785. Validation loss: 2.7067463397979736.\n",
      "Epoch 943. Training loss: 0.7747032046318054. Validation loss: 2.7067456245422363.\n",
      "Epoch 944. Training loss: 0.774699866771698. Validation loss: 2.706745147705078.\n",
      "Epoch 945. Training loss: 0.7746965885162354. Validation loss: 2.70674467086792.\n",
      "Epoch 946. Training loss: 0.7746934294700623. Validation loss: 2.7067441940307617.\n",
      "Epoch 947. Training loss: 0.7746900916099548. Validation loss: 2.7067434787750244.\n",
      "Epoch 948. Training loss: 0.7746868133544922. Validation loss: 2.7067432403564453.\n",
      "Epoch 949. Training loss: 0.7746836543083191. Validation loss: 2.706742763519287.\n",
      "Epoch 950. Training loss: 0.7746803164482117. Validation loss: 2.70674204826355.\n",
      "Epoch 951. Training loss: 0.774677038192749. Validation loss: 2.7067418098449707.\n",
      "Epoch 952. Training loss: 0.7746737599372864. Validation loss: 2.7067413330078125.\n",
      "Epoch 953. Training loss: 0.7746705412864685. Validation loss: 2.706740617752075.\n",
      "Epoch 954. Training loss: 0.7746673226356506. Validation loss: 2.706740140914917.\n",
      "Epoch 955. Training loss: 0.7746641039848328. Validation loss: 2.706739902496338.\n",
      "Epoch 956. Training loss: 0.7746607661247253. Validation loss: 2.7067389488220215.\n",
      "Epoch 957. Training loss: 0.7746575474739075. Validation loss: 2.7067384719848633.\n",
      "Epoch 958. Training loss: 0.7746543288230896. Validation loss: 2.706737995147705.\n",
      "Epoch 959. Training loss: 0.7746509909629822. Validation loss: 2.706737756729126.\n",
      "Epoch 960. Training loss: 0.7746477127075195. Validation loss: 2.7067370414733887.\n",
      "Epoch 961. Training loss: 0.7746445536613464. Validation loss: 2.7067365646362305.\n",
      "Epoch 962. Training loss: 0.7746412754058838. Validation loss: 2.7067360877990723.\n",
      "Epoch 963. Training loss: 0.7746379971504211. Validation loss: 2.706735610961914.\n",
      "Epoch 964. Training loss: 0.7746346592903137. Validation loss: 2.7067348957061768.\n",
      "Epoch 965. Training loss: 0.7746314406394958. Validation loss: 2.7067346572875977.\n",
      "Epoch 966. Training loss: 0.7746281623840332. Validation loss: 2.7067341804504395.\n",
      "Epoch 967. Training loss: 0.7746248245239258. Validation loss: 2.706733226776123.\n",
      "Epoch 968. Training loss: 0.7746216654777527. Validation loss: 2.706733226776123.\n",
      "Epoch 969. Training loss: 0.77461838722229. Validation loss: 2.706732749938965.\n",
      "Epoch 970. Training loss: 0.7746150493621826. Validation loss: 2.7067320346832275.\n",
      "Epoch 971. Training loss: 0.7746118903160095. Validation loss: 2.7067313194274902.\n",
      "Epoch 972. Training loss: 0.7746085524559021. Validation loss: 2.706730842590332.\n",
      "Epoch 973. Training loss: 0.7746053338050842. Validation loss: 2.706730365753174.\n",
      "Epoch 974. Training loss: 0.7746019959449768. Validation loss: 2.7067298889160156.\n",
      "Epoch 975. Training loss: 0.7745987772941589. Validation loss: 2.7067294120788574.\n",
      "Epoch 976. Training loss: 0.7745954394340515. Validation loss: 2.706728935241699.\n",
      "Epoch 977. Training loss: 0.7745922207832336. Validation loss: 2.706727981567383.\n",
      "Epoch 978. Training loss: 0.7745888829231262. Validation loss: 2.7067277431488037.\n",
      "Epoch 979. Training loss: 0.7745855450630188. Validation loss: 2.7067275047302246.\n",
      "Epoch 980. Training loss: 0.7745823264122009. Validation loss: 2.7067267894744873.\n",
      "Epoch 981. Training loss: 0.7745791077613831. Validation loss: 2.706726312637329.\n",
      "Epoch 982. Training loss: 0.7745757699012756. Validation loss: 2.706725597381592.\n",
      "Epoch 983. Training loss: 0.7745725512504578. Validation loss: 2.7067253589630127.\n",
      "Epoch 984. Training loss: 0.7745692133903503. Validation loss: 2.7067248821258545.\n",
      "Epoch 985. Training loss: 0.7745659351348877. Validation loss: 2.706724166870117.\n",
      "Epoch 986. Training loss: 0.774562656879425. Validation loss: 2.706723928451538.\n",
      "Epoch 987. Training loss: 0.7745594382286072. Validation loss: 2.706723213195801.\n",
      "Epoch 988. Training loss: 0.7745561599731445. Validation loss: 2.7067224979400635.\n",
      "Epoch 989. Training loss: 0.7745528817176819. Validation loss: 2.7067222595214844.\n",
      "Epoch 990. Training loss: 0.774549663066864. Validation loss: 2.706721782684326.\n",
      "Epoch 991. Training loss: 0.7745463252067566. Validation loss: 2.706721067428589.\n",
      "Epoch 992. Training loss: 0.7745429873466492. Validation loss: 2.7067205905914307.\n",
      "Epoch 993. Training loss: 0.7745397090911865. Validation loss: 2.7067198753356934.\n",
      "Epoch 994. Training loss: 0.7745364308357239. Validation loss: 2.7067196369171143.\n",
      "Epoch 995. Training loss: 0.7745330929756165. Validation loss: 2.706719398498535.\n",
      "Epoch 996. Training loss: 0.7745298743247986. Validation loss: 2.706718921661377.\n",
      "Epoch 997. Training loss: 0.7745265960693359. Validation loss: 2.7067182064056396.\n",
      "Epoch 998. Training loss: 0.7745232582092285. Validation loss: 2.7067177295684814.\n",
      "Epoch 999. Training loss: 0.7745199203491211. Validation loss: 2.706717014312744.\n",
      "Epoch 1000. Training loss: 0.774516761302948. Validation loss: 2.706716537475586.\n",
      "Epoch 1001. Training loss: 0.7745134234428406. Validation loss: 2.7067160606384277.\n",
      "Epoch 1002. Training loss: 0.7745101451873779. Validation loss: 2.7067155838012695.\n",
      "Epoch 1003. Training loss: 0.7745068073272705. Validation loss: 2.7067151069641113.\n",
      "Epoch 1004. Training loss: 0.7745035290718079. Validation loss: 2.706714630126953.\n",
      "Epoch 1005. Training loss: 0.77450031042099. Validation loss: 2.706713914871216.\n",
      "Epoch 1006. Training loss: 0.7744969725608826. Validation loss: 2.7067131996154785.\n",
      "Epoch 1007. Training loss: 0.7744935154914856. Validation loss: 2.7067127227783203.\n",
      "Epoch 1008. Training loss: 0.7744903564453125. Validation loss: 2.706712007522583.\n",
      "Epoch 1009. Training loss: 0.7744870781898499. Validation loss: 2.706711769104004.\n",
      "Epoch 1010. Training loss: 0.774483859539032. Validation loss: 2.7067108154296875.\n",
      "Epoch 1011. Training loss: 0.7744805216789246. Validation loss: 2.7067105770111084.\n",
      "Epoch 1012. Training loss: 0.7744770646095276. Validation loss: 2.706709384918213.\n",
      "Epoch 1013. Training loss: 0.7744738459587097. Validation loss: 2.7067086696624756.\n",
      "Epoch 1014. Training loss: 0.7744703888893127. Validation loss: 2.7067081928253174.\n",
      "Epoch 1015. Training loss: 0.7744671702384949. Validation loss: 2.706707715988159.\n",
      "Epoch 1016. Training loss: 0.7744638323783875. Validation loss: 2.706707000732422.\n",
      "Epoch 1017. Training loss: 0.77446049451828. Validation loss: 2.7067065238952637.\n",
      "Epoch 1018. Training loss: 0.7744571566581726. Validation loss: 2.7067058086395264.\n",
      "Epoch 1019. Training loss: 0.77445387840271. Validation loss: 2.706705093383789.\n",
      "Epoch 1020. Training loss: 0.7744505405426025. Validation loss: 2.706704616546631.\n",
      "Epoch 1021. Training loss: 0.7744472026824951. Validation loss: 2.7067041397094727.\n",
      "Epoch 1022. Training loss: 0.7744438648223877. Validation loss: 2.7067036628723145.\n",
      "Epoch 1023. Training loss: 0.7744405269622803. Validation loss: 2.706702947616577.\n",
      "Epoch 1024. Training loss: 0.7744371294975281. Validation loss: 2.7067019939422607.\n",
      "Epoch 1025. Training loss: 0.7744338512420654. Validation loss: 2.7067015171051025.\n",
      "Epoch 1026. Training loss: 0.774430513381958. Validation loss: 2.7067008018493652.\n",
      "Epoch 1027. Training loss: 0.7744271159172058. Validation loss: 2.706700086593628.\n",
      "Epoch 1028. Training loss: 0.7744238376617432. Validation loss: 2.7066996097564697.\n",
      "Epoch 1029. Training loss: 0.7744204998016357. Validation loss: 2.7066988945007324.\n",
      "Epoch 1030. Training loss: 0.7744171023368835. Validation loss: 2.706698417663574.\n",
      "Epoch 1031. Training loss: 0.7744138240814209. Validation loss: 2.706697702407837.\n",
      "Epoch 1032. Training loss: 0.7744103074073792. Validation loss: 2.7066969871520996.\n",
      "Epoch 1033. Training loss: 0.774407148361206. Validation loss: 2.7066965103149414.\n",
      "Epoch 1034. Training loss: 0.7744036316871643. Validation loss: 2.706695556640625.\n",
      "Epoch 1035. Training loss: 0.7744004130363464. Validation loss: 2.706695079803467.\n",
      "Epoch 1036. Training loss: 0.7743969559669495. Validation loss: 2.7066946029663086.\n",
      "Epoch 1037. Training loss: 0.774393618106842. Validation loss: 2.706693649291992.\n",
      "Epoch 1038. Training loss: 0.7743902802467346. Validation loss: 2.706693172454834.\n",
      "Epoch 1039. Training loss: 0.7743868827819824. Validation loss: 2.7066922187805176.\n",
      "Epoch 1040. Training loss: 0.7743836045265198. Validation loss: 2.7066919803619385.\n",
      "Epoch 1041. Training loss: 0.7743802666664124. Validation loss: 2.706691026687622.\n",
      "Epoch 1042. Training loss: 0.7743768692016602. Validation loss: 2.706690549850464.\n",
      "Epoch 1043. Training loss: 0.7743735313415527. Validation loss: 2.7066900730133057.\n",
      "Epoch 1044. Training loss: 0.7743701934814453. Validation loss: 2.7066893577575684.\n",
      "Epoch 1045. Training loss: 0.7743668556213379. Validation loss: 2.706688642501831.\n",
      "Epoch 1046. Training loss: 0.7743634581565857. Validation loss: 2.706688165664673.\n",
      "Epoch 1047. Training loss: 0.7743601202964783. Validation loss: 2.7066872119903564.\n",
      "Epoch 1048. Training loss: 0.7743567824363708. Validation loss: 2.7066869735717773.\n",
      "Epoch 1049. Training loss: 0.7743533253669739. Validation loss: 2.70668625831604.\n",
      "Epoch 1050. Training loss: 0.774350106716156. Validation loss: 2.7066853046417236.\n",
      "Epoch 1051. Training loss: 0.774346649646759. Validation loss: 2.7066848278045654.\n",
      "Epoch 1052. Training loss: 0.7743432521820068. Validation loss: 2.706684112548828.\n",
      "Epoch 1053. Training loss: 0.7743399143218994. Validation loss: 2.70668363571167.\n",
      "Epoch 1054. Training loss: 0.774336576461792. Validation loss: 2.7066831588745117.\n",
      "Epoch 1055. Training loss: 0.7743332386016846. Validation loss: 2.7066822052001953.\n",
      "Epoch 1056. Training loss: 0.7743299007415771. Validation loss: 2.706681728363037.\n",
      "Epoch 1057. Training loss: 0.774326503276825. Validation loss: 2.7066810131073.\n",
      "Epoch 1058. Training loss: 0.7743231654167175. Validation loss: 2.7066802978515625.\n",
      "Epoch 1059. Training loss: 0.7743197083473206. Validation loss: 2.7066798210144043.\n",
      "Epoch 1060. Training loss: 0.7743163704872131. Validation loss: 2.706679105758667.\n",
      "Epoch 1061. Training loss: 0.7743129730224609. Validation loss: 2.7066781520843506.\n",
      "Epoch 1062. Training loss: 0.7743096351623535. Validation loss: 2.7066776752471924.\n",
      "Epoch 1063. Training loss: 0.7743062973022461. Validation loss: 2.7066774368286133.\n",
      "Epoch 1064. Training loss: 0.7743027806282043. Validation loss: 2.706676483154297.\n",
      "Epoch 1065. Training loss: 0.7742994427680969. Validation loss: 2.7066760063171387.\n",
      "Epoch 1066. Training loss: 0.7742961049079895. Validation loss: 2.7066752910614014.\n",
      "Epoch 1067. Training loss: 0.7742927074432373. Validation loss: 2.706674814224243.\n",
      "Epoch 1068. Training loss: 0.7742893695831299. Validation loss: 2.706674098968506.\n",
      "Epoch 1069. Training loss: 0.7742860317230225. Validation loss: 2.7066733837127686.\n",
      "Epoch 1070. Training loss: 0.7742826342582703. Validation loss: 2.7066726684570312.\n",
      "Epoch 1071. Training loss: 0.7742792963981628. Validation loss: 2.706671953201294.\n",
      "Epoch 1072. Training loss: 0.7742759585380554. Validation loss: 2.7066714763641357.\n",
      "Epoch 1073. Training loss: 0.7742725014686584. Validation loss: 2.7066705226898193.\n",
      "Epoch 1074. Training loss: 0.7742691040039062. Validation loss: 2.706669807434082.\n",
      "Epoch 1075. Training loss: 0.7742657661437988. Validation loss: 2.7066690921783447.\n",
      "Epoch 1076. Training loss: 0.7742623686790466. Validation loss: 2.7066686153411865.\n",
      "Epoch 1077. Training loss: 0.7742590308189392. Validation loss: 2.706667900085449.\n",
      "Epoch 1078. Training loss: 0.7742555737495422. Validation loss: 2.706667184829712.\n",
      "Epoch 1079. Training loss: 0.7742522358894348. Validation loss: 2.706666946411133.\n",
      "Epoch 1080. Training loss: 0.7742488980293274. Validation loss: 2.7066659927368164.\n",
      "Epoch 1081. Training loss: 0.7742455005645752. Validation loss: 2.706665277481079.\n",
      "Epoch 1082. Training loss: 0.7742419838905334. Validation loss: 2.7066650390625.\n",
      "Epoch 1083. Training loss: 0.774238646030426. Validation loss: 2.706664562225342.\n",
      "Epoch 1084. Training loss: 0.7742353081703186. Validation loss: 2.7066636085510254.\n",
      "Epoch 1085. Training loss: 0.7742319107055664. Validation loss: 2.706663131713867.\n",
      "Epoch 1086. Training loss: 0.7742285132408142. Validation loss: 2.706662178039551.\n",
      "Epoch 1087. Training loss: 0.7742252349853516. Validation loss: 2.7066617012023926.\n",
      "Epoch 1088. Training loss: 0.7742217183113098. Validation loss: 2.7066612243652344.\n",
      "Epoch 1089. Training loss: 0.7742183208465576. Validation loss: 2.706660270690918.\n",
      "Epoch 1090. Training loss: 0.7742149829864502. Validation loss: 2.7066597938537598.\n",
      "Epoch 1091. Training loss: 0.7742116451263428. Validation loss: 2.7066593170166016.\n",
      "Epoch 1092. Training loss: 0.7742082476615906. Validation loss: 2.706658363342285.\n",
      "Epoch 1093. Training loss: 0.7742047905921936. Validation loss: 2.706657648086548.\n",
      "Epoch 1094. Training loss: 0.7742014527320862. Validation loss: 2.7066569328308105.\n",
      "Epoch 1095. Training loss: 0.7741979956626892. Validation loss: 2.7066564559936523.\n",
      "Epoch 1096. Training loss: 0.7741946578025818. Validation loss: 2.706655740737915.\n",
      "Epoch 1097. Training loss: 0.7741913199424744. Validation loss: 2.706655263900757.\n",
      "Epoch 1098. Training loss: 0.7741878628730774. Validation loss: 2.7066543102264404.\n",
      "Epoch 1099. Training loss: 0.7741844654083252. Validation loss: 2.7066538333892822.\n",
      "Epoch 1100. Training loss: 0.774181067943573. Validation loss: 2.706653118133545.\n",
      "Epoch 1101. Training loss: 0.7741777300834656. Validation loss: 2.7066524028778076.\n",
      "Epoch 1102. Training loss: 0.7741742730140686. Validation loss: 2.7066519260406494.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1103. Training loss: 0.7741708755493164. Validation loss: 2.706651210784912.\n",
      "Epoch 1104. Training loss: 0.774167537689209. Validation loss: 2.706650495529175.\n",
      "Epoch 1105. Training loss: 0.7741641402244568. Validation loss: 2.7066500186920166.\n",
      "Epoch 1106. Training loss: 0.7741608023643494. Validation loss: 2.7066493034362793.\n",
      "Epoch 1107. Training loss: 0.7741573452949524. Validation loss: 2.706648588180542.\n",
      "Epoch 1108. Training loss: 0.7741539478302002. Validation loss: 2.7066478729248047.\n",
      "Epoch 1109. Training loss: 0.774150550365448. Validation loss: 2.7066471576690674.\n",
      "Epoch 1110. Training loss: 0.774147093296051. Validation loss: 2.706646680831909.\n",
      "Epoch 1111. Training loss: 0.7741437554359436. Validation loss: 2.706645965576172.\n",
      "Epoch 1112. Training loss: 0.7741403579711914. Validation loss: 2.7066452503204346.\n",
      "Epoch 1113. Training loss: 0.7741369605064392. Validation loss: 2.706644296646118.\n",
      "Epoch 1114. Training loss: 0.7741336226463318. Validation loss: 2.706644058227539.\n",
      "Epoch 1115. Training loss: 0.7741300463676453. Validation loss: 2.7066431045532227.\n",
      "Epoch 1116. Training loss: 0.7741267681121826. Validation loss: 2.7066426277160645.\n",
      "Epoch 1117. Training loss: 0.7741233706474304. Validation loss: 2.7066421508789062.\n",
      "Epoch 1118. Training loss: 0.774120032787323. Validation loss: 2.706641435623169.\n",
      "Epoch 1119. Training loss: 0.7741165161132812. Validation loss: 2.7066407203674316.\n",
      "Epoch 1120. Training loss: 0.774113118648529. Validation loss: 2.7066397666931152.\n",
      "Epoch 1121. Training loss: 0.7741096615791321. Validation loss: 2.706639289855957.\n",
      "Epoch 1122. Training loss: 0.7741062641143799. Validation loss: 2.7066385746002197.\n",
      "Epoch 1123. Training loss: 0.7741029262542725. Validation loss: 2.7066380977630615.\n",
      "Epoch 1124. Training loss: 0.7740995287895203. Validation loss: 2.7066376209259033.\n",
      "Epoch 1125. Training loss: 0.7740960717201233. Validation loss: 2.706636428833008.\n",
      "Epoch 1126. Training loss: 0.7740927338600159. Validation loss: 2.7066359519958496.\n",
      "Epoch 1127. Training loss: 0.7740892767906189. Validation loss: 2.7066352367401123.\n",
      "Epoch 1128. Training loss: 0.7740859389305115. Validation loss: 2.706634998321533.\n",
      "Epoch 1129. Training loss: 0.7740824222564697. Validation loss: 2.706634044647217.\n",
      "Epoch 1130. Training loss: 0.7740790843963623. Validation loss: 2.7066335678100586.\n",
      "Epoch 1131. Training loss: 0.7740755677223206. Validation loss: 2.7066328525543213.\n",
      "Epoch 1132. Training loss: 0.7740721702575684. Validation loss: 2.706632137298584.\n",
      "Epoch 1133. Training loss: 0.7740688323974609. Validation loss: 2.706631660461426.\n",
      "Epoch 1134. Training loss: 0.7740653157234192. Validation loss: 2.7066309452056885.\n",
      "Epoch 1135. Training loss: 0.7740619778633118. Validation loss: 2.706630229949951.\n",
      "Epoch 1136. Training loss: 0.7740585207939148. Validation loss: 2.706629753112793.\n",
      "Epoch 1137. Training loss: 0.7740551829338074. Validation loss: 2.7066287994384766.\n",
      "Epoch 1138. Training loss: 0.7740516662597656. Validation loss: 2.7066283226013184.\n",
      "Epoch 1139. Training loss: 0.7740483283996582. Validation loss: 2.706627130508423.\n",
      "Epoch 1140. Training loss: 0.774044930934906. Validation loss: 2.7066268920898438.\n",
      "Epoch 1141. Training loss: 0.7740415930747986. Validation loss: 2.7066261768341064.\n",
      "Epoch 1142. Training loss: 0.7740380764007568. Validation loss: 2.7066256999969482.\n",
      "Epoch 1143. Training loss: 0.7740346789360046. Validation loss: 2.706624746322632.\n",
      "Epoch 1144. Training loss: 0.7740312218666077. Validation loss: 2.7066240310668945.\n",
      "Epoch 1145. Training loss: 0.7740278244018555. Validation loss: 2.7066235542297363.\n",
      "Epoch 1146. Training loss: 0.7740243077278137. Validation loss: 2.706622838973999.\n",
      "Epoch 1147. Training loss: 0.7740209102630615. Validation loss: 2.7066221237182617.\n",
      "Epoch 1148. Training loss: 0.7740175127983093. Validation loss: 2.7066216468811035.\n",
      "Epoch 1149. Training loss: 0.7740142345428467. Validation loss: 2.706620931625366.\n",
      "Epoch 1150. Training loss: 0.7740106582641602. Validation loss: 2.706620693206787.\n",
      "Epoch 1151. Training loss: 0.7740073204040527. Validation loss: 2.7066195011138916.\n",
      "Epoch 1152. Training loss: 0.7740039229393005. Validation loss: 2.7066192626953125.\n",
      "Epoch 1153. Training loss: 0.7740004062652588. Validation loss: 2.706618309020996.\n",
      "Epoch 1154. Training loss: 0.7739970684051514. Validation loss: 2.706617593765259.\n",
      "Epoch 1155. Training loss: 0.7739934921264648. Validation loss: 2.7066168785095215.\n",
      "Epoch 1156. Training loss: 0.7739901542663574. Validation loss: 2.706616163253784.\n",
      "Epoch 1157. Training loss: 0.7739867568016052. Validation loss: 2.706615447998047.\n",
      "Epoch 1158. Training loss: 0.7739832997322083. Validation loss: 2.7066147327423096.\n",
      "Epoch 1159. Training loss: 0.7739798426628113. Validation loss: 2.7066140174865723.\n",
      "Epoch 1160. Training loss: 0.7739765048027039. Validation loss: 2.706613779067993.\n",
      "Epoch 1161. Training loss: 0.7739729881286621. Validation loss: 2.706613063812256.\n",
      "Epoch 1162. Training loss: 0.7739694714546204. Validation loss: 2.7066121101379395.\n",
      "Epoch 1163. Training loss: 0.7739661335945129. Validation loss: 2.7066116333007812.\n",
      "Epoch 1164. Training loss: 0.7739627361297607. Validation loss: 2.706610918045044.\n",
      "Epoch 1165. Training loss: 0.773959219455719. Validation loss: 2.7066104412078857.\n",
      "Epoch 1166. Training loss: 0.7739558219909668. Validation loss: 2.7066097259521484.\n",
      "Epoch 1167. Training loss: 0.7739524245262146. Validation loss: 2.706608772277832.\n",
      "Epoch 1168. Training loss: 0.7739489674568176. Validation loss: 2.706608295440674.\n",
      "Epoch 1169. Training loss: 0.7739455699920654. Validation loss: 2.7066073417663574.\n",
      "Epoch 1170. Training loss: 0.7739421725273132. Validation loss: 2.706606864929199.\n",
      "Epoch 1171. Training loss: 0.7739388346672058. Validation loss: 2.706606388092041.\n",
      "Epoch 1172. Training loss: 0.7739354968070984. Validation loss: 2.7066054344177246.\n",
      "Epoch 1173. Training loss: 0.7739322185516357. Validation loss: 2.7066049575805664.\n",
      "Epoch 1174. Training loss: 0.7739288210868835. Validation loss: 2.706604242324829.\n",
      "Epoch 1175. Training loss: 0.7739254832267761. Validation loss: 2.706603527069092.\n",
      "Epoch 1176. Training loss: 0.7739221453666687. Validation loss: 2.7066030502319336.\n",
      "Epoch 1177. Training loss: 0.7739188075065613. Validation loss: 2.706602096557617.\n",
      "Epoch 1178. Training loss: 0.7739155292510986. Validation loss: 2.706601619720459.\n",
      "Epoch 1179. Training loss: 0.7739121317863464. Validation loss: 2.706601142883301.\n",
      "Epoch 1180. Training loss: 0.773908793926239. Validation loss: 2.7066001892089844.\n",
      "Epoch 1181. Training loss: 0.7739054560661316. Validation loss: 2.706599712371826.\n",
      "Epoch 1182. Training loss: 0.7739021182060242. Validation loss: 2.706599235534668.\n",
      "Epoch 1183. Training loss: 0.7738987803459167. Validation loss: 2.7065982818603516.\n",
      "Epoch 1184. Training loss: 0.7738954424858093. Validation loss: 2.7065978050231934.\n",
      "Epoch 1185. Training loss: 0.7738921642303467. Validation loss: 2.706596851348877.\n",
      "Epoch 1186. Training loss: 0.7738888263702393. Validation loss: 2.7065958976745605.\n",
      "Epoch 1187. Training loss: 0.7738854289054871. Validation loss: 2.7065958976745605.\n",
      "Epoch 1188. Training loss: 0.7738820910453796. Validation loss: 2.7065951824188232.\n",
      "Epoch 1189. Training loss: 0.773878812789917. Validation loss: 2.706594467163086.\n",
      "Epoch 1190. Training loss: 0.7738754749298096. Validation loss: 2.7065935134887695.\n",
      "Epoch 1191. Training loss: 0.7738721966743469. Validation loss: 2.7065930366516113.\n",
      "Epoch 1192. Training loss: 0.7738688588142395. Validation loss: 2.706592559814453.\n",
      "Epoch 1193. Training loss: 0.7738655209541321. Validation loss: 2.706592082977295.\n",
      "Epoch 1194. Training loss: 0.7738621234893799. Validation loss: 2.7065911293029785.\n",
      "Epoch 1195. Training loss: 0.7738588452339172. Validation loss: 2.7065906524658203.\n",
      "Epoch 1196. Training loss: 0.7738555073738098. Validation loss: 2.706589698791504.\n",
      "Epoch 1197. Training loss: 0.7738521695137024. Validation loss: 2.7065889835357666.\n",
      "Epoch 1198. Training loss: 0.773848831653595. Validation loss: 2.7065882682800293.\n",
      "Epoch 1199. Training loss: 0.7738456130027771. Validation loss: 2.706587791442871.\n",
      "Epoch 1200. Training loss: 0.7738422751426697. Validation loss: 2.706587076187134.\n",
      "Epoch 1201. Training loss: 0.7738388180732727. Validation loss: 2.7065863609313965.\n",
      "Epoch 1202. Training loss: 0.7738356590270996. Validation loss: 2.7065858840942383.\n",
      "Epoch 1203. Training loss: 0.7738321423530579. Validation loss: 2.70658540725708.\n",
      "Epoch 1204. Training loss: 0.77382892370224. Validation loss: 2.7065844535827637.\n",
      "Epoch 1205. Training loss: 0.7738255858421326. Validation loss: 2.7065839767456055.\n",
      "Epoch 1206. Training loss: 0.7738222479820251. Validation loss: 2.706583023071289.\n",
      "Epoch 1207. Training loss: 0.7738189101219177. Validation loss: 2.706582546234131.\n",
      "Epoch 1208. Training loss: 0.7738155722618103. Validation loss: 2.7065818309783936.\n",
      "Epoch 1209. Training loss: 0.7738122940063477. Validation loss: 2.7065811157226562.\n",
      "Epoch 1210. Training loss: 0.7738088965415955. Validation loss: 2.706580400466919.\n",
      "Epoch 1211. Training loss: 0.773805558681488. Validation loss: 2.7065799236297607.\n",
      "Epoch 1212. Training loss: 0.7738022208213806. Validation loss: 2.7065792083740234.\n",
      "Epoch 1213. Training loss: 0.773798942565918. Validation loss: 2.7065787315368652.\n",
      "Epoch 1214. Training loss: 0.7737956047058105. Validation loss: 2.706578016281128.\n",
      "Epoch 1215. Training loss: 0.7737922072410583. Validation loss: 2.7065773010253906.\n",
      "Epoch 1216. Training loss: 0.7737889289855957. Validation loss: 2.706576347351074.\n",
      "Epoch 1217. Training loss: 0.7737855911254883. Validation loss: 2.706575870513916.\n",
      "Epoch 1218. Training loss: 0.7737821936607361. Validation loss: 2.7065751552581787.\n",
      "Epoch 1219. Training loss: 0.7737788558006287. Validation loss: 2.7065744400024414.\n",
      "Epoch 1220. Training loss: 0.773775577545166. Validation loss: 2.706573724746704.\n",
      "Epoch 1221. Training loss: 0.7737722396850586. Validation loss: 2.706573247909546.\n",
      "Epoch 1222. Training loss: 0.7737689018249512. Validation loss: 2.7065725326538086.\n",
      "Epoch 1223. Training loss: 0.7737655639648438. Validation loss: 2.7065720558166504.\n",
      "Epoch 1224. Training loss: 0.7737622261047363. Validation loss: 2.706571340560913.\n",
      "Epoch 1225. Training loss: 0.7737589478492737. Validation loss: 2.706570625305176.\n",
      "Epoch 1226. Training loss: 0.7737556099891663. Validation loss: 2.7065699100494385.\n",
      "Epoch 1227. Training loss: 0.7737522721290588. Validation loss: 2.706569194793701.\n",
      "Epoch 1228. Training loss: 0.773749053478241. Validation loss: 2.706568479537964.\n",
      "Epoch 1229. Training loss: 0.7737457156181335. Validation loss: 2.7065680027008057.\n",
      "Epoch 1230. Training loss: 0.7737423777580261. Validation loss: 2.7065675258636475.\n",
      "Epoch 1231. Training loss: 0.7737390398979187. Validation loss: 2.706566572189331.\n",
      "Epoch 1232. Training loss: 0.7737357020378113. Validation loss: 2.706566095352173.\n",
      "Epoch 1233. Training loss: 0.7737324237823486. Validation loss: 2.7065653800964355.\n",
      "Epoch 1234. Training loss: 0.7737290859222412. Validation loss: 2.7065646648406982.\n",
      "Epoch 1235. Training loss: 0.773725688457489. Validation loss: 2.706563949584961.\n",
      "Epoch 1236. Training loss: 0.7737224102020264. Validation loss: 2.7065632343292236.\n",
      "Epoch 1237. Training loss: 0.7737191319465637. Validation loss: 2.7065627574920654.\n",
      "Epoch 1238. Training loss: 0.7737157940864563. Validation loss: 2.706561803817749.\n",
      "Epoch 1239. Training loss: 0.7737125754356384. Validation loss: 2.706561326980591.\n",
      "Epoch 1240. Training loss: 0.773709237575531. Validation loss: 2.7065608501434326.\n",
      "Epoch 1241. Training loss: 0.7737058997154236. Validation loss: 2.7065601348876953.\n",
      "Epoch 1242. Training loss: 0.7737026214599609. Validation loss: 2.706559658050537.\n",
      "Epoch 1243. Training loss: 0.7736992835998535. Validation loss: 2.7065587043762207.\n",
      "Epoch 1244. Training loss: 0.7736959457397461. Validation loss: 2.7065579891204834.\n",
      "Epoch 1245. Training loss: 0.7736926078796387. Validation loss: 2.706557273864746.\n",
      "Epoch 1246. Training loss: 0.7736892700195312. Validation loss: 2.706556797027588.\n",
      "Epoch 1247. Training loss: 0.7736859321594238. Validation loss: 2.7065563201904297.\n",
      "Epoch 1248. Training loss: 0.7736826539039612. Validation loss: 2.7065556049346924.\n",
      "Epoch 1249. Training loss: 0.7736794352531433. Validation loss: 2.706554889678955.\n",
      "Epoch 1250. Training loss: 0.7736760973930359. Validation loss: 2.7065541744232178.\n",
      "Epoch 1251. Training loss: 0.7736726403236389. Validation loss: 2.7065536975860596.\n",
      "Epoch 1252. Training loss: 0.773669421672821. Validation loss: 2.7065529823303223.\n",
      "Epoch 1253. Training loss: 0.7736660838127136. Validation loss: 2.706552267074585.\n",
      "Epoch 1254. Training loss: 0.7736627459526062. Validation loss: 2.7065513134002686.\n",
      "Epoch 1255. Training loss: 0.7736594080924988. Validation loss: 2.7065508365631104.\n",
      "Epoch 1256. Training loss: 0.7736560702323914. Validation loss: 2.706550359725952.\n",
      "Epoch 1257. Training loss: 0.7736527323722839. Validation loss: 2.706549882888794.\n",
      "Epoch 1258. Training loss: 0.7736494541168213. Validation loss: 2.7065486907958984.\n",
      "Epoch 1259. Training loss: 0.7736460566520691. Validation loss: 2.7065482139587402.\n",
      "Epoch 1260. Training loss: 0.7736427783966064. Validation loss: 2.706547737121582.\n",
      "Epoch 1261. Training loss: 0.773639440536499. Validation loss: 2.7065467834472656.\n",
      "Epoch 1262. Training loss: 0.7736360430717468. Validation loss: 2.7065465450286865.\n",
      "Epoch 1263. Training loss: 0.7736327648162842. Validation loss: 2.706545829772949.\n",
      "Epoch 1264. Training loss: 0.7736294269561768. Validation loss: 2.706545114517212.\n",
      "Epoch 1265. Training loss: 0.7736261487007141. Validation loss: 2.7065443992614746.\n",
      "Epoch 1266. Training loss: 0.7736227512359619. Validation loss: 2.7065436840057373.\n",
      "Epoch 1267. Training loss: 0.7736194133758545. Validation loss: 2.706543207168579.\n",
      "Epoch 1268. Training loss: 0.7736162543296814. Validation loss: 2.7065422534942627.\n",
      "Epoch 1269. Training loss: 0.7736127376556396. Validation loss: 2.7065415382385254.\n",
      "Epoch 1270. Training loss: 0.773609459400177. Validation loss: 2.706540822982788.\n",
      "Epoch 1271. Training loss: 0.7736061215400696. Validation loss: 2.70654034614563.\n",
      "Epoch 1272. Training loss: 0.7736029028892517. Validation loss: 2.7065396308898926.\n",
      "Epoch 1273. Training loss: 0.7735994458198547. Validation loss: 2.7065391540527344.\n",
      "Epoch 1274. Training loss: 0.7735962271690369. Validation loss: 2.706538200378418.\n",
      "Epoch 1275. Training loss: 0.7735927700996399. Validation loss: 2.706537961959839.\n",
      "Epoch 1276. Training loss: 0.7735894322395325. Validation loss: 2.7065370082855225.\n",
      "Epoch 1277. Training loss: 0.773586094379425. Validation loss: 2.7065367698669434.\n",
      "Epoch 1278. Training loss: 0.7735827565193176. Validation loss: 2.706536054611206.\n",
      "Epoch 1279. Training loss: 0.7735795378684998. Validation loss: 2.7065351009368896.\n",
      "Epoch 1280. Training loss: 0.7735760807991028. Validation loss: 2.7065343856811523.\n",
      "Epoch 1281. Training loss: 0.7735727429389954. Validation loss: 2.706533908843994.\n",
      "Epoch 1282. Training loss: 0.7735695242881775. Validation loss: 2.706533193588257.\n",
      "Epoch 1283. Training loss: 0.7735660672187805. Validation loss: 2.7065324783325195.\n",
      "Epoch 1284. Training loss: 0.7735627293586731. Validation loss: 2.7065322399139404.\n",
      "Epoch 1285. Training loss: 0.7735593914985657. Validation loss: 2.706531047821045.\n",
      "Epoch 1286. Training loss: 0.7735560536384583. Validation loss: 2.706530809402466.\n",
      "Epoch 1287. Training loss: 0.7735528349876404. Validation loss: 2.7065298557281494.\n",
      "Epoch 1288. Training loss: 0.7735493779182434. Validation loss: 2.706529378890991.\n",
      "Epoch 1289. Training loss: 0.773546040058136. Validation loss: 2.706528425216675.\n",
      "Epoch 1290. Training loss: 0.7735428214073181. Validation loss: 2.7065277099609375.\n",
      "Epoch 1291. Training loss: 0.7735393643379211. Validation loss: 2.7065272331237793.\n",
      "Epoch 1292. Training loss: 0.7735360264778137. Validation loss: 2.706526756286621.\n",
      "Epoch 1293. Training loss: 0.7735326886177063. Validation loss: 2.706526041030884.\n",
      "Epoch 1294. Training loss: 0.7735293507575989. Validation loss: 2.7065253257751465.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1295. Training loss: 0.7735260128974915. Validation loss: 2.7065248489379883.\n",
      "Epoch 1296. Training loss: 0.773522675037384. Validation loss: 2.706523895263672.\n",
      "Epoch 1297. Training loss: 0.7735192775726318. Validation loss: 2.7065231800079346.\n",
      "Epoch 1298. Training loss: 0.7735159993171692. Validation loss: 2.7065227031707764.\n",
      "Epoch 1299. Training loss: 0.7735126614570618. Validation loss: 2.70652174949646.\n",
      "Epoch 1300. Training loss: 0.7735092639923096. Validation loss: 2.7065212726593018.\n",
      "Epoch 1301. Training loss: 0.7735059261322021. Validation loss: 2.7065205574035645.\n",
      "Epoch 1302. Training loss: 0.7735025882720947. Validation loss: 2.7065203189849854.\n",
      "Epoch 1303. Training loss: 0.7734993100166321. Validation loss: 2.706519365310669.\n",
      "Epoch 1304. Training loss: 0.7734959125518799. Validation loss: 2.7065186500549316.\n",
      "Epoch 1305. Training loss: 0.7734925150871277. Validation loss: 2.7065179347991943.\n",
      "Epoch 1306. Training loss: 0.773489236831665. Validation loss: 2.706517457962036.\n",
      "Epoch 1307. Training loss: 0.7734858393669128. Validation loss: 2.7065162658691406.\n",
      "Epoch 1308. Training loss: 0.7734825015068054. Validation loss: 2.7065162658691406.\n",
      "Epoch 1309. Training loss: 0.773479163646698. Validation loss: 2.706515312194824.\n",
      "Epoch 1310. Training loss: 0.7734758257865906. Validation loss: 2.706514835357666.\n",
      "Epoch 1311. Training loss: 0.7734724879264832. Validation loss: 2.7065141201019287.\n",
      "Epoch 1312. Training loss: 0.7734691500663757. Validation loss: 2.7065131664276123.\n",
      "Epoch 1313. Training loss: 0.7734656929969788. Validation loss: 2.706512689590454.\n",
      "Epoch 1314. Training loss: 0.7734623551368713. Validation loss: 2.706511974334717.\n",
      "Epoch 1315. Training loss: 0.7734590172767639. Validation loss: 2.7065112590789795.\n",
      "Epoch 1316. Training loss: 0.7734556794166565. Validation loss: 2.7065107822418213.\n",
      "Epoch 1317. Training loss: 0.7734523415565491. Validation loss: 2.706510066986084.\n",
      "Epoch 1318. Training loss: 0.7734490036964417. Validation loss: 2.706509590148926.\n",
      "Epoch 1319. Training loss: 0.7734456062316895. Validation loss: 2.7065086364746094.\n",
      "Epoch 1320. Training loss: 0.773442268371582. Validation loss: 2.706508159637451.\n",
      "Epoch 1321. Training loss: 0.7734389305114746. Validation loss: 2.706507682800293.\n",
      "Epoch 1322. Training loss: 0.7734355926513672. Validation loss: 2.7065069675445557.\n",
      "Epoch 1323. Training loss: 0.773432195186615. Validation loss: 2.7065062522888184.\n",
      "Epoch 1324. Training loss: 0.7734288573265076. Validation loss: 2.706505537033081.\n",
      "Epoch 1325. Training loss: 0.7734255194664001. Validation loss: 2.7065048217773438.\n",
      "Epoch 1326. Training loss: 0.7734220623970032. Validation loss: 2.7065041065216064.\n",
      "Epoch 1327. Training loss: 0.7734187245368958. Validation loss: 2.7065036296844482.\n",
      "Epoch 1328. Training loss: 0.7734153866767883. Validation loss: 2.706502676010132.\n",
      "Epoch 1329. Training loss: 0.7734119892120361. Validation loss: 2.7065019607543945.\n",
      "Epoch 1330. Training loss: 0.7734087109565735. Validation loss: 2.7065014839172363.\n",
      "Epoch 1331. Training loss: 0.7734053730964661. Validation loss: 2.70650053024292.\n",
      "Epoch 1332. Training loss: 0.7734019756317139. Validation loss: 2.706500291824341.\n",
      "Epoch 1333. Training loss: 0.7733986377716064. Validation loss: 2.7064993381500244.\n",
      "Epoch 1334. Training loss: 0.7733951210975647. Validation loss: 2.706498622894287.\n",
      "Epoch 1335. Training loss: 0.7733917832374573. Validation loss: 2.706498146057129.\n",
      "Epoch 1336. Training loss: 0.7733884453773499. Validation loss: 2.7064976692199707.\n",
      "Epoch 1337. Training loss: 0.7733851075172424. Validation loss: 2.706496477127075.\n",
      "Epoch 1338. Training loss: 0.773381769657135. Validation loss: 2.706496000289917.\n",
      "Epoch 1339. Training loss: 0.7733783721923828. Validation loss: 2.7064952850341797.\n",
      "Epoch 1340. Training loss: 0.7733750343322754. Validation loss: 2.7064948081970215.\n",
      "Epoch 1341. Training loss: 0.7733716368675232. Validation loss: 2.7064943313598633.\n",
      "Epoch 1342. Training loss: 0.7733682990074158. Validation loss: 2.706493377685547.\n",
      "Epoch 1343. Training loss: 0.7733649611473083. Validation loss: 2.7064926624298096.\n",
      "Epoch 1344. Training loss: 0.7733615040779114. Validation loss: 2.7064919471740723.\n",
      "Epoch 1345. Training loss: 0.7733582854270935. Validation loss: 2.706490993499756.\n",
      "Epoch 1346. Training loss: 0.7733548283576965. Validation loss: 2.706490993499756.\n",
      "Epoch 1347. Training loss: 0.7733514904975891. Validation loss: 2.7064902782440186.\n",
      "Epoch 1348. Training loss: 0.7733481526374817. Validation loss: 2.7064895629882812.\n",
      "Epoch 1349. Training loss: 0.7733446955680847. Validation loss: 2.706489086151123.\n",
      "Epoch 1350. Training loss: 0.7733413577079773. Validation loss: 2.7064881324768066.\n",
      "Epoch 1351. Training loss: 0.7733380198478699. Validation loss: 2.7064876556396484.\n",
      "Epoch 1352. Training loss: 0.7733345627784729. Validation loss: 2.706486940383911.\n",
      "Epoch 1353. Training loss: 0.7733311653137207. Validation loss: 2.706486225128174.\n",
      "Epoch 1354. Training loss: 0.7733278274536133. Validation loss: 2.7064852714538574.\n",
      "Epoch 1355. Training loss: 0.7733244895935059. Validation loss: 2.706484794616699.\n",
      "Epoch 1356. Training loss: 0.7733210921287537. Validation loss: 2.706484079360962.\n",
      "Epoch 1357. Training loss: 0.7733177542686462. Validation loss: 2.706483840942383.\n",
      "Epoch 1358. Training loss: 0.7733142971992493. Validation loss: 2.7064826488494873.\n",
      "Epoch 1359. Training loss: 0.7733110785484314. Validation loss: 2.706482172012329.\n",
      "Epoch 1360. Training loss: 0.7733076214790344. Validation loss: 2.7064812183380127.\n",
      "Epoch 1361. Training loss: 0.773304283618927. Validation loss: 2.7064809799194336.\n",
      "Epoch 1362. Training loss: 0.7733008861541748. Validation loss: 2.7064805030822754.\n",
      "Epoch 1363. Training loss: 0.7732974886894226. Validation loss: 2.706479549407959.\n",
      "Epoch 1364. Training loss: 0.7732940316200256. Validation loss: 2.7064788341522217.\n",
      "Epoch 1365. Training loss: 0.7732906937599182. Validation loss: 2.7064783573150635.\n",
      "Epoch 1366. Training loss: 0.773287296295166. Validation loss: 2.7064778804779053.\n",
      "Epoch 1367. Training loss: 0.7732839584350586. Validation loss: 2.706476926803589.\n",
      "Epoch 1368. Training loss: 0.7732805609703064. Validation loss: 2.7064762115478516.\n",
      "Epoch 1369. Training loss: 0.773277223110199. Validation loss: 2.706475257873535.\n",
      "Epoch 1370. Training loss: 0.7732738852500916. Validation loss: 2.706474781036377.\n",
      "Epoch 1371. Training loss: 0.7732704281806946. Validation loss: 2.7064743041992188.\n",
      "Epoch 1372. Training loss: 0.7732670903205872. Validation loss: 2.7064733505249023.\n",
      "Epoch 1373. Training loss: 0.7732636332511902. Validation loss: 2.706472873687744.\n",
      "Epoch 1374. Training loss: 0.7732602953910828. Validation loss: 2.706472158432007.\n",
      "Epoch 1375. Training loss: 0.7732569575309753. Validation loss: 2.7064716815948486.\n",
      "Epoch 1376. Training loss: 0.7732535004615784. Validation loss: 2.7064707279205322.\n",
      "Epoch 1377. Training loss: 0.7732500433921814. Validation loss: 2.706470251083374.\n",
      "Epoch 1378. Training loss: 0.7732467651367188. Validation loss: 2.7064695358276367.\n",
      "Epoch 1379. Training loss: 0.7732433676719666. Validation loss: 2.7064685821533203.\n",
      "Epoch 1380. Training loss: 0.7732398509979248. Validation loss: 2.706468105316162.\n",
      "Epoch 1381. Training loss: 0.7732365727424622. Validation loss: 2.706467390060425.\n",
      "Epoch 1382. Training loss: 0.77323317527771. Validation loss: 2.7064671516418457.\n",
      "Epoch 1383. Training loss: 0.7732297778129578. Validation loss: 2.7064661979675293.\n",
      "Epoch 1384. Training loss: 0.7732264399528503. Validation loss: 2.706465244293213.\n",
      "Epoch 1385. Training loss: 0.7732231020927429. Validation loss: 2.7064647674560547.\n",
      "Epoch 1386. Training loss: 0.7732195854187012. Validation loss: 2.7064638137817383.\n",
      "Epoch 1387. Training loss: 0.7732162475585938. Validation loss: 2.706462860107422.\n",
      "Epoch 1388. Training loss: 0.773212730884552. Validation loss: 2.7064621448516846.\n",
      "Epoch 1389. Training loss: 0.7732093930244446. Validation loss: 2.7064614295959473.\n",
      "Epoch 1390. Training loss: 0.7732060551643372. Validation loss: 2.70646071434021.\n",
      "Epoch 1391. Training loss: 0.7732027173042297. Validation loss: 2.7064597606658936.\n",
      "Epoch 1392. Training loss: 0.7731993198394775. Validation loss: 2.7064590454101562.\n",
      "Epoch 1393. Training loss: 0.7731959223747253. Validation loss: 2.7064578533172607.\n",
      "Epoch 1394. Training loss: 0.7731924653053284. Validation loss: 2.7064571380615234.\n",
      "Epoch 1395. Training loss: 0.7731890678405762. Validation loss: 2.706456422805786.\n",
      "Epoch 1396. Training loss: 0.7731857299804688. Validation loss: 2.7064552307128906.\n",
      "Epoch 1397. Training loss: 0.7731823325157166. Validation loss: 2.7064545154571533.\n",
      "Epoch 1398. Training loss: 0.7731788754463196. Validation loss: 2.706453800201416.\n",
      "Epoch 1399. Training loss: 0.7731755375862122. Validation loss: 2.7064526081085205.\n",
      "Epoch 1400. Training loss: 0.77317214012146. Validation loss: 2.7064521312713623.\n",
      "Epoch 1401. Training loss: 0.7731688022613525. Validation loss: 2.706450939178467.\n",
      "Epoch 1402. Training loss: 0.7731654047966003. Validation loss: 2.7064497470855713.\n",
      "Epoch 1403. Training loss: 0.7731620669364929. Validation loss: 2.706449031829834.\n",
      "Epoch 1404. Training loss: 0.773158609867096. Validation loss: 2.7064480781555176.\n",
      "Epoch 1405. Training loss: 0.7731552124023438. Validation loss: 2.706447124481201.\n",
      "Epoch 1406. Training loss: 0.7731518745422363. Validation loss: 2.7064461708068848.\n",
      "Epoch 1407. Training loss: 0.7731484770774841. Validation loss: 2.7064454555511475.\n",
      "Epoch 1408. Training loss: 0.7731451392173767. Validation loss: 2.706444263458252.\n",
      "Epoch 1409. Training loss: 0.7731416821479797. Validation loss: 2.7064435482025146.\n",
      "Epoch 1410. Training loss: 0.7731383442878723. Validation loss: 2.7064428329467773.\n",
      "Epoch 1411. Training loss: 0.7731349468231201. Validation loss: 2.706441879272461.\n",
      "Epoch 1412. Training loss: 0.7731315493583679. Validation loss: 2.7064411640167236.\n",
      "Epoch 1413. Training loss: 0.7731282114982605. Validation loss: 2.7064404487609863.\n",
      "Epoch 1414. Training loss: 0.7731247544288635. Validation loss: 2.7064390182495117.\n",
      "Epoch 1415. Training loss: 0.7731215357780457. Validation loss: 2.7064383029937744.\n",
      "Epoch 1416. Training loss: 0.7731180787086487. Validation loss: 2.706437587738037.\n",
      "Epoch 1417. Training loss: 0.7731146812438965. Validation loss: 2.7064366340637207.\n",
      "Epoch 1418. Training loss: 0.7731113433837891. Validation loss: 2.7064356803894043.\n",
      "Epoch 1419. Training loss: 0.7731079459190369. Validation loss: 2.706434965133667.\n",
      "Epoch 1420. Training loss: 0.7731044888496399. Validation loss: 2.7064340114593506.\n",
      "Epoch 1421. Training loss: 0.773101270198822. Validation loss: 2.706433057785034.\n",
      "Epoch 1422. Training loss: 0.773097813129425. Validation loss: 2.7064321041107178.\n",
      "Epoch 1423. Training loss: 0.7730944156646729. Validation loss: 2.7064313888549805.\n",
      "Epoch 1424. Training loss: 0.7730910778045654. Validation loss: 2.706430435180664.\n",
      "Epoch 1425. Training loss: 0.7730876803398132. Validation loss: 2.7064294815063477.\n",
      "Epoch 1426. Training loss: 0.7730844020843506. Validation loss: 2.7064287662506104.\n",
      "Epoch 1427. Training loss: 0.7730810642242432. Validation loss: 2.706428050994873.\n",
      "Epoch 1428. Training loss: 0.7730775475502014. Validation loss: 2.7064268589019775.\n",
      "Epoch 1429. Training loss: 0.773074209690094. Validation loss: 2.7064261436462402.\n",
      "Epoch 1430. Training loss: 0.7730708718299866. Validation loss: 2.7064249515533447.\n",
      "Epoch 1431. Training loss: 0.7730674743652344. Validation loss: 2.7064242362976074.\n",
      "Epoch 1432. Training loss: 0.7730640769004822. Validation loss: 2.706423044204712.\n",
      "Epoch 1433. Training loss: 0.7730607390403748. Validation loss: 2.7064223289489746.\n",
      "Epoch 1434. Training loss: 0.7730574011802673. Validation loss: 2.706421375274658.\n",
      "Epoch 1435. Training loss: 0.7730539441108704. Validation loss: 2.706420660018921.\n",
      "Epoch 1436. Training loss: 0.7730505466461182. Validation loss: 2.7064199447631836.\n",
      "Epoch 1437. Training loss: 0.7730472087860107. Validation loss: 2.7064194679260254.\n",
      "Epoch 1438. Training loss: 0.7730438113212585. Validation loss: 2.706418514251709.\n",
      "Epoch 1439. Training loss: 0.7730404734611511. Validation loss: 2.7064177989959717.\n",
      "Epoch 1440. Training loss: 0.7730371356010437. Validation loss: 2.7064173221588135.\n",
      "Epoch 1441. Training loss: 0.7730336785316467. Validation loss: 2.706416606903076.\n",
      "Epoch 1442. Training loss: 0.7730302810668945. Validation loss: 2.7064154148101807.\n",
      "Epoch 1443. Training loss: 0.7730269432067871. Validation loss: 2.7064146995544434.\n",
      "Epoch 1444. Training loss: 0.7730236053466797. Validation loss: 2.706413984298706.\n",
      "Epoch 1445. Training loss: 0.7730202078819275. Validation loss: 2.7064132690429688.\n",
      "Epoch 1446. Training loss: 0.7730167508125305. Validation loss: 2.7064123153686523.\n",
      "Epoch 1447. Training loss: 0.7730134129524231. Validation loss: 2.706411838531494.\n",
      "Epoch 1448. Training loss: 0.7730100750923157. Validation loss: 2.706411361694336.\n",
      "Epoch 1449. Training loss: 0.7730066776275635. Validation loss: 2.7064104080200195.\n",
      "Epoch 1450. Training loss: 0.7730032801628113. Validation loss: 2.706409454345703.\n",
      "Epoch 1451. Training loss: 0.7729999423027039. Validation loss: 2.706408739089966.\n",
      "Epoch 1452. Training loss: 0.7729964256286621. Validation loss: 2.7064082622528076.\n",
      "Epoch 1453. Training loss: 0.7729930877685547. Validation loss: 2.706407308578491.\n",
      "Epoch 1454. Training loss: 0.7729897499084473. Validation loss: 2.706406593322754.\n",
      "Epoch 1455. Training loss: 0.7729863524436951. Validation loss: 2.7064058780670166.\n",
      "Epoch 1456. Training loss: 0.7729830145835876. Validation loss: 2.7064049243927.\n",
      "Epoch 1457. Training loss: 0.7729795575141907. Validation loss: 2.706403970718384.\n",
      "Epoch 1458. Training loss: 0.7729761600494385. Validation loss: 2.7064034938812256.\n",
      "Epoch 1459. Training loss: 0.772972822189331. Validation loss: 2.7064027786254883.\n",
      "Epoch 1460. Training loss: 0.7729694843292236. Validation loss: 2.70640230178833.\n",
      "Epoch 1461. Training loss: 0.7729659676551819. Validation loss: 2.7064011096954346.\n",
      "Epoch 1462. Training loss: 0.772962749004364. Validation loss: 2.706400156021118.\n",
      "Epoch 1463. Training loss: 0.772959291934967. Validation loss: 2.70639967918396.\n",
      "Epoch 1464. Training loss: 0.7729558944702148. Validation loss: 2.7063987255096436.\n",
      "Epoch 1465. Training loss: 0.7729525566101074. Validation loss: 2.7063982486724854.\n",
      "Epoch 1466. Training loss: 0.7729490399360657. Validation loss: 2.706397533416748.\n",
      "Epoch 1467. Training loss: 0.7729456424713135. Validation loss: 2.7063968181610107.\n",
      "Epoch 1468. Training loss: 0.772942304611206. Validation loss: 2.7063961029052734.\n",
      "Epoch 1469. Training loss: 0.7729389667510986. Validation loss: 2.706395149230957.\n",
      "Epoch 1470. Training loss: 0.7729356288909912. Validation loss: 2.706394672393799.\n",
      "Epoch 1471. Training loss: 0.7729321122169495. Validation loss: 2.7063937187194824.\n",
      "Epoch 1472. Training loss: 0.7729287147521973. Validation loss: 2.706392765045166.\n",
      "Epoch 1473. Training loss: 0.7729253768920898. Validation loss: 2.7063920497894287.\n",
      "Epoch 1474. Training loss: 0.7729220390319824. Validation loss: 2.7063913345336914.\n",
      "Epoch 1475. Training loss: 0.7729185223579407. Validation loss: 2.706390857696533.\n",
      "Epoch 1476. Training loss: 0.7729151844978333. Validation loss: 2.706389904022217.\n",
      "Epoch 1477. Training loss: 0.7729117274284363. Validation loss: 2.7063891887664795.\n",
      "Epoch 1478. Training loss: 0.7729082703590393. Validation loss: 2.706388473510742.\n",
      "Epoch 1479. Training loss: 0.7729050517082214. Validation loss: 2.706387519836426.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1480. Training loss: 0.7729015946388245. Validation loss: 2.7063865661621094.\n",
      "Epoch 1481. Training loss: 0.772898256778717. Validation loss: 2.706385850906372.\n",
      "Epoch 1482. Training loss: 0.7728947997093201. Validation loss: 2.706385374069214.\n",
      "Epoch 1483. Training loss: 0.7728913426399231. Validation loss: 2.7063846588134766.\n",
      "Epoch 1484. Training loss: 0.7728879451751709. Validation loss: 2.70638370513916.\n",
      "Epoch 1485. Training loss: 0.7728845477104187. Validation loss: 2.7063827514648438.\n",
      "Epoch 1486. Training loss: 0.7728812098503113. Validation loss: 2.7063825130462646.\n",
      "Epoch 1487. Training loss: 0.7728778719902039. Validation loss: 2.706381320953369.\n",
      "Epoch 1488. Training loss: 0.7728743553161621. Validation loss: 2.706380844116211.\n",
      "Epoch 1489. Training loss: 0.7728710174560547. Validation loss: 2.7063796520233154.\n",
      "Epoch 1490. Training loss: 0.7728675007820129. Validation loss: 2.7063791751861572.\n",
      "Epoch 1491. Training loss: 0.7728641629219055. Validation loss: 2.70637845993042.\n",
      "Epoch 1492. Training loss: 0.7728608250617981. Validation loss: 2.7063777446746826.\n",
      "Epoch 1493. Training loss: 0.7728574275970459. Validation loss: 2.706376791000366.\n",
      "Epoch 1494. Training loss: 0.7728539109230042. Validation loss: 2.70637583732605.\n",
      "Epoch 1495. Training loss: 0.772850513458252. Validation loss: 2.7063753604888916.\n",
      "Epoch 1496. Training loss: 0.7728471755981445. Validation loss: 2.7063746452331543.\n",
      "Epoch 1497. Training loss: 0.7728436589241028. Validation loss: 2.706373929977417.\n",
      "Epoch 1498. Training loss: 0.7728403210639954. Validation loss: 2.7063729763031006.\n",
      "Epoch 1499. Training loss: 0.7728368639945984. Validation loss: 2.7063722610473633.\n",
      "Epoch 1500. Training loss: 0.772833526134491. Validation loss: 2.706371545791626.\n",
      "Epoch 1501. Training loss: 0.772830069065094. Validation loss: 2.7063705921173096.\n",
      "Epoch 1502. Training loss: 0.7728266716003418. Validation loss: 2.7063698768615723.\n",
      "Epoch 1503. Training loss: 0.7728232741355896. Validation loss: 2.706369400024414.\n",
      "Epoch 1504. Training loss: 0.7728198170661926. Validation loss: 2.7063684463500977.\n",
      "Epoch 1505. Training loss: 0.7728164792060852. Validation loss: 2.7063677310943604.\n",
      "Epoch 1506. Training loss: 0.7728130221366882. Validation loss: 2.706367015838623.\n",
      "Epoch 1507. Training loss: 0.7728096842765808. Validation loss: 2.7063658237457275.\n",
      "Epoch 1508. Training loss: 0.7728063464164734. Validation loss: 2.7063653469085693.\n",
      "Epoch 1509. Training loss: 0.7728027701377869. Validation loss: 2.706364631652832.\n",
      "Epoch 1510. Training loss: 0.7727993130683899. Validation loss: 2.7063636779785156.\n",
      "Epoch 1511. Training loss: 0.7727959752082825. Validation loss: 2.7063629627227783.\n",
      "Epoch 1512. Training loss: 0.7727925181388855. Validation loss: 2.706362247467041.\n",
      "Epoch 1513. Training loss: 0.7727891802787781. Validation loss: 2.706361770629883.\n",
      "Epoch 1514. Training loss: 0.7727856636047363. Validation loss: 2.7063608169555664.\n",
      "Epoch 1515. Training loss: 0.7727823257446289. Validation loss: 2.70635986328125.\n",
      "Epoch 1516. Training loss: 0.7727789282798767. Validation loss: 2.706359386444092.\n",
      "Epoch 1517. Training loss: 0.7727754712104797. Validation loss: 2.7063586711883545.\n",
      "Epoch 1518. Training loss: 0.7727720141410828. Validation loss: 2.706357479095459.\n",
      "Epoch 1519. Training loss: 0.7727685570716858. Validation loss: 2.7063567638397217.\n",
      "Epoch 1520. Training loss: 0.7727651596069336. Validation loss: 2.7063558101654053.\n",
      "Epoch 1521. Training loss: 0.7727617621421814. Validation loss: 2.706355094909668.\n",
      "Epoch 1522. Training loss: 0.7727583050727844. Validation loss: 2.7063546180725098.\n",
      "Epoch 1523. Training loss: 0.7727549076080322. Validation loss: 2.7063536643981934.\n",
      "Epoch 1524. Training loss: 0.77275151014328. Validation loss: 2.706352949142456.\n",
      "Epoch 1525. Training loss: 0.7727480530738831. Validation loss: 2.706352472305298.\n",
      "Epoch 1526. Training loss: 0.7727447152137756. Validation loss: 2.7063515186309814.\n",
      "Epoch 1527. Training loss: 0.7727412581443787. Validation loss: 2.706350564956665.\n",
      "Epoch 1528. Training loss: 0.7727379202842712. Validation loss: 2.706350088119507.\n",
      "Epoch 1529. Training loss: 0.7727344036102295. Validation loss: 2.7063488960266113.\n",
      "Epoch 1530. Training loss: 0.7727310061454773. Validation loss: 2.706348419189453.\n",
      "Epoch 1531. Training loss: 0.7727276682853699. Validation loss: 2.7063474655151367.\n",
      "Epoch 1532. Training loss: 0.7727240920066833. Validation loss: 2.7063467502593994.\n",
      "Epoch 1533. Training loss: 0.7727207541465759. Validation loss: 2.706346035003662.\n",
      "Epoch 1534. Training loss: 0.7727172374725342. Validation loss: 2.7063450813293457.\n",
      "Epoch 1535. Training loss: 0.772713840007782. Validation loss: 2.7063446044921875.\n",
      "Epoch 1536. Training loss: 0.7727105021476746. Validation loss: 2.706343412399292.\n",
      "Epoch 1537. Training loss: 0.7727069854736328. Validation loss: 2.706342935562134.\n",
      "Epoch 1538. Training loss: 0.7727035880088806. Validation loss: 2.7063419818878174.\n",
      "Epoch 1539. Training loss: 0.7727001309394836. Validation loss: 2.70634126663208.\n",
      "Epoch 1540. Training loss: 0.7726967334747314. Validation loss: 2.7063403129577637.\n",
      "Epoch 1541. Training loss: 0.7726932168006897. Validation loss: 2.7063395977020264.\n",
      "Epoch 1542. Training loss: 0.7726898193359375. Validation loss: 2.70633864402771.\n",
      "Epoch 1543. Training loss: 0.7726864814758301. Validation loss: 2.706338405609131.\n",
      "Epoch 1544. Training loss: 0.7726829648017883. Validation loss: 2.7063376903533936.\n",
      "Epoch 1545. Training loss: 0.7726795673370361. Validation loss: 2.706336498260498.\n",
      "Epoch 1546. Training loss: 0.7726761698722839. Validation loss: 2.7063357830047607.\n",
      "Epoch 1547. Training loss: 0.7726726531982422. Validation loss: 2.7063350677490234.\n",
      "Epoch 1548. Training loss: 0.77266925573349. Validation loss: 2.706334114074707.\n",
      "Epoch 1549. Training loss: 0.7726659178733826. Validation loss: 2.706333637237549.\n",
      "Epoch 1550. Training loss: 0.7726624608039856. Validation loss: 2.7063326835632324.\n",
      "Epoch 1551. Training loss: 0.7726590037345886. Validation loss: 2.706331729888916.\n",
      "Epoch 1552. Training loss: 0.7726555466651917. Validation loss: 2.7063310146331787.\n",
      "Epoch 1553. Training loss: 0.7726521492004395. Validation loss: 2.7063302993774414.\n",
      "Epoch 1554. Training loss: 0.7726485729217529. Validation loss: 2.706329345703125.\n",
      "Epoch 1555. Training loss: 0.7726451754570007. Validation loss: 2.706328868865967.\n",
      "Epoch 1556. Training loss: 0.7726417183876038. Validation loss: 2.7063281536102295.\n",
      "Epoch 1557. Training loss: 0.7726383805274963. Validation loss: 2.706327199935913.\n",
      "Epoch 1558. Training loss: 0.7726349234580994. Validation loss: 2.706326484680176.\n",
      "Epoch 1559. Training loss: 0.7726314067840576. Validation loss: 2.7063257694244385.\n",
      "Epoch 1560. Training loss: 0.7726280093193054. Validation loss: 2.706324815750122.\n",
      "Epoch 1561. Training loss: 0.7726245522499084. Validation loss: 2.7063238620758057.\n",
      "Epoch 1562. Training loss: 0.7726211547851562. Validation loss: 2.7063231468200684.\n",
      "Epoch 1563. Training loss: 0.772617757320404. Validation loss: 2.706322431564331.\n",
      "Epoch 1564. Training loss: 0.7726142406463623. Validation loss: 2.7063217163085938.\n",
      "Epoch 1565. Training loss: 0.7726109027862549. Validation loss: 2.7063207626342773.\n",
      "Epoch 1566. Training loss: 0.7726073861122131. Validation loss: 2.70632004737854.\n",
      "Epoch 1567. Training loss: 0.7726038098335266. Validation loss: 2.7063193321228027.\n",
      "Epoch 1568. Training loss: 0.7726004123687744. Validation loss: 2.7063186168670654.\n",
      "Epoch 1569. Training loss: 0.7725970149040222. Validation loss: 2.706317901611328.\n",
      "Epoch 1570. Training loss: 0.7725934982299805. Validation loss: 2.7063169479370117.\n",
      "Epoch 1571. Training loss: 0.7725901007652283. Validation loss: 2.7063162326812744.\n",
      "Epoch 1572. Training loss: 0.7725866436958313. Validation loss: 2.706315040588379.\n",
      "Epoch 1573. Training loss: 0.7725832462310791. Validation loss: 2.7063143253326416.\n",
      "Epoch 1574. Training loss: 0.7725796699523926. Validation loss: 2.7063136100769043.\n",
      "Epoch 1575. Training loss: 0.7725763320922852. Validation loss: 2.706312656402588.\n",
      "Epoch 1576. Training loss: 0.7725728154182434. Validation loss: 2.7063119411468506.\n",
      "Epoch 1577. Training loss: 0.7725694179534912. Validation loss: 2.7063112258911133.\n",
      "Epoch 1578. Training loss: 0.7725659012794495. Validation loss: 2.706310272216797.\n",
      "Epoch 1579. Training loss: 0.7725625038146973. Validation loss: 2.7063093185424805.\n",
      "Epoch 1580. Training loss: 0.7725589871406555. Validation loss: 2.706308364868164.\n",
      "Epoch 1581. Training loss: 0.7725555300712585. Validation loss: 2.7063076496124268.\n",
      "Epoch 1582. Training loss: 0.7725521922111511. Validation loss: 2.7063064575195312.\n",
      "Epoch 1583. Training loss: 0.7725487351417542. Validation loss: 2.706305980682373.\n",
      "Epoch 1584. Training loss: 0.7725452780723572. Validation loss: 2.7063050270080566.\n",
      "Epoch 1585. Training loss: 0.7725417613983154. Validation loss: 2.7063040733337402.\n",
      "Epoch 1586. Training loss: 0.7725383639335632. Validation loss: 2.706303358078003.\n",
      "Epoch 1587. Training loss: 0.7725347876548767. Validation loss: 2.7063024044036865.\n",
      "Epoch 1588. Training loss: 0.7725315093994141. Validation loss: 2.70630145072937.\n",
      "Epoch 1589. Training loss: 0.7725279331207275. Validation loss: 2.706300735473633.\n",
      "Epoch 1590. Training loss: 0.7725245356559753. Validation loss: 2.7063000202178955.\n",
      "Epoch 1591. Training loss: 0.7725210189819336. Validation loss: 2.706299304962158.\n",
      "Epoch 1592. Training loss: 0.7725175023078918. Validation loss: 2.706298351287842.\n",
      "Epoch 1593. Training loss: 0.7725141048431396. Validation loss: 2.7062973976135254.\n",
      "Epoch 1594. Training loss: 0.7725105881690979. Validation loss: 2.706296443939209.\n",
      "Epoch 1595. Training loss: 0.7725071310997009. Validation loss: 2.7062954902648926.\n",
      "Epoch 1596. Training loss: 0.772503674030304. Validation loss: 2.7062950134277344.\n",
      "Epoch 1597. Training loss: 0.772500216960907. Validation loss: 2.706293821334839.\n",
      "Epoch 1598. Training loss: 0.7724967002868652. Validation loss: 2.7062933444976807.\n",
      "Epoch 1599. Training loss: 0.772493302822113. Validation loss: 2.7062923908233643.\n",
      "Epoch 1600. Training loss: 0.7724898457527161. Validation loss: 2.706291437149048.\n",
      "Epoch 1601. Training loss: 0.7724863886833191. Validation loss: 2.7062902450561523.\n",
      "Epoch 1602. Training loss: 0.7724828720092773. Validation loss: 2.706289768218994.\n",
      "Epoch 1603. Training loss: 0.7724793553352356. Validation loss: 2.7062885761260986.\n",
      "Epoch 1604. Training loss: 0.7724760174751282. Validation loss: 2.7062883377075195.\n",
      "Epoch 1605. Training loss: 0.7724725604057312. Validation loss: 2.706287384033203.\n",
      "Epoch 1606. Training loss: 0.7724689841270447. Validation loss: 2.7062864303588867.\n",
      "Epoch 1607. Training loss: 0.7724655270576477. Validation loss: 2.7062854766845703.\n",
      "Epoch 1608. Training loss: 0.7724621295928955. Validation loss: 2.706284523010254.\n",
      "Epoch 1609. Training loss: 0.7724586129188538. Validation loss: 2.7062838077545166.\n",
      "Epoch 1610. Training loss: 0.7724552154541016. Validation loss: 2.7062828540802.\n",
      "Epoch 1611. Training loss: 0.7724516987800598. Validation loss: 2.706282377243042.\n",
      "Epoch 1612. Training loss: 0.7724481225013733. Validation loss: 2.7062811851501465.\n",
      "Epoch 1613. Training loss: 0.7724447250366211. Validation loss: 2.706280469894409.\n",
      "Epoch 1614. Training loss: 0.7724411487579346. Validation loss: 2.706279754638672.\n",
      "Epoch 1615. Training loss: 0.7724378108978271. Validation loss: 2.7062783241271973.\n",
      "Epoch 1616. Training loss: 0.7724342942237854. Validation loss: 2.706277847290039.\n",
      "Epoch 1617. Training loss: 0.7724308371543884. Validation loss: 2.7062768936157227.\n",
      "Epoch 1618. Training loss: 0.7724273204803467. Validation loss: 2.7062761783599854.\n",
      "Epoch 1619. Training loss: 0.7724239230155945. Validation loss: 2.706275463104248.\n",
      "Epoch 1620. Training loss: 0.7724204063415527. Validation loss: 2.7062745094299316.\n",
      "Epoch 1621. Training loss: 0.7724170088768005. Validation loss: 2.7062740325927734.\n",
      "Epoch 1622. Training loss: 0.772413432598114. Validation loss: 2.706272840499878.\n",
      "Epoch 1623. Training loss: 0.7724099159240723. Validation loss: 2.7062718868255615.\n",
      "Epoch 1624. Training loss: 0.7724065184593201. Validation loss: 2.706271171569824.\n",
      "Epoch 1625. Training loss: 0.7724030613899231. Validation loss: 2.706270217895508.\n",
      "Epoch 1626. Training loss: 0.7723996043205261. Validation loss: 2.7062695026397705.\n",
      "Epoch 1627. Training loss: 0.7723960876464844. Validation loss: 2.706268787384033.\n",
      "Epoch 1628. Training loss: 0.7723925709724426. Validation loss: 2.706267833709717.\n",
      "Epoch 1629. Training loss: 0.7723891139030457. Validation loss: 2.7062668800354004.\n",
      "Epoch 1630. Training loss: 0.7723855972290039. Validation loss: 2.706266164779663.\n",
      "Epoch 1631. Training loss: 0.7723822593688965. Validation loss: 2.706265449523926.\n",
      "Epoch 1632. Training loss: 0.7723785042762756. Validation loss: 2.7062642574310303.\n",
      "Epoch 1633. Training loss: 0.7723751068115234. Validation loss: 2.706263303756714.\n",
      "Epoch 1634. Training loss: 0.7723717093467712. Validation loss: 2.7062628269195557.\n",
      "Epoch 1635. Training loss: 0.7723681330680847. Validation loss: 2.70626163482666.\n",
      "Epoch 1636. Training loss: 0.7723646759986877. Validation loss: 2.7062606811523438.\n",
      "Epoch 1637. Training loss: 0.7723612785339355. Validation loss: 2.7062602043151855.\n",
      "Epoch 1638. Training loss: 0.7723577618598938. Validation loss: 2.706259250640869.\n",
      "Epoch 1639. Training loss: 0.7723543047904968. Validation loss: 2.706258535385132.\n",
      "Epoch 1640. Training loss: 0.7723507881164551. Validation loss: 2.7062580585479736.\n",
      "Epoch 1641. Training loss: 0.7723472714424133. Validation loss: 2.7062573432922363.\n",
      "Epoch 1642. Training loss: 0.7723438143730164. Validation loss: 2.706256866455078.\n",
      "Epoch 1643. Training loss: 0.7723403573036194. Validation loss: 2.706256151199341.\n",
      "Epoch 1644. Training loss: 0.7723367810249329. Validation loss: 2.7062556743621826.\n",
      "Epoch 1645. Training loss: 0.7723333835601807. Validation loss: 2.7062549591064453.\n",
      "Epoch 1646. Training loss: 0.7723298668861389. Validation loss: 2.706254243850708.\n",
      "Epoch 1647. Training loss: 0.7723264098167419. Validation loss: 2.7062532901763916.\n",
      "Epoch 1648. Training loss: 0.7723228335380554. Validation loss: 2.7062528133392334.\n",
      "Epoch 1649. Training loss: 0.772319495677948. Validation loss: 2.706252098083496.\n",
      "Epoch 1650. Training loss: 0.7723159193992615. Validation loss: 2.706251621246338.\n",
      "Epoch 1651. Training loss: 0.7723124623298645. Validation loss: 2.7062511444091797.\n",
      "Epoch 1652. Training loss: 0.772308886051178. Validation loss: 2.7062501907348633.\n",
      "Epoch 1653. Training loss: 0.772305428981781. Validation loss: 2.706249713897705.\n",
      "Epoch 1654. Training loss: 0.7723019123077393. Validation loss: 2.7062489986419678.\n",
      "Epoch 1655. Training loss: 0.7722983956336975. Validation loss: 2.7062482833862305.\n",
      "Epoch 1656. Training loss: 0.772294819355011. Validation loss: 2.706247568130493.\n",
      "Epoch 1657. Training loss: 0.7722914814949036. Validation loss: 2.7062466144561768.\n",
      "Epoch 1658. Training loss: 0.7722880244255066. Validation loss: 2.7062458992004395.\n",
      "Epoch 1659. Training loss: 0.7722844481468201. Validation loss: 2.7062456607818604.\n",
      "Epoch 1660. Training loss: 0.7722809910774231. Validation loss: 2.706244945526123.\n",
      "Epoch 1661. Training loss: 0.7722773551940918. Validation loss: 2.7062439918518066.\n",
      "Epoch 1662. Training loss: 0.7722739577293396. Validation loss: 2.7062437534332275.\n",
      "Epoch 1663. Training loss: 0.7722704410552979. Validation loss: 2.7062432765960693.\n",
      "Epoch 1664. Training loss: 0.7722668647766113. Validation loss: 2.706242561340332.\n",
      "Epoch 1665. Training loss: 0.7722634673118591. Validation loss: 2.7062416076660156.\n",
      "Epoch 1666. Training loss: 0.7722599506378174. Validation loss: 2.7062411308288574.\n",
      "Epoch 1667. Training loss: 0.7722564339637756. Validation loss: 2.706240653991699.\n",
      "Epoch 1668. Training loss: 0.7722529768943787. Validation loss: 2.706239938735962.\n",
      "Epoch 1669. Training loss: 0.7722494006156921. Validation loss: 2.7062394618988037.\n",
      "Epoch 1670. Training loss: 0.7722459435462952. Validation loss: 2.706238269805908.\n",
      "Epoch 1671. Training loss: 0.7722423672676086. Validation loss: 2.706237554550171.\n",
      "Epoch 1672. Training loss: 0.7722389101982117. Validation loss: 2.7062370777130127.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1673. Training loss: 0.7722353935241699. Validation loss: 2.7062363624572754.\n",
      "Epoch 1674. Training loss: 0.7722318768501282. Validation loss: 2.706235885620117.\n",
      "Epoch 1675. Training loss: 0.7722284197807312. Validation loss: 2.70623517036438.\n",
      "Epoch 1676. Training loss: 0.7722249031066895. Validation loss: 2.7062346935272217.\n",
      "Epoch 1677. Training loss: 0.7722213864326477. Validation loss: 2.7062337398529053.\n",
      "Epoch 1678. Training loss: 0.7722178101539612. Validation loss: 2.706233263015747.\n",
      "Epoch 1679. Training loss: 0.772214412689209. Validation loss: 2.7062325477600098.\n",
      "Epoch 1680. Training loss: 0.7722108364105225. Validation loss: 2.7062318325042725.\n",
      "Epoch 1681. Training loss: 0.7722073197364807. Validation loss: 2.7062313556671143.\n",
      "Epoch 1682. Training loss: 0.7722038626670837. Validation loss: 2.706230640411377.\n",
      "Epoch 1683. Training loss: 0.7722002863883972. Validation loss: 2.7062296867370605.\n",
      "Epoch 1684. Training loss: 0.7721967697143555. Validation loss: 2.7062292098999023.\n",
      "Epoch 1685. Training loss: 0.7721932530403137. Validation loss: 2.706228733062744.\n",
      "Epoch 1686. Training loss: 0.7721898555755615. Validation loss: 2.7062277793884277.\n",
      "Epoch 1687. Training loss: 0.7721863389015198. Validation loss: 2.7062275409698486.\n",
      "Epoch 1688. Training loss: 0.7721827626228333. Validation loss: 2.7062268257141113.\n",
      "Epoch 1689. Training loss: 0.7721793055534363. Validation loss: 2.706225872039795.\n",
      "Epoch 1690. Training loss: 0.7721757292747498. Validation loss: 2.7062253952026367.\n",
      "Epoch 1691. Training loss: 0.772172212600708. Validation loss: 2.7062249183654785.\n",
      "Epoch 1692. Training loss: 0.7721686959266663. Validation loss: 2.706224203109741.\n",
      "Epoch 1693. Training loss: 0.7721651196479797. Validation loss: 2.706223487854004.\n",
      "Epoch 1694. Training loss: 0.7721617221832275. Validation loss: 2.7062225341796875.\n",
      "Epoch 1695. Training loss: 0.772158145904541. Validation loss: 2.7062220573425293.\n",
      "Epoch 1696. Training loss: 0.7721546292304993. Validation loss: 2.706221342086792.\n",
      "Epoch 1697. Training loss: 0.7721511721611023. Validation loss: 2.7062206268310547.\n",
      "Epoch 1698. Training loss: 0.7721476554870605. Validation loss: 2.7062201499938965.\n",
      "Epoch 1699. Training loss: 0.7721441388130188. Validation loss: 2.706219434738159.\n",
      "Epoch 1700. Training loss: 0.7721405625343323. Validation loss: 2.7062184810638428.\n",
      "Epoch 1701. Training loss: 0.7721371054649353. Validation loss: 2.7062182426452637.\n",
      "Epoch 1702. Training loss: 0.7721335887908936. Validation loss: 2.7062175273895264.\n",
      "Epoch 1703. Training loss: 0.772130012512207. Validation loss: 2.706216812133789.\n",
      "Epoch 1704. Training loss: 0.7721264362335205. Validation loss: 2.7062160968780518.\n",
      "Epoch 1705. Training loss: 0.7721230387687683. Validation loss: 2.7062153816223145.\n",
      "Epoch 1706. Training loss: 0.7721195220947266. Validation loss: 2.706214666366577.\n",
      "Epoch 1707. Training loss: 0.7721160054206848. Validation loss: 2.706214189529419.\n",
      "Epoch 1708. Training loss: 0.7721123695373535. Validation loss: 2.7062134742736816.\n",
      "Epoch 1709. Training loss: 0.7721089720726013. Validation loss: 2.7062127590179443.\n",
      "Epoch 1710. Training loss: 0.7721054553985596. Validation loss: 2.706212043762207.\n",
      "Epoch 1711. Training loss: 0.772101879119873. Validation loss: 2.7062113285064697.\n",
      "Epoch 1712. Training loss: 0.7720983028411865. Validation loss: 2.7062110900878906.\n",
      "Epoch 1713. Training loss: 0.7720947861671448. Validation loss: 2.706210136413574.\n",
      "Epoch 1714. Training loss: 0.7720913290977478. Validation loss: 2.706209659576416.\n",
      "Epoch 1715. Training loss: 0.772087812423706. Validation loss: 2.706209182739258.\n",
      "Epoch 1716. Training loss: 0.7720842361450195. Validation loss: 2.7062079906463623.\n",
      "Epoch 1717. Training loss: 0.7720807194709778. Validation loss: 2.706207513809204.\n",
      "Epoch 1718. Training loss: 0.7720771431922913. Validation loss: 2.706206798553467.\n",
      "Epoch 1719. Training loss: 0.7720737457275391. Validation loss: 2.7062063217163086.\n",
      "Epoch 1720. Training loss: 0.7720701694488525. Validation loss: 2.7062056064605713.\n",
      "Epoch 1721. Training loss: 0.7720666527748108. Validation loss: 2.706204891204834.\n",
      "Epoch 1722. Training loss: 0.7720631957054138. Validation loss: 2.7062041759490967.\n",
      "Epoch 1723. Training loss: 0.7720596790313721. Validation loss: 2.7062034606933594.\n",
      "Epoch 1724. Training loss: 0.7720560431480408. Validation loss: 2.706202507019043.\n",
      "Epoch 1725. Training loss: 0.772052526473999. Validation loss: 2.7062020301818848.\n",
      "Epoch 1726. Training loss: 0.7720490097999573. Validation loss: 2.7062013149261475.\n",
      "Epoch 1727. Training loss: 0.7720454335212708. Validation loss: 2.70620059967041.\n",
      "Epoch 1728. Training loss: 0.7720418572425842. Validation loss: 2.706200122833252.\n",
      "Epoch 1729. Training loss: 0.772038459777832. Validation loss: 2.7061991691589355.\n",
      "Epoch 1730. Training loss: 0.7720348238945007. Validation loss: 2.7061989307403564.\n",
      "Epoch 1731. Training loss: 0.772031307220459. Validation loss: 2.706197738647461.\n",
      "Epoch 1732. Training loss: 0.7720277905464172. Validation loss: 2.7061972618103027.\n",
      "Epoch 1733. Training loss: 0.7720243334770203. Validation loss: 2.7061965465545654.\n",
      "Epoch 1734. Training loss: 0.7720207571983337. Validation loss: 2.706195831298828.\n",
      "Epoch 1735. Training loss: 0.772017240524292. Validation loss: 2.706195116043091.\n",
      "Epoch 1736. Training loss: 0.7720136642456055. Validation loss: 2.7061946392059326.\n",
      "Epoch 1737. Training loss: 0.7720101475715637. Validation loss: 2.7061939239501953.\n",
      "Epoch 1738. Training loss: 0.7720065712928772. Validation loss: 2.706193447113037.\n",
      "Epoch 1739. Training loss: 0.7720031142234802. Validation loss: 2.7061920166015625.\n",
      "Epoch 1740. Training loss: 0.7719995379447937. Validation loss: 2.7061917781829834.\n",
      "Epoch 1741. Training loss: 0.7719959616661072. Validation loss: 2.706191062927246.\n",
      "Epoch 1742. Training loss: 0.7719924449920654. Validation loss: 2.706190347671509.\n",
      "Epoch 1743. Training loss: 0.7719888687133789. Validation loss: 2.7061896324157715.\n",
      "Epoch 1744. Training loss: 0.7719853520393372. Validation loss: 2.7061893939971924.\n",
      "Epoch 1745. Training loss: 0.7719818949699402. Validation loss: 2.706188440322876.\n",
      "Epoch 1746. Training loss: 0.7719781994819641. Validation loss: 2.7061879634857178.\n",
      "Epoch 1747. Training loss: 0.7719747424125671. Validation loss: 2.7061874866485596.\n",
      "Epoch 1748. Training loss: 0.7719711661338806. Validation loss: 2.706186532974243.\n",
      "Epoch 1749. Training loss: 0.7719677090644836. Validation loss: 2.706186056137085.\n",
      "Epoch 1750. Training loss: 0.7719641327857971. Validation loss: 2.7061851024627686.\n",
      "Epoch 1751. Training loss: 0.7719604969024658. Validation loss: 2.7061846256256104.\n",
      "Epoch 1752. Training loss: 0.7719569206237793. Validation loss: 2.706183671951294.\n",
      "Epoch 1753. Training loss: 0.7719535827636719. Validation loss: 2.706183433532715.\n",
      "Epoch 1754. Training loss: 0.7719499468803406. Validation loss: 2.7061824798583984.\n",
      "Epoch 1755. Training loss: 0.7719464302062988. Validation loss: 2.7061822414398193.\n",
      "Epoch 1756. Training loss: 0.7719427943229675. Validation loss: 2.706181526184082.\n",
      "Epoch 1757. Training loss: 0.7719393372535706. Validation loss: 2.7061805725097656.\n",
      "Epoch 1758. Training loss: 0.771935760974884. Validation loss: 2.7061800956726074.\n",
      "Epoch 1759. Training loss: 0.7719321250915527. Validation loss: 2.706179141998291.\n",
      "Epoch 1760. Training loss: 0.771928608417511. Validation loss: 2.706178665161133.\n",
      "Epoch 1761. Training loss: 0.7719250321388245. Validation loss: 2.7061781883239746.\n",
      "Epoch 1762. Training loss: 0.7719214558601379. Validation loss: 2.706177234649658.\n",
      "Epoch 1763. Training loss: 0.7719180583953857. Validation loss: 2.706176519393921.\n",
      "Epoch 1764. Training loss: 0.7719143033027649. Validation loss: 2.706176280975342.\n",
      "Epoch 1765. Training loss: 0.7719109654426575. Validation loss: 2.7061750888824463.\n",
      "Epoch 1766. Training loss: 0.7719073295593262. Validation loss: 2.706174612045288.\n",
      "Epoch 1767. Training loss: 0.7719038128852844. Validation loss: 2.70617413520813.\n",
      "Epoch 1768. Training loss: 0.7719003558158875. Validation loss: 2.7061731815338135.\n",
      "Epoch 1769. Training loss: 0.7718966603279114. Validation loss: 2.7061727046966553.\n",
      "Epoch 1770. Training loss: 0.7718930840492249. Validation loss: 2.706171989440918.\n",
      "Epoch 1771. Training loss: 0.7718896269798279. Validation loss: 2.7061715126037598.\n",
      "Epoch 1772. Training loss: 0.7718861103057861. Validation loss: 2.7061707973480225.\n",
      "Epoch 1773. Training loss: 0.7718825340270996. Validation loss: 2.706169605255127.\n",
      "Epoch 1774. Training loss: 0.7718789577484131. Validation loss: 2.7061691284179688.\n",
      "Epoch 1775. Training loss: 0.7718753814697266. Validation loss: 2.7061686515808105.\n",
      "Epoch 1776. Training loss: 0.77187180519104. Validation loss: 2.7061679363250732.\n",
      "Epoch 1777. Training loss: 0.7718682289123535. Validation loss: 2.706167221069336.\n",
      "Epoch 1778. Training loss: 0.7718647122383118. Validation loss: 2.7061662673950195.\n",
      "Epoch 1779. Training loss: 0.7718611359596252. Validation loss: 2.7061660289764404.\n",
      "Epoch 1780. Training loss: 0.7718575596809387. Validation loss: 2.706165313720703.\n",
      "Epoch 1781. Training loss: 0.7718539834022522. Validation loss: 2.706164598464966.\n",
      "Epoch 1782. Training loss: 0.7718505263328552. Validation loss: 2.7061641216278076.\n",
      "Epoch 1783. Training loss: 0.7718470096588135. Validation loss: 2.706163167953491.\n",
      "Epoch 1784. Training loss: 0.771843433380127. Validation loss: 2.706162929534912.\n",
      "Epoch 1785. Training loss: 0.7718397974967957. Validation loss: 2.7061617374420166.\n",
      "Epoch 1786. Training loss: 0.7718362212181091. Validation loss: 2.7061612606048584.\n",
      "Epoch 1787. Training loss: 0.7718327045440674. Validation loss: 2.7061607837677.\n",
      "Epoch 1788. Training loss: 0.7718291282653809. Validation loss: 2.706160068511963.\n",
      "Epoch 1789. Training loss: 0.7718255519866943. Validation loss: 2.7061595916748047.\n",
      "Epoch 1790. Training loss: 0.7718219757080078. Validation loss: 2.7061586380004883.\n",
      "Epoch 1791. Training loss: 0.7718184590339661. Validation loss: 2.70615816116333.\n",
      "Epoch 1792. Training loss: 0.7718148827552795. Validation loss: 2.7061572074890137.\n",
      "Epoch 1793. Training loss: 0.7718112468719482. Validation loss: 2.7061564922332764.\n",
      "Epoch 1794. Training loss: 0.7718077301979065. Validation loss: 2.706156015396118.\n",
      "Epoch 1795. Training loss: 0.77180415391922. Validation loss: 2.7061550617218018.\n",
      "Epoch 1796. Training loss: 0.7718005776405334. Validation loss: 2.7061548233032227.\n",
      "Epoch 1797. Training loss: 0.7717971205711365. Validation loss: 2.7061541080474854.\n",
      "Epoch 1798. Training loss: 0.7717934250831604. Validation loss: 2.706153154373169.\n",
      "Epoch 1799. Training loss: 0.7717898488044739. Validation loss: 2.7061526775360107.\n",
      "Epoch 1800. Training loss: 0.7717862725257874. Validation loss: 2.7061519622802734.\n",
      "Epoch 1801. Training loss: 0.7717828154563904. Validation loss: 2.706151247024536.\n",
      "Epoch 1802. Training loss: 0.7717792391777039. Validation loss: 2.706151008605957.\n",
      "Epoch 1803. Training loss: 0.7717756628990173. Validation loss: 2.7061500549316406.\n",
      "Epoch 1804. Training loss: 0.7717720866203308. Validation loss: 2.7061493396759033.\n",
      "Epoch 1805. Training loss: 0.7717683911323547. Validation loss: 2.706148862838745.\n",
      "Epoch 1806. Training loss: 0.7717649936676025. Validation loss: 2.7061479091644287.\n",
      "Epoch 1807. Training loss: 0.771761417388916. Validation loss: 2.7061471939086914.\n",
      "Epoch 1808. Training loss: 0.7717578411102295. Validation loss: 2.706146717071533.\n",
      "Epoch 1809. Training loss: 0.771754264831543. Validation loss: 2.706145763397217.\n",
      "Epoch 1810. Training loss: 0.7717506885528564. Validation loss: 2.7061455249786377.\n",
      "Epoch 1811. Training loss: 0.7717471122741699. Validation loss: 2.706144332885742.\n",
      "Epoch 1812. Training loss: 0.7717435956001282. Validation loss: 2.706144094467163.\n",
      "Epoch 1813. Training loss: 0.7717399597167969. Validation loss: 2.7061431407928467.\n",
      "Epoch 1814. Training loss: 0.7717363834381104. Validation loss: 2.7061429023742676.\n",
      "Epoch 1815. Training loss: 0.7717328071594238. Validation loss: 2.706141948699951.\n",
      "Epoch 1816. Training loss: 0.7717292308807373. Validation loss: 2.706141233444214.\n",
      "Epoch 1817. Training loss: 0.7717256546020508. Validation loss: 2.7061402797698975.\n",
      "Epoch 1818. Training loss: 0.7717220783233643. Validation loss: 2.7061398029327393.\n",
      "Epoch 1819. Training loss: 0.7717185020446777. Validation loss: 2.706139326095581.\n",
      "Epoch 1820. Training loss: 0.7717149257659912. Validation loss: 2.7061386108398438.\n",
      "Epoch 1821. Training loss: 0.7717113494873047. Validation loss: 2.7061378955841064.\n",
      "Epoch 1822. Training loss: 0.7717077732086182. Validation loss: 2.7061376571655273.\n",
      "Epoch 1823. Training loss: 0.7717041969299316. Validation loss: 2.706136703491211.\n",
      "Epoch 1824. Training loss: 0.7717006206512451. Validation loss: 2.7061357498168945.\n",
      "Epoch 1825. Training loss: 0.7716970443725586. Validation loss: 2.706134796142578.\n",
      "Epoch 1826. Training loss: 0.7716934680938721. Validation loss: 2.706134557723999.\n",
      "Epoch 1827. Training loss: 0.7716898918151855. Validation loss: 2.706134080886841.\n",
      "Epoch 1828. Training loss: 0.7716862559318542. Validation loss: 2.7061333656311035.\n",
      "Epoch 1829. Training loss: 0.7716827392578125. Validation loss: 2.706132411956787.\n",
      "Epoch 1830. Training loss: 0.7716791033744812. Validation loss: 2.706131935119629.\n",
      "Epoch 1831. Training loss: 0.7716755270957947. Validation loss: 2.7061312198638916.\n",
      "Epoch 1832. Training loss: 0.7716720104217529. Validation loss: 2.7061305046081543.\n",
      "Epoch 1833. Training loss: 0.7716684341430664. Validation loss: 2.706129550933838.\n",
      "Epoch 1834. Training loss: 0.7716646790504456. Validation loss: 2.706129312515259.\n",
      "Epoch 1835. Training loss: 0.7716612219810486. Validation loss: 2.7061285972595215.\n",
      "Epoch 1836. Training loss: 0.7716576457023621. Validation loss: 2.7061281204223633.\n",
      "Epoch 1837. Training loss: 0.7716540694236755. Validation loss: 2.706127166748047.\n",
      "Epoch 1838. Training loss: 0.771650493144989. Validation loss: 2.7061266899108887.\n",
      "Epoch 1839. Training loss: 0.7716467976570129. Validation loss: 2.7061257362365723.\n",
      "Epoch 1840. Training loss: 0.771643340587616. Validation loss: 2.706125020980835.\n",
      "Epoch 1841. Training loss: 0.7716396450996399. Validation loss: 2.706124782562256.\n",
      "Epoch 1842. Training loss: 0.7716360688209534. Validation loss: 2.7061238288879395.\n",
      "Epoch 1843. Training loss: 0.7716324925422668. Validation loss: 2.706123113632202.\n",
      "Epoch 1844. Training loss: 0.7716289162635803. Validation loss: 2.706122398376465.\n",
      "Epoch 1845. Training loss: 0.771625280380249. Validation loss: 2.7061219215393066.\n",
      "Epoch 1846. Training loss: 0.7716217041015625. Validation loss: 2.7061209678649902.\n",
      "Epoch 1847. Training loss: 0.771618127822876. Validation loss: 2.706120491027832.\n",
      "Epoch 1848. Training loss: 0.7716146111488342. Validation loss: 2.7061197757720947.\n",
      "Epoch 1849. Training loss: 0.7716109156608582. Validation loss: 2.7061190605163574.\n",
      "Epoch 1850. Training loss: 0.7716073989868164. Validation loss: 2.706118583679199.\n",
      "Epoch 1851. Training loss: 0.7716037631034851. Validation loss: 2.706117868423462.\n",
      "Epoch 1852. Training loss: 0.7716001868247986. Validation loss: 2.7061169147491455.\n",
      "Epoch 1853. Training loss: 0.7715966105461121. Validation loss: 2.7061164379119873.\n",
      "Epoch 1854. Training loss: 0.7715928554534912. Validation loss: 2.70611572265625.\n",
      "Epoch 1855. Training loss: 0.7715892791748047. Validation loss: 2.706115245819092.\n",
      "Epoch 1856. Training loss: 0.7715857625007629. Validation loss: 2.7061142921447754.\n",
      "Epoch 1857. Training loss: 0.7715821266174316. Validation loss: 2.706113576889038.\n",
      "Epoch 1858. Training loss: 0.7715784907341003. Validation loss: 2.706112861633301.\n",
      "Epoch 1859. Training loss: 0.7715749740600586. Validation loss: 2.7061123847961426.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1860. Training loss: 0.7715713977813721. Validation loss: 2.7061116695404053.\n",
      "Epoch 1861. Training loss: 0.7715677618980408. Validation loss: 2.706110954284668.\n",
      "Epoch 1862. Training loss: 0.771564245223999. Validation loss: 2.7061102390289307.\n",
      "Epoch 1863. Training loss: 0.7715606093406677. Validation loss: 2.7061097621917725.\n",
      "Epoch 1864. Training loss: 0.7715569138526917. Validation loss: 2.706109046936035.\n",
      "Epoch 1865. Training loss: 0.7715532779693604. Validation loss: 2.706108570098877.\n",
      "Epoch 1866. Training loss: 0.7715497612953186. Validation loss: 2.7061071395874023.\n",
      "Epoch 1867. Training loss: 0.7715460658073425. Validation loss: 2.7061069011688232.\n",
      "Epoch 1868. Training loss: 0.771542489528656. Validation loss: 2.706105947494507.\n",
      "Epoch 1869. Training loss: 0.7715389728546143. Validation loss: 2.7061054706573486.\n",
      "Epoch 1870. Training loss: 0.7715353965759277. Validation loss: 2.7061047554016113.\n",
      "Epoch 1871. Training loss: 0.7715317606925964. Validation loss: 2.706104278564453.\n",
      "Epoch 1872. Training loss: 0.7715280652046204. Validation loss: 2.706103563308716.\n",
      "Epoch 1873. Training loss: 0.7715244889259338. Validation loss: 2.7061030864715576.\n",
      "Epoch 1874. Training loss: 0.7715208530426025. Validation loss: 2.706101894378662.\n",
      "Epoch 1875. Training loss: 0.771517276763916. Validation loss: 2.706101417541504.\n",
      "Epoch 1876. Training loss: 0.7715136408805847. Validation loss: 2.7061007022857666.\n",
      "Epoch 1877. Training loss: 0.771510124206543. Validation loss: 2.7061002254486084.\n",
      "Epoch 1878. Training loss: 0.7715064883232117. Validation loss: 2.706099510192871.\n",
      "Epoch 1879. Training loss: 0.7715027928352356. Validation loss: 2.706098794937134.\n",
      "Epoch 1880. Training loss: 0.7714992165565491. Validation loss: 2.7060978412628174.\n",
      "Epoch 1881. Training loss: 0.7714955806732178. Validation loss: 2.706097364425659.\n",
      "Epoch 1882. Training loss: 0.7714920043945312. Validation loss: 2.706096649169922.\n",
      "Epoch 1883. Training loss: 0.7714883685112. Validation loss: 2.7060959339141846.\n",
      "Epoch 1884. Training loss: 0.7714846730232239. Validation loss: 2.7060954570770264.\n",
      "Epoch 1885. Training loss: 0.7714812159538269. Validation loss: 2.70609450340271.\n",
      "Epoch 1886. Training loss: 0.7714776396751404. Validation loss: 2.706094264984131.\n",
      "Epoch 1887. Training loss: 0.7714739441871643. Validation loss: 2.7060933113098145.\n",
      "Epoch 1888. Training loss: 0.7714703679084778. Validation loss: 2.706092596054077.\n",
      "Epoch 1889. Training loss: 0.7714667320251465. Validation loss: 2.70609188079834.\n",
      "Epoch 1890. Training loss: 0.77146315574646. Validation loss: 2.7060914039611816.\n",
      "Epoch 1891. Training loss: 0.7714595198631287. Validation loss: 2.7060906887054443.\n",
      "Epoch 1892. Training loss: 0.7714559435844421. Validation loss: 2.706089496612549.\n",
      "Epoch 1893. Training loss: 0.7714522480964661. Validation loss: 2.7060892581939697.\n",
      "Epoch 1894. Training loss: 0.7714486122131348. Validation loss: 2.7060885429382324.\n",
      "Epoch 1895. Training loss: 0.7714450359344482. Validation loss: 2.706087827682495.\n",
      "Epoch 1896. Training loss: 0.7714412808418274. Validation loss: 2.706087112426758.\n",
      "Epoch 1897. Training loss: 0.7714378237724304. Validation loss: 2.7060863971710205.\n",
      "Epoch 1898. Training loss: 0.7714341282844543. Validation loss: 2.7060859203338623.\n",
      "Epoch 1899. Training loss: 0.771430492401123. Validation loss: 2.706085205078125.\n",
      "Epoch 1900. Training loss: 0.7714269161224365. Validation loss: 2.7060844898223877.\n",
      "Epoch 1901. Training loss: 0.77142333984375. Validation loss: 2.7060837745666504.\n",
      "Epoch 1902. Training loss: 0.7714197635650635. Validation loss: 2.706083059310913.\n",
      "Epoch 1903. Training loss: 0.7714160084724426. Validation loss: 2.706082344055176.\n",
      "Epoch 1904. Training loss: 0.7714123725891113. Validation loss: 2.7060818672180176.\n",
      "Epoch 1905. Training loss: 0.7714088559150696. Validation loss: 2.706080913543701.\n",
      "Epoch 1906. Training loss: 0.7714051604270935. Validation loss: 2.706080913543701.\n",
      "Epoch 1907. Training loss: 0.7714014649391174. Validation loss: 2.7060797214508057.\n",
      "Epoch 1908. Training loss: 0.7713978886604309. Validation loss: 2.7060790061950684.\n",
      "Epoch 1909. Training loss: 0.7713943123817444. Validation loss: 2.706078290939331.\n",
      "Epoch 1910. Training loss: 0.7713906764984131. Validation loss: 2.7060775756835938.\n",
      "Epoch 1911. Training loss: 0.7713870406150818. Validation loss: 2.7060770988464355.\n",
      "Epoch 1912. Training loss: 0.7713833451271057. Validation loss: 2.7060763835906982.\n",
      "Epoch 1913. Training loss: 0.7713797688484192. Validation loss: 2.70607590675354.\n",
      "Epoch 1914. Training loss: 0.7713761329650879. Validation loss: 2.7060747146606445.\n",
      "Epoch 1915. Training loss: 0.7713724970817566. Validation loss: 2.7060742378234863.\n",
      "Epoch 1916. Training loss: 0.7713689804077148. Validation loss: 2.706073522567749.\n",
      "Epoch 1917. Training loss: 0.7713653445243835. Validation loss: 2.7060728073120117.\n",
      "Epoch 1918. Training loss: 0.7713615894317627. Validation loss: 2.7060723304748535.\n",
      "Epoch 1919. Training loss: 0.7713580131530762. Validation loss: 2.706071376800537.\n",
      "Epoch 1920. Training loss: 0.7713543772697449. Validation loss: 2.706070899963379.\n",
      "Epoch 1921. Training loss: 0.7713508009910583. Validation loss: 2.7060704231262207.\n",
      "Epoch 1922. Training loss: 0.7713470458984375. Validation loss: 2.706069231033325.\n",
      "Epoch 1923. Training loss: 0.771343469619751. Validation loss: 2.706068754196167.\n",
      "Epoch 1924. Training loss: 0.7713398933410645. Validation loss: 2.7060680389404297.\n",
      "Epoch 1925. Training loss: 0.7713362574577332. Validation loss: 2.7060673236846924.\n",
      "Epoch 1926. Training loss: 0.7713327407836914. Validation loss: 2.706066846847534.\n",
      "Epoch 1927. Training loss: 0.771329402923584. Validation loss: 2.706066131591797.\n",
      "Epoch 1928. Training loss: 0.7713260054588318. Validation loss: 2.7060654163360596.\n",
      "Epoch 1929. Training loss: 0.7713226675987244. Validation loss: 2.7060649394989014.\n",
      "Epoch 1930. Training loss: 0.7713192105293274. Validation loss: 2.706063985824585.\n",
      "Epoch 1931. Training loss: 0.7713158130645752. Validation loss: 2.7060632705688477.\n",
      "Epoch 1932. Training loss: 0.771312415599823. Validation loss: 2.7060627937316895.\n",
      "Epoch 1933. Training loss: 0.7713090777397156. Validation loss: 2.706061840057373.\n",
      "Epoch 1934. Training loss: 0.7713057398796082. Validation loss: 2.706061601638794.\n",
      "Epoch 1935. Training loss: 0.7713022828102112. Validation loss: 2.7060608863830566.\n",
      "Epoch 1936. Training loss: 0.771298885345459. Validation loss: 2.7060601711273193.\n",
      "Epoch 1937. Training loss: 0.7712955474853516. Validation loss: 2.706059455871582.\n",
      "Epoch 1938. Training loss: 0.7712921500205994. Validation loss: 2.706058979034424.\n",
      "Epoch 1939. Training loss: 0.7712886929512024. Validation loss: 2.7060580253601074.\n",
      "Epoch 1940. Training loss: 0.771285355091095. Validation loss: 2.70605731010437.\n",
      "Epoch 1941. Training loss: 0.7712820172309875. Validation loss: 2.706057071685791.\n",
      "Epoch 1942. Training loss: 0.7712785601615906. Validation loss: 2.7060558795928955.\n",
      "Epoch 1943. Training loss: 0.7712752223014832. Validation loss: 2.7060556411743164.\n",
      "Epoch 1944. Training loss: 0.7712717056274414. Validation loss: 2.706054925918579.\n",
      "Epoch 1945. Training loss: 0.7712685465812683. Validation loss: 2.706054449081421.\n",
      "Epoch 1946. Training loss: 0.7712650299072266. Validation loss: 2.7060537338256836.\n",
      "Epoch 1947. Training loss: 0.7712616920471191. Validation loss: 2.706052780151367.\n",
      "Epoch 1948. Training loss: 0.7712581753730774. Validation loss: 2.706052303314209.\n",
      "Epoch 1949. Training loss: 0.7712549567222595. Validation loss: 2.7060513496398926.\n",
      "Epoch 1950. Training loss: 0.7712514996528625. Validation loss: 2.7060508728027344.\n",
      "Epoch 1951. Training loss: 0.7712481021881104. Validation loss: 2.706050157546997.\n",
      "Epoch 1952. Training loss: 0.7712447047233582. Validation loss: 2.7060494422912598.\n",
      "Epoch 1953. Training loss: 0.7712412476539612. Validation loss: 2.7060489654541016.\n",
      "Epoch 1954. Training loss: 0.7712379097938538. Validation loss: 2.7060482501983643.\n",
      "Epoch 1955. Training loss: 0.7712346911430359. Validation loss: 2.706047296524048.\n",
      "Epoch 1956. Training loss: 0.7712311744689941. Validation loss: 2.7060465812683105.\n",
      "Epoch 1957. Training loss: 0.7712277770042419. Validation loss: 2.7060461044311523.\n",
      "Epoch 1958. Training loss: 0.771224319934845. Validation loss: 2.706045627593994.\n",
      "Epoch 1959. Training loss: 0.7712209820747375. Validation loss: 2.706045150756836.\n",
      "Epoch 1960. Training loss: 0.7712176442146301. Validation loss: 2.7060446739196777.\n",
      "Epoch 1961. Training loss: 0.7712142467498779. Validation loss: 2.7060437202453613.\n",
      "Epoch 1962. Training loss: 0.7712109088897705. Validation loss: 2.706043243408203.\n",
      "Epoch 1963. Training loss: 0.7712073922157288. Validation loss: 2.7060422897338867.\n",
      "Epoch 1964. Training loss: 0.7712040543556213. Validation loss: 2.7060420513153076.\n",
      "Epoch 1965. Training loss: 0.7712006568908691. Validation loss: 2.7060413360595703.\n",
      "Epoch 1966. Training loss: 0.7711972594261169. Validation loss: 2.706040382385254.\n",
      "Epoch 1967. Training loss: 0.7711939215660095. Validation loss: 2.7060396671295166.\n",
      "Epoch 1968. Training loss: 0.7711905837059021. Validation loss: 2.7060389518737793.\n",
      "Epoch 1969. Training loss: 0.7711870074272156. Validation loss: 2.706038475036621.\n",
      "Epoch 1970. Training loss: 0.7711837291717529. Validation loss: 2.706037998199463.\n",
      "Epoch 1971. Training loss: 0.7711803317070007. Validation loss: 2.7060370445251465.\n",
      "Epoch 1972. Training loss: 0.7711769938468933. Validation loss: 2.7060365676879883.\n",
      "Epoch 1973. Training loss: 0.7711736559867859. Validation loss: 2.70603609085083.\n",
      "Epoch 1974. Training loss: 0.7711701989173889. Validation loss: 2.7060348987579346.\n",
      "Epoch 1975. Training loss: 0.7711668610572815. Validation loss: 2.7060346603393555.\n",
      "Epoch 1976. Training loss: 0.7711634635925293. Validation loss: 2.706033706665039.\n",
      "Epoch 1977. Training loss: 0.7711601257324219. Validation loss: 2.7060329914093018.\n",
      "Epoch 1978. Training loss: 0.7711567878723145. Validation loss: 2.7060322761535645.\n",
      "Epoch 1979. Training loss: 0.771153450012207. Validation loss: 2.706031560897827.\n",
      "Epoch 1980. Training loss: 0.7711499333381653. Validation loss: 2.706031084060669.\n",
      "Epoch 1981. Training loss: 0.7711465954780579. Validation loss: 2.7060303688049316.\n",
      "Epoch 1982. Training loss: 0.7711432576179504. Validation loss: 2.7060298919677734.\n",
      "Epoch 1983. Training loss: 0.771139919757843. Validation loss: 2.706029176712036.\n",
      "Epoch 1984. Training loss: 0.7711365222930908. Validation loss: 2.706028938293457.\n",
      "Epoch 1985. Training loss: 0.7711331844329834. Validation loss: 2.706028461456299.\n",
      "Epoch 1986. Training loss: 0.7711297869682312. Validation loss: 2.7060279846191406.\n",
      "Epoch 1987. Training loss: 0.7711265087127686. Validation loss: 2.706027030944824.\n",
      "Epoch 1988. Training loss: 0.7711231112480164. Validation loss: 2.706026554107666.\n",
      "Epoch 1989. Training loss: 0.7711197733879089. Validation loss: 2.706026077270508.\n",
      "Epoch 1990. Training loss: 0.771116316318512. Validation loss: 2.7060256004333496.\n",
      "Epoch 1991. Training loss: 0.7711129188537598. Validation loss: 2.7060251235961914.\n",
      "Epoch 1992. Training loss: 0.7711095809936523. Validation loss: 2.7060248851776123.\n",
      "Epoch 1993. Training loss: 0.7711062431335449. Validation loss: 2.706023931503296.\n",
      "Epoch 1994. Training loss: 0.7711028456687927. Validation loss: 2.706023693084717.\n",
      "Epoch 1995. Training loss: 0.7710995078086853. Validation loss: 2.7060229778289795.\n",
      "Epoch 1996. Training loss: 0.7710960507392883. Validation loss: 2.7060225009918213.\n",
      "Epoch 1997. Training loss: 0.7710926532745361. Validation loss: 2.706022024154663.\n",
      "Epoch 1998. Training loss: 0.7710893154144287. Validation loss: 2.706021547317505.\n",
      "Epoch 1999. Training loss: 0.7710859775543213. Validation loss: 2.7060208320617676.\n",
      "Epoch 2000. Training loss: 0.7710826396942139. Validation loss: 2.7060205936431885.\n",
      "Epoch 2001. Training loss: 0.7710793018341064. Validation loss: 2.7060201168060303.\n",
      "Epoch 2002. Training loss: 0.7710759043693542. Validation loss: 2.706019401550293.\n",
      "Epoch 2003. Training loss: 0.7710724472999573. Validation loss: 2.7060189247131348.\n",
      "Epoch 2004. Training loss: 0.7710690498352051. Validation loss: 2.7060184478759766.\n",
      "Epoch 2005. Training loss: 0.7710656523704529. Validation loss: 2.7060179710388184.\n",
      "Epoch 2006. Training loss: 0.7710623741149902. Validation loss: 2.70601749420166.\n",
      "Epoch 2007. Training loss: 0.7710588574409485. Validation loss: 2.706016778945923.\n",
      "Epoch 2008. Training loss: 0.7710554599761963. Validation loss: 2.7060160636901855.\n",
      "Epoch 2009. Training loss: 0.7710521817207336. Validation loss: 2.7060155868530273.\n",
      "Epoch 2010. Training loss: 0.7710488438606262. Validation loss: 2.706015110015869.\n",
      "Epoch 2011. Training loss: 0.771045446395874. Validation loss: 2.706014633178711.\n",
      "Epoch 2012. Training loss: 0.7710420489311218. Validation loss: 2.7060141563415527.\n",
      "Epoch 2013. Training loss: 0.7710387110710144. Validation loss: 2.7060136795043945.\n",
      "Epoch 2014. Training loss: 0.7710352540016174. Validation loss: 2.7060132026672363.\n",
      "Epoch 2015. Training loss: 0.77103191614151. Validation loss: 2.706012725830078.\n",
      "Epoch 2016. Training loss: 0.7710285186767578. Validation loss: 2.70601224899292.\n",
      "Epoch 2017. Training loss: 0.7710251808166504. Validation loss: 2.7060117721557617.\n",
      "Epoch 2018. Training loss: 0.771021842956543. Validation loss: 2.7060110569000244.\n",
      "Epoch 2019. Training loss: 0.7710184454917908. Validation loss: 2.706010341644287.\n",
      "Epoch 2020. Training loss: 0.7710149884223938. Validation loss: 2.706010103225708.\n",
      "Epoch 2021. Training loss: 0.7710115909576416. Validation loss: 2.7060093879699707.\n",
      "Epoch 2022. Training loss: 0.7710082530975342. Validation loss: 2.7060089111328125.\n",
      "Epoch 2023. Training loss: 0.771004855632782. Validation loss: 2.7060084342956543.\n",
      "Epoch 2024. Training loss: 0.7710015177726746. Validation loss: 2.706007957458496.\n",
      "Epoch 2025. Training loss: 0.7709980607032776. Validation loss: 2.706007242202759.\n",
      "Epoch 2026. Training loss: 0.7709946632385254. Validation loss: 2.7060067653656006.\n",
      "Epoch 2027. Training loss: 0.7709912657737732. Validation loss: 2.7060062885284424.\n",
      "Epoch 2028. Training loss: 0.7709879279136658. Validation loss: 2.706005811691284.\n",
      "Epoch 2029. Training loss: 0.7709845900535583. Validation loss: 2.706005096435547.\n",
      "Epoch 2030. Training loss: 0.7709811329841614. Validation loss: 2.7060043811798096.\n",
      "Epoch 2031. Training loss: 0.770977795124054. Validation loss: 2.7060039043426514.\n",
      "Epoch 2032. Training loss: 0.7709743976593018. Validation loss: 2.7060036659240723.\n",
      "Epoch 2033. Training loss: 0.7709710001945496. Validation loss: 2.706003189086914.\n",
      "Epoch 2034. Training loss: 0.7709676623344421. Validation loss: 2.7060022354125977.\n",
      "Epoch 2035. Training loss: 0.7709642052650452. Validation loss: 2.7060019969940186.\n",
      "Epoch 2036. Training loss: 0.770960807800293. Validation loss: 2.7060017585754395.\n",
      "Epoch 2037. Training loss: 0.7709574699401855. Validation loss: 2.706000804901123.\n",
      "Epoch 2038. Training loss: 0.7709540724754333. Validation loss: 2.706000328063965.\n",
      "Epoch 2039. Training loss: 0.7709507346153259. Validation loss: 2.7059996128082275.\n",
      "Epoch 2040. Training loss: 0.770947277545929. Validation loss: 2.7059991359710693.\n",
      "Epoch 2041. Training loss: 0.7709439396858215. Validation loss: 2.7059988975524902.\n",
      "Epoch 2042. Training loss: 0.7709404826164246. Validation loss: 2.705998420715332.\n",
      "Epoch 2043. Training loss: 0.7709372043609619. Validation loss: 2.705997943878174.\n",
      "Epoch 2044. Training loss: 0.7709336876869202. Validation loss: 2.7059974670410156.\n",
      "Epoch 2045. Training loss: 0.770930290222168. Validation loss: 2.705996513366699.\n",
      "Epoch 2046. Training loss: 0.7709269523620605. Validation loss: 2.705996036529541.\n",
      "Epoch 2047. Training loss: 0.7709235548973083. Validation loss: 2.705995559692383.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2048. Training loss: 0.7709202170372009. Validation loss: 2.7059950828552246.\n",
      "Epoch 2049. Training loss: 0.770916759967804. Validation loss: 2.7059948444366455.\n",
      "Epoch 2050. Training loss: 0.7709133625030518. Validation loss: 2.70599365234375.\n",
      "Epoch 2051. Training loss: 0.7709100246429443. Validation loss: 2.70599365234375.\n",
      "Epoch 2052. Training loss: 0.7709066867828369. Validation loss: 2.705993175506592.\n",
      "Epoch 2053. Training loss: 0.7709031105041504. Validation loss: 2.7059926986694336.\n",
      "Epoch 2054. Training loss: 0.770899772644043. Validation loss: 2.7059919834136963.\n",
      "Epoch 2055. Training loss: 0.7708964347839355. Validation loss: 2.705991506576538.\n",
      "Epoch 2056. Training loss: 0.7708929181098938. Validation loss: 2.70599102973938.\n",
      "Epoch 2057. Training loss: 0.7708895802497864. Validation loss: 2.7059903144836426.\n",
      "Epoch 2058. Training loss: 0.7708861827850342. Validation loss: 2.7059895992279053.\n",
      "Epoch 2059. Training loss: 0.7708828449249268. Validation loss: 2.705989360809326.\n",
      "Epoch 2060. Training loss: 0.7708794474601746. Validation loss: 2.705988883972168.\n",
      "Epoch 2061. Training loss: 0.7708759903907776. Validation loss: 2.7059884071350098.\n",
      "Epoch 2062. Training loss: 0.7708725929260254. Validation loss: 2.7059879302978516.\n",
      "Epoch 2063. Training loss: 0.7708691954612732. Validation loss: 2.7059874534606934.\n",
      "Epoch 2064. Training loss: 0.7708657383918762. Validation loss: 2.705986499786377.\n",
      "Epoch 2065. Training loss: 0.7708624005317688. Validation loss: 2.7059860229492188.\n",
      "Epoch 2066. Training loss: 0.7708589434623718. Validation loss: 2.7059855461120605.\n",
      "Epoch 2067. Training loss: 0.7708556056022644. Validation loss: 2.7059850692749023.\n",
      "Epoch 2068. Training loss: 0.7708521485328674. Validation loss: 2.7059848308563232.\n",
      "Epoch 2069. Training loss: 0.77084881067276. Validation loss: 2.705984115600586.\n",
      "Epoch 2070. Training loss: 0.7708454132080078. Validation loss: 2.7059834003448486.\n",
      "Epoch 2071. Training loss: 0.7708418965339661. Validation loss: 2.7059831619262695.\n",
      "Epoch 2072. Training loss: 0.7708384990692139. Validation loss: 2.7059824466705322.\n",
      "Epoch 2073. Training loss: 0.7708352208137512. Validation loss: 2.705981969833374.\n",
      "Epoch 2074. Training loss: 0.7708317637443542. Validation loss: 2.705981492996216.\n",
      "Epoch 2075. Training loss: 0.7708284258842468. Validation loss: 2.7059812545776367.\n",
      "Epoch 2076. Training loss: 0.7708249092102051. Validation loss: 2.705980062484741.\n",
      "Epoch 2077. Training loss: 0.7708215713500977. Validation loss: 2.705980062484741.\n",
      "Epoch 2078. Training loss: 0.7708181738853455. Validation loss: 2.705979585647583.\n",
      "Epoch 2079. Training loss: 0.7708147168159485. Validation loss: 2.705979108810425.\n",
      "Epoch 2080. Training loss: 0.7708113193511963. Validation loss: 2.7059783935546875.\n",
      "Epoch 2081. Training loss: 0.7708079218864441. Validation loss: 2.70597767829895.\n",
      "Epoch 2082. Training loss: 0.7708044648170471. Validation loss: 2.705977439880371.\n",
      "Epoch 2083. Training loss: 0.7708010673522949. Validation loss: 2.705976963043213.\n",
      "Epoch 2084. Training loss: 0.7707976698875427. Validation loss: 2.7059764862060547.\n",
      "Epoch 2085. Training loss: 0.7707943320274353. Validation loss: 2.7059757709503174.\n",
      "Epoch 2086. Training loss: 0.7707908749580383. Validation loss: 2.705975294113159.\n",
      "Epoch 2087. Training loss: 0.7707875370979309. Validation loss: 2.705974817276001.\n",
      "Epoch 2088. Training loss: 0.7707840800285339. Validation loss: 2.7059741020202637.\n",
      "Epoch 2089. Training loss: 0.770780622959137. Validation loss: 2.7059736251831055.\n",
      "Epoch 2090. Training loss: 0.7707772254943848. Validation loss: 2.7059731483459473.\n",
      "Epoch 2091. Training loss: 0.7707738876342773. Validation loss: 2.705972671508789.\n",
      "Epoch 2092. Training loss: 0.7707703709602356. Validation loss: 2.7059719562530518.\n",
      "Epoch 2093. Training loss: 0.7707669734954834. Validation loss: 2.7059714794158936.\n",
      "Epoch 2094. Training loss: 0.7707635760307312. Validation loss: 2.7059710025787354.\n",
      "Epoch 2095. Training loss: 0.7707600593566895. Validation loss: 2.7059707641601562.\n",
      "Epoch 2096. Training loss: 0.7707567811012268. Validation loss: 2.70596981048584.\n",
      "Epoch 2097. Training loss: 0.7707532048225403. Validation loss: 2.7059693336486816.\n",
      "Epoch 2098. Training loss: 0.7707499861717224. Validation loss: 2.7059688568115234.\n",
      "Epoch 2099. Training loss: 0.7707464694976807. Validation loss: 2.7059683799743652.\n",
      "Epoch 2100. Training loss: 0.7707431316375732. Validation loss: 2.705967903137207.\n",
      "Epoch 2101. Training loss: 0.770739734172821. Validation loss: 2.705967426300049.\n",
      "Epoch 2102. Training loss: 0.7707362174987793. Validation loss: 2.7059669494628906.\n",
      "Epoch 2103. Training loss: 0.7707328796386719. Validation loss: 2.705965995788574.\n",
      "Epoch 2104. Training loss: 0.7707294821739197. Validation loss: 2.705965518951416.\n",
      "Epoch 2105. Training loss: 0.7707259654998779. Validation loss: 2.705965280532837.\n",
      "Epoch 2106. Training loss: 0.7707225680351257. Validation loss: 2.7059648036956787.\n",
      "Epoch 2107. Training loss: 0.7707191109657288. Validation loss: 2.7059640884399414.\n",
      "Epoch 2108. Training loss: 0.7707157135009766. Validation loss: 2.705963611602783.\n",
      "Epoch 2109. Training loss: 0.7707123160362244. Validation loss: 2.705963373184204.\n",
      "Epoch 2110. Training loss: 0.7707088589668274. Validation loss: 2.705962896347046.\n",
      "Epoch 2111. Training loss: 0.7707054615020752. Validation loss: 2.7059619426727295.\n",
      "Epoch 2112. Training loss: 0.7707021236419678. Validation loss: 2.7059614658355713.\n",
      "Epoch 2113. Training loss: 0.770698606967926. Validation loss: 2.705960988998413.\n",
      "Epoch 2114. Training loss: 0.770695149898529. Validation loss: 2.705960750579834.\n",
      "Epoch 2115. Training loss: 0.7706918120384216. Validation loss: 2.7059600353240967.\n",
      "Epoch 2116. Training loss: 0.7706883549690247. Validation loss: 2.7059595584869385.\n",
      "Epoch 2117. Training loss: 0.7706849575042725. Validation loss: 2.705958843231201.\n",
      "Epoch 2118. Training loss: 0.7706815600395203. Validation loss: 2.705958366394043.\n",
      "Epoch 2119. Training loss: 0.7706781029701233. Validation loss: 2.7059578895568848.\n",
      "Epoch 2120. Training loss: 0.7706746459007263. Validation loss: 2.7059574127197266.\n",
      "Epoch 2121. Training loss: 0.7706711888313293. Validation loss: 2.7059566974639893.\n",
      "Epoch 2122. Training loss: 0.7706678509712219. Validation loss: 2.70595645904541.\n",
      "Epoch 2123. Training loss: 0.770664393901825. Validation loss: 2.705955743789673.\n",
      "Epoch 2124. Training loss: 0.7706610560417175. Validation loss: 2.7059552669525146.\n",
      "Epoch 2125. Training loss: 0.7706575393676758. Validation loss: 2.7059547901153564.\n",
      "Epoch 2126. Training loss: 0.7706542015075684. Validation loss: 2.705954074859619.\n",
      "Epoch 2127. Training loss: 0.7706506848335266. Validation loss: 2.705953598022461.\n",
      "Epoch 2128. Training loss: 0.7706472873687744. Validation loss: 2.705953359603882.\n",
      "Epoch 2129. Training loss: 0.7706437706947327. Validation loss: 2.7059526443481445.\n",
      "Epoch 2130. Training loss: 0.7706403732299805. Validation loss: 2.7059524059295654.\n",
      "Epoch 2131. Training loss: 0.770637035369873. Validation loss: 2.705951452255249.\n",
      "Epoch 2132. Training loss: 0.7706335186958313. Validation loss: 2.7059507369995117.\n",
      "Epoch 2133. Training loss: 0.7706301808357239. Validation loss: 2.7059504985809326.\n",
      "Epoch 2134. Training loss: 0.7706267237663269. Validation loss: 2.7059500217437744.\n",
      "Epoch 2135. Training loss: 0.7706232666969299. Validation loss: 2.705949306488037.\n",
      "Epoch 2136. Training loss: 0.770619809627533. Validation loss: 2.705948829650879.\n",
      "Epoch 2137. Training loss: 0.770616352558136. Validation loss: 2.7059483528137207.\n",
      "Epoch 2138. Training loss: 0.770612895488739. Validation loss: 2.7059478759765625.\n",
      "Epoch 2139. Training loss: 0.7706095576286316. Validation loss: 2.7059473991394043.\n",
      "Epoch 2140. Training loss: 0.7706060409545898. Validation loss: 2.705946922302246.\n",
      "Epoch 2141. Training loss: 0.7706027030944824. Validation loss: 2.705946445465088.\n",
      "Epoch 2142. Training loss: 0.7705991864204407. Validation loss: 2.7059454917907715.\n",
      "Epoch 2143. Training loss: 0.7705957293510437. Validation loss: 2.7059450149536133.\n",
      "Epoch 2144. Training loss: 0.7705923914909363. Validation loss: 2.705944538116455.\n",
      "Epoch 2145. Training loss: 0.7705888748168945. Validation loss: 2.705944538116455.\n",
      "Epoch 2146. Training loss: 0.7705854773521423. Validation loss: 2.7059435844421387.\n",
      "Epoch 2147. Training loss: 0.7705821394920349. Validation loss: 2.7059433460235596.\n",
      "Epoch 2148. Training loss: 0.7705785632133484. Validation loss: 2.7059428691864014.\n",
      "Epoch 2149. Training loss: 0.770575225353241. Validation loss: 2.705942153930664.\n",
      "Epoch 2150. Training loss: 0.7705717086791992. Validation loss: 2.7059414386749268.\n",
      "Epoch 2151. Training loss: 0.7705681920051575. Validation loss: 2.7059407234191895.\n",
      "Epoch 2152. Training loss: 0.77056485414505. Validation loss: 2.7059402465820312.\n",
      "Epoch 2153. Training loss: 0.7705613970756531. Validation loss: 2.705939769744873.\n",
      "Epoch 2154. Training loss: 0.7705578804016113. Validation loss: 2.705939292907715.\n",
      "Epoch 2155. Training loss: 0.7705545425415039. Validation loss: 2.7059388160705566.\n",
      "Epoch 2156. Training loss: 0.7705510258674622. Validation loss: 2.7059381008148193.\n",
      "Epoch 2157. Training loss: 0.77054762840271. Validation loss: 2.7059378623962402.\n",
      "Epoch 2158. Training loss: 0.7705442309379578. Validation loss: 2.705937147140503.\n",
      "Epoch 2159. Training loss: 0.770540714263916. Validation loss: 2.705936908721924.\n",
      "Epoch 2160. Training loss: 0.7705373167991638. Validation loss: 2.7059359550476074.\n",
      "Epoch 2161. Training loss: 0.7705338001251221. Validation loss: 2.705935478210449.\n",
      "Epoch 2162. Training loss: 0.7705304026603699. Validation loss: 2.70593523979187.\n",
      "Epoch 2163. Training loss: 0.7705269455909729. Validation loss: 2.705934524536133.\n",
      "Epoch 2164. Training loss: 0.7705234885215759. Validation loss: 2.7059340476989746.\n",
      "Epoch 2165. Training loss: 0.770520031452179. Validation loss: 2.7059333324432373.\n",
      "Epoch 2166. Training loss: 0.7705166339874268. Validation loss: 2.705932855606079.\n",
      "Epoch 2167. Training loss: 0.770513117313385. Validation loss: 2.7059326171875.\n",
      "Epoch 2168. Training loss: 0.7705097794532776. Validation loss: 2.705932140350342.\n",
      "Epoch 2169. Training loss: 0.7705063223838806. Validation loss: 2.7059314250946045.\n",
      "Epoch 2170. Training loss: 0.7705028057098389. Validation loss: 2.705930709838867.\n",
      "Epoch 2171. Training loss: 0.7704992890357971. Validation loss: 2.705930471420288.\n",
      "Epoch 2172. Training loss: 0.7704959511756897. Validation loss: 2.705929756164551.\n",
      "Epoch 2173. Training loss: 0.7704924941062927. Validation loss: 2.7059292793273926.\n",
      "Epoch 2174. Training loss: 0.7704890370368958. Validation loss: 2.7059288024902344.\n",
      "Epoch 2175. Training loss: 0.7704855799674988. Validation loss: 2.705928087234497.\n",
      "Epoch 2176. Training loss: 0.770482063293457. Validation loss: 2.705927848815918.\n",
      "Epoch 2177. Training loss: 0.7704786658287048. Validation loss: 2.7059268951416016.\n",
      "Epoch 2178. Training loss: 0.7704751491546631. Validation loss: 2.7059266567230225.\n",
      "Epoch 2179. Training loss: 0.7704717516899109. Validation loss: 2.705925941467285.\n",
      "Epoch 2180. Training loss: 0.7704682946205139. Validation loss: 2.705925703048706.\n",
      "Epoch 2181. Training loss: 0.7704648971557617. Validation loss: 2.7059249877929688.\n",
      "Epoch 2182. Training loss: 0.77046138048172. Validation loss: 2.7059245109558105.\n",
      "Epoch 2183. Training loss: 0.770457923412323. Validation loss: 2.7059240341186523.\n",
      "Epoch 2184. Training loss: 0.770454466342926. Validation loss: 2.705923080444336.\n",
      "Epoch 2185. Training loss: 0.770451009273529. Validation loss: 2.705922842025757.\n",
      "Epoch 2186. Training loss: 0.7704475522041321. Validation loss: 2.7059226036071777.\n",
      "Epoch 2187. Training loss: 0.7704440951347351. Validation loss: 2.7059218883514404.\n",
      "Epoch 2188. Training loss: 0.7704406380653381. Validation loss: 2.705921173095703.\n",
      "Epoch 2189. Training loss: 0.7704372406005859. Validation loss: 2.705920696258545.\n",
      "Epoch 2190. Training loss: 0.7704337239265442. Validation loss: 2.7059202194213867.\n",
      "Epoch 2191. Training loss: 0.7704302668571472. Validation loss: 2.7059197425842285.\n",
      "Epoch 2192. Training loss: 0.7704268097877502. Validation loss: 2.705919027328491.\n",
      "Epoch 2193. Training loss: 0.770423412322998. Validation loss: 2.705918788909912.\n",
      "Epoch 2194. Training loss: 0.7704198956489563. Validation loss: 2.7059178352355957.\n",
      "Epoch 2195. Training loss: 0.7704164385795593. Validation loss: 2.7059173583984375.\n",
      "Epoch 2196. Training loss: 0.7704129815101624. Validation loss: 2.7059171199798584.\n",
      "Epoch 2197. Training loss: 0.7704095244407654. Validation loss: 2.705916404724121.\n",
      "Epoch 2198. Training loss: 0.7704060077667236. Validation loss: 2.705915689468384.\n",
      "Epoch 2199. Training loss: 0.7704026103019714. Validation loss: 2.7059152126312256.\n",
      "Epoch 2200. Training loss: 0.7703990936279297. Validation loss: 2.7059149742126465.\n",
      "Epoch 2201. Training loss: 0.7703956961631775. Validation loss: 2.7059144973754883.\n",
      "Epoch 2202. Training loss: 0.7703921794891357. Validation loss: 2.705913543701172.\n",
      "Epoch 2203. Training loss: 0.7703887820243835. Validation loss: 2.7059130668640137.\n",
      "Epoch 2204. Training loss: 0.7703852653503418. Validation loss: 2.7059128284454346.\n",
      "Epoch 2205. Training loss: 0.7703818678855896. Validation loss: 2.7059123516082764.\n",
      "Epoch 2206. Training loss: 0.7703783512115479. Validation loss: 2.705911636352539.\n",
      "Epoch 2207. Training loss: 0.7703748345375061. Validation loss: 2.7059109210968018.\n",
      "Epoch 2208. Training loss: 0.7703714370727539. Validation loss: 2.7059106826782227.\n",
      "Epoch 2209. Training loss: 0.7703679203987122. Validation loss: 2.7059097290039062.\n",
      "Epoch 2210. Training loss: 0.77036452293396. Validation loss: 2.705909490585327.\n",
      "Epoch 2211. Training loss: 0.7703609466552734. Validation loss: 2.705909252166748.\n",
      "Epoch 2212. Training loss: 0.7703574299812317. Validation loss: 2.7059082984924316.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2213. Training loss: 0.7703542113304138. Validation loss: 2.7059080600738525.\n",
      "Epoch 2214. Training loss: 0.7703506350517273. Validation loss: 2.7059073448181152.\n",
      "Epoch 2215. Training loss: 0.7703471183776855. Validation loss: 2.705906867980957.\n",
      "Epoch 2216. Training loss: 0.7703437209129333. Validation loss: 2.705906391143799.\n",
      "Epoch 2217. Training loss: 0.7703402042388916. Validation loss: 2.7059059143066406.\n",
      "Epoch 2218. Training loss: 0.7703366875648499. Validation loss: 2.7059051990509033.\n",
      "Epoch 2219. Training loss: 0.7703332304954529. Validation loss: 2.705904483795166.\n",
      "Epoch 2220. Training loss: 0.7703297138214111. Validation loss: 2.705904006958008.\n",
      "Epoch 2221. Training loss: 0.7703263163566589. Validation loss: 2.7059035301208496.\n",
      "Epoch 2222. Training loss: 0.770322859287262. Validation loss: 2.7059030532836914.\n",
      "Epoch 2223. Training loss: 0.7703192830085754. Validation loss: 2.705902576446533.\n",
      "Epoch 2224. Training loss: 0.7703158259391785. Validation loss: 2.705902099609375.\n",
      "Epoch 2225. Training loss: 0.770312488079071. Validation loss: 2.705901622772217.\n",
      "Epoch 2226. Training loss: 0.7703089118003845. Validation loss: 2.7059009075164795.\n",
      "Epoch 2227. Training loss: 0.7703053951263428. Validation loss: 2.705900192260742.\n",
      "Epoch 2228. Training loss: 0.7703019976615906. Validation loss: 2.705899715423584.\n",
      "Epoch 2229. Training loss: 0.7702984809875488. Validation loss: 2.705899238586426.\n",
      "Epoch 2230. Training loss: 0.7702949643135071. Validation loss: 2.7058987617492676.\n",
      "Epoch 2231. Training loss: 0.7702915668487549. Validation loss: 2.7058982849121094.\n",
      "Epoch 2232. Training loss: 0.7702880501747131. Validation loss: 2.705897808074951.\n",
      "Epoch 2233. Training loss: 0.7702845931053162. Validation loss: 2.705897092819214.\n",
      "Epoch 2234. Training loss: 0.7702811360359192. Validation loss: 2.7058966159820557.\n",
      "Epoch 2235. Training loss: 0.7702775597572327. Validation loss: 2.7058959007263184.\n",
      "Epoch 2236. Training loss: 0.7702741622924805. Validation loss: 2.70589542388916.\n",
      "Epoch 2237. Training loss: 0.7702706456184387. Validation loss: 2.705895185470581.\n",
      "Epoch 2238. Training loss: 0.7702671885490417. Validation loss: 2.705894708633423.\n",
      "Epoch 2239. Training loss: 0.770263671875. Validation loss: 2.7058937549591064.\n",
      "Epoch 2240. Training loss: 0.7702601552009583. Validation loss: 2.7058932781219482.\n",
      "Epoch 2241. Training loss: 0.7702566981315613. Validation loss: 2.70589280128479.\n",
      "Epoch 2242. Training loss: 0.7702532410621643. Validation loss: 2.705892562866211.\n",
      "Epoch 2243. Training loss: 0.7702497839927673. Validation loss: 2.7058916091918945.\n",
      "Epoch 2244. Training loss: 0.7702462077140808. Validation loss: 2.7058911323547363.\n",
      "Epoch 2245. Training loss: 0.7702427506446838. Validation loss: 2.7058908939361572.\n",
      "Epoch 2246. Training loss: 0.7702393531799316. Validation loss: 2.70589017868042.\n",
      "Epoch 2247. Training loss: 0.7702358365058899. Validation loss: 2.7058897018432617.\n",
      "Epoch 2248. Training loss: 0.7702323794364929. Validation loss: 2.7058892250061035.\n",
      "Epoch 2249. Training loss: 0.7702288031578064. Validation loss: 2.705888509750366.\n",
      "Epoch 2250. Training loss: 0.7702253460884094. Validation loss: 2.705888271331787.\n",
      "Epoch 2251. Training loss: 0.7702218890190125. Validation loss: 2.70588755607605.\n",
      "Epoch 2252. Training loss: 0.7702184319496155. Validation loss: 2.7058868408203125.\n",
      "Epoch 2253. Training loss: 0.770214855670929. Validation loss: 2.7058863639831543.\n",
      "Epoch 2254. Training loss: 0.770211398601532. Validation loss: 2.705885887145996.\n",
      "Epoch 2255. Training loss: 0.7702078819274902. Validation loss: 2.705885171890259.\n",
      "Epoch 2256. Training loss: 0.770204484462738. Validation loss: 2.7058849334716797.\n",
      "Epoch 2257. Training loss: 0.7702009081840515. Validation loss: 2.7058844566345215.\n",
      "Epoch 2258. Training loss: 0.7701973915100098. Validation loss: 2.705883741378784.\n",
      "Epoch 2259. Training loss: 0.7701939940452576. Validation loss: 2.705883264541626.\n",
      "Epoch 2260. Training loss: 0.7701904773712158. Validation loss: 2.7058825492858887.\n",
      "Epoch 2261. Training loss: 0.7701869010925293. Validation loss: 2.7058820724487305.\n",
      "Epoch 2262. Training loss: 0.7701835036277771. Validation loss: 2.7058815956115723.\n",
      "Epoch 2263. Training loss: 0.7701799273490906. Validation loss: 2.705881118774414.\n",
      "Epoch 2264. Training loss: 0.7701764106750488. Validation loss: 2.7058804035186768.\n",
      "Epoch 2265. Training loss: 0.7701728940010071. Validation loss: 2.7058796882629395.\n",
      "Epoch 2266. Training loss: 0.7701694965362549. Validation loss: 2.7058792114257812.\n",
      "Epoch 2267. Training loss: 0.7701659202575684. Validation loss: 2.705878973007202.\n",
      "Epoch 2268. Training loss: 0.7701624035835266. Validation loss: 2.705878257751465.\n",
      "Epoch 2269. Training loss: 0.7701589465141296. Validation loss: 2.7058777809143066.\n",
      "Epoch 2270. Training loss: 0.7701554298400879. Validation loss: 2.7058770656585693.\n",
      "Epoch 2271. Training loss: 0.7701519131660461. Validation loss: 2.705876350402832.\n",
      "Epoch 2272. Training loss: 0.7701484560966492. Validation loss: 2.705875873565674.\n",
      "Epoch 2273. Training loss: 0.7701449394226074. Validation loss: 2.7058756351470947.\n",
      "Epoch 2274. Training loss: 0.7701414227485657. Validation loss: 2.7058749198913574.\n",
      "Epoch 2275. Training loss: 0.7701379656791687. Validation loss: 2.7058746814727783.\n",
      "Epoch 2276. Training loss: 0.770134449005127. Validation loss: 2.705873966217041.\n",
      "Epoch 2277. Training loss: 0.7701308727264404. Validation loss: 2.7058732509613037.\n",
      "Epoch 2278. Training loss: 0.7701273560523987. Validation loss: 2.7058730125427246.\n",
      "Epoch 2279. Training loss: 0.7701239585876465. Validation loss: 2.7058722972869873.\n",
      "Epoch 2280. Training loss: 0.7701204419136047. Validation loss: 2.70587158203125.\n",
      "Epoch 2281. Training loss: 0.7701168656349182. Validation loss: 2.705871105194092.\n",
      "Epoch 2282. Training loss: 0.7701134085655212. Validation loss: 2.7058706283569336.\n",
      "Epoch 2283. Training loss: 0.7701098918914795. Validation loss: 2.7058701515197754.\n",
      "Epoch 2284. Training loss: 0.7701063752174377. Validation loss: 2.705869674682617.\n",
      "Epoch 2285. Training loss: 0.7701029181480408. Validation loss: 2.705869197845459.\n",
      "Epoch 2286. Training loss: 0.7700993418693542. Validation loss: 2.7058684825897217.\n",
      "Epoch 2287. Training loss: 0.7700958847999573. Validation loss: 2.7058682441711426.\n",
      "Epoch 2288. Training loss: 0.7700923085212708. Validation loss: 2.705867290496826.\n",
      "Epoch 2289. Training loss: 0.7700889110565186. Validation loss: 2.705867052078247.\n",
      "Epoch 2290. Training loss: 0.770085334777832. Validation loss: 2.7058663368225098.\n",
      "Epoch 2291. Training loss: 0.7700818181037903. Validation loss: 2.7058658599853516.\n",
      "Epoch 2292. Training loss: 0.7700782418251038. Validation loss: 2.7058651447296143.\n",
      "Epoch 2293. Training loss: 0.7700748443603516. Validation loss: 2.705864906311035.\n",
      "Epoch 2294. Training loss: 0.7700713276863098. Validation loss: 2.705864429473877.\n",
      "Epoch 2295. Training loss: 0.7700677514076233. Validation loss: 2.7058632373809814.\n",
      "Epoch 2296. Training loss: 0.770064115524292. Validation loss: 2.7058632373809814.\n",
      "Epoch 2297. Training loss: 0.7700607180595398. Validation loss: 2.705862522125244.\n",
      "Epoch 2298. Training loss: 0.7700571417808533. Validation loss: 2.705862283706665.\n",
      "Epoch 2299. Training loss: 0.7700536847114563. Validation loss: 2.7058615684509277.\n",
      "Epoch 2300. Training loss: 0.7700502276420593. Validation loss: 2.7058610916137695.\n",
      "Epoch 2301. Training loss: 0.7700466513633728. Validation loss: 2.7058606147766113.\n",
      "Epoch 2302. Training loss: 0.7700431942939758. Validation loss: 2.705859899520874.\n",
      "Epoch 2303. Training loss: 0.7700395584106445. Validation loss: 2.7058591842651367.\n",
      "Epoch 2304. Training loss: 0.7700360417366028. Validation loss: 2.7058587074279785.\n",
      "Epoch 2305. Training loss: 0.7700325846672058. Validation loss: 2.7058582305908203.\n",
      "Epoch 2306. Training loss: 0.7700290679931641. Validation loss: 2.705857515335083.\n",
      "Epoch 2307. Training loss: 0.7700254917144775. Validation loss: 2.705857276916504.\n",
      "Epoch 2308. Training loss: 0.7700220942497253. Validation loss: 2.7058568000793457.\n",
      "Epoch 2309. Training loss: 0.7700185775756836. Validation loss: 2.7058560848236084.\n",
      "Epoch 2310. Training loss: 0.7700149416923523. Validation loss: 2.705855369567871.\n",
      "Epoch 2311. Training loss: 0.7700114846229553. Validation loss: 2.705855131149292.\n",
      "Epoch 2312. Training loss: 0.7700079083442688. Validation loss: 2.705854654312134.\n",
      "Epoch 2313. Training loss: 0.7700044512748718. Validation loss: 2.7058539390563965.\n",
      "Epoch 2314. Training loss: 0.7700009346008301. Validation loss: 2.705853223800659.\n",
      "Epoch 2315. Training loss: 0.7699974179267883. Validation loss: 2.705852508544922.\n",
      "Epoch 2316. Training loss: 0.7699939608573914. Validation loss: 2.7058522701263428.\n",
      "Epoch 2317. Training loss: 0.7699902653694153. Validation loss: 2.7058515548706055.\n",
      "Epoch 2318. Training loss: 0.7699868679046631. Validation loss: 2.7058513164520264.\n",
      "Epoch 2319. Training loss: 0.7699833512306213. Validation loss: 2.705850839614868.\n",
      "Epoch 2320. Training loss: 0.7699797749519348. Validation loss: 2.7058498859405518.\n",
      "Epoch 2321. Training loss: 0.7699763178825378. Validation loss: 2.7058491706848145.\n",
      "Epoch 2322. Training loss: 0.7699728012084961. Validation loss: 2.7058486938476562.\n",
      "Epoch 2323. Training loss: 0.7699691653251648. Validation loss: 2.7058486938476562.\n",
      "Epoch 2324. Training loss: 0.7699657082557678. Validation loss: 2.70584774017334.\n",
      "Epoch 2325. Training loss: 0.7699621319770813. Validation loss: 2.7058470249176025.\n",
      "Epoch 2326. Training loss: 0.7699586749076843. Validation loss: 2.7058467864990234.\n",
      "Epoch 2327. Training loss: 0.7699549794197083. Validation loss: 2.7058463096618652.\n",
      "Epoch 2328. Training loss: 0.769951581954956. Validation loss: 2.705845832824707.\n",
      "Epoch 2329. Training loss: 0.7699480652809143. Validation loss: 2.7058451175689697.\n",
      "Epoch 2330. Training loss: 0.769944429397583. Validation loss: 2.7058446407318115.\n",
      "Epoch 2331. Training loss: 0.7699409127235413. Validation loss: 2.705843925476074.\n",
      "Epoch 2332. Training loss: 0.7699373364448547. Validation loss: 2.705843210220337.\n",
      "Epoch 2333. Training loss: 0.7699339389801025. Validation loss: 2.7058427333831787.\n",
      "Epoch 2334. Training loss: 0.769930362701416. Validation loss: 2.7058424949645996.\n",
      "Epoch 2335. Training loss: 0.7699268460273743. Validation loss: 2.705841541290283.\n",
      "Epoch 2336. Training loss: 0.7699233889579773. Validation loss: 2.705841541290283.\n",
      "Epoch 2337. Training loss: 0.7699198126792908. Validation loss: 2.705841064453125.\n",
      "Epoch 2338. Training loss: 0.769916296005249. Validation loss: 2.7058398723602295.\n",
      "Epoch 2339. Training loss: 0.7699127197265625. Validation loss: 2.7058396339416504.\n",
      "Epoch 2340. Training loss: 0.7699090838432312. Validation loss: 2.705839157104492.\n",
      "Epoch 2341. Training loss: 0.7699056267738342. Validation loss: 2.705838680267334.\n",
      "Epoch 2342. Training loss: 0.7699021697044373. Validation loss: 2.7058377265930176.\n",
      "Epoch 2343. Training loss: 0.7698984742164612. Validation loss: 2.7058374881744385.\n",
      "Epoch 2344. Training loss: 0.7698948979377747. Validation loss: 2.705836772918701.\n",
      "Epoch 2345. Training loss: 0.7698915004730225. Validation loss: 2.705836296081543.\n",
      "Epoch 2346. Training loss: 0.7698879241943359. Validation loss: 2.7058355808258057.\n",
      "Epoch 2347. Training loss: 0.7698845267295837. Validation loss: 2.7058353424072266.\n",
      "Epoch 2348. Training loss: 0.7698807716369629. Validation loss: 2.7058346271514893.\n",
      "Epoch 2349. Training loss: 0.7698772549629211. Validation loss: 2.705833911895752.\n",
      "Epoch 2350. Training loss: 0.7698737978935242. Validation loss: 2.7058334350585938.\n",
      "Epoch 2351. Training loss: 0.7698702812194824. Validation loss: 2.7058329582214355.\n",
      "Epoch 2352. Training loss: 0.7698666453361511. Validation loss: 2.7058324813842773.\n",
      "Epoch 2353. Training loss: 0.7698631286621094. Validation loss: 2.705832004547119.\n",
      "Epoch 2354. Training loss: 0.7698596119880676. Validation loss: 2.705831527709961.\n",
      "Epoch 2355. Training loss: 0.7698560357093811. Validation loss: 2.7058310508728027.\n",
      "Epoch 2356. Training loss: 0.7698525786399841. Validation loss: 2.7058300971984863.\n",
      "Epoch 2357. Training loss: 0.7698490023612976. Validation loss: 2.705829620361328.\n",
      "Epoch 2358. Training loss: 0.7698454260826111. Validation loss: 2.70582914352417.\n",
      "Epoch 2359. Training loss: 0.7698418498039246. Validation loss: 2.7058286666870117.\n",
      "Epoch 2360. Training loss: 0.7698383331298828. Validation loss: 2.7058281898498535.\n",
      "Epoch 2361. Training loss: 0.7698347568511963. Validation loss: 2.705827474594116.\n",
      "Epoch 2362. Training loss: 0.7698311805725098. Validation loss: 2.705826997756958.\n",
      "Epoch 2363. Training loss: 0.769827663898468. Validation loss: 2.7058262825012207.\n",
      "Epoch 2364. Training loss: 0.7698240876197815. Validation loss: 2.7058260440826416.\n",
      "Epoch 2365. Training loss: 0.769820511341095. Validation loss: 2.705825090408325.\n",
      "Epoch 2366. Training loss: 0.769817054271698. Validation loss: 2.705824851989746.\n",
      "Epoch 2367. Training loss: 0.7698135375976562. Validation loss: 2.705824613571167.\n",
      "Epoch 2368. Training loss: 0.769809901714325. Validation loss: 2.7058234214782715.\n",
      "Epoch 2369. Training loss: 0.7698063850402832. Validation loss: 2.7058231830596924.\n",
      "Epoch 2370. Training loss: 0.7698028087615967. Validation loss: 2.705822467803955.\n",
      "Epoch 2371. Training loss: 0.7697992324829102. Validation loss: 2.705821990966797.\n",
      "Epoch 2372. Training loss: 0.7697957158088684. Validation loss: 2.7058215141296387.\n",
      "Epoch 2373. Training loss: 0.7697922587394714. Validation loss: 2.7058210372924805.\n",
      "Epoch 2374. Training loss: 0.7697885632514954. Validation loss: 2.705820083618164.\n",
      "Epoch 2375. Training loss: 0.7697849869728088. Validation loss: 2.705819845199585.\n",
      "Epoch 2376. Training loss: 0.7697815299034119. Validation loss: 2.7058191299438477.\n",
      "Epoch 2377. Training loss: 0.7697778344154358. Validation loss: 2.7058186531066895.\n",
      "Epoch 2378. Training loss: 0.7697743773460388. Validation loss: 2.705817937850952.\n",
      "Epoch 2379. Training loss: 0.7697708606719971. Validation loss: 2.705817699432373.\n",
      "Epoch 2380. Training loss: 0.7697672843933105. Validation loss: 2.7058169841766357.\n",
      "Epoch 2381. Training loss: 0.7697637677192688. Validation loss: 2.7058162689208984.\n",
      "Epoch 2382. Training loss: 0.7697601318359375. Validation loss: 2.7058160305023193.\n",
      "Epoch 2383. Training loss: 0.769756555557251. Validation loss: 2.705815315246582.\n",
      "Epoch 2384. Training loss: 0.7697530388832092. Validation loss: 2.705815076828003.\n",
      "Epoch 2385. Training loss: 0.7697494626045227. Validation loss: 2.7058143615722656.\n",
      "Epoch 2386. Training loss: 0.7697458863258362. Validation loss: 2.7058138847351074.\n",
      "Epoch 2387. Training loss: 0.7697423100471497. Validation loss: 2.70581316947937.\n",
      "Epoch 2388. Training loss: 0.7697387337684631. Validation loss: 2.705812692642212.\n",
      "Epoch 2389. Training loss: 0.7697351574897766. Validation loss: 2.7058119773864746.\n",
      "Epoch 2390. Training loss: 0.7697315812110901. Validation loss: 2.7058115005493164.\n",
      "Epoch 2391. Training loss: 0.7697281837463379. Validation loss: 2.7058112621307373.\n",
      "Epoch 2392. Training loss: 0.7697245478630066. Validation loss: 2.705810546875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2393. Training loss: 0.7697209715843201. Validation loss: 2.7058098316192627.\n",
      "Epoch 2394. Training loss: 0.7697173953056335. Validation loss: 2.7058091163635254.\n",
      "Epoch 2395. Training loss: 0.769713819026947. Validation loss: 2.7058088779449463.\n",
      "Epoch 2396. Training loss: 0.7697103023529053. Validation loss: 2.705808639526367.\n",
      "Epoch 2397. Training loss: 0.769706666469574. Validation loss: 2.7058074474334717.\n",
      "Epoch 2398. Training loss: 0.7697030901908875. Validation loss: 2.7058074474334717.\n",
      "Epoch 2399. Training loss: 0.7696996331214905. Validation loss: 2.7058067321777344.\n",
      "Epoch 2400. Training loss: 0.7696959972381592. Validation loss: 2.705806016921997.\n",
      "Epoch 2401. Training loss: 0.7696924209594727. Validation loss: 2.705805540084839.\n",
      "Epoch 2402. Training loss: 0.7696889042854309. Validation loss: 2.7058048248291016.\n",
      "Epoch 2403. Training loss: 0.7696852684020996. Validation loss: 2.7058048248291016.\n",
      "Epoch 2404. Training loss: 0.7696816921234131. Validation loss: 2.705803632736206.\n",
      "Epoch 2405. Training loss: 0.7696781754493713. Validation loss: 2.705803155899048.\n",
      "Epoch 2406. Training loss: 0.7696745991706848. Validation loss: 2.7058024406433105.\n",
      "Epoch 2407. Training loss: 0.7696710228919983. Validation loss: 2.7058022022247314.\n",
      "Epoch 2408. Training loss: 0.7696674466133118. Validation loss: 2.7058017253875732.\n",
      "Epoch 2409. Training loss: 0.7696638107299805. Validation loss: 2.705801486968994.\n",
      "Epoch 2410. Training loss: 0.7696602940559387. Validation loss: 2.7058005332946777.\n",
      "Epoch 2411. Training loss: 0.7696567177772522. Validation loss: 2.7057998180389404.\n",
      "Epoch 2412. Training loss: 0.7696530818939209. Validation loss: 2.705799102783203.\n",
      "Epoch 2413. Training loss: 0.7696495056152344. Validation loss: 2.7057993412017822.\n",
      "Epoch 2414. Training loss: 0.7696459889411926. Validation loss: 2.705798864364624.\n",
      "Epoch 2415. Training loss: 0.7696424126625061. Validation loss: 2.7057981491088867.\n",
      "Epoch 2416. Training loss: 0.7696388363838196. Validation loss: 2.7057976722717285.\n",
      "Epoch 2417. Training loss: 0.7696352601051331. Validation loss: 2.7057974338531494.\n",
      "Epoch 2418. Training loss: 0.7696316838264465. Validation loss: 2.7057974338531494.\n",
      "Epoch 2419. Training loss: 0.7696280479431152. Validation loss: 2.7057971954345703.\n",
      "Epoch 2420. Training loss: 0.7696245312690735. Validation loss: 2.705796241760254.\n",
      "Epoch 2421. Training loss: 0.769620954990387. Validation loss: 2.7057957649230957.\n",
      "Epoch 2422. Training loss: 0.7696173787117004. Validation loss: 2.7057957649230957.\n",
      "Epoch 2423. Training loss: 0.7696137428283691. Validation loss: 2.7057952880859375.\n",
      "Epoch 2424. Training loss: 0.7696102261543274. Validation loss: 2.7057945728302.\n",
      "Epoch 2425. Training loss: 0.7696065902709961. Validation loss: 2.7057945728302.\n",
      "Epoch 2426. Training loss: 0.7696030139923096. Validation loss: 2.705794334411621.\n",
      "Epoch 2427. Training loss: 0.769599437713623. Validation loss: 2.705793857574463.\n",
      "Epoch 2428. Training loss: 0.7695958018302917. Validation loss: 2.7057933807373047.\n",
      "Epoch 2429. Training loss: 0.76959228515625. Validation loss: 2.7057931423187256.\n",
      "Epoch 2430. Training loss: 0.7695886492729187. Validation loss: 2.7057926654815674.\n",
      "Epoch 2431. Training loss: 0.7695850729942322. Validation loss: 2.705792188644409.\n",
      "Epoch 2432. Training loss: 0.7695815563201904. Validation loss: 2.70579195022583.\n",
      "Epoch 2433. Training loss: 0.7695779204368591. Validation loss: 2.705791473388672.\n",
      "Epoch 2434. Training loss: 0.7695743441581726. Validation loss: 2.705791473388672.\n",
      "Epoch 2435. Training loss: 0.7695707678794861. Validation loss: 2.7057909965515137.\n",
      "Epoch 2436. Training loss: 0.7695671916007996. Validation loss: 2.7057909965515137.\n",
      "Epoch 2437. Training loss: 0.769563615322113. Validation loss: 2.7057900428771973.\n",
      "Epoch 2438. Training loss: 0.7695600390434265. Validation loss: 2.705789566040039.\n",
      "Epoch 2439. Training loss: 0.7695563435554504. Validation loss: 2.70578932762146.\n",
      "Epoch 2440. Training loss: 0.7695527672767639. Validation loss: 2.705789089202881.\n",
      "Epoch 2441. Training loss: 0.7695493102073669. Validation loss: 2.7057888507843018.\n",
      "Epoch 2442. Training loss: 0.7695456147193909. Validation loss: 2.7057883739471436.\n",
      "Epoch 2443. Training loss: 0.7695419788360596. Validation loss: 2.7057881355285645.\n",
      "Epoch 2444. Training loss: 0.7695384621620178. Validation loss: 2.7057876586914062.\n",
      "Epoch 2445. Training loss: 0.7695348262786865. Validation loss: 2.705787420272827.\n",
      "Epoch 2446. Training loss: 0.76953125. Validation loss: 2.70578670501709.\n",
      "Epoch 2447. Training loss: 0.7695276141166687. Validation loss: 2.70578670501709.\n",
      "Epoch 2448. Training loss: 0.7695240378379822. Validation loss: 2.7057862281799316.\n",
      "Epoch 2449. Training loss: 0.7695204615592957. Validation loss: 2.7057857513427734.\n",
      "Epoch 2450. Training loss: 0.7695167660713196. Validation loss: 2.705785036087036.\n",
      "Epoch 2451. Training loss: 0.7695133090019226. Validation loss: 2.705784797668457.\n",
      "Epoch 2452. Training loss: 0.7695096135139465. Validation loss: 2.705784797668457.\n",
      "Epoch 2453. Training loss: 0.76950603723526. Validation loss: 2.7057840824127197.\n",
      "Epoch 2454. Training loss: 0.7695024013519287. Validation loss: 2.7057838439941406.\n",
      "Epoch 2455. Training loss: 0.769498884677887. Validation loss: 2.7057836055755615.\n",
      "Epoch 2456. Training loss: 0.7694952487945557. Validation loss: 2.7057833671569824.\n",
      "Epoch 2457. Training loss: 0.7694916129112244. Validation loss: 2.705782413482666.\n",
      "Epoch 2458. Training loss: 0.7694880366325378. Validation loss: 2.705782413482666.\n",
      "Epoch 2459. Training loss: 0.7694844603538513. Validation loss: 2.705781936645508.\n",
      "Epoch 2460. Training loss: 0.7694807648658752. Validation loss: 2.7057814598083496.\n",
      "Epoch 2461. Training loss: 0.7694771885871887. Validation loss: 2.7057814598083496.\n",
      "Epoch 2462. Training loss: 0.7694736123085022. Validation loss: 2.7057809829711914.\n",
      "Epoch 2463. Training loss: 0.7694700360298157. Validation loss: 2.7057807445526123.\n",
      "Epoch 2464. Training loss: 0.7694664001464844. Validation loss: 2.705780267715454.\n",
      "Epoch 2465. Training loss: 0.7694627642631531. Validation loss: 2.705780029296875.\n",
      "Epoch 2466. Training loss: 0.7694591879844666. Validation loss: 2.705779552459717.\n",
      "Epoch 2467. Training loss: 0.76945561170578. Validation loss: 2.7057788372039795.\n",
      "Epoch 2468. Training loss: 0.7694520950317383. Validation loss: 2.7057785987854004.\n",
      "Epoch 2469. Training loss: 0.769448459148407. Validation loss: 2.7057785987854004.\n",
      "Epoch 2470. Training loss: 0.7694447636604309. Validation loss: 2.705778121948242.\n",
      "Epoch 2471. Training loss: 0.7694411873817444. Validation loss: 2.705777883529663.\n",
      "Epoch 2472. Training loss: 0.7694374918937683. Validation loss: 2.7057769298553467.\n",
      "Epoch 2473. Training loss: 0.7694337964057922. Validation loss: 2.7057766914367676.\n",
      "Epoch 2474. Training loss: 0.7694303393363953. Validation loss: 2.7057762145996094.\n",
      "Epoch 2475. Training loss: 0.7694266438484192. Validation loss: 2.7057759761810303.\n",
      "Epoch 2476. Training loss: 0.7694231867790222. Validation loss: 2.705775737762451.\n",
      "Epoch 2477. Training loss: 0.7694194912910461. Validation loss: 2.705775499343872.\n",
      "Epoch 2478. Training loss: 0.7694158554077148. Validation loss: 2.7057747840881348.\n",
      "Epoch 2479. Training loss: 0.7694122791290283. Validation loss: 2.705775260925293.\n",
      "Epoch 2480. Training loss: 0.769408643245697. Validation loss: 2.7057740688323975.\n",
      "Epoch 2481. Training loss: 0.7694050669670105. Validation loss: 2.7057738304138184.\n",
      "Epoch 2482. Training loss: 0.769401490688324. Validation loss: 2.7057735919952393.\n",
      "Epoch 2483. Training loss: 0.7693977355957031. Validation loss: 2.705773115158081.\n",
      "Epoch 2484. Training loss: 0.7693941593170166. Validation loss: 2.705773115158081.\n",
      "Epoch 2485. Training loss: 0.7693905830383301. Validation loss: 2.705772638320923.\n",
      "Epoch 2486. Training loss: 0.7693870663642883. Validation loss: 2.7057723999023438.\n",
      "Epoch 2487. Training loss: 0.7693833708763123. Validation loss: 2.7057719230651855.\n",
      "Epoch 2488. Training loss: 0.7693797945976257. Validation loss: 2.7057714462280273.\n",
      "Epoch 2489. Training loss: 0.7693760991096497. Validation loss: 2.705770969390869.\n",
      "Epoch 2490. Training loss: 0.7693725228309631. Validation loss: 2.705770492553711.\n",
      "Epoch 2491. Training loss: 0.7693689465522766. Validation loss: 2.705770254135132.\n",
      "Epoch 2492. Training loss: 0.7693652510643005. Validation loss: 2.7057695388793945.\n",
      "Epoch 2493. Training loss: 0.769361674785614. Validation loss: 2.7057695388793945.\n",
      "Epoch 2494. Training loss: 0.7693580985069275. Validation loss: 2.7057690620422363.\n",
      "Epoch 2495. Training loss: 0.7693544030189514. Validation loss: 2.7057688236236572.\n",
      "Epoch 2496. Training loss: 0.7693507075309753. Validation loss: 2.705768346786499.\n",
      "Epoch 2497. Training loss: 0.7693471908569336. Validation loss: 2.70576810836792.\n",
      "Epoch 2498. Training loss: 0.7693434357643127. Validation loss: 2.7057676315307617.\n",
      "Epoch 2499. Training loss: 0.7693398594856262. Validation loss: 2.7057673931121826.\n",
      "Epoch 2500. Training loss: 0.7693362832069397. Validation loss: 2.7057669162750244.\n",
      "Epoch 2501. Training loss: 0.7693325877189636. Validation loss: 2.705766201019287.\n",
      "Epoch 2502. Training loss: 0.7693290114402771. Validation loss: 2.705766201019287.\n",
      "Epoch 2503. Training loss: 0.769325315952301. Validation loss: 2.705765724182129.\n",
      "Epoch 2504. Training loss: 0.7693216800689697. Validation loss: 2.70576548576355.\n",
      "Epoch 2505. Training loss: 0.769318163394928. Validation loss: 2.7057652473449707.\n",
      "Epoch 2506. Training loss: 0.7693145275115967. Validation loss: 2.7057647705078125.\n",
      "Epoch 2507. Training loss: 0.7693107724189758. Validation loss: 2.7057642936706543.\n",
      "Epoch 2508. Training loss: 0.7693071961402893. Validation loss: 2.7057642936706543.\n",
      "Epoch 2509. Training loss: 0.769303560256958. Validation loss: 2.705763578414917.\n",
      "Epoch 2510. Training loss: 0.7692999839782715. Validation loss: 2.705763339996338.\n",
      "Epoch 2511. Training loss: 0.7692963480949402. Validation loss: 2.705763101577759.\n",
      "Epoch 2512. Training loss: 0.7692927718162537. Validation loss: 2.7057623863220215.\n",
      "Epoch 2513. Training loss: 0.7692890763282776. Validation loss: 2.7057621479034424.\n",
      "Epoch 2514. Training loss: 0.7692854404449463. Validation loss: 2.7057619094848633.\n",
      "Epoch 2515. Training loss: 0.7692818641662598. Validation loss: 2.705760955810547.\n",
      "Epoch 2516. Training loss: 0.7692780494689941. Validation loss: 2.705760955810547.\n",
      "Epoch 2517. Training loss: 0.7692745327949524. Validation loss: 2.7057602405548096.\n",
      "Epoch 2518. Training loss: 0.7692708373069763. Validation loss: 2.7057602405548096.\n",
      "Epoch 2519. Training loss: 0.7692671418190002. Validation loss: 2.7057597637176514.\n",
      "Epoch 2520. Training loss: 0.7692635655403137. Validation loss: 2.7057597637176514.\n",
      "Epoch 2521. Training loss: 0.7692599892616272. Validation loss: 2.705759048461914.\n",
      "Epoch 2522. Training loss: 0.7692562937736511. Validation loss: 2.705758810043335.\n",
      "Epoch 2523. Training loss: 0.7692527770996094. Validation loss: 2.7057583332061768.\n",
      "Epoch 2524. Training loss: 0.7692490220069885. Validation loss: 2.7057578563690186.\n",
      "Epoch 2525. Training loss: 0.7692453861236572. Validation loss: 2.7057576179504395.\n",
      "Epoch 2526. Training loss: 0.7692418098449707. Validation loss: 2.7057573795318604.\n",
      "Epoch 2527. Training loss: 0.7692381739616394. Validation loss: 2.705756902694702.\n",
      "Epoch 2528. Training loss: 0.7692344188690186. Validation loss: 2.705756425857544.\n",
      "Epoch 2529. Training loss: 0.769230842590332. Validation loss: 2.705756187438965.\n",
      "Epoch 2530. Training loss: 0.7692272067070007. Validation loss: 2.7057557106018066.\n",
      "Epoch 2531. Training loss: 0.7692236304283142. Validation loss: 2.7057552337646484.\n",
      "Epoch 2532. Training loss: 0.7692198753356934. Validation loss: 2.7057549953460693.\n",
      "Epoch 2533. Training loss: 0.7692162990570068. Validation loss: 2.7057547569274902.\n",
      "Epoch 2534. Training loss: 0.769212543964386. Validation loss: 2.705754280090332.\n",
      "Epoch 2535. Training loss: 0.7692089676856995. Validation loss: 2.705754041671753.\n",
      "Epoch 2536. Training loss: 0.7692053318023682. Validation loss: 2.7057533264160156.\n",
      "Epoch 2537. Training loss: 0.7692016959190369. Validation loss: 2.7057530879974365.\n",
      "Epoch 2538. Training loss: 0.7691981196403503. Validation loss: 2.7057526111602783.\n",
      "Epoch 2539. Training loss: 0.7691944241523743. Validation loss: 2.70575213432312.\n",
      "Epoch 2540. Training loss: 0.7691907286643982. Validation loss: 2.70575213432312.\n",
      "Epoch 2541. Training loss: 0.7691871523857117. Validation loss: 2.705751657485962.\n",
      "Epoch 2542. Training loss: 0.7691833972930908. Validation loss: 2.7057509422302246.\n",
      "Epoch 2543. Training loss: 0.7691797614097595. Validation loss: 2.7057509422302246.\n",
      "Epoch 2544. Training loss: 0.769176185131073. Validation loss: 2.7057502269744873.\n",
      "Epoch 2545. Training loss: 0.7691724896430969. Validation loss: 2.705749750137329.\n",
      "Epoch 2546. Training loss: 0.7691688537597656. Validation loss: 2.705749750137329.\n",
      "Epoch 2547. Training loss: 0.7691652178764343. Validation loss: 2.705749273300171.\n",
      "Epoch 2548. Training loss: 0.7691615223884583. Validation loss: 2.705749273300171.\n",
      "Epoch 2549. Training loss: 0.769157886505127. Validation loss: 2.7057485580444336.\n",
      "Epoch 2550. Training loss: 0.7691541314125061. Validation loss: 2.7057480812072754.\n",
      "Epoch 2551. Training loss: 0.7691505551338196. Validation loss: 2.7057478427886963.\n",
      "Epoch 2552. Training loss: 0.7691468596458435. Validation loss: 2.705747604370117.\n",
      "Epoch 2553. Training loss: 0.7691431641578674. Validation loss: 2.705747127532959.\n",
      "Epoch 2554. Training loss: 0.7691397070884705. Validation loss: 2.70574688911438.\n",
      "Epoch 2555. Training loss: 0.7691357731819153. Validation loss: 2.7057459354400635.\n",
      "Epoch 2556. Training loss: 0.7691323161125183. Validation loss: 2.7057461738586426.\n",
      "Epoch 2557. Training loss: 0.7691285610198975. Validation loss: 2.7057456970214844.\n",
      "Epoch 2558. Training loss: 0.7691249251365662. Validation loss: 2.705744981765747.\n",
      "Epoch 2559. Training loss: 0.7691212296485901. Validation loss: 2.705744743347168.\n",
      "Epoch 2560. Training loss: 0.7691176533699036. Validation loss: 2.705744504928589.\n",
      "Epoch 2561. Training loss: 0.7691140174865723. Validation loss: 2.7057442665100098.\n",
      "Epoch 2562. Training loss: 0.769110381603241. Validation loss: 2.7057437896728516.\n",
      "Epoch 2563. Training loss: 0.7691066861152649. Validation loss: 2.7057435512542725.\n",
      "Epoch 2564. Training loss: 0.7691029906272888. Validation loss: 2.7057430744171143.\n",
      "Epoch 2565. Training loss: 0.7690992951393127. Validation loss: 2.705742597579956.\n",
      "Epoch 2566. Training loss: 0.7690955996513367. Validation loss: 2.705742359161377.\n",
      "Epoch 2567. Training loss: 0.7690920829772949. Validation loss: 2.705742120742798.\n",
      "Epoch 2568. Training loss: 0.7690882682800293. Validation loss: 2.7057416439056396.\n",
      "Epoch 2569. Training loss: 0.769084632396698. Validation loss: 2.7057411670684814.\n",
      "Epoch 2570. Training loss: 0.7690810561180115. Validation loss: 2.7057409286499023.\n",
      "Epoch 2571. Training loss: 0.7690773010253906. Validation loss: 2.705739974975586.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2572. Training loss: 0.7690736651420593. Validation loss: 2.705739736557007.\n",
      "Epoch 2573. Training loss: 0.7690699696540833. Validation loss: 2.7057394981384277.\n",
      "Epoch 2574. Training loss: 0.769066333770752. Validation loss: 2.7057392597198486.\n",
      "Epoch 2575. Training loss: 0.7690625786781311. Validation loss: 2.7057390213012695.\n",
      "Epoch 2576. Training loss: 0.7690590023994446. Validation loss: 2.7057387828826904.\n",
      "Epoch 2577. Training loss: 0.7690553069114685. Validation loss: 2.705738067626953.\n",
      "Epoch 2578. Training loss: 0.769051730632782. Validation loss: 2.705737829208374.\n",
      "Epoch 2579. Training loss: 0.7690480351448059. Validation loss: 2.705737352371216.\n",
      "Epoch 2580. Training loss: 0.7690442204475403. Validation loss: 2.7057368755340576.\n",
      "Epoch 2581. Training loss: 0.769040584564209. Validation loss: 2.7057363986968994.\n",
      "Epoch 2582. Training loss: 0.7690368294715881. Validation loss: 2.7057363986968994.\n",
      "Epoch 2583. Training loss: 0.7690333724021912. Validation loss: 2.7057361602783203.\n",
      "Epoch 2584. Training loss: 0.7690296173095703. Validation loss: 2.705735445022583.\n",
      "Epoch 2585. Training loss: 0.769025981426239. Validation loss: 2.705735206604004.\n",
      "Epoch 2586. Training loss: 0.7690222859382629. Validation loss: 2.7057347297668457.\n",
      "Epoch 2587. Training loss: 0.7690185904502869. Validation loss: 2.7057342529296875.\n",
      "Epoch 2588. Training loss: 0.7690148949623108. Validation loss: 2.7057340145111084.\n",
      "Epoch 2589. Training loss: 0.7690111994743347. Validation loss: 2.7057337760925293.\n",
      "Epoch 2590. Training loss: 0.7690076231956482. Validation loss: 2.705733060836792.\n",
      "Epoch 2591. Training loss: 0.7690039277076721. Validation loss: 2.705733060836792.\n",
      "Epoch 2592. Training loss: 0.7690001130104065. Validation loss: 2.705732822418213.\n",
      "Epoch 2593. Training loss: 0.76899653673172. Validation loss: 2.7057321071624756.\n",
      "Epoch 2594. Training loss: 0.7689927220344543. Validation loss: 2.7057316303253174.\n",
      "Epoch 2595. Training loss: 0.7689892649650574. Validation loss: 2.705731153488159.\n",
      "Epoch 2596. Training loss: 0.7689855098724365. Validation loss: 2.705731153488159.\n",
      "Epoch 2597. Training loss: 0.7689817547798157. Validation loss: 2.705730676651001.\n",
      "Epoch 2598. Training loss: 0.7689781188964844. Validation loss: 2.7057301998138428.\n",
      "Epoch 2599. Training loss: 0.7689743638038635. Validation loss: 2.7057299613952637.\n",
      "Epoch 2600. Training loss: 0.7689707279205322. Validation loss: 2.7057294845581055.\n",
      "Epoch 2601. Training loss: 0.7689670920372009. Validation loss: 2.7057292461395264.\n",
      "Epoch 2602. Training loss: 0.7689633369445801. Validation loss: 2.705728769302368.\n",
      "Epoch 2603. Training loss: 0.7689597010612488. Validation loss: 2.70572829246521.\n",
      "Epoch 2604. Training loss: 0.7689560055732727. Validation loss: 2.705728054046631.\n",
      "Epoch 2605. Training loss: 0.7689523100852966. Validation loss: 2.7057275772094727.\n",
      "Epoch 2606. Training loss: 0.7689486145973206. Validation loss: 2.7057271003723145.\n",
      "Epoch 2607. Training loss: 0.7689449787139893. Validation loss: 2.7057268619537354.\n",
      "Epoch 2608. Training loss: 0.7689412236213684. Validation loss: 2.705726385116577.\n",
      "Epoch 2609. Training loss: 0.7689375877380371. Validation loss: 2.705725908279419.\n",
      "Epoch 2610. Training loss: 0.7689338326454163. Validation loss: 2.70572566986084.\n",
      "Epoch 2611. Training loss: 0.768930196762085. Validation loss: 2.7057254314422607.\n",
      "Epoch 2612. Training loss: 0.7689265608787537. Validation loss: 2.7057249546051025.\n",
      "Epoch 2613. Training loss: 0.7689228057861328. Validation loss: 2.7057247161865234.\n",
      "Epoch 2614. Training loss: 0.768919050693512. Validation loss: 2.705724000930786.\n",
      "Epoch 2615. Training loss: 0.7689153552055359. Validation loss: 2.705723762512207.\n",
      "Epoch 2616. Training loss: 0.7689117789268494. Validation loss: 2.705723285675049.\n",
      "Epoch 2617. Training loss: 0.7689080238342285. Validation loss: 2.705723285675049.\n",
      "Epoch 2618. Training loss: 0.7689043879508972. Validation loss: 2.7057230472564697.\n",
      "Epoch 2619. Training loss: 0.7689006328582764. Validation loss: 2.7057223320007324.\n",
      "Epoch 2620. Training loss: 0.7688969969749451. Validation loss: 2.705721855163574.\n",
      "Epoch 2621. Training loss: 0.7688932418823242. Validation loss: 2.705721378326416.\n",
      "Epoch 2622. Training loss: 0.7688894867897034. Validation loss: 2.705721139907837.\n",
      "Epoch 2623. Training loss: 0.7688858509063721. Validation loss: 2.7057206630706787.\n",
      "Epoch 2624. Training loss: 0.7688820958137512. Validation loss: 2.7057201862335205.\n",
      "Epoch 2625. Training loss: 0.7688784599304199. Validation loss: 2.7057199478149414.\n",
      "Epoch 2626. Training loss: 0.7688748240470886. Validation loss: 2.705719470977783.\n",
      "Epoch 2627. Training loss: 0.768871009349823. Validation loss: 2.705718994140625.\n",
      "Epoch 2628. Training loss: 0.7688673138618469. Validation loss: 2.705718994140625.\n",
      "Epoch 2629. Training loss: 0.7688636779785156. Validation loss: 2.705718517303467.\n",
      "Epoch 2630. Training loss: 0.7688599228858948. Validation loss: 2.7057180404663086.\n",
      "Epoch 2631. Training loss: 0.7688562870025635. Validation loss: 2.7057178020477295.\n",
      "Epoch 2632. Training loss: 0.7688525319099426. Validation loss: 2.7057173252105713.\n",
      "Epoch 2633. Training loss: 0.768848717212677. Validation loss: 2.705717086791992.\n",
      "Epoch 2634. Training loss: 0.7688451409339905. Validation loss: 2.705716609954834.\n",
      "Epoch 2635. Training loss: 0.7688414454460144. Validation loss: 2.705716371536255.\n",
      "Epoch 2636. Training loss: 0.7688376903533936. Validation loss: 2.7057156562805176.\n",
      "Epoch 2637. Training loss: 0.7688340544700623. Validation loss: 2.7057154178619385.\n",
      "Epoch 2638. Training loss: 0.7688302993774414. Validation loss: 2.7057149410247803.\n",
      "Epoch 2639. Training loss: 0.7688266634941101. Validation loss: 2.7057149410247803.\n",
      "Epoch 2640. Training loss: 0.7688229084014893. Validation loss: 2.705714464187622.\n",
      "Epoch 2641. Training loss: 0.7688191533088684. Validation loss: 2.7057137489318848.\n",
      "Epoch 2642. Training loss: 0.7688153386116028. Validation loss: 2.7057137489318848.\n",
      "Epoch 2643. Training loss: 0.7688117623329163. Validation loss: 2.7057132720947266.\n",
      "Epoch 2644. Training loss: 0.7688080668449402. Validation loss: 2.7057127952575684.\n",
      "Epoch 2645. Training loss: 0.7688043713569641. Validation loss: 2.70571231842041.\n",
      "Epoch 2646. Training loss: 0.768800675868988. Validation loss: 2.705712080001831.\n",
      "Epoch 2647. Training loss: 0.7687969207763672. Validation loss: 2.705711841583252.\n",
      "Epoch 2648. Training loss: 0.7687932848930359. Validation loss: 2.7057111263275146.\n",
      "Epoch 2649. Training loss: 0.7687894701957703. Validation loss: 2.7057108879089355.\n",
      "Epoch 2650. Training loss: 0.7687857151031494. Validation loss: 2.7057106494903564.\n",
      "Epoch 2651. Training loss: 0.7687819600105286. Validation loss: 2.7057101726531982.\n",
      "Epoch 2652. Training loss: 0.7687783241271973. Validation loss: 2.705709934234619.\n",
      "Epoch 2653. Training loss: 0.768774688243866. Validation loss: 2.705709457397461.\n",
      "Epoch 2654. Training loss: 0.7687708735466003. Validation loss: 2.7057089805603027.\n",
      "Epoch 2655. Training loss: 0.7687671780586243. Validation loss: 2.7057085037231445.\n",
      "Epoch 2656. Training loss: 0.7687634825706482. Validation loss: 2.7057082653045654.\n",
      "Epoch 2657. Training loss: 0.7687597274780273. Validation loss: 2.7057077884674072.\n",
      "Epoch 2658. Training loss: 0.768756091594696. Validation loss: 2.705707550048828.\n",
      "Epoch 2659. Training loss: 0.7687523365020752. Validation loss: 2.705707311630249.\n",
      "Epoch 2660. Training loss: 0.7687485814094543. Validation loss: 2.7057065963745117.\n",
      "Epoch 2661. Training loss: 0.7687448859214783. Validation loss: 2.7057065963745117.\n",
      "Epoch 2662. Training loss: 0.7687411308288574. Validation loss: 2.7057061195373535.\n",
      "Epoch 2663. Training loss: 0.7687374949455261. Validation loss: 2.705705404281616.\n",
      "Epoch 2664. Training loss: 0.7687336802482605. Validation loss: 2.705704927444458.\n",
      "Epoch 2665. Training loss: 0.768730103969574. Validation loss: 2.705704689025879.\n",
      "Epoch 2666. Training loss: 0.7687262892723083. Validation loss: 2.7057044506073.\n",
      "Epoch 2667. Training loss: 0.7687225341796875. Validation loss: 2.7057039737701416.\n",
      "Epoch 2668. Training loss: 0.7687188982963562. Validation loss: 2.7057037353515625.\n",
      "Epoch 2669. Training loss: 0.7687151432037354. Validation loss: 2.7057032585144043.\n",
      "Epoch 2670. Training loss: 0.7687113881111145. Validation loss: 2.705702781677246.\n",
      "Epoch 2671. Training loss: 0.7687077522277832. Validation loss: 2.705702543258667.\n",
      "Epoch 2672. Training loss: 0.7687039375305176. Validation loss: 2.7057018280029297.\n",
      "Epoch 2673. Training loss: 0.7687003016471863. Validation loss: 2.7057018280029297.\n",
      "Epoch 2674. Training loss: 0.7686965465545654. Validation loss: 2.7057015895843506.\n",
      "Epoch 2675. Training loss: 0.7686927318572998. Validation loss: 2.7057013511657715.\n",
      "Epoch 2676. Training loss: 0.7686890959739685. Validation loss: 2.7057008743286133.\n",
      "Epoch 2677. Training loss: 0.7686853408813477. Validation loss: 2.705700159072876.\n",
      "Epoch 2678. Training loss: 0.768681526184082. Validation loss: 2.7056996822357178.\n",
      "Epoch 2679. Training loss: 0.7686778903007507. Validation loss: 2.7056994438171387.\n",
      "Epoch 2680. Training loss: 0.7686741352081299. Validation loss: 2.7056989669799805.\n",
      "Epoch 2681. Training loss: 0.768670380115509. Validation loss: 2.7056987285614014.\n",
      "Epoch 2682. Training loss: 0.7686665654182434. Validation loss: 2.7056984901428223.\n",
      "Epoch 2683. Training loss: 0.7686628699302673. Validation loss: 2.705698013305664.\n",
      "Epoch 2684. Training loss: 0.7686591148376465. Validation loss: 2.705697536468506.\n",
      "Epoch 2685. Training loss: 0.76865553855896. Validation loss: 2.7056972980499268.\n",
      "Epoch 2686. Training loss: 0.7686516642570496. Validation loss: 2.7056965827941895.\n",
      "Epoch 2687. Training loss: 0.7686479687690735. Validation loss: 2.7056965827941895.\n",
      "Epoch 2688. Training loss: 0.7686442732810974. Validation loss: 2.7056961059570312.\n",
      "Epoch 2689. Training loss: 0.7686405181884766. Validation loss: 2.705695629119873.\n",
      "Epoch 2690. Training loss: 0.7686367630958557. Validation loss: 2.705695629119873.\n",
      "Epoch 2691. Training loss: 0.7686330676078796. Validation loss: 2.7056949138641357.\n",
      "Epoch 2692. Training loss: 0.7686293125152588. Validation loss: 2.7056944370269775.\n",
      "Epoch 2693. Training loss: 0.7686255574226379. Validation loss: 2.7056941986083984.\n",
      "Epoch 2694. Training loss: 0.7686217427253723. Validation loss: 2.705693483352661.\n",
      "Epoch 2695. Training loss: 0.768618106842041. Validation loss: 2.705693244934082.\n",
      "Epoch 2696. Training loss: 0.7686142921447754. Validation loss: 2.705692768096924.\n",
      "Epoch 2697. Training loss: 0.7686106562614441. Validation loss: 2.7056922912597656.\n",
      "Epoch 2698. Training loss: 0.7686067223548889. Validation loss: 2.7056922912597656.\n",
      "Epoch 2699. Training loss: 0.7686031460762024. Validation loss: 2.7056918144226074.\n",
      "Epoch 2700. Training loss: 0.7685994505882263. Validation loss: 2.705691337585449.\n",
      "Epoch 2701. Training loss: 0.7685956358909607. Validation loss: 2.705690860748291.\n",
      "Epoch 2702. Training loss: 0.7685918807983398. Validation loss: 2.705690860748291.\n",
      "Epoch 2703. Training loss: 0.768588125705719. Validation loss: 2.7056899070739746.\n",
      "Epoch 2704. Training loss: 0.7685844302177429. Validation loss: 2.7056899070739746.\n",
      "Epoch 2705. Training loss: 0.7685806751251221. Validation loss: 2.7056894302368164.\n",
      "Epoch 2706. Training loss: 0.7685769200325012. Validation loss: 2.705688953399658.\n",
      "Epoch 2707. Training loss: 0.7685731053352356. Validation loss: 2.705688714981079.\n",
      "Epoch 2708. Training loss: 0.7685694694519043. Validation loss: 2.7056884765625.\n",
      "Epoch 2709. Training loss: 0.7685656547546387. Validation loss: 2.705687999725342.\n",
      "Epoch 2710. Training loss: 0.7685618996620178. Validation loss: 2.7056875228881836.\n",
      "Epoch 2711. Training loss: 0.7685582041740417. Validation loss: 2.7056875228881836.\n",
      "Epoch 2712. Training loss: 0.7685543894767761. Validation loss: 2.7056870460510254.\n",
      "Epoch 2713. Training loss: 0.7685506343841553. Validation loss: 2.705686330795288.\n",
      "Epoch 2714. Training loss: 0.768546998500824. Validation loss: 2.70568585395813.\n",
      "Epoch 2715. Training loss: 0.7685431838035583. Validation loss: 2.70568585395813.\n",
      "Epoch 2716. Training loss: 0.7685393691062927. Validation loss: 2.7056851387023926.\n",
      "Epoch 2717. Training loss: 0.7685357928276062. Validation loss: 2.7056846618652344.\n",
      "Epoch 2718. Training loss: 0.7685319781303406. Validation loss: 2.7056846618652344.\n",
      "Epoch 2719. Training loss: 0.768528163433075. Validation loss: 2.705684185028076.\n",
      "Epoch 2720. Training loss: 0.7685244083404541. Validation loss: 2.705683708190918.\n",
      "Epoch 2721. Training loss: 0.7685205936431885. Validation loss: 2.705683469772339.\n",
      "Epoch 2722. Training loss: 0.7685168385505676. Validation loss: 2.7056827545166016.\n",
      "Epoch 2723. Training loss: 0.768513023853302. Validation loss: 2.7056827545166016.\n",
      "Epoch 2724. Training loss: 0.7685093879699707. Validation loss: 2.7056825160980225.\n",
      "Epoch 2725. Training loss: 0.7685055732727051. Validation loss: 2.705681800842285.\n",
      "Epoch 2726. Training loss: 0.7685018181800842. Validation loss: 2.705681324005127.\n",
      "Epoch 2727. Training loss: 0.7684980034828186. Validation loss: 2.7056808471679688.\n",
      "Epoch 2728. Training loss: 0.7684943675994873. Validation loss: 2.7056808471679688.\n",
      "Epoch 2729. Training loss: 0.7684905529022217. Validation loss: 2.7056803703308105.\n",
      "Epoch 2730. Training loss: 0.7684867978096008. Validation loss: 2.7056798934936523.\n",
      "Epoch 2731. Training loss: 0.7684829831123352. Validation loss: 2.705679416656494.\n",
      "Epoch 2732. Training loss: 0.7684792876243591. Validation loss: 2.705678939819336.\n",
      "Epoch 2733. Training loss: 0.7684755325317383. Validation loss: 2.7056784629821777.\n",
      "Epoch 2734. Training loss: 0.7684717178344727. Validation loss: 2.7056782245635986.\n",
      "Epoch 2735. Training loss: 0.768467903137207. Validation loss: 2.7056779861450195.\n",
      "Epoch 2736. Training loss: 0.7684642672538757. Validation loss: 2.7056775093078613.\n",
      "Epoch 2737. Training loss: 0.7684605121612549. Validation loss: 2.7056775093078613.\n",
      "Epoch 2738. Training loss: 0.7684566378593445. Validation loss: 2.705676555633545.\n",
      "Epoch 2739. Training loss: 0.7684529423713684. Validation loss: 2.705676555633545.\n",
      "Epoch 2740. Training loss: 0.7684491276741028. Validation loss: 2.7056760787963867.\n",
      "Epoch 2741. Training loss: 0.7684454321861267. Validation loss: 2.7056756019592285.\n",
      "Epoch 2742. Training loss: 0.7684416770935059. Validation loss: 2.7056756019592285.\n",
      "Epoch 2743. Training loss: 0.7684378027915955. Validation loss: 2.705674886703491.\n",
      "Epoch 2744. Training loss: 0.7684341073036194. Validation loss: 2.705674648284912.\n",
      "Epoch 2745. Training loss: 0.768430233001709. Validation loss: 2.705674171447754.\n",
      "Epoch 2746. Training loss: 0.7684265971183777. Validation loss: 2.7056736946105957.\n",
      "Epoch 2747. Training loss: 0.7684228420257568. Validation loss: 2.7056732177734375.\n",
      "Epoch 2748. Training loss: 0.768419086933136. Validation loss: 2.7056729793548584.\n",
      "Epoch 2749. Training loss: 0.7684152722358704. Validation loss: 2.7056727409362793.\n",
      "Epoch 2750. Training loss: 0.7684114575386047. Validation loss: 2.705672264099121.\n",
      "Epoch 2751. Training loss: 0.7684076428413391. Validation loss: 2.705671787261963.\n",
      "Epoch 2752. Training loss: 0.7684040069580078. Validation loss: 2.705671787261963.\n",
      "Epoch 2753. Training loss: 0.7684000134468079. Validation loss: 2.7056708335876465.\n",
      "Epoch 2754. Training loss: 0.7683963179588318. Validation loss: 2.7056708335876465.\n",
      "Epoch 2755. Training loss: 0.7683925628662109. Validation loss: 2.705670118331909.\n",
      "Epoch 2756. Training loss: 0.7683888077735901. Validation loss: 2.70566987991333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2757. Training loss: 0.7683849930763245. Validation loss: 2.705669641494751.\n",
      "Epoch 2758. Training loss: 0.7683812975883484. Validation loss: 2.7056689262390137.\n",
      "Epoch 2759. Training loss: 0.7683773636817932. Validation loss: 2.7056689262390137.\n",
      "Epoch 2760. Training loss: 0.7683737874031067. Validation loss: 2.7056682109832764.\n",
      "Epoch 2761. Training loss: 0.7683699131011963. Validation loss: 2.7056679725646973.\n",
      "Epoch 2762. Training loss: 0.7683660984039307. Validation loss: 2.705667495727539.\n",
      "Epoch 2763. Training loss: 0.7683623433113098. Validation loss: 2.70566725730896.\n",
      "Epoch 2764. Training loss: 0.7683585286140442. Validation loss: 2.7056665420532227.\n",
      "Epoch 2765. Training loss: 0.7683547139167786. Validation loss: 2.7056660652160645.\n",
      "Epoch 2766. Training loss: 0.7683510184288025. Validation loss: 2.7056660652160645.\n",
      "Epoch 2767. Training loss: 0.7683472633361816. Validation loss: 2.7056655883789062.\n",
      "Epoch 2768. Training loss: 0.768343448638916. Validation loss: 2.705665111541748.\n",
      "Epoch 2769. Training loss: 0.7683396339416504. Validation loss: 2.705664873123169.\n",
      "Epoch 2770. Training loss: 0.7683358788490295. Validation loss: 2.7056643962860107.\n",
      "Epoch 2771. Training loss: 0.7683320045471191. Validation loss: 2.7056636810302734.\n",
      "Epoch 2772. Training loss: 0.7683283686637878. Validation loss: 2.7056634426116943.\n",
      "Epoch 2773. Training loss: 0.7683245539665222. Validation loss: 2.7056632041931152.\n",
      "Epoch 2774. Training loss: 0.7683207392692566. Validation loss: 2.705662965774536.\n",
      "Epoch 2775. Training loss: 0.7683169841766357. Validation loss: 2.705662488937378.\n",
      "Epoch 2776. Training loss: 0.7683131694793701. Validation loss: 2.705662250518799.\n",
      "Epoch 2777. Training loss: 0.7683093547821045. Validation loss: 2.7056617736816406.\n",
      "Epoch 2778. Training loss: 0.7683054804801941. Validation loss: 2.7056612968444824.\n",
      "Epoch 2779. Training loss: 0.7683017253875732. Validation loss: 2.705660820007324.\n",
      "Epoch 2780. Training loss: 0.7682979702949524. Validation loss: 2.705660820007324.\n",
      "Epoch 2781. Training loss: 0.7682941555976868. Validation loss: 2.705660104751587.\n",
      "Epoch 2782. Training loss: 0.7682904601097107. Validation loss: 2.7056596279144287.\n",
      "Epoch 2783. Training loss: 0.7682866454124451. Validation loss: 2.7056596279144287.\n",
      "Epoch 2784. Training loss: 0.7682827115058899. Validation loss: 2.7056591510772705.\n",
      "Epoch 2785. Training loss: 0.7682790756225586. Validation loss: 2.7056589126586914.\n",
      "Epoch 2786. Training loss: 0.7682752013206482. Validation loss: 2.705657958984375.\n",
      "Epoch 2787. Training loss: 0.7682714462280273. Validation loss: 2.705657958984375.\n",
      "Epoch 2788. Training loss: 0.7682676315307617. Validation loss: 2.705657482147217.\n",
      "Epoch 2789. Training loss: 0.7682638168334961. Validation loss: 2.705657482147217.\n",
      "Epoch 2790. Training loss: 0.7682600617408752. Validation loss: 2.7056570053100586.\n",
      "Epoch 2791. Training loss: 0.7682562470436096. Validation loss: 2.7056562900543213.\n",
      "Epoch 2792. Training loss: 0.768252432346344. Validation loss: 2.705656051635742.\n",
      "Epoch 2793. Training loss: 0.7682485580444336. Validation loss: 2.705655574798584.\n",
      "Epoch 2794. Training loss: 0.768244743347168. Validation loss: 2.705655097961426.\n",
      "Epoch 2795. Training loss: 0.7682409286499023. Validation loss: 2.7056546211242676.\n",
      "Epoch 2796. Training loss: 0.7682371735572815. Validation loss: 2.7056541442871094.\n",
      "Epoch 2797. Training loss: 0.7682333588600159. Validation loss: 2.7056541442871094.\n",
      "Epoch 2798. Training loss: 0.7682295441627502. Validation loss: 2.705653667449951.\n",
      "Epoch 2799. Training loss: 0.7682257294654846. Validation loss: 2.705653190612793.\n",
      "Epoch 2800. Training loss: 0.768221914768219. Validation loss: 2.705653190612793.\n",
      "Epoch 2801. Training loss: 0.7682181000709534. Validation loss: 2.7056524753570557.\n",
      "Epoch 2802. Training loss: 0.768214225769043. Validation loss: 2.7056519985198975.\n",
      "Epoch 2803. Training loss: 0.7682104110717773. Validation loss: 2.7056517601013184.\n",
      "Epoch 2804. Training loss: 0.7682065963745117. Validation loss: 2.7056515216827393.\n",
      "Epoch 2805. Training loss: 0.7682028412818909. Validation loss: 2.705651044845581.\n",
      "Epoch 2806. Training loss: 0.7681990265846252. Validation loss: 2.7056500911712646.\n",
      "Epoch 2807. Training loss: 0.7681951522827148. Validation loss: 2.7056500911712646.\n",
      "Epoch 2808. Training loss: 0.7681913375854492. Validation loss: 2.7056498527526855.\n",
      "Epoch 2809. Training loss: 0.7681875228881836. Validation loss: 2.7056491374969482.\n",
      "Epoch 2810. Training loss: 0.7681836485862732. Validation loss: 2.7056491374969482.\n",
      "Epoch 2811. Training loss: 0.7681798338890076. Validation loss: 2.70564866065979.\n",
      "Epoch 2812. Training loss: 0.7681760191917419. Validation loss: 2.705648422241211.\n",
      "Epoch 2813. Training loss: 0.7681722044944763. Validation loss: 2.7056479454040527.\n",
      "Epoch 2814. Training loss: 0.7681682705879211. Validation loss: 2.7056474685668945.\n",
      "Epoch 2815. Training loss: 0.7681645750999451. Validation loss: 2.7056467533111572.\n",
      "Epoch 2816. Training loss: 0.7681606411933899. Validation loss: 2.7056469917297363.\n",
      "Epoch 2817. Training loss: 0.7681568264961243. Validation loss: 2.705646276473999.\n",
      "Epoch 2818. Training loss: 0.7681530117988586. Validation loss: 2.705645799636841.\n",
      "Epoch 2819. Training loss: 0.7681491374969482. Validation loss: 2.7056453227996826.\n",
      "Epoch 2820. Training loss: 0.7681453227996826. Validation loss: 2.7056450843811035.\n",
      "Epoch 2821. Training loss: 0.7681414484977722. Validation loss: 2.705644369125366.\n",
      "Epoch 2822. Training loss: 0.7681376338005066. Validation loss: 2.705644130706787.\n",
      "Epoch 2823. Training loss: 0.7681336998939514. Validation loss: 2.705643892288208.\n",
      "Epoch 2824. Training loss: 0.7681298851966858. Validation loss: 2.705643653869629.\n",
      "Epoch 2825. Training loss: 0.7681260108947754. Validation loss: 2.7056431770324707.\n",
      "Epoch 2826. Training loss: 0.7681222558021545. Validation loss: 2.7056427001953125.\n",
      "Epoch 2827. Training loss: 0.7681183815002441. Validation loss: 2.7056427001953125.\n",
      "Epoch 2828. Training loss: 0.7681145668029785. Validation loss: 2.705641984939575.\n",
      "Epoch 2829. Training loss: 0.7681107521057129. Validation loss: 2.705641508102417.\n",
      "Epoch 2830. Training loss: 0.7681069374084473. Validation loss: 2.705641269683838.\n",
      "Epoch 2831. Training loss: 0.7681029438972473. Validation loss: 2.7056407928466797.\n",
      "Epoch 2832. Training loss: 0.7680991291999817. Validation loss: 2.7056405544281006.\n",
      "Epoch 2833. Training loss: 0.7680953145027161. Validation loss: 2.7056398391723633.\n",
      "Epoch 2834. Training loss: 0.7680914402008057. Validation loss: 2.7056398391723633.\n",
      "Epoch 2835. Training loss: 0.76808762550354. Validation loss: 2.705639123916626.\n",
      "Epoch 2836. Training loss: 0.7680837512016296. Validation loss: 2.7056386470794678.\n",
      "Epoch 2837. Training loss: 0.768079936504364. Validation loss: 2.7056386470794678.\n",
      "Epoch 2838. Training loss: 0.7680761218070984. Validation loss: 2.7056381702423096.\n",
      "Epoch 2839. Training loss: 0.7680721879005432. Validation loss: 2.7056374549865723.\n",
      "Epoch 2840. Training loss: 0.7680683135986328. Validation loss: 2.705636978149414.\n",
      "Epoch 2841. Training loss: 0.7680644989013672. Validation loss: 2.705636978149414.\n",
      "Epoch 2842. Training loss: 0.7680606245994568. Validation loss: 2.7056362628936768.\n",
      "Epoch 2843. Training loss: 0.7680566906929016. Validation loss: 2.7056360244750977.\n",
      "Epoch 2844. Training loss: 0.768052875995636. Validation loss: 2.7056357860565186.\n",
      "Epoch 2845. Training loss: 0.7680490612983704. Validation loss: 2.7056355476379395.\n",
      "Epoch 2846. Training loss: 0.76804518699646. Validation loss: 2.7056350708007812.\n",
      "Epoch 2847. Training loss: 0.7680413722991943. Validation loss: 2.705634355545044.\n",
      "Epoch 2848. Training loss: 0.7680374979972839. Validation loss: 2.7056338787078857.\n",
      "Epoch 2849. Training loss: 0.7680335640907288. Validation loss: 2.7056336402893066.\n",
      "Epoch 2850. Training loss: 0.7680296897888184. Validation loss: 2.7056331634521484.\n",
      "Epoch 2851. Training loss: 0.7680258750915527. Validation loss: 2.7056331634521484.\n",
      "Epoch 2852. Training loss: 0.7680221199989319. Validation loss: 2.7056326866149902.\n",
      "Epoch 2853. Training loss: 0.7680180668830872. Validation loss: 2.705632209777832.\n",
      "Epoch 2854. Training loss: 0.7680143713951111. Validation loss: 2.705632209777832.\n",
      "Epoch 2855. Training loss: 0.7680104374885559. Validation loss: 2.7056314945220947.\n",
      "Epoch 2856. Training loss: 0.7680065035820007. Validation loss: 2.7056310176849365.\n",
      "Epoch 2857. Training loss: 0.7680026888847351. Validation loss: 2.7056305408477783.\n",
      "Epoch 2858. Training loss: 0.7679988741874695. Validation loss: 2.70563006401062.\n",
      "Epoch 2859. Training loss: 0.7679950594902039. Validation loss: 2.705629587173462.\n",
      "Epoch 2860. Training loss: 0.7679912447929382. Validation loss: 2.705629348754883.\n",
      "Epoch 2861. Training loss: 0.7679874300956726. Validation loss: 2.7056291103363037.\n",
      "Epoch 2862. Training loss: 0.7679833769798279. Validation loss: 2.7056283950805664.\n",
      "Epoch 2863. Training loss: 0.7679794430732727. Validation loss: 2.7056283950805664.\n",
      "Epoch 2864. Training loss: 0.7679757475852966. Validation loss: 2.705627918243408.\n",
      "Epoch 2865. Training loss: 0.7679718136787415. Validation loss: 2.705627679824829.\n",
      "Epoch 2866. Training loss: 0.7679678797721863. Validation loss: 2.705626964569092.\n",
      "Epoch 2867. Training loss: 0.7679640650749207. Validation loss: 2.705626964569092.\n",
      "Epoch 2868. Training loss: 0.7679601311683655. Validation loss: 2.7056264877319336.\n",
      "Epoch 2869. Training loss: 0.7679562568664551. Validation loss: 2.7056260108947754.\n",
      "Epoch 2870. Training loss: 0.7679525017738342. Validation loss: 2.705625534057617.\n",
      "Epoch 2871. Training loss: 0.7679484486579895. Validation loss: 2.70562481880188.\n",
      "Epoch 2872. Training loss: 0.7679446339607239. Validation loss: 2.70562481880188.\n",
      "Epoch 2873. Training loss: 0.7679407596588135. Validation loss: 2.7056243419647217.\n",
      "Epoch 2874. Training loss: 0.7679368853569031. Validation loss: 2.7056238651275635.\n",
      "Epoch 2875. Training loss: 0.7679329514503479. Validation loss: 2.7056236267089844.\n",
      "Epoch 2876. Training loss: 0.7679290771484375. Validation loss: 2.705623149871826.\n",
      "Epoch 2877. Training loss: 0.7679252624511719. Validation loss: 2.705622911453247.\n",
      "Epoch 2878. Training loss: 0.7679213881492615. Validation loss: 2.705622434616089.\n",
      "Epoch 2879. Training loss: 0.7679175734519958. Validation loss: 2.7056221961975098.\n",
      "Epoch 2880. Training loss: 0.7679136395454407. Validation loss: 2.7056217193603516.\n",
      "Epoch 2881. Training loss: 0.7679097652435303. Validation loss: 2.7056212425231934.\n",
      "Epoch 2882. Training loss: 0.7679058909416199. Validation loss: 2.7056210041046143.\n",
      "Epoch 2883. Training loss: 0.7679019570350647. Validation loss: 2.705620050430298.\n",
      "Epoch 2884. Training loss: 0.7678981423377991. Validation loss: 2.705620050430298.\n",
      "Epoch 2885. Training loss: 0.7678942084312439. Validation loss: 2.7056195735931396.\n",
      "Epoch 2886. Training loss: 0.7678903937339783. Validation loss: 2.7056193351745605.\n",
      "Epoch 2887. Training loss: 0.7678864002227783. Validation loss: 2.7056190967559814.\n",
      "Epoch 2888. Training loss: 0.7678825855255127. Validation loss: 2.7056186199188232.\n",
      "Epoch 2889. Training loss: 0.7678785920143127. Validation loss: 2.705618381500244.\n",
      "Epoch 2890. Training loss: 0.7678747773170471. Validation loss: 2.705617666244507.\n",
      "Epoch 2891. Training loss: 0.7678709030151367. Validation loss: 2.7056174278259277.\n",
      "Epoch 2892. Training loss: 0.7678670883178711. Validation loss: 2.7056171894073486.\n",
      "Epoch 2893. Training loss: 0.7678630948066711. Validation loss: 2.7056164741516113.\n",
      "Epoch 2894. Training loss: 0.7678592801094055. Validation loss: 2.7056164741516113.\n",
      "Epoch 2895. Training loss: 0.7678553462028503. Validation loss: 2.705615520477295.\n",
      "Epoch 2896. Training loss: 0.7678515315055847. Validation loss: 2.705615520477295.\n",
      "Epoch 2897. Training loss: 0.7678475975990295. Validation loss: 2.7056150436401367.\n",
      "Epoch 2898. Training loss: 0.7678437232971191. Validation loss: 2.7056145668029785.\n",
      "Epoch 2899. Training loss: 0.7678397297859192. Validation loss: 2.7056140899658203.\n",
      "Epoch 2900. Training loss: 0.7678358554840088. Validation loss: 2.705613851547241.\n",
      "Epoch 2901. Training loss: 0.7678319811820984. Validation loss: 2.705613613128662.\n",
      "Epoch 2902. Training loss: 0.7678281664848328. Validation loss: 2.705613136291504.\n",
      "Epoch 2903. Training loss: 0.7678241729736328. Validation loss: 2.7056126594543457.\n",
      "Epoch 2904. Training loss: 0.7678202986717224. Validation loss: 2.7056126594543457.\n",
      "Epoch 2905. Training loss: 0.7678163647651672. Validation loss: 2.7056119441986084.\n",
      "Epoch 2906. Training loss: 0.7678125500679016. Validation loss: 2.7056117057800293.\n",
      "Epoch 2907. Training loss: 0.7678086161613464. Validation loss: 2.705611228942871.\n",
      "Epoch 2908. Training loss: 0.7678048014640808. Validation loss: 2.705610752105713.\n",
      "Epoch 2909. Training loss: 0.7678008675575256. Validation loss: 2.7056100368499756.\n",
      "Epoch 2910. Training loss: 0.7677969932556152. Validation loss: 2.7056097984313965.\n",
      "Epoch 2911. Training loss: 0.7677929997444153. Validation loss: 2.7056095600128174.\n",
      "Epoch 2912. Training loss: 0.7677890658378601. Validation loss: 2.70560884475708.\n",
      "Epoch 2913. Training loss: 0.7677851319313049. Validation loss: 2.705608367919922.\n",
      "Epoch 2914. Training loss: 0.7677813172340393. Validation loss: 2.705608367919922.\n",
      "Epoch 2915. Training loss: 0.7677774429321289. Validation loss: 2.7056078910827637.\n",
      "Epoch 2916. Training loss: 0.7677735686302185. Validation loss: 2.7056076526641846.\n",
      "Epoch 2917. Training loss: 0.7677695751190186. Validation loss: 2.7056071758270264.\n",
      "Epoch 2918. Training loss: 0.7677658200263977. Validation loss: 2.705606460571289.\n",
      "Epoch 2919. Training loss: 0.767761766910553. Validation loss: 2.705605983734131.\n",
      "Epoch 2920. Training loss: 0.7677578926086426. Validation loss: 2.70560622215271.\n",
      "Epoch 2921. Training loss: 0.7677538990974426. Validation loss: 2.7056055068969727.\n",
      "Epoch 2922. Training loss: 0.7677500247955322. Validation loss: 2.7056047916412354.\n",
      "Epoch 2923. Training loss: 0.7677461504936218. Validation loss: 2.7056047916412354.\n",
      "Epoch 2924. Training loss: 0.7677422165870667. Validation loss: 2.705604314804077.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2925. Training loss: 0.7677383422851562. Validation loss: 2.705603837966919.\n",
      "Epoch 2926. Training loss: 0.7677344679832458. Validation loss: 2.705603837966919.\n",
      "Epoch 2927. Training loss: 0.7677305340766907. Validation loss: 2.7056033611297607.\n",
      "Epoch 2928. Training loss: 0.767726480960846. Validation loss: 2.7056026458740234.\n",
      "Epoch 2929. Training loss: 0.7677226662635803. Validation loss: 2.7056021690368652.\n",
      "Epoch 2930. Training loss: 0.7677187919616699. Validation loss: 2.705601930618286.\n",
      "Epoch 2931. Training loss: 0.7677149176597595. Validation loss: 2.705601692199707.\n",
      "Epoch 2932. Training loss: 0.7677109241485596. Validation loss: 2.705601453781128.\n",
      "Epoch 2933. Training loss: 0.7677070498466492. Validation loss: 2.7056009769439697.\n",
      "Epoch 2934. Training loss: 0.767703115940094. Validation loss: 2.7056007385253906.\n",
      "Epoch 2935. Training loss: 0.7676992416381836. Validation loss: 2.7056000232696533.\n",
      "Epoch 2936. Training loss: 0.7676952481269836. Validation loss: 2.705599546432495.\n",
      "Epoch 2937. Training loss: 0.7676913738250732. Validation loss: 2.705599308013916.\n",
      "Epoch 2938. Training loss: 0.7676874995231628. Validation loss: 2.705598831176758.\n",
      "Epoch 2939. Training loss: 0.7676835060119629. Validation loss: 2.7055983543395996.\n",
      "Epoch 2940. Training loss: 0.7676796317100525. Validation loss: 2.7055981159210205.\n",
      "Epoch 2941. Training loss: 0.7676756381988525. Validation loss: 2.7055976390838623.\n",
      "Epoch 2942. Training loss: 0.7676717638969421. Validation loss: 2.705597400665283.\n",
      "Epoch 2943. Training loss: 0.7676677703857422. Validation loss: 2.705596685409546.\n",
      "Epoch 2944. Training loss: 0.7676639556884766. Validation loss: 2.7055962085723877.\n",
      "Epoch 2945. Training loss: 0.7676600813865662. Validation loss: 2.7055959701538086.\n",
      "Epoch 2946. Training loss: 0.7676560878753662. Validation loss: 2.7055959701538086.\n",
      "Epoch 2947. Training loss: 0.7676520943641663. Validation loss: 2.7055952548980713.\n",
      "Epoch 2948. Training loss: 0.7676482200622559. Validation loss: 2.705594778060913.\n",
      "Epoch 2949. Training loss: 0.7676443457603455. Validation loss: 2.705594301223755.\n",
      "Epoch 2950. Training loss: 0.7676402926445007. Validation loss: 2.705594062805176.\n",
      "Epoch 2951. Training loss: 0.7676363587379456. Validation loss: 2.7055938243865967.\n",
      "Epoch 2952. Training loss: 0.7676324844360352. Validation loss: 2.7055933475494385.\n",
      "Epoch 2953. Training loss: 0.7676286101341248. Validation loss: 2.7055928707122803.\n",
      "Epoch 2954. Training loss: 0.7676246762275696. Validation loss: 2.705592632293701.\n",
      "Epoch 2955. Training loss: 0.7676207423210144. Validation loss: 2.705592155456543.\n",
      "Epoch 2956. Training loss: 0.7676167488098145. Validation loss: 2.7055916786193848.\n",
      "Epoch 2957. Training loss: 0.7676129341125488. Validation loss: 2.7055912017822266.\n",
      "Epoch 2958. Training loss: 0.7676088809967041. Validation loss: 2.7055907249450684.\n",
      "Epoch 2959. Training loss: 0.7676050662994385. Validation loss: 2.7055907249450684.\n",
      "Epoch 2960. Training loss: 0.7676010131835938. Validation loss: 2.70559024810791.\n",
      "Epoch 2961. Training loss: 0.7675971388816833. Validation loss: 2.705589771270752.\n",
      "Epoch 2962. Training loss: 0.7675932049751282. Validation loss: 2.7055892944335938.\n",
      "Epoch 2963. Training loss: 0.767589271068573. Validation loss: 2.7055888175964355.\n",
      "Epoch 2964. Training loss: 0.7675853371620178. Validation loss: 2.7055883407592773.\n",
      "Epoch 2965. Training loss: 0.7675812840461731. Validation loss: 2.7055881023406982.\n",
      "Epoch 2966. Training loss: 0.7675773501396179. Validation loss: 2.705587863922119.\n",
      "Epoch 2967. Training loss: 0.7675734162330627. Validation loss: 2.705587387084961.\n",
      "Epoch 2968. Training loss: 0.7675694823265076. Validation loss: 2.7055869102478027.\n",
      "Epoch 2969. Training loss: 0.7675655484199524. Validation loss: 2.7055864334106445.\n",
      "Epoch 2970. Training loss: 0.767561674118042. Validation loss: 2.7055859565734863.\n",
      "Epoch 2971. Training loss: 0.767557680606842. Validation loss: 2.705585479736328.\n",
      "Epoch 2972. Training loss: 0.7675537467002869. Validation loss: 2.705585241317749.\n",
      "Epoch 2973. Training loss: 0.7675498127937317. Validation loss: 2.70558500289917.\n",
      "Epoch 2974. Training loss: 0.7675458788871765. Validation loss: 2.7055845260620117.\n",
      "Epoch 2975. Training loss: 0.7675419449806213. Validation loss: 2.7055840492248535.\n",
      "Epoch 2976. Training loss: 0.7675380110740662. Validation loss: 2.7055835723876953.\n",
      "Epoch 2977. Training loss: 0.7675340175628662. Validation loss: 2.705583095550537.\n",
      "Epoch 2978. Training loss: 0.7675300240516663. Validation loss: 2.705582857131958.\n",
      "Epoch 2979. Training loss: 0.7675261497497559. Validation loss: 2.705582618713379.\n",
      "Epoch 2980. Training loss: 0.7675221562385559. Validation loss: 2.7055821418762207.\n",
      "Epoch 2981. Training loss: 0.7675182223320007. Validation loss: 2.7055816650390625.\n",
      "Epoch 2982. Training loss: 0.7675142288208008. Validation loss: 2.7055816650390625.\n",
      "Epoch 2983. Training loss: 0.7675103545188904. Validation loss: 2.705580949783325.\n",
      "Epoch 2984. Training loss: 0.7675063014030457. Validation loss: 2.705580711364746.\n",
      "Epoch 2985. Training loss: 0.7675023674964905. Validation loss: 2.705579996109009.\n",
      "Epoch 2986. Training loss: 0.7674984931945801. Validation loss: 2.7055797576904297.\n",
      "Epoch 2987. Training loss: 0.7674944996833801. Validation loss: 2.7055792808532715.\n",
      "Epoch 2988. Training loss: 0.767490565776825. Validation loss: 2.7055788040161133.\n",
      "Epoch 2989. Training loss: 0.767486572265625. Validation loss: 2.705578327178955.\n",
      "Epoch 2990. Training loss: 0.767482578754425. Validation loss: 2.705578088760376.\n",
      "Epoch 2991. Training loss: 0.7674786448478699. Validation loss: 2.7055773735046387.\n",
      "Epoch 2992. Training loss: 0.7674747109413147. Validation loss: 2.7055771350860596.\n",
      "Epoch 2993. Training loss: 0.7674707770347595. Validation loss: 2.7055771350860596.\n",
      "Epoch 2994. Training loss: 0.7674667835235596. Validation loss: 2.7055764198303223.\n",
      "Epoch 2995. Training loss: 0.7674627900123596. Validation loss: 2.705575942993164.\n",
      "Epoch 2996. Training loss: 0.7674589157104492. Validation loss: 2.705575466156006.\n",
      "Epoch 2997. Training loss: 0.7674550414085388. Validation loss: 2.7055752277374268.\n",
      "Epoch 2998. Training loss: 0.7674509882926941. Validation loss: 2.7055749893188477.\n",
      "Epoch 2999. Training loss: 0.7674470543861389. Validation loss: 2.7055745124816895.\n",
      "Epoch 3000. Training loss: 0.7674431204795837. Validation loss: 2.7055742740631104.\n",
      "Epoch 3001. Training loss: 0.7674391269683838. Validation loss: 2.7055740356445312.\n",
      "Epoch 3002. Training loss: 0.7674350738525391. Validation loss: 2.705573081970215.\n",
      "Epoch 3003. Training loss: 0.7674311995506287. Validation loss: 2.7055728435516357.\n",
      "Epoch 3004. Training loss: 0.7674272060394287. Validation loss: 2.7055726051330566.\n",
      "Epoch 3005. Training loss: 0.7674232125282288. Validation loss: 2.7055721282958984.\n",
      "Epoch 3006. Training loss: 0.7674192786216736. Validation loss: 2.7055718898773193.\n",
      "Epoch 3007. Training loss: 0.7674152255058289. Validation loss: 2.705571174621582.\n",
      "Epoch 3008. Training loss: 0.7674112915992737. Validation loss: 2.705570697784424.\n",
      "Epoch 3009. Training loss: 0.7674073576927185. Validation loss: 2.7055702209472656.\n",
      "Epoch 3010. Training loss: 0.7674035429954529. Validation loss: 2.7055702209472656.\n",
      "Epoch 3011. Training loss: 0.7673996090888977. Validation loss: 2.7055697441101074.\n",
      "Epoch 3012. Training loss: 0.7673957943916321. Validation loss: 2.7055695056915283.\n",
      "Epoch 3013. Training loss: 0.7673919200897217. Validation loss: 2.70556902885437.\n",
      "Epoch 3014. Training loss: 0.767388105392456. Validation loss: 2.705568313598633.\n",
      "Epoch 3015. Training loss: 0.7673844695091248. Validation loss: 2.705568313598633.\n",
      "Epoch 3016. Training loss: 0.7673805356025696. Validation loss: 2.7055675983428955.\n",
      "Epoch 3017. Training loss: 0.7673768401145935. Validation loss: 2.7055673599243164.\n",
      "Epoch 3018. Training loss: 0.7673729062080383. Validation loss: 2.7055671215057373.\n",
      "Epoch 3019. Training loss: 0.7673692107200623. Validation loss: 2.705566644668579.\n",
      "Epoch 3020. Training loss: 0.7673653960227966. Validation loss: 2.705566167831421.\n",
      "Epoch 3021. Training loss: 0.767361581325531. Validation loss: 2.7055654525756836.\n",
      "Epoch 3022. Training loss: 0.7673578262329102. Validation loss: 2.7055649757385254.\n",
      "Epoch 3023. Training loss: 0.7673540115356445. Validation loss: 2.7055647373199463.\n",
      "Epoch 3024. Training loss: 0.7673502564430237. Validation loss: 2.705564498901367.\n",
      "Epoch 3025. Training loss: 0.7673464417457581. Validation loss: 2.705564022064209.\n",
      "Epoch 3026. Training loss: 0.767342746257782. Validation loss: 2.705563545227051.\n",
      "Epoch 3027. Training loss: 0.7673389315605164. Validation loss: 2.7055633068084717.\n",
      "Epoch 3028. Training loss: 0.7673351764678955. Validation loss: 2.7055625915527344.\n",
      "Epoch 3029. Training loss: 0.7673313617706299. Validation loss: 2.7055623531341553.\n",
      "Epoch 3030. Training loss: 0.767327606678009. Validation loss: 2.705561876296997.\n",
      "Epoch 3031. Training loss: 0.7673237919807434. Validation loss: 2.705561399459839.\n",
      "Epoch 3032. Training loss: 0.7673200964927673. Validation loss: 2.7055611610412598.\n",
      "Epoch 3033. Training loss: 0.7673162817955017. Validation loss: 2.7055606842041016.\n",
      "Epoch 3034. Training loss: 0.7673125267028809. Validation loss: 2.7055604457855225.\n",
      "Epoch 3035. Training loss: 0.7673088908195496. Validation loss: 2.705559492111206.\n",
      "Epoch 3036. Training loss: 0.7673051357269287. Validation loss: 2.705559253692627.\n",
      "Epoch 3037. Training loss: 0.7673015594482422. Validation loss: 2.7055587768554688.\n",
      "Epoch 3038. Training loss: 0.7672978043556213. Validation loss: 2.7055585384368896.\n",
      "Epoch 3039. Training loss: 0.7672942280769348. Validation loss: 2.7055583000183105.\n",
      "Epoch 3040. Training loss: 0.7672905921936035. Validation loss: 2.7055578231811523.\n",
      "Epoch 3041. Training loss: 0.7672869563102722. Validation loss: 2.705557346343994.\n",
      "Epoch 3042. Training loss: 0.7672834396362305. Validation loss: 2.705556869506836.\n",
      "Epoch 3043. Training loss: 0.7672798037528992. Validation loss: 2.705556631088257.\n",
      "Epoch 3044. Training loss: 0.7672761082649231. Validation loss: 2.7055561542510986.\n",
      "Epoch 3045. Training loss: 0.7672724723815918. Validation loss: 2.7055556774139404.\n",
      "Epoch 3046. Training loss: 0.7672688961029053. Validation loss: 2.7055552005767822.\n",
      "Epoch 3047. Training loss: 0.767265260219574. Validation loss: 2.705554485321045.\n",
      "Epoch 3048. Training loss: 0.7672617435455322. Validation loss: 2.705554246902466.\n",
      "Epoch 3049. Training loss: 0.7672581672668457. Validation loss: 2.7055535316467285.\n",
      "Epoch 3050. Training loss: 0.7672544121742249. Validation loss: 2.7055535316467285.\n",
      "Epoch 3051. Training loss: 0.7672510147094727. Validation loss: 2.7055530548095703.\n",
      "Epoch 3052. Training loss: 0.7672473788261414. Validation loss: 2.705552816390991.\n",
      "Epoch 3053. Training loss: 0.7672436833381653. Validation loss: 2.705552101135254.\n",
      "Epoch 3054. Training loss: 0.7672401070594788. Validation loss: 2.705551862716675.\n",
      "Epoch 3055. Training loss: 0.7672364711761475. Validation loss: 2.7055513858795166.\n",
      "Epoch 3056. Training loss: 0.7672328948974609. Validation loss: 2.7055506706237793.\n",
      "Epoch 3057. Training loss: 0.7672293782234192. Validation loss: 2.7055504322052.\n",
      "Epoch 3058. Training loss: 0.7672258019447327. Validation loss: 2.705549716949463.\n",
      "Epoch 3059. Training loss: 0.7672221660614014. Validation loss: 2.705549478530884.\n",
      "Epoch 3060. Training loss: 0.7672185897827148. Validation loss: 2.7055490016937256.\n",
      "Epoch 3061. Training loss: 0.7672149538993835. Validation loss: 2.7055487632751465.\n",
      "Epoch 3062. Training loss: 0.7672112584114075. Validation loss: 2.705548048019409.\n",
      "Epoch 3063. Training loss: 0.7672078013420105. Validation loss: 2.70554780960083.\n",
      "Epoch 3064. Training loss: 0.7672041058540344. Validation loss: 2.705547332763672.\n",
      "Epoch 3065. Training loss: 0.7672005295753479. Validation loss: 2.7055466175079346.\n",
      "Epoch 3066. Training loss: 0.7671969532966614. Validation loss: 2.7055461406707764.\n",
      "Epoch 3067. Training loss: 0.7671933770179749. Validation loss: 2.7055461406707764.\n",
      "Epoch 3068. Training loss: 0.7671898007392883. Validation loss: 2.705545425415039.\n",
      "Epoch 3069. Training loss: 0.767186164855957. Validation loss: 2.70554518699646.\n",
      "Epoch 3070. Training loss: 0.7671825289726257. Validation loss: 2.705544948577881.\n",
      "Epoch 3071. Training loss: 0.767179012298584. Validation loss: 2.7055444717407227.\n",
      "Epoch 3072. Training loss: 0.7671754360198975. Validation loss: 2.7055437564849854.\n",
      "Epoch 3073. Training loss: 0.7671718001365662. Validation loss: 2.7055435180664062.\n",
      "Epoch 3074. Training loss: 0.7671682238578796. Validation loss: 2.705543041229248.\n",
      "Epoch 3075. Training loss: 0.7671645283699036. Validation loss: 2.70554256439209.\n",
      "Epoch 3076. Training loss: 0.767160952091217. Validation loss: 2.7055418491363525.\n",
      "Epoch 3077. Training loss: 0.7671573758125305. Validation loss: 2.7055413722991943.\n",
      "Epoch 3078. Training loss: 0.7671537399291992. Validation loss: 2.705540895462036.\n",
      "Epoch 3079. Training loss: 0.7671501636505127. Validation loss: 2.705540895462036.\n",
      "Epoch 3080. Training loss: 0.7671465873718262. Validation loss: 2.705540418624878.\n",
      "Epoch 3081. Training loss: 0.7671429514884949. Validation loss: 2.7055399417877197.\n",
      "Epoch 3082. Training loss: 0.7671394348144531. Validation loss: 2.7055394649505615.\n",
      "Epoch 3083. Training loss: 0.7671357989311218. Validation loss: 2.7055392265319824.\n",
      "Epoch 3084. Training loss: 0.7671321034431458. Validation loss: 2.705538511276245.\n",
      "Epoch 3085. Training loss: 0.7671284675598145. Validation loss: 2.705538034439087.\n",
      "Epoch 3086. Training loss: 0.7671249508857727. Validation loss: 2.7055375576019287.\n",
      "Epoch 3087. Training loss: 0.7671214938163757. Validation loss: 2.7055373191833496.\n",
      "Epoch 3088. Training loss: 0.7671177387237549. Validation loss: 2.7055368423461914.\n",
      "Epoch 3089. Training loss: 0.7671141624450684. Validation loss: 2.705536365509033.\n",
      "Epoch 3090. Training loss: 0.7671105265617371. Validation loss: 2.705536127090454.\n",
      "Epoch 3091. Training loss: 0.7671069502830505. Validation loss: 2.705535411834717.\n",
      "Epoch 3092. Training loss: 0.767103374004364. Validation loss: 2.7055349349975586.\n",
      "Epoch 3093. Training loss: 0.7670996785163879. Validation loss: 2.7055346965789795.\n",
      "Epoch 3094. Training loss: 0.7670961022377014. Validation loss: 2.7055344581604004.\n",
      "Epoch 3095. Training loss: 0.7670925259590149. Validation loss: 2.705533504486084.\n",
      "Epoch 3096. Training loss: 0.7670887112617493. Validation loss: 2.705533027648926.\n",
      "Epoch 3097. Training loss: 0.7670853137969971. Validation loss: 2.705533027648926.\n",
      "Epoch 3098. Training loss: 0.7670816779136658. Validation loss: 2.7055325508117676.\n",
      "Epoch 3099. Training loss: 0.7670779824256897. Validation loss: 2.7055320739746094.\n",
      "Epoch 3100. Training loss: 0.7670743465423584. Validation loss: 2.7055318355560303.\n",
      "Epoch 3101. Training loss: 0.7670707702636719. Validation loss: 2.705531358718872.\n",
      "Epoch 3102. Training loss: 0.7670671343803406. Validation loss: 2.7055306434631348.\n",
      "Epoch 3103. Training loss: 0.7670636177062988. Validation loss: 2.7055304050445557.\n",
      "Epoch 3104. Training loss: 0.767059862613678. Validation loss: 2.7055299282073975.\n",
      "Epoch 3105. Training loss: 0.7670562863349915. Validation loss: 2.7055294513702393.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3106. Training loss: 0.7670526504516602. Validation loss: 2.705528736114502.\n",
      "Epoch 3107. Training loss: 0.7670490741729736. Validation loss: 2.705528497695923.\n",
      "Epoch 3108. Training loss: 0.7670454978942871. Validation loss: 2.7055280208587646.\n",
      "Epoch 3109. Training loss: 0.7670416831970215. Validation loss: 2.7055275440216064.\n",
      "Epoch 3110. Training loss: 0.767038106918335. Validation loss: 2.7055273056030273.\n",
      "Epoch 3111. Training loss: 0.7670345306396484. Validation loss: 2.705526828765869.\n",
      "Epoch 3112. Training loss: 0.7670309543609619. Validation loss: 2.705526351928711.\n",
      "Epoch 3113. Training loss: 0.7670273184776306. Validation loss: 2.7055256366729736.\n",
      "Epoch 3114. Training loss: 0.7670236229896545. Validation loss: 2.7055253982543945.\n",
      "Epoch 3115. Training loss: 0.767020046710968. Validation loss: 2.7055249214172363.\n",
      "Epoch 3116. Training loss: 0.7670164704322815. Validation loss: 2.7055246829986572.\n",
      "Epoch 3117. Training loss: 0.7670127749443054. Validation loss: 2.705524206161499.\n",
      "Epoch 3118. Training loss: 0.7670091986656189. Validation loss: 2.70552396774292.\n",
      "Epoch 3119. Training loss: 0.7670055031776428. Validation loss: 2.7055234909057617.\n",
      "Epoch 3120. Training loss: 0.7670019268989563. Validation loss: 2.7055227756500244.\n",
      "Epoch 3121. Training loss: 0.7669982314109802. Validation loss: 2.705522298812866.\n",
      "Epoch 3122. Training loss: 0.7669946551322937. Validation loss: 2.705522060394287.\n",
      "Epoch 3123. Training loss: 0.7669910788536072. Validation loss: 2.705521583557129.\n",
      "Epoch 3124. Training loss: 0.7669873833656311. Validation loss: 2.7055211067199707.\n",
      "Epoch 3125. Training loss: 0.7669838070869446. Validation loss: 2.7055208683013916.\n",
      "Epoch 3126. Training loss: 0.7669801115989685. Validation loss: 2.705519914627075.\n",
      "Epoch 3127. Training loss: 0.766976535320282. Validation loss: 2.705519676208496.\n",
      "Epoch 3128. Training loss: 0.7669728398323059. Validation loss: 2.705519199371338.\n",
      "Epoch 3129. Training loss: 0.7669692039489746. Validation loss: 2.705518960952759.\n",
      "Epoch 3130. Training loss: 0.7669656276702881. Validation loss: 2.7055187225341797.\n",
      "Epoch 3131. Training loss: 0.7669619917869568. Validation loss: 2.7055180072784424.\n",
      "Epoch 3132. Training loss: 0.7669584155082703. Validation loss: 2.705517530441284.\n",
      "Epoch 3133. Training loss: 0.7669547200202942. Validation loss: 2.705517292022705.\n",
      "Epoch 3134. Training loss: 0.7669510245323181. Validation loss: 2.705516815185547.\n",
      "Epoch 3135. Training loss: 0.7669474482536316. Validation loss: 2.7055165767669678.\n",
      "Epoch 3136. Training loss: 0.7669438719749451. Validation loss: 2.7055158615112305.\n",
      "Epoch 3137. Training loss: 0.7669401168823242. Validation loss: 2.705515146255493.\n",
      "Epoch 3138. Training loss: 0.7669365406036377. Validation loss: 2.705514907836914.\n",
      "Epoch 3139. Training loss: 0.7669329047203064. Validation loss: 2.705514430999756.\n",
      "Epoch 3140. Training loss: 0.7669292092323303. Validation loss: 2.7055139541625977.\n",
      "Epoch 3141. Training loss: 0.7669256329536438. Validation loss: 2.7055137157440186.\n",
      "Epoch 3142. Training loss: 0.7669219970703125. Validation loss: 2.7055132389068604.\n",
      "Epoch 3143. Training loss: 0.7669183611869812. Validation loss: 2.705512762069702.\n",
      "Epoch 3144. Training loss: 0.7669146656990051. Validation loss: 2.705512046813965.\n",
      "Epoch 3145. Training loss: 0.7669110894203186. Validation loss: 2.705512046813965.\n",
      "Epoch 3146. Training loss: 0.7669073939323425. Validation loss: 2.7055118083953857.\n",
      "Epoch 3147. Training loss: 0.7669036984443665. Validation loss: 2.7055110931396484.\n",
      "Epoch 3148. Training loss: 0.7669001221656799. Validation loss: 2.7055110931396484.\n",
      "Epoch 3149. Training loss: 0.7668963074684143. Validation loss: 2.705510139465332.\n",
      "Epoch 3150. Training loss: 0.7668927311897278. Validation loss: 2.7055094242095947.\n",
      "Epoch 3151. Training loss: 0.7668890357017517. Validation loss: 2.7055091857910156.\n",
      "Epoch 3152. Training loss: 0.7668853402137756. Validation loss: 2.7055089473724365.\n",
      "Epoch 3153. Training loss: 0.7668817639350891. Validation loss: 2.7055084705352783.\n",
      "Epoch 3154. Training loss: 0.7668781876564026. Validation loss: 2.70550799369812.\n",
      "Epoch 3155. Training loss: 0.7668744921684265. Validation loss: 2.705507278442383.\n",
      "Epoch 3156. Training loss: 0.7668707966804504. Validation loss: 2.7055068016052246.\n",
      "Epoch 3157. Training loss: 0.7668672204017639. Validation loss: 2.7055068016052246.\n",
      "Epoch 3158. Training loss: 0.7668635845184326. Validation loss: 2.7055063247680664.\n",
      "Epoch 3159. Training loss: 0.7668598294258118. Validation loss: 2.705505847930908.\n",
      "Epoch 3160. Training loss: 0.7668562531471252. Validation loss: 2.70550537109375.\n",
      "Epoch 3161. Training loss: 0.7668525576591492. Validation loss: 2.705504894256592.\n",
      "Epoch 3162. Training loss: 0.7668488621711731. Validation loss: 2.7055041790008545.\n",
      "Epoch 3163. Training loss: 0.7668452262878418. Validation loss: 2.7055039405822754.\n",
      "Epoch 3164. Training loss: 0.7668415904045105. Validation loss: 2.7055039405822754.\n",
      "Epoch 3165. Training loss: 0.766838014125824. Validation loss: 2.705503225326538.\n",
      "Epoch 3166. Training loss: 0.7668342590332031. Validation loss: 2.705502510070801.\n",
      "Epoch 3167. Training loss: 0.7668306827545166. Validation loss: 2.7055022716522217.\n",
      "Epoch 3168. Training loss: 0.7668269276618958. Validation loss: 2.7055020332336426.\n",
      "Epoch 3169. Training loss: 0.7668232917785645. Validation loss: 2.7055015563964844.\n",
      "Epoch 3170. Training loss: 0.7668196558952332. Validation loss: 2.705501079559326.\n",
      "Epoch 3171. Training loss: 0.7668159604072571. Validation loss: 2.705500364303589.\n",
      "Epoch 3172. Training loss: 0.7668123245239258. Validation loss: 2.7055001258850098.\n",
      "Epoch 3173. Training loss: 0.7668086886405945. Validation loss: 2.7054996490478516.\n",
      "Epoch 3174. Training loss: 0.7668049931526184. Validation loss: 2.7054991722106934.\n",
      "Epoch 3175. Training loss: 0.7668012976646423. Validation loss: 2.705498695373535.\n",
      "Epoch 3176. Training loss: 0.7667976021766663. Validation loss: 2.705498218536377.\n",
      "Epoch 3177. Training loss: 0.766793966293335. Validation loss: 2.705497980117798.\n",
      "Epoch 3178. Training loss: 0.7667903304100037. Validation loss: 2.7054972648620605.\n",
      "Epoch 3179. Training loss: 0.7667866349220276. Validation loss: 2.7054967880249023.\n",
      "Epoch 3180. Training loss: 0.7667829990386963. Validation loss: 2.7054967880249023.\n",
      "Epoch 3181. Training loss: 0.766779363155365. Validation loss: 2.705496311187744.\n",
      "Epoch 3182. Training loss: 0.7667756080627441. Validation loss: 2.705495834350586.\n",
      "Epoch 3183. Training loss: 0.7667720317840576. Validation loss: 2.7054953575134277.\n",
      "Epoch 3184. Training loss: 0.7667682766914368. Validation loss: 2.7054948806762695.\n",
      "Epoch 3185. Training loss: 0.7667645812034607. Validation loss: 2.7054944038391113.\n",
      "Epoch 3186. Training loss: 0.7667610049247742. Validation loss: 2.705493927001953.\n",
      "Epoch 3187. Training loss: 0.7667573094367981. Validation loss: 2.705493688583374.\n",
      "Epoch 3188. Training loss: 0.7667536735534668. Validation loss: 2.7054929733276367.\n",
      "Epoch 3189. Training loss: 0.7667500376701355. Validation loss: 2.7054927349090576.\n",
      "Epoch 3190. Training loss: 0.7667462229728699. Validation loss: 2.7054920196533203.\n",
      "Epoch 3191. Training loss: 0.7667426466941833. Validation loss: 2.705491542816162.\n",
      "Epoch 3192. Training loss: 0.7667388916015625. Validation loss: 2.705491065979004.\n",
      "Epoch 3193. Training loss: 0.7667352557182312. Validation loss: 2.705491065979004.\n",
      "Epoch 3194. Training loss: 0.7667315602302551. Validation loss: 2.7054905891418457.\n",
      "Epoch 3195. Training loss: 0.766727864742279. Validation loss: 2.7054901123046875.\n",
      "Epoch 3196. Training loss: 0.766724169254303. Validation loss: 2.7054896354675293.\n",
      "Epoch 3197. Training loss: 0.7667205333709717. Validation loss: 2.705489158630371.\n",
      "Epoch 3198. Training loss: 0.7667168974876404. Validation loss: 2.705488681793213.\n",
      "Epoch 3199. Training loss: 0.7667131423950195. Validation loss: 2.705488681793213.\n",
      "Epoch 3200. Training loss: 0.7667093873023987. Validation loss: 2.7054877281188965.\n",
      "Epoch 3201. Training loss: 0.7667058110237122. Validation loss: 2.7054874897003174.\n",
      "Epoch 3202. Training loss: 0.7667021155357361. Validation loss: 2.705487012863159.\n",
      "Epoch 3203. Training loss: 0.76669842004776. Validation loss: 2.705486297607422.\n",
      "Epoch 3204. Training loss: 0.7666947245597839. Validation loss: 2.7054858207702637.\n",
      "Epoch 3205. Training loss: 0.7666911482810974. Validation loss: 2.7054855823516846.\n",
      "Epoch 3206. Training loss: 0.7666873931884766. Validation loss: 2.7054848670959473.\n",
      "Epoch 3207. Training loss: 0.7666837573051453. Validation loss: 2.7054848670959473.\n",
      "Epoch 3208. Training loss: 0.7666800022125244. Validation loss: 2.705484390258789.\n",
      "Epoch 3209. Training loss: 0.7666762471199036. Validation loss: 2.705483913421631.\n",
      "Epoch 3210. Training loss: 0.7666726112365723. Validation loss: 2.7054834365844727.\n",
      "Epoch 3211. Training loss: 0.7666690349578857. Validation loss: 2.7054831981658936.\n",
      "Epoch 3212. Training loss: 0.7666653990745544. Validation loss: 2.7054824829101562.\n",
      "Epoch 3213. Training loss: 0.7666616439819336. Validation loss: 2.705482006072998.\n",
      "Epoch 3214. Training loss: 0.7666578888893127. Validation loss: 2.705482006072998.\n",
      "Epoch 3215. Training loss: 0.7666541934013367. Validation loss: 2.7054810523986816.\n",
      "Epoch 3216. Training loss: 0.7666504979133606. Validation loss: 2.7054805755615234.\n",
      "Epoch 3217. Training loss: 0.7666468620300293. Validation loss: 2.7054803371429443.\n",
      "Epoch 3218. Training loss: 0.7666431069374084. Validation loss: 2.705479860305786.\n",
      "Epoch 3219. Training loss: 0.7666394114494324. Validation loss: 2.705479383468628.\n",
      "Epoch 3220. Training loss: 0.7666357159614563. Validation loss: 2.705479145050049.\n",
      "Epoch 3221. Training loss: 0.766632080078125. Validation loss: 2.7054786682128906.\n",
      "Epoch 3222. Training loss: 0.7666282653808594. Validation loss: 2.7054781913757324.\n",
      "Epoch 3223. Training loss: 0.7666246294975281. Validation loss: 2.7054779529571533.\n",
      "Epoch 3224. Training loss: 0.766620934009552. Validation loss: 2.705477476119995.\n",
      "Epoch 3225. Training loss: 0.7666172385215759. Validation loss: 2.705476999282837.\n",
      "Epoch 3226. Training loss: 0.7666135430335999. Validation loss: 2.7054765224456787.\n",
      "Epoch 3227. Training loss: 0.7666098475456238. Validation loss: 2.7054758071899414.\n",
      "Epoch 3228. Training loss: 0.7666061520576477. Validation loss: 2.705475330352783.\n",
      "Epoch 3229. Training loss: 0.7666024565696716. Validation loss: 2.705475091934204.\n",
      "Epoch 3230. Training loss: 0.7665987610816956. Validation loss: 2.705474853515625.\n",
      "Epoch 3231. Training loss: 0.7665950655937195. Validation loss: 2.7054741382598877.\n",
      "Epoch 3232. Training loss: 0.7665913701057434. Validation loss: 2.7054738998413086.\n",
      "Epoch 3233. Training loss: 0.7665877342224121. Validation loss: 2.7054734230041504.\n",
      "Epoch 3234. Training loss: 0.7665839195251465. Validation loss: 2.705472707748413.\n",
      "Epoch 3235. Training loss: 0.7665802836418152. Validation loss: 2.705472469329834.\n",
      "Epoch 3236. Training loss: 0.7665765881538391. Validation loss: 2.705471992492676.\n",
      "Epoch 3237. Training loss: 0.7665729522705078. Validation loss: 2.7054717540740967.\n",
      "Epoch 3238. Training loss: 0.7665691375732422. Validation loss: 2.7054715156555176.\n",
      "Epoch 3239. Training loss: 0.7665653824806213. Validation loss: 2.705470561981201.\n",
      "Epoch 3240. Training loss: 0.7665616869926453. Validation loss: 2.705470323562622.\n",
      "Epoch 3241. Training loss: 0.7665579319000244. Validation loss: 2.705469846725464.\n",
      "Epoch 3242. Training loss: 0.7665542960166931. Validation loss: 2.7054693698883057.\n",
      "Epoch 3243. Training loss: 0.7665505409240723. Validation loss: 2.7054688930511475.\n",
      "Epoch 3244. Training loss: 0.7665467858314514. Validation loss: 2.7054684162139893.\n",
      "Epoch 3245. Training loss: 0.7665430903434753. Validation loss: 2.705467939376831.\n",
      "Epoch 3246. Training loss: 0.7665393948554993. Validation loss: 2.70546817779541.\n",
      "Epoch 3247. Training loss: 0.7665356993675232. Validation loss: 2.7054672241210938.\n",
      "Epoch 3248. Training loss: 0.7665321230888367. Validation loss: 2.7054669857025146.\n",
      "Epoch 3249. Training loss: 0.766528308391571. Validation loss: 2.7054665088653564.\n",
      "Epoch 3250. Training loss: 0.7665245532989502. Validation loss: 2.7054660320281982.\n",
      "Epoch 3251. Training loss: 0.7665207982063293. Validation loss: 2.705465316772461.\n",
      "Epoch 3252. Training loss: 0.7665171027183533. Validation loss: 2.7054648399353027.\n",
      "Epoch 3253. Training loss: 0.7665134072303772. Validation loss: 2.7054646015167236.\n",
      "Epoch 3254. Training loss: 0.7665095925331116. Validation loss: 2.7054638862609863.\n",
      "Epoch 3255. Training loss: 0.766506016254425. Validation loss: 2.7054636478424072.\n",
      "Epoch 3256. Training loss: 0.766502320766449. Validation loss: 2.705463409423828.\n",
      "Epoch 3257. Training loss: 0.7664985656738281. Validation loss: 2.70546293258667.\n",
      "Epoch 3258. Training loss: 0.7664947509765625. Validation loss: 2.7054622173309326.\n",
      "Epoch 3259. Training loss: 0.7664909958839417. Validation loss: 2.7054617404937744.\n",
      "Epoch 3260. Training loss: 0.7664874196052551. Validation loss: 2.7054615020751953.\n",
      "Epoch 3261. Training loss: 0.766483724117279. Validation loss: 2.705461025238037.\n",
      "Epoch 3262. Training loss: 0.7664799094200134. Validation loss: 2.705460548400879.\n",
      "Epoch 3263. Training loss: 0.7664761543273926. Validation loss: 2.7054603099823.\n",
      "Epoch 3264. Training loss: 0.7664723992347717. Validation loss: 2.7054595947265625.\n",
      "Epoch 3265. Training loss: 0.7664687037467957. Validation loss: 2.7054595947265625.\n",
      "Epoch 3266. Training loss: 0.7664650082588196. Validation loss: 2.705458641052246.\n",
      "Epoch 3267. Training loss: 0.7664613723754883. Validation loss: 2.705458641052246.\n",
      "Epoch 3268. Training loss: 0.7664575576782227. Validation loss: 2.705458164215088.\n",
      "Epoch 3269. Training loss: 0.766453742980957. Validation loss: 2.7054576873779297.\n",
      "Epoch 3270. Training loss: 0.7664501070976257. Validation loss: 2.7054569721221924.\n",
      "Epoch 3271. Training loss: 0.7664463520050049. Validation loss: 2.705456495285034.\n",
      "Epoch 3272. Training loss: 0.766442596912384. Validation loss: 2.705456018447876.\n",
      "Epoch 3273. Training loss: 0.766438901424408. Validation loss: 2.7054555416107178.\n",
      "Epoch 3274. Training loss: 0.7664351463317871. Validation loss: 2.7054553031921387.\n",
      "Epoch 3275. Training loss: 0.7664313912391663. Validation loss: 2.7054553031921387.\n",
      "Epoch 3276. Training loss: 0.7664276957511902. Validation loss: 2.7054543495178223.\n",
      "Epoch 3277. Training loss: 0.7664239406585693. Validation loss: 2.705453872680664.\n",
      "Epoch 3278. Training loss: 0.766420304775238. Validation loss: 2.705453634262085.\n",
      "Epoch 3279. Training loss: 0.7664164900779724. Validation loss: 2.7054529190063477.\n",
      "Epoch 3280. Training loss: 0.7664127349853516. Validation loss: 2.7054529190063477.\n",
      "Epoch 3281. Training loss: 0.7664089202880859. Validation loss: 2.7054522037506104.\n",
      "Epoch 3282. Training loss: 0.7664052844047546. Validation loss: 2.7054519653320312.\n",
      "Epoch 3283. Training loss: 0.7664015889167786. Validation loss: 2.705451488494873.\n",
      "Epoch 3284. Training loss: 0.7663977742195129. Validation loss: 2.7054507732391357.\n",
      "Epoch 3285. Training loss: 0.7663940787315369. Validation loss: 2.7054505348205566.\n",
      "Epoch 3286. Training loss: 0.766390323638916. Validation loss: 2.7054502964019775.\n",
      "Epoch 3287. Training loss: 0.7663865089416504. Validation loss: 2.7054498195648193.\n",
      "Epoch 3288. Training loss: 0.7663828730583191. Validation loss: 2.705448865890503.\n",
      "Epoch 3289. Training loss: 0.7663790583610535. Validation loss: 2.705448627471924.\n",
      "Epoch 3290. Training loss: 0.7663753628730774. Validation loss: 2.7054481506347656.\n",
      "Epoch 3291. Training loss: 0.7663716673851013. Validation loss: 2.7054479122161865.\n",
      "Epoch 3292. Training loss: 0.7663679122924805. Validation loss: 2.705447196960449.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3293. Training loss: 0.7663640975952148. Validation loss: 2.705447196960449.\n",
      "Epoch 3294. Training loss: 0.7663602828979492. Validation loss: 2.705446481704712.\n",
      "Epoch 3295. Training loss: 0.7663565278053284. Validation loss: 2.7054460048675537.\n",
      "Epoch 3296. Training loss: 0.7663528919219971. Validation loss: 2.7054455280303955.\n",
      "Epoch 3297. Training loss: 0.7663490772247314. Validation loss: 2.7054452896118164.\n",
      "Epoch 3298. Training loss: 0.7663453221321106. Validation loss: 2.705444812774658.\n",
      "Epoch 3299. Training loss: 0.7663416266441345. Validation loss: 2.7054443359375.\n",
      "Epoch 3300. Training loss: 0.7663378715515137. Validation loss: 2.705443859100342.\n",
      "Epoch 3301. Training loss: 0.7663341164588928. Validation loss: 2.7054433822631836.\n",
      "Epoch 3302. Training loss: 0.7663303017616272. Validation loss: 2.7054429054260254.\n",
      "Epoch 3303. Training loss: 0.7663266658782959. Validation loss: 2.705442428588867.\n",
      "Epoch 3304. Training loss: 0.7663228511810303. Validation loss: 2.705441951751709.\n",
      "Epoch 3305. Training loss: 0.7663190364837646. Validation loss: 2.705441474914551.\n",
      "Epoch 3306. Training loss: 0.7663154006004333. Validation loss: 2.7054409980773926.\n",
      "Epoch 3307. Training loss: 0.7663115859031677. Validation loss: 2.7054405212402344.\n",
      "Epoch 3308. Training loss: 0.7663078308105469. Validation loss: 2.705440044403076.\n",
      "Epoch 3309. Training loss: 0.766304075717926. Validation loss: 2.705440044403076.\n",
      "Epoch 3310. Training loss: 0.7663002610206604. Validation loss: 2.705439329147339.\n",
      "Epoch 3311. Training loss: 0.7662965655326843. Validation loss: 2.7054390907287598.\n",
      "Epoch 3312. Training loss: 0.7662927508354187. Validation loss: 2.7054386138916016.\n",
      "Epoch 3313. Training loss: 0.7662889957427979. Validation loss: 2.7054383754730225.\n",
      "Epoch 3314. Training loss: 0.7662851810455322. Validation loss: 2.7054378986358643.\n",
      "Epoch 3315. Training loss: 0.7662814259529114. Validation loss: 2.705437183380127.\n",
      "Epoch 3316. Training loss: 0.7662776112556458. Validation loss: 2.705436944961548.\n",
      "Epoch 3317. Training loss: 0.7662739753723145. Validation loss: 2.7054362297058105.\n",
      "Epoch 3318. Training loss: 0.7662701606750488. Validation loss: 2.7054357528686523.\n",
      "Epoch 3319. Training loss: 0.7662665247917175. Validation loss: 2.7054355144500732.\n",
      "Epoch 3320. Training loss: 0.7662627100944519. Validation loss: 2.705434799194336.\n",
      "Epoch 3321. Training loss: 0.7662588953971863. Validation loss: 2.705434560775757.\n",
      "Epoch 3322. Training loss: 0.7662551403045654. Validation loss: 2.7054338455200195.\n",
      "Epoch 3323. Training loss: 0.7662513852119446. Validation loss: 2.7054338455200195.\n",
      "Epoch 3324. Training loss: 0.7662476897239685. Validation loss: 2.7054333686828613.\n",
      "Epoch 3325. Training loss: 0.7662438750267029. Validation loss: 2.705432891845703.\n",
      "Epoch 3326. Training loss: 0.7662400603294373. Validation loss: 2.705432415008545.\n",
      "Epoch 3327. Training loss: 0.7662363052368164. Validation loss: 2.705432176589966.\n",
      "Epoch 3328. Training loss: 0.7662325501441956. Validation loss: 2.7054314613342285.\n",
      "Epoch 3329. Training loss: 0.7662288546562195. Validation loss: 2.7054312229156494.\n",
      "Epoch 3330. Training loss: 0.7662250399589539. Validation loss: 2.705430507659912.\n",
      "Epoch 3331. Training loss: 0.7662212252616882. Validation loss: 2.705430030822754.\n",
      "Epoch 3332. Training loss: 0.7662174105644226. Validation loss: 2.705429792404175.\n",
      "Epoch 3333. Training loss: 0.7662136554718018. Validation loss: 2.7054293155670166.\n",
      "Epoch 3334. Training loss: 0.7662099003791809. Validation loss: 2.7054288387298584.\n",
      "Epoch 3335. Training loss: 0.7662060856819153. Validation loss: 2.7054286003112793.\n",
      "Epoch 3336. Training loss: 0.7662022113800049. Validation loss: 2.705428123474121.\n",
      "Epoch 3337. Training loss: 0.7661985754966736. Validation loss: 2.705427646636963.\n",
      "Epoch 3338. Training loss: 0.7661946415901184. Validation loss: 2.705427408218384.\n",
      "Epoch 3339. Training loss: 0.7661910057067871. Validation loss: 2.7054266929626465.\n",
      "Epoch 3340. Training loss: 0.7661871910095215. Validation loss: 2.7054262161254883.\n",
      "Epoch 3341. Training loss: 0.7661834359169006. Validation loss: 2.70542573928833.\n",
      "Epoch 3342. Training loss: 0.766179621219635. Validation loss: 2.7054250240325928.\n",
      "Epoch 3343. Training loss: 0.7661758065223694. Validation loss: 2.7054247856140137.\n",
      "Epoch 3344. Training loss: 0.7661719918251038. Validation loss: 2.7054243087768555.\n",
      "Epoch 3345. Training loss: 0.7661682963371277. Validation loss: 2.7054238319396973.\n",
      "Epoch 3346. Training loss: 0.7661644816398621. Validation loss: 2.705423355102539.\n",
      "Epoch 3347. Training loss: 0.7661607265472412. Validation loss: 2.705422878265381.\n",
      "Epoch 3348. Training loss: 0.7661569118499756. Validation loss: 2.7054226398468018.\n",
      "Epoch 3349. Training loss: 0.7661531567573547. Validation loss: 2.7054219245910645.\n",
      "Epoch 3350. Training loss: 0.7661493420600891. Validation loss: 2.7054216861724854.\n",
      "Epoch 3351. Training loss: 0.7661455273628235. Validation loss: 2.7054214477539062.\n",
      "Epoch 3352. Training loss: 0.7661417126655579. Validation loss: 2.705420970916748.\n",
      "Epoch 3353. Training loss: 0.7661378979682922. Validation loss: 2.7054200172424316.\n",
      "Epoch 3354. Training loss: 0.7661342620849609. Validation loss: 2.7054200172424316.\n",
      "Epoch 3355. Training loss: 0.7661304473876953. Validation loss: 2.7054195404052734.\n",
      "Epoch 3356. Training loss: 0.7661266326904297. Validation loss: 2.7054190635681152.\n",
      "Epoch 3357. Training loss: 0.7661228179931641. Validation loss: 2.705418825149536.\n",
      "Epoch 3358. Training loss: 0.7661190032958984. Validation loss: 2.705418348312378.\n",
      "Epoch 3359. Training loss: 0.7661152482032776. Validation loss: 2.7054176330566406.\n",
      "Epoch 3360. Training loss: 0.7661113739013672. Validation loss: 2.7054171562194824.\n",
      "Epoch 3361. Training loss: 0.7661077380180359. Validation loss: 2.705416679382324.\n",
      "Epoch 3362. Training loss: 0.7661038041114807. Validation loss: 2.705416440963745.\n",
      "Epoch 3363. Training loss: 0.7660999894142151. Validation loss: 2.705415725708008.\n",
      "Epoch 3364. Training loss: 0.766096293926239. Validation loss: 2.7054154872894287.\n",
      "Epoch 3365. Training loss: 0.7660924792289734. Validation loss: 2.7054152488708496.\n",
      "Epoch 3366. Training loss: 0.7660887241363525. Validation loss: 2.7054147720336914.\n",
      "Epoch 3367. Training loss: 0.7660848498344421. Validation loss: 2.705414056777954.\n",
      "Epoch 3368. Training loss: 0.7660810351371765. Validation loss: 2.705413818359375.\n",
      "Epoch 3369. Training loss: 0.7660772800445557. Validation loss: 2.705413341522217.\n",
      "Epoch 3370. Training loss: 0.76607346534729. Validation loss: 2.7054128646850586.\n",
      "Epoch 3371. Training loss: 0.7660697102546692. Validation loss: 2.7054126262664795.\n",
      "Epoch 3372. Training loss: 0.7660658359527588. Validation loss: 2.705411911010742.\n",
      "Epoch 3373. Training loss: 0.7660620212554932. Validation loss: 2.705411434173584.\n",
      "Epoch 3374. Training loss: 0.7660582661628723. Validation loss: 2.705410957336426.\n",
      "Epoch 3375. Training loss: 0.7660543918609619. Validation loss: 2.7054104804992676.\n",
      "Epoch 3376. Training loss: 0.7660506367683411. Validation loss: 2.7054104804992676.\n",
      "Epoch 3377. Training loss: 0.7660467624664307. Validation loss: 2.7054097652435303.\n",
      "Epoch 3378. Training loss: 0.766042947769165. Validation loss: 2.705409288406372.\n",
      "Epoch 3379. Training loss: 0.7660391926765442. Validation loss: 2.705409049987793.\n",
      "Epoch 3380. Training loss: 0.7660353779792786. Validation loss: 2.7054085731506348.\n",
      "Epoch 3381. Training loss: 0.7660315632820129. Validation loss: 2.7054080963134766.\n",
      "Epoch 3382. Training loss: 0.7660276889801025. Validation loss: 2.7054073810577393.\n",
      "Epoch 3383. Training loss: 0.7660240530967712. Validation loss: 2.70540714263916.\n",
      "Epoch 3384. Training loss: 0.7660202383995056. Validation loss: 2.705406665802002.\n",
      "Epoch 3385. Training loss: 0.7660163044929504. Validation loss: 2.7054061889648438.\n",
      "Epoch 3386. Training loss: 0.7660124897956848. Validation loss: 2.7054057121276855.\n",
      "Epoch 3387. Training loss: 0.7660086750984192. Validation loss: 2.7054054737091064.\n",
      "Epoch 3388. Training loss: 0.7660048604011536. Validation loss: 2.705404758453369.\n",
      "Epoch 3389. Training loss: 0.7660011649131775. Validation loss: 2.705404281616211.\n",
      "Epoch 3390. Training loss: 0.7659974098205566. Validation loss: 2.7054035663604736.\n",
      "Epoch 3391. Training loss: 0.7659935355186462. Validation loss: 2.7054035663604736.\n",
      "Epoch 3392. Training loss: 0.7659896016120911. Validation loss: 2.7054030895233154.\n",
      "Epoch 3393. Training loss: 0.7659857869148254. Validation loss: 2.7054028511047363.\n",
      "Epoch 3394. Training loss: 0.7659819722175598. Validation loss: 2.705402135848999.\n",
      "Epoch 3395. Training loss: 0.7659781575202942. Validation loss: 2.70540189743042.\n",
      "Epoch 3396. Training loss: 0.7659743428230286. Validation loss: 2.7054009437561035.\n",
      "Epoch 3397. Training loss: 0.7659704685211182. Validation loss: 2.7054007053375244.\n",
      "Epoch 3398. Training loss: 0.7659667134284973. Validation loss: 2.705400228500366.\n",
      "Epoch 3399. Training loss: 0.7659628987312317. Validation loss: 2.705400228500366.\n",
      "Epoch 3400. Training loss: 0.7659590244293213. Validation loss: 2.705399751663208.\n",
      "Epoch 3401. Training loss: 0.7659552693367004. Validation loss: 2.70539927482605.\n",
      "Epoch 3402. Training loss: 0.76595139503479. Validation loss: 2.7053985595703125.\n",
      "Epoch 3403. Training loss: 0.7659477591514587. Validation loss: 2.7053980827331543.\n",
      "Epoch 3404. Training loss: 0.7659438252449036. Validation loss: 2.705397367477417.\n",
      "Epoch 3405. Training loss: 0.7659399509429932. Validation loss: 2.705397367477417.\n",
      "Epoch 3406. Training loss: 0.7659361362457275. Validation loss: 2.705397129058838.\n",
      "Epoch 3407. Training loss: 0.7659323215484619. Validation loss: 2.7053961753845215.\n",
      "Epoch 3408. Training loss: 0.7659284472465515. Validation loss: 2.7053956985473633.\n",
      "Epoch 3409. Training loss: 0.7659246325492859. Validation loss: 2.705395460128784.\n",
      "Epoch 3410. Training loss: 0.765920877456665. Validation loss: 2.705394744873047.\n",
      "Epoch 3411. Training loss: 0.7659168839454651. Validation loss: 2.705394744873047.\n",
      "Epoch 3412. Training loss: 0.765913188457489. Validation loss: 2.7053942680358887.\n",
      "Epoch 3413. Training loss: 0.7659091949462891. Validation loss: 2.7053940296173096.\n",
      "Epoch 3414. Training loss: 0.7659054398536682. Validation loss: 2.7053937911987305.\n",
      "Epoch 3415. Training loss: 0.7659015655517578. Validation loss: 2.705393075942993.\n",
      "Epoch 3416. Training loss: 0.765897810459137. Validation loss: 2.7053921222686768.\n",
      "Epoch 3417. Training loss: 0.7658939957618713. Validation loss: 2.7053921222686768.\n",
      "Epoch 3418. Training loss: 0.7658901214599609. Validation loss: 2.7053916454315186.\n",
      "Epoch 3419. Training loss: 0.7658862471580505. Validation loss: 2.7053909301757812.\n",
      "Epoch 3420. Training loss: 0.7658824324607849. Validation loss: 2.705390691757202.\n",
      "Epoch 3421. Training loss: 0.7658786773681641. Validation loss: 2.705390453338623.\n",
      "Epoch 3422. Training loss: 0.7658748030662537. Validation loss: 2.7053897380828857.\n",
      "Epoch 3423. Training loss: 0.765870988368988. Validation loss: 2.7053892612457275.\n",
      "Epoch 3424. Training loss: 0.7658671736717224. Validation loss: 2.7053890228271484.\n",
      "Epoch 3425. Training loss: 0.7658633589744568. Validation loss: 2.705388307571411.\n",
      "Epoch 3426. Training loss: 0.7658593654632568. Validation loss: 2.705388069152832.\n",
      "Epoch 3427. Training loss: 0.7658555507659912. Validation loss: 2.7053873538970947.\n",
      "Epoch 3428. Training loss: 0.7658517956733704. Validation loss: 2.7053868770599365.\n",
      "Epoch 3429. Training loss: 0.76584792137146. Validation loss: 2.7053864002227783.\n",
      "Epoch 3430. Training loss: 0.7658440470695496. Validation loss: 2.70538592338562.\n",
      "Epoch 3431. Training loss: 0.7658402323722839. Validation loss: 2.705385684967041.\n",
      "Epoch 3432. Training loss: 0.7658364176750183. Validation loss: 2.705385208129883.\n",
      "Epoch 3433. Training loss: 0.7658324837684631. Validation loss: 2.7053847312927246.\n",
      "Epoch 3434. Training loss: 0.7658286690711975. Validation loss: 2.7053842544555664.\n",
      "Epoch 3435. Training loss: 0.7658247947692871. Validation loss: 2.7053842544555664.\n",
      "Epoch 3436. Training loss: 0.7658209800720215. Validation loss: 2.70538330078125.\n",
      "Epoch 3437. Training loss: 0.7658171653747559. Validation loss: 2.70538330078125.\n",
      "Epoch 3438. Training loss: 0.7658132910728455. Validation loss: 2.705382823944092.\n",
      "Epoch 3439. Training loss: 0.7658093571662903. Validation loss: 2.7053821086883545.\n",
      "Epoch 3440. Training loss: 0.7658055424690247. Validation loss: 2.705381393432617.\n",
      "Epoch 3441. Training loss: 0.765801727771759. Validation loss: 2.705381393432617.\n",
      "Epoch 3442. Training loss: 0.7657978534698486. Validation loss: 2.705380916595459.\n",
      "Epoch 3443. Training loss: 0.765794038772583. Validation loss: 2.7053799629211426.\n",
      "Epoch 3444. Training loss: 0.7657902240753174. Validation loss: 2.7053799629211426.\n",
      "Epoch 3445. Training loss: 0.7657862305641174. Validation loss: 2.7053794860839844.\n",
      "Epoch 3446. Training loss: 0.7657824158668518. Validation loss: 2.705379009246826.\n",
      "Epoch 3447. Training loss: 0.7657785415649414. Validation loss: 2.705378770828247.\n",
      "Epoch 3448. Training loss: 0.765774667263031. Validation loss: 2.7053780555725098.\n",
      "Epoch 3449. Training loss: 0.7657708525657654. Validation loss: 2.7053775787353516.\n",
      "Epoch 3450. Training loss: 0.7657669186592102. Validation loss: 2.7053768634796143.\n",
      "Epoch 3451. Training loss: 0.7657632231712341. Validation loss: 2.7053768634796143.\n",
      "Epoch 3452. Training loss: 0.765759289264679. Validation loss: 2.705376386642456.\n",
      "Epoch 3453. Training loss: 0.7657553553581238. Validation loss: 2.705375909805298.\n",
      "Epoch 3454. Training loss: 0.7657515406608582. Validation loss: 2.7053754329681396.\n",
      "Epoch 3455. Training loss: 0.7657477259635925. Validation loss: 2.7053747177124023.\n",
      "Epoch 3456. Training loss: 0.7657437920570374. Validation loss: 2.7053747177124023.\n",
      "Epoch 3457. Training loss: 0.765739917755127. Validation loss: 2.705374002456665.\n",
      "Epoch 3458. Training loss: 0.7657361030578613. Validation loss: 2.705373764038086.\n",
      "Epoch 3459. Training loss: 0.7657321095466614. Validation loss: 2.7053732872009277.\n",
      "Epoch 3460. Training loss: 0.7657284140586853. Validation loss: 2.7053728103637695.\n",
      "Epoch 3461. Training loss: 0.7657244801521301. Validation loss: 2.7053725719451904.\n",
      "Epoch 3462. Training loss: 0.7657206654548645. Validation loss: 2.705371618270874.\n",
      "Epoch 3463. Training loss: 0.7657167315483093. Validation loss: 2.705371379852295.\n",
      "Epoch 3464. Training loss: 0.7657129764556885. Validation loss: 2.705371141433716.\n",
      "Epoch 3465. Training loss: 0.7657091021537781. Validation loss: 2.7053704261779785.\n",
      "Epoch 3466. Training loss: 0.7657051682472229. Validation loss: 2.7053699493408203.\n",
      "Epoch 3467. Training loss: 0.7657012939453125. Validation loss: 2.705369472503662.\n",
      "Epoch 3468. Training loss: 0.7656974792480469. Validation loss: 2.705369234085083.\n",
      "Epoch 3469. Training loss: 0.7656934857368469. Validation loss: 2.705368757247925.\n",
      "Epoch 3470. Training loss: 0.7656896710395813. Validation loss: 2.7053680419921875.\n",
      "Epoch 3471. Training loss: 0.7656857967376709. Validation loss: 2.7053678035736084.\n",
      "Epoch 3472. Training loss: 0.76568204164505. Validation loss: 2.705367088317871.\n",
      "Epoch 3473. Training loss: 0.765678346157074. Validation loss: 2.705367088317871.\n",
      "Epoch 3474. Training loss: 0.7656745910644531. Validation loss: 2.705366373062134.\n",
      "Epoch 3475. Training loss: 0.7656707763671875. Validation loss: 2.7053658962249756.\n",
      "Epoch 3476. Training loss: 0.7656670212745667. Validation loss: 2.7053658962249756.\n",
      "Epoch 3477. Training loss: 0.7656633257865906. Validation loss: 2.7053651809692383.\n",
      "Epoch 3478. Training loss: 0.7656596302986145. Validation loss: 2.705364465713501.\n",
      "Epoch 3479. Training loss: 0.7656558156013489. Validation loss: 2.7053639888763428.\n",
      "Epoch 3480. Training loss: 0.7656521201133728. Validation loss: 2.7053637504577637.\n",
      "Epoch 3481. Training loss: 0.765648365020752. Validation loss: 2.7053632736206055.\n",
      "Epoch 3482. Training loss: 0.7656447291374207. Validation loss: 2.7053627967834473.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3483. Training loss: 0.7656407952308655. Validation loss: 2.705362558364868.\n",
      "Epoch 3484. Training loss: 0.765637218952179. Validation loss: 2.705361843109131.\n",
      "Epoch 3485. Training loss: 0.7656334042549133. Validation loss: 2.7053613662719727.\n",
      "Epoch 3486. Training loss: 0.765629768371582. Validation loss: 2.7053608894348145.\n",
      "Epoch 3487. Training loss: 0.7656260132789612. Validation loss: 2.7053604125976562.\n",
      "Epoch 3488. Training loss: 0.7656223177909851. Validation loss: 2.705359935760498.\n",
      "Epoch 3489. Training loss: 0.7656185030937195. Validation loss: 2.705359935760498.\n",
      "Epoch 3490. Training loss: 0.7656148076057434. Validation loss: 2.7053592205047607.\n",
      "Epoch 3491. Training loss: 0.7656111121177673. Validation loss: 2.7053587436676025.\n",
      "Epoch 3492. Training loss: 0.7656074166297913. Validation loss: 2.7053585052490234.\n",
      "Epoch 3493. Training loss: 0.7656037211418152. Validation loss: 2.705357551574707.\n",
      "Epoch 3494. Training loss: 0.7655999064445496. Validation loss: 2.705357074737549.\n",
      "Epoch 3495. Training loss: 0.7655962109565735. Validation loss: 2.705357074737549.\n",
      "Epoch 3496. Training loss: 0.7655925154685974. Validation loss: 2.7053561210632324.\n",
      "Epoch 3497. Training loss: 0.7655887603759766. Validation loss: 2.705355644226074.\n",
      "Epoch 3498. Training loss: 0.7655850052833557. Validation loss: 2.705355167388916.\n",
      "Epoch 3499. Training loss: 0.7655814290046692. Validation loss: 2.705354690551758.\n",
      "Epoch 3500. Training loss: 0.7655777335166931. Validation loss: 2.7053544521331787.\n",
      "Epoch 3501. Training loss: 0.7655739784240723. Validation loss: 2.7053537368774414.\n",
      "Epoch 3502. Training loss: 0.7655701637268066. Validation loss: 2.7053534984588623.\n",
      "Epoch 3503. Training loss: 0.7655664086341858. Validation loss: 2.705353260040283.\n",
      "Epoch 3504. Training loss: 0.7655627727508545. Validation loss: 2.705352306365967.\n",
      "Epoch 3505. Training loss: 0.7655591368675232. Validation loss: 2.7053520679473877.\n",
      "Epoch 3506. Training loss: 0.7655553817749023. Validation loss: 2.7053515911102295.\n",
      "Epoch 3507. Training loss: 0.7655516266822815. Validation loss: 2.705350875854492.\n",
      "Epoch 3508. Training loss: 0.7655479311943054. Validation loss: 2.705350399017334.\n",
      "Epoch 3509. Training loss: 0.7655441164970398. Validation loss: 2.705350399017334.\n",
      "Epoch 3510. Training loss: 0.7655404210090637. Validation loss: 2.7053496837615967.\n",
      "Epoch 3511. Training loss: 0.7655366063117981. Validation loss: 2.7053494453430176.\n",
      "Epoch 3512. Training loss: 0.7655329704284668. Validation loss: 2.7053489685058594.\n",
      "Epoch 3513. Training loss: 0.765529215335846. Validation loss: 2.705348253250122.\n",
      "Epoch 3514. Training loss: 0.7655255198478699. Validation loss: 2.705348014831543.\n",
      "Epoch 3515. Training loss: 0.7655218243598938. Validation loss: 2.7053470611572266.\n",
      "Epoch 3516. Training loss: 0.7655180096626282. Validation loss: 2.7053470611572266.\n",
      "Epoch 3517. Training loss: 0.7655143737792969. Validation loss: 2.7053465843200684.\n",
      "Epoch 3518. Training loss: 0.7655107378959656. Validation loss: 2.70534610748291.\n",
      "Epoch 3519. Training loss: 0.7655069231987. Validation loss: 2.705345630645752.\n",
      "Epoch 3520. Training loss: 0.7655031681060791. Validation loss: 2.7053449153900146.\n",
      "Epoch 3521. Training loss: 0.7654994130134583. Validation loss: 2.7053444385528564.\n",
      "Epoch 3522. Training loss: 0.7654955983161926. Validation loss: 2.7053439617156982.\n",
      "Epoch 3523. Training loss: 0.7654919028282166. Validation loss: 2.705343723297119.\n",
      "Epoch 3524. Training loss: 0.7654882073402405. Validation loss: 2.705343246459961.\n",
      "Epoch 3525. Training loss: 0.7654845118522644. Validation loss: 2.7053427696228027.\n",
      "Epoch 3526. Training loss: 0.7654808163642883. Validation loss: 2.7053420543670654.\n",
      "Epoch 3527. Training loss: 0.7654771208763123. Validation loss: 2.7053418159484863.\n",
      "Epoch 3528. Training loss: 0.7654731869697571. Validation loss: 2.705341339111328.\n",
      "Epoch 3529. Training loss: 0.7654695510864258. Validation loss: 2.70534086227417.\n",
      "Epoch 3530. Training loss: 0.7654657363891602. Validation loss: 2.7053403854370117.\n",
      "Epoch 3531. Training loss: 0.7654619812965393. Validation loss: 2.7053399085998535.\n",
      "Epoch 3532. Training loss: 0.765458345413208. Validation loss: 2.7053394317626953.\n",
      "Epoch 3533. Training loss: 0.7654545903205872. Validation loss: 2.705338954925537.\n",
      "Epoch 3534. Training loss: 0.7654508948326111. Validation loss: 2.705338716506958.\n",
      "Epoch 3535. Training loss: 0.7654470801353455. Validation loss: 2.7053380012512207.\n",
      "Epoch 3536. Training loss: 0.7654433846473694. Validation loss: 2.7053375244140625.\n",
      "Epoch 3537. Training loss: 0.7654396891593933. Validation loss: 2.705336809158325.\n",
      "Epoch 3538. Training loss: 0.7654359340667725. Validation loss: 2.705336570739746.\n",
      "Epoch 3539. Training loss: 0.7654321789741516. Validation loss: 2.705336093902588.\n",
      "Epoch 3540. Training loss: 0.765428364276886. Validation loss: 2.705336093902588.\n",
      "Epoch 3541. Training loss: 0.7654246687889099. Validation loss: 2.7053351402282715.\n",
      "Epoch 3542. Training loss: 0.7654209733009338. Validation loss: 2.7053346633911133.\n",
      "Epoch 3543. Training loss: 0.7654171586036682. Validation loss: 2.705334424972534.\n",
      "Epoch 3544. Training loss: 0.7654134631156921. Validation loss: 2.7053334712982178.\n",
      "Epoch 3545. Training loss: 0.7654096484184265. Validation loss: 2.7053334712982178.\n",
      "Epoch 3546. Training loss: 0.7654059529304504. Validation loss: 2.7053329944610596.\n",
      "Epoch 3547. Training loss: 0.7654021382331848. Validation loss: 2.7053322792053223.\n",
      "Epoch 3548. Training loss: 0.7653984427452087. Validation loss: 2.705331802368164.\n",
      "Epoch 3549. Training loss: 0.7653946876525879. Validation loss: 2.705331325531006.\n",
      "Epoch 3550. Training loss: 0.765390932559967. Validation loss: 2.705331325531006.\n",
      "Epoch 3551. Training loss: 0.7653871178627014. Validation loss: 2.7053308486938477.\n",
      "Epoch 3552. Training loss: 0.7653834819793701. Validation loss: 2.7053298950195312.\n",
      "Epoch 3553. Training loss: 0.7653797268867493. Validation loss: 2.705329656600952.\n",
      "Epoch 3554. Training loss: 0.7653759121894836. Validation loss: 2.705329179763794.\n",
      "Epoch 3555. Training loss: 0.7653722167015076. Validation loss: 2.7053287029266357.\n",
      "Epoch 3556. Training loss: 0.7653684616088867. Validation loss: 2.7053279876708984.\n",
      "Epoch 3557. Training loss: 0.7653646469116211. Validation loss: 2.7053277492523193.\n",
      "Epoch 3558. Training loss: 0.7653608918190002. Validation loss: 2.705327033996582.\n",
      "Epoch 3559. Training loss: 0.7653570771217346. Validation loss: 2.705326557159424.\n",
      "Epoch 3560. Training loss: 0.7653533816337585. Validation loss: 2.705326557159424.\n",
      "Epoch 3561. Training loss: 0.7653496861457825. Validation loss: 2.7053260803222656.\n",
      "Epoch 3562. Training loss: 0.7653458714485168. Validation loss: 2.7053256034851074.\n",
      "Epoch 3563. Training loss: 0.7653421759605408. Validation loss: 2.705325126647949.\n",
      "Epoch 3564. Training loss: 0.7653384208679199. Validation loss: 2.705324649810791.\n",
      "Epoch 3565. Training loss: 0.7653346061706543. Validation loss: 2.705324172973633.\n",
      "Epoch 3566. Training loss: 0.7653308510780334. Validation loss: 2.7053234577178955.\n",
      "Epoch 3567. Training loss: 0.7653272747993469. Validation loss: 2.7053229808807373.\n",
      "Epoch 3568. Training loss: 0.7653236389160156. Validation loss: 2.705322504043579.\n",
      "Epoch 3569. Training loss: 0.7653200030326843. Validation loss: 2.705322027206421.\n",
      "Epoch 3570. Training loss: 0.7653163075447083. Validation loss: 2.7053215503692627.\n",
      "Epoch 3571. Training loss: 0.7653126120567322. Validation loss: 2.7053210735321045.\n",
      "Epoch 3572. Training loss: 0.7653090357780457. Validation loss: 2.7053205966949463.\n",
      "Epoch 3573. Training loss: 0.7653053402900696. Validation loss: 2.705320358276367.\n",
      "Epoch 3574. Training loss: 0.7653016448020935. Validation loss: 2.705319881439209.\n",
      "Epoch 3575. Training loss: 0.765298068523407. Validation loss: 2.705319404602051.\n",
      "Epoch 3576. Training loss: 0.7652944922447205. Validation loss: 2.7053189277648926.\n",
      "Epoch 3577. Training loss: 0.7652907371520996. Validation loss: 2.7053182125091553.\n",
      "Epoch 3578. Training loss: 0.7652871012687683. Validation loss: 2.7053182125091553.\n",
      "Epoch 3579. Training loss: 0.7652835249900818. Validation loss: 2.705317258834839.\n",
      "Epoch 3580. Training loss: 0.7652798295021057. Validation loss: 2.7053170204162598.\n",
      "Epoch 3581. Training loss: 0.7652761340141296. Validation loss: 2.7053163051605225.\n",
      "Epoch 3582. Training loss: 0.7652725577354431. Validation loss: 2.7053158283233643.\n",
      "Epoch 3583. Training loss: 0.7652689814567566. Validation loss: 2.705315351486206.\n",
      "Epoch 3584. Training loss: 0.7652652263641357. Validation loss: 2.705315113067627.\n",
      "Epoch 3585. Training loss: 0.765261709690094. Validation loss: 2.7053146362304688.\n",
      "Epoch 3586. Training loss: 0.7652578949928284. Validation loss: 2.7053139209747314.\n",
      "Epoch 3587. Training loss: 0.7652542591094971. Validation loss: 2.7053134441375732.\n",
      "Epoch 3588. Training loss: 0.7652506828308105. Validation loss: 2.705312967300415.\n",
      "Epoch 3589. Training loss: 0.7652470469474792. Validation loss: 2.705312728881836.\n",
      "Epoch 3590. Training loss: 0.7652433514595032. Validation loss: 2.7053120136260986.\n",
      "Epoch 3591. Training loss: 0.7652397751808167. Validation loss: 2.7053117752075195.\n",
      "Epoch 3592. Training loss: 0.7652361989021301. Validation loss: 2.7053110599517822.\n",
      "Epoch 3593. Training loss: 0.765232503414154. Validation loss: 2.705310583114624.\n",
      "Epoch 3594. Training loss: 0.7652287483215332. Validation loss: 2.705310106277466.\n",
      "Epoch 3595. Training loss: 0.7652252316474915. Validation loss: 2.7053096294403076.\n",
      "Epoch 3596. Training loss: 0.7652215361595154. Validation loss: 2.7053091526031494.\n",
      "Epoch 3597. Training loss: 0.7652179598808289. Validation loss: 2.7053091526031494.\n",
      "Epoch 3598. Training loss: 0.7652143836021423. Validation loss: 2.705308198928833.\n",
      "Epoch 3599. Training loss: 0.7652106881141663. Validation loss: 2.7053072452545166.\n",
      "Epoch 3600. Training loss: 0.765207052230835. Validation loss: 2.7053070068359375.\n",
      "Epoch 3601. Training loss: 0.7652034163475037. Validation loss: 2.7053067684173584.\n",
      "Epoch 3602. Training loss: 0.7651996612548828. Validation loss: 2.7053062915802.\n",
      "Epoch 3603. Training loss: 0.7651960253715515. Validation loss: 2.705305576324463.\n",
      "Epoch 3604. Training loss: 0.765192449092865. Validation loss: 2.7053050994873047.\n",
      "Epoch 3605. Training loss: 0.7651886940002441. Validation loss: 2.7053046226501465.\n",
      "Epoch 3606. Training loss: 0.7651851177215576. Validation loss: 2.7053041458129883.\n",
      "Epoch 3607. Training loss: 0.7651814818382263. Validation loss: 2.70530366897583.\n",
      "Epoch 3608. Training loss: 0.7651779055595398. Validation loss: 2.705303192138672.\n",
      "Epoch 3609. Training loss: 0.765174150466919. Validation loss: 2.7053027153015137.\n",
      "Epoch 3610. Training loss: 0.7651705741882324. Validation loss: 2.7053024768829346.\n",
      "Epoch 3611. Training loss: 0.7651668190956116. Validation loss: 2.7053017616271973.\n",
      "Epoch 3612. Training loss: 0.7651631236076355. Validation loss: 2.705301284790039.\n",
      "Epoch 3613. Training loss: 0.765159547328949. Validation loss: 2.705300807952881.\n",
      "Epoch 3614. Training loss: 0.7651559710502625. Validation loss: 2.7053003311157227.\n",
      "Epoch 3615. Training loss: 0.7651522159576416. Validation loss: 2.7052998542785645.\n",
      "Epoch 3616. Training loss: 0.7651486396789551. Validation loss: 2.7052993774414062.\n",
      "Epoch 3617. Training loss: 0.7651450037956238. Validation loss: 2.705298900604248.\n",
      "Epoch 3618. Training loss: 0.7651414275169373. Validation loss: 2.705298662185669.\n",
      "Epoch 3619. Training loss: 0.7651377320289612. Validation loss: 2.7052977085113525.\n",
      "Epoch 3620. Training loss: 0.7651340365409851. Validation loss: 2.7052972316741943.\n",
      "Epoch 3621. Training loss: 0.7651302814483643. Validation loss: 2.7052969932556152.\n",
      "Epoch 3622. Training loss: 0.765126645565033. Validation loss: 2.705296516418457.\n",
      "Epoch 3623. Training loss: 0.7651230692863464. Validation loss: 2.705296039581299.\n",
      "Epoch 3624. Training loss: 0.7651193737983704. Validation loss: 2.7052958011627197.\n",
      "Epoch 3625. Training loss: 0.7651157379150391. Validation loss: 2.7052950859069824.\n",
      "Epoch 3626. Training loss: 0.7651121020317078. Validation loss: 2.705294609069824.\n",
      "Epoch 3627. Training loss: 0.7651083469390869. Validation loss: 2.705294132232666.\n",
      "Epoch 3628. Training loss: 0.7651047706604004. Validation loss: 2.705293655395508.\n",
      "Epoch 3629. Training loss: 0.7651011347770691. Validation loss: 2.7052929401397705.\n",
      "Epoch 3630. Training loss: 0.765097439289093. Validation loss: 2.705292224884033.\n",
      "Epoch 3631. Training loss: 0.7650937438011169. Validation loss: 2.705292224884033.\n",
      "Epoch 3632. Training loss: 0.7650901675224304. Validation loss: 2.705291271209717.\n",
      "Epoch 3633. Training loss: 0.7650864124298096. Validation loss: 2.7052910327911377.\n",
      "Epoch 3634. Training loss: 0.7650827765464783. Validation loss: 2.7052907943725586.\n",
      "Epoch 3635. Training loss: 0.7650790214538574. Validation loss: 2.7052903175354004.\n",
      "Epoch 3636. Training loss: 0.7650753855705261. Validation loss: 2.705289840698242.\n",
      "Epoch 3637. Training loss: 0.7650718092918396. Validation loss: 2.705289363861084.\n",
      "Epoch 3638. Training loss: 0.7650681138038635. Validation loss: 2.7052884101867676.\n",
      "Epoch 3639. Training loss: 0.7650644183158875. Validation loss: 2.7052881717681885.\n",
      "Epoch 3640. Training loss: 0.7650607228279114. Validation loss: 2.7052876949310303.\n",
      "Epoch 3641. Training loss: 0.7650571465492249. Validation loss: 2.705286979675293.\n",
      "Epoch 3642. Training loss: 0.7650534510612488. Validation loss: 2.7052865028381348.\n",
      "Epoch 3643. Training loss: 0.7650496959686279. Validation loss: 2.7052860260009766.\n",
      "Epoch 3644. Training loss: 0.7650461196899414. Validation loss: 2.7052857875823975.\n",
      "Epoch 3645. Training loss: 0.7650424838066101. Validation loss: 2.70528507232666.\n",
      "Epoch 3646. Training loss: 0.765038788318634. Validation loss: 2.705284595489502.\n",
      "Epoch 3647. Training loss: 0.7650352120399475. Validation loss: 2.705284357070923.\n",
      "Epoch 3648. Training loss: 0.7650313973426819. Validation loss: 2.7052836418151855.\n",
      "Epoch 3649. Training loss: 0.7650277614593506. Validation loss: 2.7052831649780273.\n",
      "Epoch 3650. Training loss: 0.7650240063667297. Validation loss: 2.705282688140869.\n",
      "Epoch 3651. Training loss: 0.7650203704833984. Validation loss: 2.705282211303711.\n",
      "Epoch 3652. Training loss: 0.7650167346000671. Validation loss: 2.7052817344665527.\n",
      "Epoch 3653. Training loss: 0.7650131583213806. Validation loss: 2.7052812576293945.\n",
      "Epoch 3654. Training loss: 0.7650094032287598. Validation loss: 2.7052807807922363.\n",
      "Epoch 3655. Training loss: 0.7650057673454285. Validation loss: 2.705280303955078.\n",
      "Epoch 3656. Training loss: 0.7650020718574524. Validation loss: 2.705280303955078.\n",
      "Epoch 3657. Training loss: 0.7649984359741211. Validation loss: 2.7052793502807617.\n",
      "Epoch 3658. Training loss: 0.7649946808815002. Validation loss: 2.7052788734436035.\n",
      "Epoch 3659. Training loss: 0.764991044998169. Validation loss: 2.7052783966064453.\n",
      "Epoch 3660. Training loss: 0.7649872899055481. Validation loss: 2.705277681350708.\n",
      "Epoch 3661. Training loss: 0.7649836540222168. Validation loss: 2.705277442932129.\n",
      "Epoch 3662. Training loss: 0.7649800777435303. Validation loss: 2.7052767276763916.\n",
      "Epoch 3663. Training loss: 0.7649763226509094. Validation loss: 2.7052764892578125.\n",
      "Epoch 3664. Training loss: 0.7649726271629333. Validation loss: 2.705275774002075.\n",
      "Epoch 3665. Training loss: 0.7649690508842468. Validation loss: 2.705275535583496.\n",
      "Epoch 3666. Training loss: 0.7649652361869812. Validation loss: 2.705274820327759.\n",
      "Epoch 3667. Training loss: 0.7649615406990051. Validation loss: 2.7052743434906006.\n",
      "Epoch 3668. Training loss: 0.7649579644203186. Validation loss: 2.7052736282348633.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3669. Training loss: 0.7649542689323425. Validation loss: 2.7052736282348633.\n",
      "Epoch 3670. Training loss: 0.7649505138397217. Validation loss: 2.705272912979126.\n",
      "Epoch 3671. Training loss: 0.7649468779563904. Validation loss: 2.7052724361419678.\n",
      "Epoch 3672. Training loss: 0.7649431824684143. Validation loss: 2.7052721977233887.\n",
      "Epoch 3673. Training loss: 0.7649394869804382. Validation loss: 2.7052712440490723.\n",
      "Epoch 3674. Training loss: 0.7649357914924622. Validation loss: 2.7052712440490723.\n",
      "Epoch 3675. Training loss: 0.7649321556091309. Validation loss: 2.705270528793335.\n",
      "Epoch 3676. Training loss: 0.76492840051651. Validation loss: 2.7052700519561768.\n",
      "Epoch 3677. Training loss: 0.7649247646331787. Validation loss: 2.7052695751190186.\n",
      "Epoch 3678. Training loss: 0.7649210095405579. Validation loss: 2.7052688598632812.\n",
      "Epoch 3679. Training loss: 0.7649173140525818. Validation loss: 2.7052688598632812.\n",
      "Epoch 3680. Training loss: 0.7649136185646057. Validation loss: 2.705267906188965.\n",
      "Epoch 3681. Training loss: 0.7649100422859192. Validation loss: 2.705267906188965.\n",
      "Epoch 3682. Training loss: 0.7649063467979431. Validation loss: 2.7052671909332275.\n",
      "Epoch 3683. Training loss: 0.7649025917053223. Validation loss: 2.7052667140960693.\n",
      "Epoch 3684. Training loss: 0.7648988366127014. Validation loss: 2.705266237258911.\n",
      "Epoch 3685. Training loss: 0.7648951411247253. Validation loss: 2.705265760421753.\n",
      "Epoch 3686. Training loss: 0.7648914456367493. Validation loss: 2.7052652835845947.\n",
      "Epoch 3687. Training loss: 0.764887809753418. Validation loss: 2.7052648067474365.\n",
      "Epoch 3688. Training loss: 0.7648840546607971. Validation loss: 2.7052643299102783.\n",
      "Epoch 3689. Training loss: 0.7648804187774658. Validation loss: 2.705263614654541.\n",
      "Epoch 3690. Training loss: 0.7648766040802002. Validation loss: 2.705263137817383.\n",
      "Epoch 3691. Training loss: 0.7648730278015137. Validation loss: 2.7052624225616455.\n",
      "Epoch 3692. Training loss: 0.764869213104248. Validation loss: 2.7052621841430664.\n",
      "Epoch 3693. Training loss: 0.7648656368255615. Validation loss: 2.705261468887329.\n",
      "Epoch 3694. Training loss: 0.7648618817329407. Validation loss: 2.70526123046875.\n",
      "Epoch 3695. Training loss: 0.7648582458496094. Validation loss: 2.705260753631592.\n",
      "Epoch 3696. Training loss: 0.7648544907569885. Validation loss: 2.7052602767944336.\n",
      "Epoch 3697. Training loss: 0.7648507952690125. Validation loss: 2.7052597999572754.\n",
      "Epoch 3698. Training loss: 0.7648470997810364. Validation loss: 2.705259084701538.\n",
      "Epoch 3699. Training loss: 0.7648434042930603. Validation loss: 2.70525860786438.\n",
      "Epoch 3700. Training loss: 0.7648396492004395. Validation loss: 2.705258369445801.\n",
      "Epoch 3701. Training loss: 0.7648360729217529. Validation loss: 2.7052576541900635.\n",
      "Epoch 3702. Training loss: 0.7648322582244873. Validation loss: 2.7052571773529053.\n",
      "Epoch 3703. Training loss: 0.7648285031318665. Validation loss: 2.705256938934326.\n",
      "Epoch 3704. Training loss: 0.7648248672485352. Validation loss: 2.705256462097168.\n",
      "Epoch 3705. Training loss: 0.7648212313652039. Validation loss: 2.7052555084228516.\n",
      "Epoch 3706. Training loss: 0.7648174166679382. Validation loss: 2.7052555084228516.\n",
      "Epoch 3707. Training loss: 0.7648137211799622. Validation loss: 2.7052547931671143.\n",
      "Epoch 3708. Training loss: 0.7648100852966309. Validation loss: 2.705254554748535.\n",
      "Epoch 3709. Training loss: 0.7648062705993652. Validation loss: 2.705254316329956.\n",
      "Epoch 3710. Training loss: 0.7648026347160339. Validation loss: 2.7052533626556396.\n",
      "Epoch 3711. Training loss: 0.7647988200187683. Validation loss: 2.7052526473999023.\n",
      "Epoch 3712. Training loss: 0.7647951245307922. Validation loss: 2.705252170562744.\n",
      "Epoch 3713. Training loss: 0.7647914290428162. Validation loss: 2.705251455307007.\n",
      "Epoch 3714. Training loss: 0.7647876143455505. Validation loss: 2.705251693725586.\n",
      "Epoch 3715. Training loss: 0.764784038066864. Validation loss: 2.7052512168884277.\n",
      "Epoch 3716. Training loss: 0.7647802829742432. Validation loss: 2.7052502632141113.\n",
      "Epoch 3717. Training loss: 0.7647765278816223. Validation loss: 2.705249786376953.\n",
      "Epoch 3718. Training loss: 0.764772891998291. Validation loss: 2.705249309539795.\n",
      "Epoch 3719. Training loss: 0.7647691369056702. Validation loss: 2.705249071121216.\n",
      "Epoch 3720. Training loss: 0.7647654414176941. Validation loss: 2.7052483558654785.\n",
      "Epoch 3721. Training loss: 0.7647616863250732. Validation loss: 2.7052478790283203.\n",
      "Epoch 3722. Training loss: 0.7647581100463867. Validation loss: 2.705247402191162.\n",
      "Epoch 3723. Training loss: 0.7647542357444763. Validation loss: 2.705246925354004.\n",
      "Epoch 3724. Training loss: 0.7647505402565002. Validation loss: 2.705246686935425.\n",
      "Epoch 3725. Training loss: 0.7647468447685242. Validation loss: 2.7052459716796875.\n",
      "Epoch 3726. Training loss: 0.7647431492805481. Validation loss: 2.7052457332611084.\n",
      "Epoch 3727. Training loss: 0.7647393345832825. Validation loss: 2.705245018005371.\n",
      "Epoch 3728. Training loss: 0.7647356390953064. Validation loss: 2.705244302749634.\n",
      "Epoch 3729. Training loss: 0.7647319436073303. Validation loss: 2.7052440643310547.\n",
      "Epoch 3730. Training loss: 0.7647281289100647. Validation loss: 2.7052435874938965.\n",
      "Epoch 3731. Training loss: 0.7647245526313782. Validation loss: 2.705242872238159.\n",
      "Epoch 3732. Training loss: 0.7647208571434021. Validation loss: 2.70524263381958.\n",
      "Epoch 3733. Training loss: 0.7647170424461365. Validation loss: 2.705242156982422.\n",
      "Epoch 3734. Training loss: 0.7647132277488708. Validation loss: 2.7052414417266846.\n",
      "Epoch 3735. Training loss: 0.76470947265625. Validation loss: 2.7052409648895264.\n",
      "Epoch 3736. Training loss: 0.7647057175636292. Validation loss: 2.7052407264709473.\n",
      "Epoch 3737. Training loss: 0.7647020816802979. Validation loss: 2.705239772796631.\n",
      "Epoch 3738. Training loss: 0.7646982669830322. Validation loss: 2.7052395343780518.\n",
      "Epoch 3739. Training loss: 0.7646946310997009. Validation loss: 2.7052390575408936.\n",
      "Epoch 3740. Training loss: 0.7646908760070801. Validation loss: 2.7052383422851562.\n",
      "Epoch 3741. Training loss: 0.7646872401237488. Validation loss: 2.7052383422851562.\n",
      "Epoch 3742. Training loss: 0.7646834254264832. Validation loss: 2.705237865447998.\n",
      "Epoch 3743. Training loss: 0.7646797299385071. Validation loss: 2.7052371501922607.\n",
      "Epoch 3744. Training loss: 0.7646758556365967. Validation loss: 2.7052369117736816.\n",
      "Epoch 3745. Training loss: 0.7646722197532654. Validation loss: 2.7052364349365234.\n",
      "Epoch 3746. Training loss: 0.7646684646606445. Validation loss: 2.7052359580993652.\n",
      "Epoch 3747. Training loss: 0.7646648287773132. Validation loss: 2.705235481262207.\n",
      "Epoch 3748. Training loss: 0.7646610140800476. Validation loss: 2.705235004425049.\n",
      "Epoch 3749. Training loss: 0.7646572589874268. Validation loss: 2.7052340507507324.\n",
      "Epoch 3750. Training loss: 0.7646535038948059. Validation loss: 2.7052338123321533.\n",
      "Epoch 3751. Training loss: 0.7646496891975403. Validation loss: 2.705233097076416.\n",
      "Epoch 3752. Training loss: 0.7646459937095642. Validation loss: 2.705232858657837.\n",
      "Epoch 3753. Training loss: 0.7646422386169434. Validation loss: 2.7052321434020996.\n",
      "Epoch 3754. Training loss: 0.7646386027336121. Validation loss: 2.7052319049835205.\n",
      "Epoch 3755. Training loss: 0.7646348476409912. Validation loss: 2.705231189727783.\n",
      "Epoch 3756. Training loss: 0.7646310925483704. Validation loss: 2.705230474472046.\n",
      "Epoch 3757. Training loss: 0.7646274566650391. Validation loss: 2.705230236053467.\n",
      "Epoch 3758. Training loss: 0.7646236419677734. Validation loss: 2.7052297592163086.\n",
      "Epoch 3759. Training loss: 0.7646198272705078. Validation loss: 2.7052292823791504.\n",
      "Epoch 3760. Training loss: 0.764616072177887. Validation loss: 2.705228805541992.\n",
      "Epoch 3761. Training loss: 0.7646123766899109. Validation loss: 2.705228090286255.\n",
      "Epoch 3762. Training loss: 0.7646085619926453. Validation loss: 2.7052276134490967.\n",
      "Epoch 3763. Training loss: 0.7646048665046692. Validation loss: 2.7052273750305176.\n",
      "Epoch 3764. Training loss: 0.7646010518074036. Validation loss: 2.7052268981933594.\n",
      "Epoch 3765. Training loss: 0.7645973563194275. Validation loss: 2.705226421356201.\n",
      "Epoch 3766. Training loss: 0.7645935416221619. Validation loss: 2.705225944519043.\n",
      "Epoch 3767. Training loss: 0.7645899653434753. Validation loss: 2.7052254676818848.\n",
      "Epoch 3768. Training loss: 0.7645861506462097. Validation loss: 2.7052247524261475.\n",
      "Epoch 3769. Training loss: 0.7645823359489441. Validation loss: 2.70522403717041.\n",
      "Epoch 3770. Training loss: 0.7645785808563232. Validation loss: 2.705223560333252.\n",
      "Epoch 3771. Training loss: 0.7645748257637024. Validation loss: 2.705223560333252.\n",
      "Epoch 3772. Training loss: 0.7645711302757263. Validation loss: 2.7052228450775146.\n",
      "Epoch 3773. Training loss: 0.7645673751831055. Validation loss: 2.7052223682403564.\n",
      "Epoch 3774. Training loss: 0.7645635604858398. Validation loss: 2.7052221298217773.\n",
      "Epoch 3775. Training loss: 0.764559805393219. Validation loss: 2.705221652984619.\n",
      "Epoch 3776. Training loss: 0.7645561099052429. Validation loss: 2.705220937728882.\n",
      "Epoch 3777. Training loss: 0.7645523548126221. Validation loss: 2.7052202224731445.\n",
      "Epoch 3778. Training loss: 0.7645485997200012. Validation loss: 2.7052197456359863.\n",
      "Epoch 3779. Training loss: 0.7645447850227356. Validation loss: 2.705219268798828.\n",
      "Epoch 3780. Training loss: 0.7645410895347595. Validation loss: 2.70521879196167.\n",
      "Epoch 3781. Training loss: 0.7645372748374939. Validation loss: 2.70521879196167.\n",
      "Epoch 3782. Training loss: 0.764533519744873. Validation loss: 2.7052178382873535.\n",
      "Epoch 3783. Training loss: 0.7645297646522522. Validation loss: 2.7052173614501953.\n",
      "Epoch 3784. Training loss: 0.7645259499549866. Validation loss: 2.705216646194458.\n",
      "Epoch 3785. Training loss: 0.7645222544670105. Validation loss: 2.705216646194458.\n",
      "Epoch 3786. Training loss: 0.7645183205604553. Validation loss: 2.7052161693573.\n",
      "Epoch 3787. Training loss: 0.764514684677124. Validation loss: 2.7052154541015625.\n",
      "Epoch 3788. Training loss: 0.7645108699798584. Validation loss: 2.7052149772644043.\n",
      "Epoch 3789. Training loss: 0.7645071148872375. Validation loss: 2.705214262008667.\n",
      "Epoch 3790. Training loss: 0.7645034193992615. Validation loss: 2.705214023590088.\n",
      "Epoch 3791. Training loss: 0.7644996047019958. Validation loss: 2.7052133083343506.\n",
      "Epoch 3792. Training loss: 0.764495849609375. Validation loss: 2.7052130699157715.\n",
      "Epoch 3793. Training loss: 0.7644920945167542. Validation loss: 2.7052125930786133.\n",
      "Epoch 3794. Training loss: 0.7644882798194885. Validation loss: 2.705212116241455.\n",
      "Epoch 3795. Training loss: 0.7644845843315125. Validation loss: 2.7052114009857178.\n",
      "Epoch 3796. Training loss: 0.7644807696342468. Validation loss: 2.7052114009857178.\n",
      "Epoch 3797. Training loss: 0.7644769549369812. Validation loss: 2.7052106857299805.\n",
      "Epoch 3798. Training loss: 0.7644731402397156. Validation loss: 2.7052102088928223.\n",
      "Epoch 3799. Training loss: 0.7644694447517395. Validation loss: 2.705209732055664.\n",
      "Epoch 3800. Training loss: 0.7644656300544739. Validation loss: 2.7052090167999268.\n",
      "Epoch 3801. Training loss: 0.7644619345664978. Validation loss: 2.7052087783813477.\n",
      "Epoch 3802. Training loss: 0.7644581198692322. Validation loss: 2.7052083015441895.\n",
      "Epoch 3803. Training loss: 0.7644543051719666. Validation loss: 2.7052078247070312.\n",
      "Epoch 3804. Training loss: 0.7644505500793457. Validation loss: 2.705206871032715.\n",
      "Epoch 3805. Training loss: 0.7644467949867249. Validation loss: 2.7052066326141357.\n",
      "Epoch 3806. Training loss: 0.7644430994987488. Validation loss: 2.7052061557769775.\n",
      "Epoch 3807. Training loss: 0.7644392848014832. Validation loss: 2.7052056789398193.\n",
      "Epoch 3808. Training loss: 0.7644354701042175. Validation loss: 2.7052054405212402.\n",
      "Epoch 3809. Training loss: 0.7644316554069519. Validation loss: 2.7052042484283447.\n",
      "Epoch 3810. Training loss: 0.7644278407096863. Validation loss: 2.7052040100097656.\n",
      "Epoch 3811. Training loss: 0.7644240856170654. Validation loss: 2.7052035331726074.\n",
      "Epoch 3812. Training loss: 0.7644202709197998. Validation loss: 2.705203056335449.\n",
      "Epoch 3813. Training loss: 0.7644166350364685. Validation loss: 2.705202579498291.\n",
      "Epoch 3814. Training loss: 0.7644128203392029. Validation loss: 2.7052018642425537.\n",
      "Epoch 3815. Training loss: 0.7644088864326477. Validation loss: 2.7052016258239746.\n",
      "Epoch 3816. Training loss: 0.7644050717353821. Validation loss: 2.7052011489868164.\n",
      "Epoch 3817. Training loss: 0.7644012570381165. Validation loss: 2.7052011489868164.\n",
      "Epoch 3818. Training loss: 0.7643976211547852. Validation loss: 2.7052001953125.\n",
      "Epoch 3819. Training loss: 0.7643938660621643. Validation loss: 2.705199718475342.\n",
      "Epoch 3820. Training loss: 0.7643899917602539. Validation loss: 2.7051992416381836.\n",
      "Epoch 3821. Training loss: 0.7643861770629883. Validation loss: 2.7051985263824463.\n",
      "Epoch 3822. Training loss: 0.7643824219703674. Validation loss: 2.705198287963867.\n",
      "Epoch 3823. Training loss: 0.7643786072731018. Validation loss: 2.705197811126709.\n",
      "Epoch 3824. Training loss: 0.7643749117851257. Validation loss: 2.705197334289551.\n",
      "Epoch 3825. Training loss: 0.7643710970878601. Validation loss: 2.7051966190338135.\n",
      "Epoch 3826. Training loss: 0.7643672823905945. Validation loss: 2.7051961421966553.\n",
      "Epoch 3827. Training loss: 0.7643635272979736. Validation loss: 2.705195903778076.\n",
      "Epoch 3828. Training loss: 0.7643596529960632. Validation loss: 2.705195188522339.\n",
      "Epoch 3829. Training loss: 0.7643559575080872. Validation loss: 2.7051947116851807.\n",
      "Epoch 3830. Training loss: 0.764352023601532. Validation loss: 2.7051944732666016.\n",
      "Epoch 3831. Training loss: 0.7643482685089111. Validation loss: 2.705193519592285.\n",
      "Epoch 3832. Training loss: 0.7643444538116455. Validation loss: 2.705193042755127.\n",
      "Epoch 3833. Training loss: 0.7643406987190247. Validation loss: 2.705192804336548.\n",
      "Epoch 3834. Training loss: 0.764336884021759. Validation loss: 2.7051923274993896.\n",
      "Epoch 3835. Training loss: 0.764333188533783. Validation loss: 2.7051916122436523.\n",
      "Epoch 3836. Training loss: 0.7643292546272278. Validation loss: 2.705190896987915.\n",
      "Epoch 3837. Training loss: 0.7643254399299622. Validation loss: 2.705190420150757.\n",
      "Epoch 3838. Training loss: 0.7643216252326965. Validation loss: 2.7051901817321777.\n",
      "Epoch 3839. Training loss: 0.7643179297447205. Validation loss: 2.7051894664764404.\n",
      "Epoch 3840. Training loss: 0.7643141150474548. Validation loss: 2.7051894664764404.\n",
      "Epoch 3841. Training loss: 0.7643103003501892. Validation loss: 2.705188751220703.\n",
      "Epoch 3842. Training loss: 0.7643064856529236. Validation loss: 2.705188274383545.\n",
      "Epoch 3843. Training loss: 0.764302670955658. Validation loss: 2.7051877975463867.\n",
      "Epoch 3844. Training loss: 0.7642988562583923. Validation loss: 2.7051873207092285.\n",
      "Epoch 3845. Training loss: 0.7642950415611267. Validation loss: 2.7051870822906494.\n",
      "Epoch 3846. Training loss: 0.7642912268638611. Validation loss: 2.705186367034912.\n",
      "Epoch 3847. Training loss: 0.7642874717712402. Validation loss: 2.705185651779175.\n",
      "Epoch 3848. Training loss: 0.7642836570739746. Validation loss: 2.7051851749420166.\n",
      "Epoch 3849. Training loss: 0.764279842376709. Validation loss: 2.7051844596862793.\n",
      "Epoch 3850. Training loss: 0.7642759680747986. Validation loss: 2.7051842212677.\n",
      "Epoch 3851. Training loss: 0.7642722725868225. Validation loss: 2.705183982849121.\n",
      "Epoch 3852. Training loss: 0.7642683982849121. Validation loss: 2.705183506011963.\n",
      "Epoch 3853. Training loss: 0.7642645835876465. Validation loss: 2.7051830291748047.\n",
      "Epoch 3854. Training loss: 0.7642607688903809. Validation loss: 2.7051823139190674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3855. Training loss: 0.7642569541931152. Validation loss: 2.7051820755004883.\n",
      "Epoch 3856. Training loss: 0.7642531394958496. Validation loss: 2.70518159866333.\n",
      "Epoch 3857. Training loss: 0.764249324798584. Validation loss: 2.705181121826172.\n",
      "Epoch 3858. Training loss: 0.7642455101013184. Validation loss: 2.7051804065704346.\n",
      "Epoch 3859. Training loss: 0.764241635799408. Validation loss: 2.7051799297332764.\n",
      "Epoch 3860. Training loss: 0.7642378807067871. Validation loss: 2.705179452896118.\n",
      "Epoch 3861. Training loss: 0.7642340660095215. Validation loss: 2.705178737640381.\n",
      "Epoch 3862. Training loss: 0.7642302513122559. Validation loss: 2.7051782608032227.\n",
      "Epoch 3863. Training loss: 0.7642264366149902. Validation loss: 2.7051777839660645.\n",
      "Epoch 3864. Training loss: 0.7642226219177246. Validation loss: 2.7051773071289062.\n",
      "Epoch 3865. Training loss: 0.764218807220459. Validation loss: 2.705177068710327.\n",
      "Epoch 3866. Training loss: 0.7642149329185486. Validation loss: 2.705176591873169.\n",
      "Epoch 3867. Training loss: 0.764211118221283. Validation loss: 2.7051758766174316.\n",
      "Epoch 3868. Training loss: 0.7642073631286621. Validation loss: 2.7051753997802734.\n",
      "Epoch 3869. Training loss: 0.7642035484313965. Validation loss: 2.705174684524536.\n",
      "Epoch 3870. Training loss: 0.7641996741294861. Validation loss: 2.705174446105957.\n",
      "Epoch 3871. Training loss: 0.7641959190368652. Validation loss: 2.705173969268799.\n",
      "Epoch 3872. Training loss: 0.7641920447349548. Validation loss: 2.7051734924316406.\n",
      "Epoch 3873. Training loss: 0.7641882300376892. Validation loss: 2.7051730155944824.\n",
      "Epoch 3874. Training loss: 0.7641844153404236. Validation loss: 2.705172538757324.\n",
      "Epoch 3875. Training loss: 0.7641804814338684. Validation loss: 2.705172061920166.\n",
      "Epoch 3876. Training loss: 0.7641767859458923. Validation loss: 2.705171585083008.\n",
      "Epoch 3877. Training loss: 0.7641728520393372. Validation loss: 2.7051711082458496.\n",
      "Epoch 3878. Training loss: 0.7641690373420715. Validation loss: 2.7051706314086914.\n",
      "Epoch 3879. Training loss: 0.7641651630401611. Validation loss: 2.705169677734375.\n",
      "Epoch 3880. Training loss: 0.7641613483428955. Validation loss: 2.705169439315796.\n",
      "Epoch 3881. Training loss: 0.7641575336456299. Validation loss: 2.7051689624786377.\n",
      "Epoch 3882. Training loss: 0.7641536593437195. Validation loss: 2.7051684856414795.\n",
      "Epoch 3883. Training loss: 0.7641499042510986. Validation loss: 2.7051680088043213.\n",
      "Epoch 3884. Training loss: 0.7641460299491882. Validation loss: 2.705167293548584.\n",
      "Epoch 3885. Training loss: 0.7641422152519226. Validation loss: 2.705167055130005.\n",
      "Epoch 3886. Training loss: 0.764138400554657. Validation loss: 2.7051663398742676.\n",
      "Epoch 3887. Training loss: 0.7641345858573914. Validation loss: 2.7051663398742676.\n",
      "Epoch 3888. Training loss: 0.7641306519508362. Validation loss: 2.7051658630371094.\n",
      "Epoch 3889. Training loss: 0.7641267776489258. Validation loss: 2.705164909362793.\n",
      "Epoch 3890. Training loss: 0.7641230225563049. Validation loss: 2.7051641941070557.\n",
      "Epoch 3891. Training loss: 0.7641192078590393. Validation loss: 2.7051639556884766.\n",
      "Epoch 3892. Training loss: 0.7641153335571289. Validation loss: 2.7051634788513184.\n",
      "Epoch 3893. Training loss: 0.7641115188598633. Validation loss: 2.7051632404327393.\n",
      "Epoch 3894. Training loss: 0.7641077041625977. Validation loss: 2.7051620483398438.\n",
      "Epoch 3895. Training loss: 0.7641037106513977. Validation loss: 2.7051620483398438.\n",
      "Epoch 3896. Training loss: 0.7640998959541321. Validation loss: 2.7051610946655273.\n",
      "Epoch 3897. Training loss: 0.764096200466156. Validation loss: 2.7051610946655273.\n",
      "Epoch 3898. Training loss: 0.764092206954956. Validation loss: 2.705160140991211.\n",
      "Epoch 3899. Training loss: 0.7640883922576904. Validation loss: 2.705160140991211.\n",
      "Epoch 3900. Training loss: 0.7640845775604248. Validation loss: 2.7051596641540527.\n",
      "Epoch 3901. Training loss: 0.7640807032585144. Validation loss: 2.7051591873168945.\n",
      "Epoch 3902. Training loss: 0.7640769481658936. Validation loss: 2.7051587104797363.\n",
      "Epoch 3903. Training loss: 0.7640731334686279. Validation loss: 2.705157995223999.\n",
      "Epoch 3904. Training loss: 0.7640692591667175. Validation loss: 2.70515775680542.\n",
      "Epoch 3905. Training loss: 0.7640653252601624. Validation loss: 2.7051572799682617.\n",
      "Epoch 3906. Training loss: 0.7640615105628967. Validation loss: 2.7051563262939453.\n",
      "Epoch 3907. Training loss: 0.7640576362609863. Validation loss: 2.705156087875366.\n",
      "Epoch 3908. Training loss: 0.7640538215637207. Validation loss: 2.705155849456787.\n",
      "Epoch 3909. Training loss: 0.7640500068664551. Validation loss: 2.7051548957824707.\n",
      "Epoch 3910. Training loss: 0.7640461325645447. Validation loss: 2.7051548957824707.\n",
      "Epoch 3911. Training loss: 0.764042317867279. Validation loss: 2.7051539421081543.\n",
      "Epoch 3912. Training loss: 0.7640383839607239. Validation loss: 2.705153465270996.\n",
      "Epoch 3913. Training loss: 0.7640345096588135. Validation loss: 2.705152988433838.\n",
      "Epoch 3914. Training loss: 0.7640306949615479. Validation loss: 2.705152750015259.\n",
      "Epoch 3915. Training loss: 0.7640268206596375. Validation loss: 2.7051517963409424.\n",
      "Epoch 3916. Training loss: 0.7640230059623718. Validation loss: 2.7051515579223633.\n",
      "Epoch 3917. Training loss: 0.7640190720558167. Validation loss: 2.705151081085205.\n",
      "Epoch 3918. Training loss: 0.764015257358551. Validation loss: 2.705150604248047.\n",
      "Epoch 3919. Training loss: 0.7640113830566406. Validation loss: 2.7051498889923096.\n",
      "Epoch 3920. Training loss: 0.764007568359375. Validation loss: 2.7051496505737305.\n",
      "Epoch 3921. Training loss: 0.7640036940574646. Validation loss: 2.7051491737365723.\n",
      "Epoch 3922. Training loss: 0.763999879360199. Validation loss: 2.705148696899414.\n",
      "Epoch 3923. Training loss: 0.7639959454536438. Validation loss: 2.7051479816436768.\n",
      "Epoch 3924. Training loss: 0.7639920711517334. Validation loss: 2.7051475048065186.\n",
      "Epoch 3925. Training loss: 0.7639882564544678. Validation loss: 2.7051472663879395.\n",
      "Epoch 3926. Training loss: 0.7639843821525574. Validation loss: 2.705146551132202.\n",
      "Epoch 3927. Training loss: 0.7639804482460022. Validation loss: 2.705146074295044.\n",
      "Epoch 3928. Training loss: 0.7639766335487366. Validation loss: 2.7051453590393066.\n",
      "Epoch 3929. Training loss: 0.7639727592468262. Validation loss: 2.7051448822021484.\n",
      "Epoch 3930. Training loss: 0.7639689445495605. Validation loss: 2.7051444053649902.\n",
      "Epoch 3931. Training loss: 0.7639649510383606. Validation loss: 2.705144166946411.\n",
      "Epoch 3932. Training loss: 0.7639610767364502. Validation loss: 2.705143690109253.\n",
      "Epoch 3933. Training loss: 0.7639572620391846. Validation loss: 2.7051429748535156.\n",
      "Epoch 3934. Training loss: 0.763953447341919. Validation loss: 2.7051424980163574.\n",
      "Epoch 3935. Training loss: 0.7639495730400085. Validation loss: 2.705142021179199.\n",
      "Epoch 3936. Training loss: 0.7639455795288086. Validation loss: 2.705141305923462.\n",
      "Epoch 3937. Training loss: 0.7639418244361877. Validation loss: 2.705141305923462.\n",
      "Epoch 3938. Training loss: 0.7639378905296326. Validation loss: 2.7051408290863037.\n",
      "Epoch 3939. Training loss: 0.7639340758323669. Validation loss: 2.7051401138305664.\n",
      "Epoch 3940. Training loss: 0.763930082321167. Validation loss: 2.705139398574829.\n",
      "Epoch 3941. Training loss: 0.7639262676239014. Validation loss: 2.705138683319092.\n",
      "Epoch 3942. Training loss: 0.7639224529266357. Validation loss: 2.705138683319092.\n",
      "Epoch 3943. Training loss: 0.7639185786247253. Validation loss: 2.7051382064819336.\n",
      "Epoch 3944. Training loss: 0.7639146447181702. Validation loss: 2.705137252807617.\n",
      "Epoch 3945. Training loss: 0.763910710811615. Validation loss: 2.705137014389038.\n",
      "Epoch 3946. Training loss: 0.7639068961143494. Validation loss: 2.705136299133301.\n",
      "Epoch 3947. Training loss: 0.7639030814170837. Validation loss: 2.7051360607147217.\n",
      "Epoch 3948. Training loss: 0.7638990879058838. Validation loss: 2.7051353454589844.\n",
      "Epoch 3949. Training loss: 0.7638952136039734. Validation loss: 2.705134868621826.\n",
      "Epoch 3950. Training loss: 0.7638913989067078. Validation loss: 2.705134391784668.\n",
      "Epoch 3951. Training loss: 0.7638874650001526. Validation loss: 2.7051339149475098.\n",
      "Epoch 3952. Training loss: 0.7638835906982422. Validation loss: 2.7051336765289307.\n",
      "Epoch 3953. Training loss: 0.7638797163963318. Validation loss: 2.7051331996917725.\n",
      "Epoch 3954. Training loss: 0.7638759016990662. Validation loss: 2.7051327228546143.\n",
      "Epoch 3955. Training loss: 0.7638719081878662. Validation loss: 2.705132007598877.\n",
      "Epoch 3956. Training loss: 0.7638680934906006. Validation loss: 2.705131769180298.\n",
      "Epoch 3957. Training loss: 0.7638642191886902. Validation loss: 2.7051310539245605.\n",
      "Epoch 3958. Training loss: 0.763860285282135. Validation loss: 2.7051303386688232.\n",
      "Epoch 3959. Training loss: 0.7638564109802246. Validation loss: 2.7051303386688232.\n",
      "Epoch 3960. Training loss: 0.7638525366783142. Validation loss: 2.705129623413086.\n",
      "Epoch 3961. Training loss: 0.763848602771759. Validation loss: 2.7051291465759277.\n",
      "Epoch 3962. Training loss: 0.7638447284698486. Validation loss: 2.7051284313201904.\n",
      "Epoch 3963. Training loss: 0.7638408541679382. Validation loss: 2.7051281929016113.\n",
      "Epoch 3964. Training loss: 0.7638370394706726. Validation loss: 2.705127716064453.\n",
      "Epoch 3965. Training loss: 0.7638331055641174. Validation loss: 2.705127239227295.\n",
      "Epoch 3966. Training loss: 0.7638290524482727. Validation loss: 2.7051265239715576.\n",
      "Epoch 3967. Training loss: 0.7638252377510071. Validation loss: 2.7051258087158203.\n",
      "Epoch 3968. Training loss: 0.7638214230537415. Validation loss: 2.7051260471343994.\n",
      "Epoch 3969. Training loss: 0.7638174891471863. Validation loss: 2.705124855041504.\n",
      "Epoch 3970. Training loss: 0.7638135552406311. Validation loss: 2.7051243782043457.\n",
      "Epoch 3971. Training loss: 0.7638096809387207. Validation loss: 2.7051239013671875.\n",
      "Epoch 3972. Training loss: 0.7638056874275208. Validation loss: 2.7051234245300293.\n",
      "Epoch 3973. Training loss: 0.7638018727302551. Validation loss: 2.70512318611145.\n",
      "Epoch 3974. Training loss: 0.7637979984283447. Validation loss: 2.705122709274292.\n",
      "Epoch 3975. Training loss: 0.7637941241264343. Validation loss: 2.7051219940185547.\n",
      "Epoch 3976. Training loss: 0.7637901902198792. Validation loss: 2.7051215171813965.\n",
      "Epoch 3977. Training loss: 0.763786256313324. Validation loss: 2.705120801925659.\n",
      "Epoch 3978. Training loss: 0.7637824416160583. Validation loss: 2.705120325088501.\n",
      "Epoch 3979. Training loss: 0.7637785077095032. Validation loss: 2.705120325088501.\n",
      "Epoch 3980. Training loss: 0.763774573802948. Validation loss: 2.7051196098327637.\n",
      "Epoch 3981. Training loss: 0.7637706398963928. Validation loss: 2.7051188945770264.\n",
      "Epoch 3982. Training loss: 0.7637667655944824. Validation loss: 2.7051186561584473.\n",
      "Epoch 3983. Training loss: 0.7637627720832825. Validation loss: 2.705118179321289.\n",
      "Epoch 3984. Training loss: 0.7637589573860168. Validation loss: 2.7051174640655518.\n",
      "Epoch 3985. Training loss: 0.7637550234794617. Validation loss: 2.7051172256469727.\n",
      "Epoch 3986. Training loss: 0.763751208782196. Validation loss: 2.7051167488098145.\n",
      "Epoch 3987. Training loss: 0.7637472152709961. Validation loss: 2.705116033554077.\n",
      "Epoch 3988. Training loss: 0.7637433409690857. Validation loss: 2.70511531829834.\n",
      "Epoch 3989. Training loss: 0.7637393474578857. Validation loss: 2.7051148414611816.\n",
      "Epoch 3990. Training loss: 0.7637355327606201. Validation loss: 2.7051148414611816.\n",
      "Epoch 3991. Training loss: 0.7637314796447754. Validation loss: 2.7051141262054443.\n",
      "Epoch 3992. Training loss: 0.763727605342865. Validation loss: 2.705113410949707.\n",
      "Epoch 3993. Training loss: 0.7637236714363098. Validation loss: 2.705112934112549.\n",
      "Epoch 3994. Training loss: 0.7637198567390442. Validation loss: 2.7051124572753906.\n",
      "Epoch 3995. Training loss: 0.763715922832489. Validation loss: 2.7051119804382324.\n",
      "Epoch 3996. Training loss: 0.7637119293212891. Validation loss: 2.705111503601074.\n",
      "Epoch 3997. Training loss: 0.7637081146240234. Validation loss: 2.705111026763916.\n",
      "Epoch 3998. Training loss: 0.7637041211128235. Validation loss: 2.7051103115081787.\n",
      "Epoch 3999. Training loss: 0.7637001872062683. Validation loss: 2.7051098346710205.\n",
      "Epoch 4000. Training loss: 0.7636963725090027. Validation loss: 2.7051093578338623.\n",
      "Epoch 4001. Training loss: 0.763692319393158. Validation loss: 2.705108880996704.\n",
      "Epoch 4002. Training loss: 0.7636883854866028. Validation loss: 2.705108880996704.\n",
      "Epoch 4003. Training loss: 0.7636845111846924. Validation loss: 2.7051076889038086.\n",
      "Epoch 4004. Training loss: 0.7636805176734924. Validation loss: 2.7051074504852295.\n",
      "Epoch 4005. Training loss: 0.763676643371582. Validation loss: 2.705106735229492.\n",
      "Epoch 4006. Training loss: 0.7636727690696716. Validation loss: 2.705106258392334.\n",
      "Epoch 4007. Training loss: 0.7636688351631165. Validation loss: 2.705106258392334.\n",
      "Epoch 4008. Training loss: 0.7636649012565613. Validation loss: 2.7051053047180176.\n",
      "Epoch 4009. Training loss: 0.7636609077453613. Validation loss: 2.7051048278808594.\n",
      "Epoch 4010. Training loss: 0.7636570930480957. Validation loss: 2.705104351043701.\n",
      "Epoch 4011. Training loss: 0.7636530995368958. Validation loss: 2.705103874206543.\n",
      "Epoch 4012. Training loss: 0.7636492252349854. Validation loss: 2.7051033973693848.\n",
      "Epoch 4013. Training loss: 0.7636452317237854. Validation loss: 2.7051029205322266.\n",
      "Epoch 4014. Training loss: 0.763641357421875. Validation loss: 2.7051022052764893.\n",
      "Epoch 4015. Training loss: 0.7636374831199646. Validation loss: 2.70510196685791.\n",
      "Epoch 4016. Training loss: 0.7636334300041199. Validation loss: 2.705101490020752.\n",
      "Epoch 4017. Training loss: 0.7636296153068542. Validation loss: 2.7051010131835938.\n",
      "Epoch 4018. Training loss: 0.7636256217956543. Validation loss: 2.7050998210906982.\n",
      "Epoch 4019. Training loss: 0.7636216282844543. Validation loss: 2.7051000595092773.\n",
      "Epoch 4020. Training loss: 0.7636176943778992. Validation loss: 2.705099105834961.\n",
      "Epoch 4021. Training loss: 0.763613760471344. Validation loss: 2.705098867416382.\n",
      "Epoch 4022. Training loss: 0.7636098265647888. Validation loss: 2.7050983905792236.\n",
      "Epoch 4023. Training loss: 0.7636058926582336. Validation loss: 2.7050981521606445.\n",
      "Epoch 4024. Training loss: 0.7636020183563232. Validation loss: 2.7050976753234863.\n",
      "Epoch 4025. Training loss: 0.7635980248451233. Validation loss: 2.705096960067749.\n",
      "Epoch 4026. Training loss: 0.7635941505432129. Validation loss: 2.705096483230591.\n",
      "Epoch 4027. Training loss: 0.7635901570320129. Validation loss: 2.7050957679748535.\n",
      "Epoch 4028. Training loss: 0.7635862231254578. Validation loss: 2.7050952911376953.\n",
      "Epoch 4029. Training loss: 0.7635822296142578. Validation loss: 2.705094814300537.\n",
      "Epoch 4030. Training loss: 0.7635783553123474. Validation loss: 2.705094337463379.\n",
      "Epoch 4031. Training loss: 0.7635743021965027. Validation loss: 2.7050938606262207.\n",
      "Epoch 4032. Training loss: 0.7635704874992371. Validation loss: 2.7050931453704834.\n",
      "Epoch 4033. Training loss: 0.7635664939880371. Validation loss: 2.7050929069519043.\n",
      "Epoch 4034. Training loss: 0.7635625004768372. Validation loss: 2.705092430114746.\n",
      "Epoch 4035. Training loss: 0.7635586261749268. Validation loss: 2.705091953277588.\n",
      "Epoch 4036. Training loss: 0.763554573059082. Validation loss: 2.7050909996032715.\n",
      "Epoch 4037. Training loss: 0.7635507583618164. Validation loss: 2.7050907611846924.\n",
      "Epoch 4038. Training loss: 0.7635467648506165. Validation loss: 2.7050905227661133.\n",
      "Epoch 4039. Training loss: 0.7635428309440613. Validation loss: 2.705089807510376.\n",
      "Epoch 4040. Training loss: 0.7635388374328613. Validation loss: 2.705089569091797.\n",
      "Epoch 4041. Training loss: 0.7635349631309509. Validation loss: 2.7050886154174805.\n",
      "Epoch 4042. Training loss: 0.763530969619751. Validation loss: 2.7050883769989014.\n",
      "Epoch 4043. Training loss: 0.7635269165039062. Validation loss: 2.705087900161743.\n",
      "Epoch 4044. Training loss: 0.7635230422019958. Validation loss: 2.705087423324585.\n",
      "Epoch 4045. Training loss: 0.7635190486907959. Validation loss: 2.7050867080688477.\n",
      "Epoch 4046. Training loss: 0.7635151743888855. Validation loss: 2.7050859928131104.\n",
      "Epoch 4047. Training loss: 0.7635111212730408. Validation loss: 2.705085515975952.\n",
      "Epoch 4048. Training loss: 0.7635071873664856. Validation loss: 2.705085277557373.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4049. Training loss: 0.7635032534599304. Validation loss: 2.7050845623016357.\n",
      "Epoch 4050. Training loss: 0.7634992599487305. Validation loss: 2.7050843238830566.\n",
      "Epoch 4051. Training loss: 0.7634952664375305. Validation loss: 2.7050838470458984.\n",
      "Epoch 4052. Training loss: 0.7634913325309753. Validation loss: 2.7050833702087402.\n",
      "Epoch 4053. Training loss: 0.7634873390197754. Validation loss: 2.705082893371582.\n",
      "Epoch 4054. Training loss: 0.7634835243225098. Validation loss: 2.7050821781158447.\n",
      "Epoch 4055. Training loss: 0.763479471206665. Validation loss: 2.7050817012786865.\n",
      "Epoch 4056. Training loss: 0.7634754776954651. Validation loss: 2.7050812244415283.\n",
      "Epoch 4057. Training loss: 0.7634714245796204. Validation loss: 2.70508074760437.\n",
      "Epoch 4058. Training loss: 0.7634676098823547. Validation loss: 2.705080032348633.\n",
      "Epoch 4059. Training loss: 0.7634636759757996. Validation loss: 2.7050795555114746.\n",
      "Epoch 4060. Training loss: 0.7634596824645996. Validation loss: 2.7050793170928955.\n",
      "Epoch 4061. Training loss: 0.7634556293487549. Validation loss: 2.705078601837158.\n",
      "Epoch 4062. Training loss: 0.7634517550468445. Validation loss: 2.705078125.\n",
      "Epoch 4063. Training loss: 0.7634477615356445. Validation loss: 2.705077886581421.\n",
      "Epoch 4064. Training loss: 0.7634438872337341. Validation loss: 2.7050774097442627.\n",
      "Epoch 4065. Training loss: 0.7634398937225342. Validation loss: 2.7050769329071045.\n",
      "Epoch 4066. Training loss: 0.7634358406066895. Validation loss: 2.705075979232788.\n",
      "Epoch 4067. Training loss: 0.7634318470954895. Validation loss: 2.705075740814209.\n",
      "Epoch 4068. Training loss: 0.7634279131889343. Validation loss: 2.705075263977051.\n",
      "Epoch 4069. Training loss: 0.7634239196777344. Validation loss: 2.7050747871398926.\n",
      "Epoch 4070. Training loss: 0.7634199261665344. Validation loss: 2.7050740718841553.\n",
      "Epoch 4071. Training loss: 0.7634159922599792. Validation loss: 2.705073833465576.\n",
      "Epoch 4072. Training loss: 0.7634119987487793. Validation loss: 2.705073595046997.\n",
      "Epoch 4073. Training loss: 0.7634079456329346. Validation loss: 2.7050724029541016.\n",
      "Epoch 4074. Training loss: 0.7634039521217346. Validation loss: 2.7050719261169434.\n",
      "Epoch 4075. Training loss: 0.7634000182151794. Validation loss: 2.7050716876983643.\n",
      "Epoch 4076. Training loss: 0.7633960843086243. Validation loss: 2.705071210861206.\n",
      "Epoch 4077. Training loss: 0.7633921504020691. Validation loss: 2.7050704956054688.\n",
      "Epoch 4078. Training loss: 0.7633881568908691. Validation loss: 2.7050700187683105.\n",
      "Epoch 4079. Training loss: 0.7633841037750244. Validation loss: 2.7050697803497314.\n",
      "Epoch 4080. Training loss: 0.763380229473114. Validation loss: 2.705069065093994.\n",
      "Epoch 4081. Training loss: 0.7633761763572693. Validation loss: 2.705068588256836.\n",
      "Epoch 4082. Training loss: 0.7633722424507141. Validation loss: 2.7050681114196777.\n",
      "Epoch 4083. Training loss: 0.7633681297302246. Validation loss: 2.7050676345825195.\n",
      "Epoch 4084. Training loss: 0.7633642554283142. Validation loss: 2.7050669193267822.\n",
      "Epoch 4085. Training loss: 0.763360321521759. Validation loss: 2.705066442489624.\n",
      "Epoch 4086. Training loss: 0.7633563876152039. Validation loss: 2.705065965652466.\n",
      "Epoch 4087. Training loss: 0.7633523344993591. Validation loss: 2.7050654888153076.\n",
      "Epoch 4088. Training loss: 0.7633482813835144. Validation loss: 2.705064535140991.\n",
      "Epoch 4089. Training loss: 0.7633442878723145. Validation loss: 2.705064058303833.\n",
      "Epoch 4090. Training loss: 0.7633402943611145. Validation loss: 2.705064058303833.\n",
      "Epoch 4091. Training loss: 0.7633363604545593. Validation loss: 2.705063581466675.\n",
      "Epoch 4092. Training loss: 0.7633323073387146. Validation loss: 2.7050628662109375.\n",
      "Epoch 4093. Training loss: 0.7633283138275146. Validation loss: 2.7050626277923584.\n",
      "Epoch 4094. Training loss: 0.7633244395256042. Validation loss: 2.705061912536621.\n",
      "Epoch 4095. Training loss: 0.7633203864097595. Validation loss: 2.705061435699463.\n",
      "Epoch 4096. Training loss: 0.7633164525032043. Validation loss: 2.7050609588623047.\n",
      "Epoch 4097. Training loss: 0.7633123397827148. Validation loss: 2.7050604820251465.\n",
      "Epoch 4098. Training loss: 0.7633083462715149. Validation loss: 2.705059766769409.\n",
      "Epoch 4099. Training loss: 0.7633042931556702. Validation loss: 2.70505952835083.\n",
      "Epoch 4100. Training loss: 0.7633004188537598. Validation loss: 2.705059051513672.\n",
      "Epoch 4101. Training loss: 0.763296365737915. Validation loss: 2.7050583362579346.\n",
      "Epoch 4102. Training loss: 0.7632924914360046. Validation loss: 2.7050580978393555.\n",
      "Epoch 4103. Training loss: 0.7632883191108704. Validation loss: 2.7050576210021973.\n",
      "Epoch 4104. Training loss: 0.76328444480896. Validation loss: 2.705056667327881.\n",
      "Epoch 4105. Training loss: 0.7632803916931152. Validation loss: 2.7050564289093018.\n",
      "Epoch 4106. Training loss: 0.7632763981819153. Validation loss: 2.7050557136535645.\n",
      "Epoch 4107. Training loss: 0.7632724642753601. Validation loss: 2.7050554752349854.\n",
      "Epoch 4108. Training loss: 0.7632684111595154. Validation loss: 2.705054998397827.\n",
      "Epoch 4109. Training loss: 0.7632643580436707. Validation loss: 2.70505428314209.\n",
      "Epoch 4110. Training loss: 0.7632603645324707. Validation loss: 2.7050540447235107.\n",
      "Epoch 4111. Training loss: 0.7632563710212708. Validation loss: 2.7050533294677734.\n",
      "Epoch 4112. Training loss: 0.7632524371147156. Validation loss: 2.705052614212036.\n",
      "Epoch 4113. Training loss: 0.7632483839988708. Validation loss: 2.705052137374878.\n",
      "Epoch 4114. Training loss: 0.7632443904876709. Validation loss: 2.7050514221191406.\n",
      "Epoch 4115. Training loss: 0.7632403373718262. Validation loss: 2.7050514221191406.\n",
      "Epoch 4116. Training loss: 0.7632363438606262. Validation loss: 2.7050507068634033.\n",
      "Epoch 4117. Training loss: 0.763232409954071. Validation loss: 2.705050230026245.\n",
      "Epoch 4118. Training loss: 0.7632283568382263. Validation loss: 2.705049753189087.\n",
      "Epoch 4119. Training loss: 0.7632243037223816. Validation loss: 2.7050492763519287.\n",
      "Epoch 4120. Training loss: 0.7632203102111816. Validation loss: 2.7050485610961914.\n",
      "Epoch 4121. Training loss: 0.7632163166999817. Validation loss: 2.705048084259033.\n",
      "Epoch 4122. Training loss: 0.763212263584137. Validation loss: 2.705047607421875.\n",
      "Epoch 4123. Training loss: 0.7632083296775818. Validation loss: 2.705047130584717.\n",
      "Epoch 4124. Training loss: 0.7632042765617371. Validation loss: 2.705047130584717.\n",
      "Epoch 4125. Training loss: 0.7632002830505371. Validation loss: 2.7050461769104004.\n",
      "Epoch 4126. Training loss: 0.7631962299346924. Validation loss: 2.705045700073242.\n",
      "Epoch 4127. Training loss: 0.7631921768188477. Validation loss: 2.705045223236084.\n",
      "Epoch 4128. Training loss: 0.7631881833076477. Validation loss: 2.705044984817505.\n",
      "Epoch 4129. Training loss: 0.763184130191803. Validation loss: 2.7050445079803467.\n",
      "Epoch 4130. Training loss: 0.7631801962852478. Validation loss: 2.7050435543060303.\n",
      "Epoch 4131. Training loss: 0.7631761431694031. Validation loss: 2.705043077468872.\n",
      "Epoch 4132. Training loss: 0.7631721496582031. Validation loss: 2.705042839050293.\n",
      "Epoch 4133. Training loss: 0.7631680369377136. Validation loss: 2.7050418853759766.\n",
      "Epoch 4134. Training loss: 0.7631641030311584. Validation loss: 2.7050416469573975.\n",
      "Epoch 4135. Training loss: 0.7631600499153137. Validation loss: 2.7050414085388184.\n",
      "Epoch 4136. Training loss: 0.763155996799469. Validation loss: 2.705040693283081.\n",
      "Epoch 4137. Training loss: 0.7631519436836243. Validation loss: 2.7050399780273438.\n",
      "Epoch 4138. Training loss: 0.7631478309631348. Validation loss: 2.7050395011901855.\n",
      "Epoch 4139. Training loss: 0.7631439566612244. Validation loss: 2.7050392627716064.\n",
      "Epoch 4140. Training loss: 0.7631399035453796. Validation loss: 2.70503830909729.\n",
      "Epoch 4141. Training loss: 0.7631359100341797. Validation loss: 2.70503830909729.\n",
      "Epoch 4142. Training loss: 0.763131856918335. Validation loss: 2.7050375938415527.\n",
      "Epoch 4143. Training loss: 0.7631278038024902. Validation loss: 2.7050368785858154.\n",
      "Epoch 4144. Training loss: 0.7631238102912903. Validation loss: 2.7050364017486572.\n",
      "Epoch 4145. Training loss: 0.7631197571754456. Validation loss: 2.705035924911499.\n",
      "Epoch 4146. Training loss: 0.7631157040596008. Validation loss: 2.705035448074341.\n",
      "Epoch 4147. Training loss: 0.7631116509437561. Validation loss: 2.7050349712371826.\n",
      "Epoch 4148. Training loss: 0.7631077170372009. Validation loss: 2.7050347328186035.\n",
      "Epoch 4149. Training loss: 0.763103723526001. Validation loss: 2.705034017562866.\n",
      "Epoch 4150. Training loss: 0.7630996704101562. Validation loss: 2.705033540725708.\n",
      "Epoch 4151. Training loss: 0.7630955576896667. Validation loss: 2.7050328254699707.\n",
      "Epoch 4152. Training loss: 0.763091504573822. Validation loss: 2.7050323486328125.\n",
      "Epoch 4153. Training loss: 0.7630874514579773. Validation loss: 2.705031394958496.\n",
      "Epoch 4154. Training loss: 0.7630834579467773. Validation loss: 2.705031394958496.\n",
      "Epoch 4155. Training loss: 0.7630794048309326. Validation loss: 2.705030918121338.\n",
      "Epoch 4156. Training loss: 0.7630754113197327. Validation loss: 2.7050304412841797.\n",
      "Epoch 4157. Training loss: 0.7630713582038879. Validation loss: 2.7050299644470215.\n",
      "Epoch 4158. Training loss: 0.7630672454833984. Validation loss: 2.7050294876098633.\n",
      "Epoch 4159. Training loss: 0.763063371181488. Validation loss: 2.705028772354126.\n",
      "Epoch 4160. Training loss: 0.7630591988563538. Validation loss: 2.705028533935547.\n",
      "Epoch 4161. Training loss: 0.763055145740509. Validation loss: 2.7050278186798096.\n",
      "Epoch 4162. Training loss: 0.7630510926246643. Validation loss: 2.7050273418426514.\n",
      "Epoch 4163. Training loss: 0.7630471587181091. Validation loss: 2.7050271034240723.\n",
      "Epoch 4164. Training loss: 0.7630431056022644. Validation loss: 2.705026149749756.\n",
      "Epoch 4165. Training loss: 0.7630390524864197. Validation loss: 2.7050259113311768.\n",
      "Epoch 4166. Training loss: 0.763034999370575. Validation loss: 2.7050254344940186.\n",
      "Epoch 4167. Training loss: 0.763031005859375. Validation loss: 2.7050247192382812.\n",
      "Epoch 4168. Training loss: 0.7630269527435303. Validation loss: 2.705024242401123.\n",
      "Epoch 4169. Training loss: 0.7630227208137512. Validation loss: 2.705023765563965.\n",
      "Epoch 4170. Training loss: 0.763018786907196. Validation loss: 2.7050230503082275.\n",
      "Epoch 4171. Training loss: 0.7630147337913513. Validation loss: 2.7050225734710693.\n",
      "Epoch 4172. Training loss: 0.7630106806755066. Validation loss: 2.705022096633911.\n",
      "Epoch 4173. Training loss: 0.7630065083503723. Validation loss: 2.705021619796753.\n",
      "Epoch 4174. Training loss: 0.7630025744438171. Validation loss: 2.7050209045410156.\n",
      "Epoch 4175. Training loss: 0.7629985213279724. Validation loss: 2.7050204277038574.\n",
      "Epoch 4176. Training loss: 0.7629944682121277. Validation loss: 2.7050201892852783.\n",
      "Epoch 4177. Training loss: 0.762990415096283. Validation loss: 2.70501971244812.\n",
      "Epoch 4178. Training loss: 0.7629863619804382. Validation loss: 2.705018997192383.\n",
      "Epoch 4179. Training loss: 0.7629823088645935. Validation loss: 2.7050182819366455.\n",
      "Epoch 4180. Training loss: 0.7629782557487488. Validation loss: 2.7050180435180664.\n",
      "Epoch 4181. Training loss: 0.762974202632904. Validation loss: 2.705017566680908.\n",
      "Epoch 4182. Training loss: 0.7629701495170593. Validation loss: 2.70501708984375.\n",
      "Epoch 4183. Training loss: 0.7629661560058594. Validation loss: 2.705016613006592.\n",
      "Epoch 4184. Training loss: 0.7629620432853699. Validation loss: 2.7050161361694336.\n",
      "Epoch 4185. Training loss: 0.7629580497741699. Validation loss: 2.7050156593322754.\n",
      "Epoch 4186. Training loss: 0.7629539370536804. Validation loss: 2.7050154209136963.\n",
      "Epoch 4187. Training loss: 0.7629498839378357. Validation loss: 2.705014705657959.\n",
      "Epoch 4188. Training loss: 0.762945830821991. Validation loss: 2.7050137519836426.\n",
      "Epoch 4189. Training loss: 0.7629416584968567. Validation loss: 2.7050137519836426.\n",
      "Epoch 4190. Training loss: 0.762937605381012. Validation loss: 2.7050132751464844.\n",
      "Epoch 4191. Training loss: 0.7629336714744568. Validation loss: 2.705012559890747.\n",
      "Epoch 4192. Training loss: 0.7629296183586121. Validation loss: 2.705012083053589.\n",
      "Epoch 4193. Training loss: 0.7629254460334778. Validation loss: 2.7050116062164307.\n",
      "Epoch 4194. Training loss: 0.7629213333129883. Validation loss: 2.7050108909606934.\n",
      "Epoch 4195. Training loss: 0.7629173398017883. Validation loss: 2.705010414123535.\n",
      "Epoch 4196. Training loss: 0.7629132270812988. Validation loss: 2.705010175704956.\n",
      "Epoch 4197. Training loss: 0.7629091739654541. Validation loss: 2.705009698867798.\n",
      "Epoch 4198. Training loss: 0.7629051208496094. Validation loss: 2.7050089836120605.\n",
      "Epoch 4199. Training loss: 0.7629010677337646. Validation loss: 2.7050087451934814.\n",
      "Epoch 4200. Training loss: 0.7628969550132751. Validation loss: 2.705007553100586.\n",
      "Epoch 4201. Training loss: 0.7628929615020752. Validation loss: 2.7050070762634277.\n",
      "Epoch 4202. Training loss: 0.7628888487815857. Validation loss: 2.7050068378448486.\n",
      "Epoch 4203. Training loss: 0.762884795665741. Validation loss: 2.7050063610076904.\n",
      "Epoch 4204. Training loss: 0.7628806233406067. Validation loss: 2.705005645751953.\n",
      "Epoch 4205. Training loss: 0.762876570224762. Validation loss: 2.705005168914795.\n",
      "Epoch 4206. Training loss: 0.7628725171089172. Validation loss: 2.7050046920776367.\n",
      "Epoch 4207. Training loss: 0.7628684043884277. Validation loss: 2.7050042152404785.\n",
      "Epoch 4208. Training loss: 0.7628644108772278. Validation loss: 2.7050037384033203.\n",
      "Epoch 4209. Training loss: 0.7628602981567383. Validation loss: 2.705003261566162.\n",
      "Epoch 4210. Training loss: 0.7628562450408936. Validation loss: 2.705002784729004.\n",
      "Epoch 4211. Training loss: 0.7628521919250488. Validation loss: 2.7050023078918457.\n",
      "Epoch 4212. Training loss: 0.7628480792045593. Validation loss: 2.7050015926361084.\n",
      "Epoch 4213. Training loss: 0.762843906879425. Validation loss: 2.70500111579895.\n",
      "Epoch 4214. Training loss: 0.7628398537635803. Validation loss: 2.705000638961792.\n",
      "Epoch 4215. Training loss: 0.7628357410430908. Validation loss: 2.705000400543213.\n",
      "Epoch 4216. Training loss: 0.7628316879272461. Validation loss: 2.7049999237060547.\n",
      "Epoch 4217. Training loss: 0.7628276944160461. Validation loss: 2.7049989700317383.\n",
      "Epoch 4218. Training loss: 0.7628235220909119. Validation loss: 2.704998731613159.\n",
      "Epoch 4219. Training loss: 0.7628194689750671. Validation loss: 2.704998254776001.\n",
      "Epoch 4220. Training loss: 0.7628154158592224. Validation loss: 2.7049977779388428.\n",
      "Epoch 4221. Training loss: 0.7628112435340881. Validation loss: 2.7049970626831055.\n",
      "Epoch 4222. Training loss: 0.7628071308135986. Validation loss: 2.7049965858459473.\n",
      "Epoch 4223. Training loss: 0.7628030180931091. Validation loss: 2.704996109008789.\n",
      "Epoch 4224. Training loss: 0.7627990245819092. Validation loss: 2.704995632171631.\n",
      "Epoch 4225. Training loss: 0.7627949118614197. Validation loss: 2.7049951553344727.\n",
      "Epoch 4226. Training loss: 0.762790858745575. Validation loss: 2.7049946784973145.\n",
      "Epoch 4227. Training loss: 0.7627866864204407. Validation loss: 2.704993724822998.\n",
      "Epoch 4228. Training loss: 0.762782633304596. Validation loss: 2.70499324798584.\n",
      "Epoch 4229. Training loss: 0.7627785205841064. Validation loss: 2.7049927711486816.\n",
      "Epoch 4230. Training loss: 0.7627744078636169. Validation loss: 2.7049922943115234.\n",
      "Epoch 4231. Training loss: 0.7627703547477722. Validation loss: 2.7049920558929443.\n",
      "Epoch 4232. Training loss: 0.7627661824226379. Validation loss: 2.704991340637207.\n",
      "Epoch 4233. Training loss: 0.7627621293067932. Validation loss: 2.704990863800049.\n",
      "Epoch 4234. Training loss: 0.7627580761909485. Validation loss: 2.7049901485443115.\n",
      "Epoch 4235. Training loss: 0.7627539038658142. Validation loss: 2.7049901485443115.\n",
      "Epoch 4236. Training loss: 0.7627498507499695. Validation loss: 2.704988956451416.\n",
      "Epoch 4237. Training loss: 0.7627456784248352. Validation loss: 2.704988479614258.\n",
      "Epoch 4238. Training loss: 0.7627415657043457. Validation loss: 2.7049882411956787.\n",
      "Epoch 4239. Training loss: 0.7627375721931458. Validation loss: 2.7049877643585205.\n",
      "Epoch 4240. Training loss: 0.7627334594726562. Validation loss: 2.7049872875213623.\n",
      "Epoch 4241. Training loss: 0.7627293467521667. Validation loss: 2.704986810684204.\n",
      "Epoch 4242. Training loss: 0.7627251744270325. Validation loss: 2.704986572265625.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4243. Training loss: 0.7627211213111877. Validation loss: 2.7049856185913086.\n",
      "Epoch 4244. Training loss: 0.7627169489860535. Validation loss: 2.7049849033355713.\n",
      "Epoch 4245. Training loss: 0.7627129554748535. Validation loss: 2.7049851417541504.\n",
      "Epoch 4246. Training loss: 0.762708842754364. Validation loss: 2.704984188079834.\n",
      "Epoch 4247. Training loss: 0.7627046704292297. Validation loss: 2.704983949661255.\n",
      "Epoch 4248. Training loss: 0.7627005577087402. Validation loss: 2.7049829959869385.\n",
      "Epoch 4249. Training loss: 0.7626964449882507. Validation loss: 2.7049825191497803.\n",
      "Epoch 4250. Training loss: 0.762692391872406. Validation loss: 2.704982280731201.\n",
      "Epoch 4251. Training loss: 0.7626882195472717. Validation loss: 2.704981803894043.\n",
      "Epoch 4252. Training loss: 0.762684166431427. Validation loss: 2.7049810886383057.\n",
      "Epoch 4253. Training loss: 0.7626799941062927. Validation loss: 2.7049808502197266.\n",
      "Epoch 4254. Training loss: 0.762675940990448. Validation loss: 2.7049803733825684.\n",
      "Epoch 4255. Training loss: 0.7626717686653137. Validation loss: 2.704979658126831.\n",
      "Epoch 4256. Training loss: 0.7626676559448242. Validation loss: 2.704979181289673.\n",
      "Epoch 4257. Training loss: 0.7626635432243347. Validation loss: 2.7049789428710938.\n",
      "Epoch 4258. Training loss: 0.7626593708992004. Validation loss: 2.7049779891967773.\n",
      "Epoch 4259. Training loss: 0.7626552581787109. Validation loss: 2.70497727394104.\n",
      "Epoch 4260. Training loss: 0.7626512050628662. Validation loss: 2.704977035522461.\n",
      "Epoch 4261. Training loss: 0.7626469731330872. Validation loss: 2.7049763202667236.\n",
      "Epoch 4262. Training loss: 0.7626428604125977. Validation loss: 2.7049758434295654.\n",
      "Epoch 4263. Training loss: 0.7626388072967529. Validation loss: 2.7049756050109863.\n",
      "Epoch 4264. Training loss: 0.7626346945762634. Validation loss: 2.704974889755249.\n",
      "Epoch 4265. Training loss: 0.7626306414604187. Validation loss: 2.704974412918091.\n",
      "Epoch 4266. Training loss: 0.7626264095306396. Validation loss: 2.7049736976623535.\n",
      "Epoch 4267. Training loss: 0.7626223564147949. Validation loss: 2.7049734592437744.\n",
      "Epoch 4268. Training loss: 0.7626181244850159. Validation loss: 2.704972982406616.\n",
      "Epoch 4269. Training loss: 0.7626140713691711. Validation loss: 2.704972267150879.\n",
      "Epoch 4270. Training loss: 0.7626099586486816. Validation loss: 2.7049717903137207.\n",
      "Epoch 4271. Training loss: 0.7626058459281921. Validation loss: 2.7049713134765625.\n",
      "Epoch 4272. Training loss: 0.7626016736030579. Validation loss: 2.7049708366394043.\n",
      "Epoch 4273. Training loss: 0.7625975012779236. Validation loss: 2.704970359802246.\n",
      "Epoch 4274. Training loss: 0.7625933289527893. Validation loss: 2.704969882965088.\n",
      "Epoch 4275. Training loss: 0.7625892758369446. Validation loss: 2.7049694061279297.\n",
      "Epoch 4276. Training loss: 0.7625851631164551. Validation loss: 2.7049686908721924.\n",
      "Epoch 4277. Training loss: 0.7625810503959656. Validation loss: 2.704967975616455.\n",
      "Epoch 4278. Training loss: 0.7625768780708313. Validation loss: 2.704967737197876.\n",
      "Epoch 4279. Training loss: 0.7625727653503418. Validation loss: 2.7049670219421387.\n",
      "Epoch 4280. Training loss: 0.7625686526298523. Validation loss: 2.7049670219421387.\n",
      "Epoch 4281. Training loss: 0.762564480304718. Validation loss: 2.7049663066864014.\n",
      "Epoch 4282. Training loss: 0.7625603079795837. Validation loss: 2.704965829849243.\n",
      "Epoch 4283. Training loss: 0.762556254863739. Validation loss: 2.704965353012085.\n",
      "Epoch 4284. Training loss: 0.7625520825386047. Validation loss: 2.7049648761749268.\n",
      "Epoch 4285. Training loss: 0.7625479698181152. Validation loss: 2.7049639225006104.\n",
      "Epoch 4286. Training loss: 0.7625437378883362. Validation loss: 2.704963445663452.\n",
      "Epoch 4287. Training loss: 0.7625396847724915. Validation loss: 2.704963207244873.\n",
      "Epoch 4288. Training loss: 0.7625355124473572. Validation loss: 2.704962730407715.\n",
      "Epoch 4289. Training loss: 0.7625313401222229. Validation loss: 2.7049617767333984.\n",
      "Epoch 4290. Training loss: 0.7625272274017334. Validation loss: 2.7049615383148193.\n",
      "Epoch 4291. Training loss: 0.7625229954719543. Validation loss: 2.704961061477661.\n",
      "Epoch 4292. Training loss: 0.7625188827514648. Validation loss: 2.704960584640503.\n",
      "Epoch 4293. Training loss: 0.7625148296356201. Validation loss: 2.7049598693847656.\n",
      "Epoch 4294. Training loss: 0.7625105977058411. Validation loss: 2.7049598693847656.\n",
      "Epoch 4295. Training loss: 0.7625064849853516. Validation loss: 2.7049593925476074.\n",
      "Epoch 4296. Training loss: 0.7625022530555725. Validation loss: 2.704958438873291.\n",
      "Epoch 4297. Training loss: 0.7624981999397278. Validation loss: 2.704957962036133.\n",
      "Epoch 4298. Training loss: 0.7624940276145935. Validation loss: 2.7049574851989746.\n",
      "Epoch 4299. Training loss: 0.7624898552894592. Validation loss: 2.7049570083618164.\n",
      "Epoch 4300. Training loss: 0.762485682964325. Validation loss: 2.704956531524658.\n",
      "Epoch 4301. Training loss: 0.7624815106391907. Validation loss: 2.7049560546875.\n",
      "Epoch 4302. Training loss: 0.762477457523346. Validation loss: 2.7049551010131836.\n",
      "Epoch 4303. Training loss: 0.7624732851982117. Validation loss: 2.7049546241760254.\n",
      "Epoch 4304. Training loss: 0.7624691128730774. Validation loss: 2.704954147338867.\n",
      "Epoch 4305. Training loss: 0.7624650001525879. Validation loss: 2.704953670501709.\n",
      "Epoch 4306. Training loss: 0.7624607086181641. Validation loss: 2.704953670501709.\n",
      "Epoch 4307. Training loss: 0.7624565958976746. Validation loss: 2.7049524784088135.\n",
      "Epoch 4308. Training loss: 0.7624524235725403. Validation loss: 2.7049522399902344.\n",
      "Epoch 4309. Training loss: 0.762448251247406. Validation loss: 2.704951763153076.\n",
      "Epoch 4310. Training loss: 0.7624441981315613. Validation loss: 2.704951286315918.\n",
      "Epoch 4311. Training loss: 0.7624399065971375. Validation loss: 2.7049505710601807.\n",
      "Epoch 4312. Training loss: 0.7624358534812927. Validation loss: 2.7049500942230225.\n",
      "Epoch 4313. Training loss: 0.7624316215515137. Validation loss: 2.7049496173858643.\n",
      "Epoch 4314. Training loss: 0.7624275088310242. Validation loss: 2.704948902130127.\n",
      "Epoch 4315. Training loss: 0.7624232769012451. Validation loss: 2.704948663711548.\n",
      "Epoch 4316. Training loss: 0.7624191641807556. Validation loss: 2.7049479484558105.\n",
      "Epoch 4317. Training loss: 0.7624149918556213. Validation loss: 2.7049474716186523.\n",
      "Epoch 4318. Training loss: 0.7624108195304871. Validation loss: 2.704946994781494.\n",
      "Epoch 4319. Training loss: 0.762406587600708. Validation loss: 2.704946756362915.\n",
      "Epoch 4320. Training loss: 0.7624025344848633. Validation loss: 2.704946279525757.\n",
      "Epoch 4321. Training loss: 0.7623983025550842. Validation loss: 2.7049450874328613.\n",
      "Epoch 4322. Training loss: 0.76239413022995. Validation loss: 2.704944610595703.\n",
      "Epoch 4323. Training loss: 0.7623899579048157. Validation loss: 2.704944610595703.\n",
      "Epoch 4324. Training loss: 0.7623858451843262. Validation loss: 2.704944133758545.\n",
      "Epoch 4325. Training loss: 0.7623817324638367. Validation loss: 2.7049436569213867.\n",
      "Epoch 4326. Training loss: 0.7623775005340576. Validation loss: 2.7049431800842285.\n",
      "Epoch 4327. Training loss: 0.7623732686042786. Validation loss: 2.7049427032470703.\n",
      "Epoch 4328. Training loss: 0.7623691558837891. Validation loss: 2.704941987991333.\n",
      "Epoch 4329. Training loss: 0.7623650431632996. Validation loss: 2.704941511154175.\n",
      "Epoch 4330. Training loss: 0.7623608112335205. Validation loss: 2.7049410343170166.\n",
      "Epoch 4331. Training loss: 0.7623565793037415. Validation loss: 2.704939842224121.\n",
      "Epoch 4332. Training loss: 0.7623524069786072. Validation loss: 2.704939365386963.\n",
      "Epoch 4333. Training loss: 0.7623482346534729. Validation loss: 2.704939126968384.\n",
      "Epoch 4334. Training loss: 0.7623440623283386. Validation loss: 2.7049386501312256.\n",
      "Epoch 4335. Training loss: 0.7623398303985596. Validation loss: 2.7049381732940674.\n",
      "Epoch 4336. Training loss: 0.7623357176780701. Validation loss: 2.7049379348754883.\n",
      "Epoch 4337. Training loss: 0.7623315453529358. Validation loss: 2.704936981201172.\n",
      "Epoch 4338. Training loss: 0.762327253818512. Validation loss: 2.7049365043640137.\n",
      "Epoch 4339. Training loss: 0.7623231410980225. Validation loss: 2.7049360275268555.\n",
      "Epoch 4340. Training loss: 0.762319028377533. Validation loss: 2.7049357891082764.\n",
      "Epoch 4341. Training loss: 0.7623147964477539. Validation loss: 2.704935312271118.\n",
      "Epoch 4342. Training loss: 0.7623105645179749. Validation loss: 2.70493483543396.\n",
      "Epoch 4343. Training loss: 0.7623063921928406. Validation loss: 2.7049338817596436.\n",
      "Epoch 4344. Training loss: 0.7623021006584167. Validation loss: 2.7049334049224854.\n",
      "Epoch 4345. Training loss: 0.762298047542572. Validation loss: 2.704932928085327.\n",
      "Epoch 4346. Training loss: 0.762293815612793. Validation loss: 2.704932451248169.\n",
      "Epoch 4347. Training loss: 0.7622895836830139. Validation loss: 2.7049317359924316.\n",
      "Epoch 4348. Training loss: 0.7622854709625244. Validation loss: 2.7049312591552734.\n",
      "Epoch 4349. Training loss: 0.7622811794281006. Validation loss: 2.704930543899536.\n",
      "Epoch 4350. Training loss: 0.7622771263122559. Validation loss: 2.704930305480957.\n",
      "Epoch 4351. Training loss: 0.762272834777832. Validation loss: 2.704930067062378.\n",
      "Epoch 4352. Training loss: 0.7622687220573425. Validation loss: 2.7049293518066406.\n",
      "Epoch 4353. Training loss: 0.7622645497322083. Validation loss: 2.7049286365509033.\n",
      "Epoch 4354. Training loss: 0.7622602581977844. Validation loss: 2.704928159713745.\n",
      "Epoch 4355. Training loss: 0.7622560858726501. Validation loss: 2.704927921295166.\n",
      "Epoch 4356. Training loss: 0.7622518539428711. Validation loss: 2.7049272060394287.\n",
      "Epoch 4357. Training loss: 0.7622477412223816. Validation loss: 2.7049267292022705.\n",
      "Epoch 4358. Training loss: 0.7622434496879578. Validation loss: 2.7049264907836914.\n",
      "Epoch 4359. Training loss: 0.7622392773628235. Validation loss: 2.704925775527954.\n",
      "Epoch 4360. Training loss: 0.762235164642334. Validation loss: 2.704925298690796.\n",
      "Epoch 4361. Training loss: 0.7622308731079102. Validation loss: 2.7049248218536377.\n",
      "Epoch 4362. Training loss: 0.7622265815734863. Validation loss: 2.7049243450164795.\n",
      "Epoch 4363. Training loss: 0.7622224688529968. Validation loss: 2.704923629760742.\n",
      "Epoch 4364. Training loss: 0.7622182369232178. Validation loss: 2.704923152923584.\n",
      "Epoch 4365. Training loss: 0.7622140049934387. Validation loss: 2.7049224376678467.\n",
      "Epoch 4366. Training loss: 0.7622098326683044. Validation loss: 2.7049221992492676.\n",
      "Epoch 4367. Training loss: 0.7622056603431702. Validation loss: 2.7049217224121094.\n",
      "Epoch 4368. Training loss: 0.7622013688087463. Validation loss: 2.7049214839935303.\n",
      "Epoch 4369. Training loss: 0.7621971964836121. Validation loss: 2.704920530319214.\n",
      "Epoch 4370. Training loss: 0.762192964553833. Validation loss: 2.7049200534820557.\n",
      "Epoch 4371. Training loss: 0.762188732624054. Validation loss: 2.7049193382263184.\n",
      "Epoch 4372. Training loss: 0.7621846199035645. Validation loss: 2.7049190998077393.\n",
      "Epoch 4373. Training loss: 0.7621803879737854. Validation loss: 2.704918622970581.\n",
      "Epoch 4374. Training loss: 0.7621760964393616. Validation loss: 2.7049179077148438.\n",
      "Epoch 4375. Training loss: 0.7621719241142273. Validation loss: 2.7049174308776855.\n",
      "Epoch 4376. Training loss: 0.7621676921844482. Validation loss: 2.7049167156219482.\n",
      "Epoch 4377. Training loss: 0.7621634602546692. Validation loss: 2.704916000366211.\n",
      "Epoch 4378. Training loss: 0.7621592879295349. Validation loss: 2.704915761947632.\n",
      "Epoch 4379. Training loss: 0.7621550559997559. Validation loss: 2.7049152851104736.\n",
      "Epoch 4380. Training loss: 0.7621508240699768. Validation loss: 2.7049145698547363.\n",
      "Epoch 4381. Training loss: 0.7621466517448425. Validation loss: 2.7049143314361572.\n",
      "Epoch 4382. Training loss: 0.7621424794197083. Validation loss: 2.70491361618042.\n",
      "Epoch 4383. Training loss: 0.7621381878852844. Validation loss: 2.704913377761841.\n",
      "Epoch 4384. Training loss: 0.7621340155601501. Validation loss: 2.7049124240875244.\n",
      "Epoch 4385. Training loss: 0.7621297836303711. Validation loss: 2.7049124240875244.\n",
      "Epoch 4386. Training loss: 0.762125551700592. Validation loss: 2.704911470413208.\n",
      "Epoch 4387. Training loss: 0.7621212601661682. Validation loss: 2.70491099357605.\n",
      "Epoch 4388. Training loss: 0.7621171474456787. Validation loss: 2.7049105167388916.\n",
      "Epoch 4389. Training loss: 0.7621129155158997. Validation loss: 2.7049098014831543.\n",
      "Epoch 4390. Training loss: 0.7621087431907654. Validation loss: 2.704909563064575.\n",
      "Epoch 4391. Training loss: 0.7621044516563416. Validation loss: 2.704908847808838.\n",
      "Epoch 4392. Training loss: 0.7621002197265625. Validation loss: 2.704908609390259.\n",
      "Epoch 4393. Training loss: 0.7620959877967834. Validation loss: 2.7049078941345215.\n",
      "Epoch 4394. Training loss: 0.7620916962623596. Validation loss: 2.7049074172973633.\n",
      "Epoch 4395. Training loss: 0.7620875239372253. Validation loss: 2.704907178878784.\n",
      "Epoch 4396. Training loss: 0.7620832920074463. Validation loss: 2.7049062252044678.\n",
      "Epoch 4397. Training loss: 0.7620790600776672. Validation loss: 2.7049057483673096.\n",
      "Epoch 4398. Training loss: 0.7620747685432434. Validation loss: 2.7049055099487305.\n",
      "Epoch 4399. Training loss: 0.7620705962181091. Validation loss: 2.704904794692993.\n",
      "Epoch 4400. Training loss: 0.7620663642883301. Validation loss: 2.704904556274414.\n",
      "Epoch 4401. Training loss: 0.762062132358551. Validation loss: 2.7049038410186768.\n",
      "Epoch 4402. Training loss: 0.7620579600334167. Validation loss: 2.7049028873443604.\n",
      "Epoch 4403. Training loss: 0.7620536684989929. Validation loss: 2.7049028873443604.\n",
      "Epoch 4404. Training loss: 0.7620494365692139. Validation loss: 2.704902410507202.\n",
      "Epoch 4405. Training loss: 0.76204514503479. Validation loss: 2.704901933670044.\n",
      "Epoch 4406. Training loss: 0.7620410323143005. Validation loss: 2.7049012184143066.\n",
      "Epoch 4407. Training loss: 0.7620367407798767. Validation loss: 2.7049005031585693.\n",
      "Epoch 4408. Training loss: 0.7620324492454529. Validation loss: 2.7049002647399902.\n",
      "Epoch 4409. Training loss: 0.7620282769203186. Validation loss: 2.704899549484253.\n",
      "Epoch 4410. Training loss: 0.7620239853858948. Validation loss: 2.7048990726470947.\n",
      "Epoch 4411. Training loss: 0.762019693851471. Validation loss: 2.7048988342285156.\n",
      "Epoch 4412. Training loss: 0.7620155215263367. Validation loss: 2.70489764213562.\n",
      "Epoch 4413. Training loss: 0.7620112299919128. Validation loss: 2.70489764213562.\n",
      "Epoch 4414. Training loss: 0.7620069980621338. Validation loss: 2.7048966884613037.\n",
      "Epoch 4415. Training loss: 0.76200270652771. Validation loss: 2.7048964500427246.\n",
      "Epoch 4416. Training loss: 0.7619984745979309. Validation loss: 2.7048957347869873.\n",
      "Epoch 4417. Training loss: 0.7619941830635071. Validation loss: 2.704895496368408.\n",
      "Epoch 4418. Training loss: 0.7619900107383728. Validation loss: 2.704894781112671.\n",
      "Epoch 4419. Training loss: 0.7619857788085938. Validation loss: 2.7048943042755127.\n",
      "Epoch 4420. Training loss: 0.7619814872741699. Validation loss: 2.7048935890197754.\n",
      "Epoch 4421. Training loss: 0.7619772553443909. Validation loss: 2.704893112182617.\n",
      "Epoch 4422. Training loss: 0.7619730830192566. Validation loss: 2.704892873764038.\n",
      "Epoch 4423. Training loss: 0.7619687914848328. Validation loss: 2.7048919200897217.\n",
      "Epoch 4424. Training loss: 0.7619645595550537. Validation loss: 2.7048916816711426.\n",
      "Epoch 4425. Training loss: 0.7619602680206299. Validation loss: 2.7048909664154053.\n",
      "Epoch 4426. Training loss: 0.761955976486206. Validation loss: 2.704890727996826.\n",
      "Epoch 4427. Training loss: 0.761951744556427. Validation loss: 2.704890251159668.\n",
      "Epoch 4428. Training loss: 0.7619474530220032. Validation loss: 2.7048895359039307.\n",
      "Epoch 4429. Training loss: 0.7619431614875793. Validation loss: 2.7048890590667725.\n",
      "Epoch 4430. Training loss: 0.7619388699531555. Validation loss: 2.7048888206481934.\n",
      "Epoch 4431. Training loss: 0.761934757232666. Validation loss: 2.704887866973877.\n",
      "Epoch 4432. Training loss: 0.7619304656982422. Validation loss: 2.704887866973877.\n",
      "Epoch 4433. Training loss: 0.7619261741638184. Validation loss: 2.7048873901367188.\n",
      "Epoch 4434. Training loss: 0.7619219422340393. Validation loss: 2.7048861980438232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4435. Training loss: 0.7619176506996155. Validation loss: 2.704885959625244.\n",
      "Epoch 4436. Training loss: 0.7619133591651917. Validation loss: 2.704885482788086.\n",
      "Epoch 4437. Training loss: 0.761909008026123. Validation loss: 2.7048850059509277.\n",
      "Epoch 4438. Training loss: 0.7619049549102783. Validation loss: 2.7048845291137695.\n",
      "Epoch 4439. Training loss: 0.7619006037712097. Validation loss: 2.7048838138580322.\n",
      "Epoch 4440. Training loss: 0.7618964314460754. Validation loss: 2.704883575439453.\n",
      "Epoch 4441. Training loss: 0.7618920803070068. Validation loss: 2.704883098602295.\n",
      "Epoch 4442. Training loss: 0.761887788772583. Validation loss: 2.7048821449279785.\n",
      "Epoch 4443. Training loss: 0.761883556842804. Validation loss: 2.7048821449279785.\n",
      "Epoch 4444. Training loss: 0.7618792653083801. Validation loss: 2.704880952835083.\n",
      "Epoch 4445. Training loss: 0.7618749737739563. Validation loss: 2.704880475997925.\n",
      "Epoch 4446. Training loss: 0.7618706822395325. Validation loss: 2.7048802375793457.\n",
      "Epoch 4447. Training loss: 0.7618663907051086. Validation loss: 2.7048797607421875.\n",
      "Epoch 4448. Training loss: 0.7618620991706848. Validation loss: 2.70487904548645.\n",
      "Epoch 4449. Training loss: 0.7618579268455505. Validation loss: 2.704878807067871.\n",
      "Epoch 4450. Training loss: 0.7618535161018372. Validation loss: 2.704878091812134.\n",
      "Epoch 4451. Training loss: 0.7618494033813477. Validation loss: 2.7048776149749756.\n",
      "Epoch 4452. Training loss: 0.7618451118469238. Validation loss: 2.704876661300659.\n",
      "Epoch 4453. Training loss: 0.7618408203125. Validation loss: 2.70487642288208.\n",
      "Epoch 4454. Training loss: 0.7618365287780762. Validation loss: 2.704875946044922.\n",
      "Epoch 4455. Training loss: 0.7618322372436523. Validation loss: 2.7048754692077637.\n",
      "Epoch 4456. Training loss: 0.7618279457092285. Validation loss: 2.7048749923706055.\n",
      "Epoch 4457. Training loss: 0.7618236541748047. Validation loss: 2.7048745155334473.\n",
      "Epoch 4458. Training loss: 0.7618193626403809. Validation loss: 2.704874038696289.\n",
      "Epoch 4459. Training loss: 0.761815071105957. Validation loss: 2.7048730850219727.\n",
      "Epoch 4460. Training loss: 0.761810839176178. Validation loss: 2.7048730850219727.\n",
      "Epoch 4461. Training loss: 0.7618065476417542. Validation loss: 2.7048726081848145.\n",
      "Epoch 4462. Training loss: 0.7618021965026855. Validation loss: 2.704871654510498.\n",
      "Epoch 4463. Training loss: 0.7617979049682617. Validation loss: 2.70487117767334.\n",
      "Epoch 4464. Training loss: 0.7617936730384827. Validation loss: 2.7048704624176025.\n",
      "Epoch 4465. Training loss: 0.7617893815040588. Validation loss: 2.7048702239990234.\n",
      "Epoch 4466. Training loss: 0.761785089969635. Validation loss: 2.7048697471618652.\n",
      "Epoch 4467. Training loss: 0.7617807984352112. Validation loss: 2.704869508743286.\n",
      "Epoch 4468. Training loss: 0.7617764472961426. Validation loss: 2.7048683166503906.\n",
      "Epoch 4469. Training loss: 0.7617722153663635. Validation loss: 2.7048678398132324.\n",
      "Epoch 4470. Training loss: 0.7617679238319397. Validation loss: 2.704867362976074.\n",
      "Epoch 4471. Training loss: 0.7617637515068054. Validation loss: 2.704866886138916.\n",
      "Epoch 4472. Training loss: 0.761759340763092. Validation loss: 2.704866409301758.\n",
      "Epoch 4473. Training loss: 0.7617551684379578. Validation loss: 2.7048659324645996.\n",
      "Epoch 4474. Training loss: 0.7617506980895996. Validation loss: 2.7048654556274414.\n",
      "Epoch 4475. Training loss: 0.7617464661598206. Validation loss: 2.704864978790283.\n",
      "Epoch 4476. Training loss: 0.7617421746253967. Validation loss: 2.704864263534546.\n",
      "Epoch 4477. Training loss: 0.7617378830909729. Validation loss: 2.7048637866973877.\n",
      "Epoch 4478. Training loss: 0.7617335319519043. Validation loss: 2.7048630714416504.\n",
      "Epoch 4479. Training loss: 0.7617293000221252. Validation loss: 2.704862594604492.\n",
      "Epoch 4480. Training loss: 0.7617249488830566. Validation loss: 2.704862117767334.\n",
      "Epoch 4481. Training loss: 0.7617207169532776. Validation loss: 2.704861640930176.\n",
      "Epoch 4482. Training loss: 0.7617164254188538. Validation loss: 2.7048611640930176.\n",
      "Epoch 4483. Training loss: 0.7617120742797852. Validation loss: 2.7048604488372803.\n",
      "Epoch 4484. Training loss: 0.7617077827453613. Validation loss: 2.704860210418701.\n",
      "Epoch 4485. Training loss: 0.7617034912109375. Validation loss: 2.7048592567443848.\n",
      "Epoch 4486. Training loss: 0.7616991996765137. Validation loss: 2.7048590183258057.\n",
      "Epoch 4487. Training loss: 0.7616948485374451. Validation loss: 2.7048585414886475.\n",
      "Epoch 4488. Training loss: 0.7616905570030212. Validation loss: 2.7048580646514893.\n",
      "Epoch 4489. Training loss: 0.7616862654685974. Validation loss: 2.704857587814331.\n",
      "Epoch 4490. Training loss: 0.7616820335388184. Validation loss: 2.7048566341400146.\n",
      "Epoch 4491. Training loss: 0.7616776823997498. Validation loss: 2.7048563957214355.\n",
      "Epoch 4492. Training loss: 0.7616732716560364. Validation loss: 2.7048559188842773.\n",
      "Epoch 4493. Training loss: 0.7616689801216125. Validation loss: 2.70485520362854.\n",
      "Epoch 4494. Training loss: 0.7616646885871887. Validation loss: 2.704854965209961.\n",
      "Epoch 4495. Training loss: 0.7616603970527649. Validation loss: 2.7048542499542236.\n",
      "Epoch 4496. Training loss: 0.7616561055183411. Validation loss: 2.7048540115356445.\n",
      "Epoch 4497. Training loss: 0.7616517543792725. Validation loss: 2.7048532962799072.\n",
      "Epoch 4498. Training loss: 0.7616474628448486. Validation loss: 2.704852819442749.\n",
      "Epoch 4499. Training loss: 0.7616431713104248. Validation loss: 2.7048521041870117.\n",
      "Epoch 4500. Training loss: 0.7616388201713562. Validation loss: 2.7048516273498535.\n",
      "Epoch 4501. Training loss: 0.7616345286369324. Validation loss: 2.7048511505126953.\n",
      "Epoch 4502. Training loss: 0.761630117893219. Validation loss: 2.704850673675537.\n",
      "Epoch 4503. Training loss: 0.7616258263587952. Validation loss: 2.704850435256958.\n",
      "Epoch 4504. Training loss: 0.7616214752197266. Validation loss: 2.7048497200012207.\n",
      "Epoch 4505. Training loss: 0.7616171836853027. Validation loss: 2.7048490047454834.\n",
      "Epoch 4506. Training loss: 0.7616128325462341. Validation loss: 2.704848527908325.\n",
      "Epoch 4507. Training loss: 0.7616085410118103. Validation loss: 2.704848051071167.\n",
      "Epoch 4508. Training loss: 0.7616042494773865. Validation loss: 2.704847574234009.\n",
      "Epoch 4509. Training loss: 0.7615998387336731. Validation loss: 2.7048470973968506.\n",
      "Epoch 4510. Training loss: 0.7615955471992493. Validation loss: 2.7048463821411133.\n",
      "Epoch 4511. Training loss: 0.7615911960601807. Validation loss: 2.704845666885376.\n",
      "Epoch 4512. Training loss: 0.7615869045257568. Validation loss: 2.704845428466797.\n",
      "Epoch 4513. Training loss: 0.7615825533866882. Validation loss: 2.7048447132110596.\n",
      "Epoch 4514. Training loss: 0.7615782618522644. Validation loss: 2.7048444747924805.\n",
      "Epoch 4515. Training loss: 0.761573851108551. Validation loss: 2.7048439979553223.\n",
      "Epoch 4516. Training loss: 0.7615695595741272. Validation loss: 2.704843521118164.\n",
      "Epoch 4517. Training loss: 0.7615651488304138. Validation loss: 2.7048425674438477.\n",
      "Epoch 4518. Training loss: 0.76156085729599. Validation loss: 2.7048420906066895.\n",
      "Epoch 4519. Training loss: 0.7615565657615662. Validation loss: 2.7048416137695312.\n",
      "Epoch 4520. Training loss: 0.7615522742271423. Validation loss: 2.704841136932373.\n",
      "Epoch 4521. Training loss: 0.761547863483429. Validation loss: 2.7048404216766357.\n",
      "Epoch 4522. Training loss: 0.7615434527397156. Validation loss: 2.7048399448394775.\n",
      "Epoch 4523. Training loss: 0.7615392208099365. Validation loss: 2.7048394680023193.\n",
      "Epoch 4524. Training loss: 0.7615346908569336. Validation loss: 2.704838752746582.\n",
      "Epoch 4525. Training loss: 0.7615304589271545. Validation loss: 2.704838514328003.\n",
      "Epoch 4526. Training loss: 0.7615261077880859. Validation loss: 2.7048377990722656.\n",
      "Epoch 4527. Training loss: 0.7615217566490173. Validation loss: 2.7048370838165283.\n",
      "Epoch 4528. Training loss: 0.7615175247192383. Validation loss: 2.704836845397949.\n",
      "Epoch 4529. Training loss: 0.7615131735801697. Validation loss: 2.704836368560791.\n",
      "Epoch 4530. Training loss: 0.7615087628364563. Validation loss: 2.7048354148864746.\n",
      "Epoch 4531. Training loss: 0.7615044713020325. Validation loss: 2.7048349380493164.\n",
      "Epoch 4532. Training loss: 0.7615000605583191. Validation loss: 2.704834461212158.\n",
      "Epoch 4533. Training loss: 0.7614957690238953. Validation loss: 2.704834222793579.\n",
      "Epoch 4534. Training loss: 0.7614913582801819. Validation loss: 2.7048332691192627.\n",
      "Epoch 4535. Training loss: 0.7614870667457581. Validation loss: 2.7048327922821045.\n",
      "Epoch 4536. Training loss: 0.7614826560020447. Validation loss: 2.7048325538635254.\n",
      "Epoch 4537. Training loss: 0.7614782452583313. Validation loss: 2.704831838607788.\n",
      "Epoch 4538. Training loss: 0.7614739537239075. Validation loss: 2.704831600189209.\n",
      "Epoch 4539. Training loss: 0.7614696621894836. Validation loss: 2.7048308849334717.\n",
      "Epoch 4540. Training loss: 0.7614652514457703. Validation loss: 2.7048304080963135.\n",
      "Epoch 4541. Training loss: 0.7614609599113464. Validation loss: 2.704829692840576.\n",
      "Epoch 4542. Training loss: 0.7614564895629883. Validation loss: 2.704829216003418.\n",
      "Epoch 4543. Training loss: 0.7614521980285645. Validation loss: 2.7048287391662598.\n",
      "Epoch 4544. Training loss: 0.7614479064941406. Validation loss: 2.7048282623291016.\n",
      "Epoch 4545. Training loss: 0.7614434361457825. Validation loss: 2.7048277854919434.\n",
      "Epoch 4546. Training loss: 0.7614391446113586. Validation loss: 2.704827308654785.\n",
      "Epoch 4547. Training loss: 0.7614347338676453. Validation loss: 2.704826831817627.\n",
      "Epoch 4548. Training loss: 0.7614303231239319. Validation loss: 2.7048261165618896.\n",
      "Epoch 4549. Training loss: 0.7614260315895081. Validation loss: 2.7048258781433105.\n",
      "Epoch 4550. Training loss: 0.7614216804504395. Validation loss: 2.7048251628875732.\n",
      "Epoch 4551. Training loss: 0.7614173293113708. Validation loss: 2.704824924468994.\n",
      "Epoch 4552. Training loss: 0.761413037776947. Validation loss: 2.704824209213257.\n",
      "Epoch 4553. Training loss: 0.7614085674285889. Validation loss: 2.7048234939575195.\n",
      "Epoch 4554. Training loss: 0.7614042162895203. Validation loss: 2.7048227787017822.\n",
      "Epoch 4555. Training loss: 0.7613998055458069. Validation loss: 2.704822540283203.\n",
      "Epoch 4556. Training loss: 0.7613954544067383. Validation loss: 2.704822063446045.\n",
      "Epoch 4557. Training loss: 0.7613911032676697. Validation loss: 2.7048215866088867.\n",
      "Epoch 4558. Training loss: 0.7613866925239563. Validation loss: 2.7048211097717285.\n",
      "Epoch 4559. Training loss: 0.7613823413848877. Validation loss: 2.704820394515991.\n",
      "Epoch 4560. Training loss: 0.7613779902458191. Validation loss: 2.704820156097412.\n",
      "Epoch 4561. Training loss: 0.7613736987113953. Validation loss: 2.7048192024230957.\n",
      "Epoch 4562. Training loss: 0.7613692879676819. Validation loss: 2.7048187255859375.\n",
      "Epoch 4563. Training loss: 0.7613649368286133. Validation loss: 2.7048182487487793.\n",
      "Epoch 4564. Training loss: 0.7613604664802551. Validation loss: 2.704817533493042.\n",
      "Epoch 4565. Training loss: 0.7613560557365417. Validation loss: 2.704817295074463.\n",
      "Epoch 4566. Training loss: 0.7613518238067627. Validation loss: 2.7048168182373047.\n",
      "Epoch 4567. Training loss: 0.7613474726676941. Validation loss: 2.7048163414001465.\n",
      "Epoch 4568. Training loss: 0.7613430619239807. Validation loss: 2.704815626144409.\n",
      "Epoch 4569. Training loss: 0.7613385319709778. Validation loss: 2.704814910888672.\n",
      "Epoch 4570. Training loss: 0.761334240436554. Validation loss: 2.7048146724700928.\n",
      "Epoch 4571. Training loss: 0.7613298892974854. Validation loss: 2.7048141956329346.\n",
      "Epoch 4572. Training loss: 0.7613255381584167. Validation loss: 2.7048134803771973.\n",
      "Epoch 4573. Training loss: 0.7613210678100586. Validation loss: 2.70481276512146.\n",
      "Epoch 4574. Training loss: 0.7613167762756348. Validation loss: 2.704812526702881.\n",
      "Epoch 4575. Training loss: 0.7613124251365662. Validation loss: 2.7048120498657227.\n",
      "Epoch 4576. Training loss: 0.761307954788208. Validation loss: 2.7048115730285645.\n",
      "Epoch 4577. Training loss: 0.7613036036491394. Validation loss: 2.7048110961914062.\n",
      "Epoch 4578. Training loss: 0.7612991333007812. Validation loss: 2.70481014251709.\n",
      "Epoch 4579. Training loss: 0.7612948417663574. Validation loss: 2.7048094272613525.\n",
      "Epoch 4580. Training loss: 0.7612903714179993. Validation loss: 2.7048091888427734.\n",
      "Epoch 4581. Training loss: 0.7612860202789307. Validation loss: 2.704808473587036.\n",
      "Epoch 4582. Training loss: 0.7612816691398621. Validation loss: 2.704808235168457.\n",
      "Epoch 4583. Training loss: 0.7612772583961487. Validation loss: 2.704807758331299.\n",
      "Epoch 4584. Training loss: 0.7612729072570801. Validation loss: 2.7048070430755615.\n",
      "Epoch 4585. Training loss: 0.7612684369087219. Validation loss: 2.704806327819824.\n",
      "Epoch 4586. Training loss: 0.7612640261650085. Validation loss: 2.704806089401245.\n",
      "Epoch 4587. Training loss: 0.7612597346305847. Validation loss: 2.7048051357269287.\n",
      "Epoch 4588. Training loss: 0.7612552642822266. Validation loss: 2.7048048973083496.\n",
      "Epoch 4589. Training loss: 0.761250913143158. Validation loss: 2.7048044204711914.\n",
      "Epoch 4590. Training loss: 0.7612465023994446. Validation loss: 2.704803705215454.\n",
      "Epoch 4591. Training loss: 0.7612420916557312. Validation loss: 2.704803466796875.\n",
      "Epoch 4592. Training loss: 0.7612376809120178. Validation loss: 2.7048025131225586.\n",
      "Epoch 4593. Training loss: 0.7612333297729492. Validation loss: 2.7048025131225586.\n",
      "Epoch 4594. Training loss: 0.7612289786338806. Validation loss: 2.704801559448242.\n",
      "Epoch 4595. Training loss: 0.7612244486808777. Validation loss: 2.704801082611084.\n",
      "Epoch 4596. Training loss: 0.7612200379371643. Validation loss: 2.7048003673553467.\n",
      "Epoch 4597. Training loss: 0.7612156867980957. Validation loss: 2.7047998905181885.\n",
      "Epoch 4598. Training loss: 0.7612113356590271. Validation loss: 2.7047994136810303.\n",
      "Epoch 4599. Training loss: 0.761206865310669. Validation loss: 2.704799175262451.\n",
      "Epoch 4600. Training loss: 0.7612025141716003. Validation loss: 2.704798460006714.\n",
      "Epoch 4601. Training loss: 0.761198103427887. Validation loss: 2.7047979831695557.\n",
      "Epoch 4602. Training loss: 0.7611936926841736. Validation loss: 2.7047972679138184.\n",
      "Epoch 4603. Training loss: 0.7611892819404602. Validation loss: 2.70479679107666.\n",
      "Epoch 4604. Training loss: 0.7611848711967468. Validation loss: 2.704796552658081.\n",
      "Epoch 4605. Training loss: 0.7611804604530334. Validation loss: 2.7047958374023438.\n",
      "Epoch 4606. Training loss: 0.7611761093139648. Validation loss: 2.7047951221466064.\n",
      "Epoch 4607. Training loss: 0.7611716389656067. Validation loss: 2.7047948837280273.\n",
      "Epoch 4608. Training loss: 0.7611672282218933. Validation loss: 2.704793930053711.\n",
      "Epoch 4609. Training loss: 0.7611627578735352. Validation loss: 2.7047934532165527.\n",
      "Epoch 4610. Training loss: 0.7611584067344666. Validation loss: 2.7047929763793945.\n",
      "Epoch 4611. Training loss: 0.7611539959907532. Validation loss: 2.7047922611236572.\n",
      "Epoch 4612. Training loss: 0.7611495852470398. Validation loss: 2.704792022705078.\n",
      "Epoch 4613. Training loss: 0.7611451745033264. Validation loss: 2.704791307449341.\n",
      "Epoch 4614. Training loss: 0.761140763759613. Validation loss: 2.7047905921936035.\n",
      "Epoch 4615. Training loss: 0.7611362934112549. Validation loss: 2.7047903537750244.\n",
      "Epoch 4616. Training loss: 0.7611319422721863. Validation loss: 2.704789638519287.\n",
      "Epoch 4617. Training loss: 0.7611275315284729. Validation loss: 2.704789161682129.\n",
      "Epoch 4618. Training loss: 0.76112300157547. Validation loss: 2.70478892326355.\n",
      "Epoch 4619. Training loss: 0.7611186504364014. Validation loss: 2.7047882080078125.\n",
      "Epoch 4620. Training loss: 0.7611141800880432. Validation loss: 2.7047877311706543.\n",
      "Epoch 4621. Training loss: 0.7611098289489746. Validation loss: 2.704787015914917.\n",
      "Epoch 4622. Training loss: 0.7611053586006165. Validation loss: 2.704786777496338.\n",
      "Epoch 4623. Training loss: 0.7611009478569031. Validation loss: 2.7047860622406006.\n",
      "Epoch 4624. Training loss: 0.7610965371131897. Validation loss: 2.7047858238220215.\n",
      "Epoch 4625. Training loss: 0.7610921859741211. Validation loss: 2.704784870147705.\n",
      "Epoch 4626. Training loss: 0.7610877156257629. Validation loss: 2.704784393310547.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4627. Training loss: 0.7610833048820496. Validation loss: 2.7047841548919678.\n",
      "Epoch 4628. Training loss: 0.7610788345336914. Validation loss: 2.7047836780548096.\n",
      "Epoch 4629. Training loss: 0.7610744833946228. Validation loss: 2.7047829627990723.\n",
      "Epoch 4630. Training loss: 0.7610699534416199. Validation loss: 2.704782247543335.\n",
      "Epoch 4631. Training loss: 0.7610655426979065. Validation loss: 2.704782009124756.\n",
      "Epoch 4632. Training loss: 0.7610610127449036. Validation loss: 2.7047812938690186.\n",
      "Epoch 4633. Training loss: 0.761056661605835. Validation loss: 2.7047808170318604.\n",
      "Epoch 4634. Training loss: 0.7610523104667664. Validation loss: 2.704780101776123.\n",
      "Epoch 4635. Training loss: 0.7610478401184082. Validation loss: 2.704779624938965.\n",
      "Epoch 4636. Training loss: 0.76104336977005. Validation loss: 2.7047793865203857.\n",
      "Epoch 4637. Training loss: 0.7610389590263367. Validation loss: 2.7047786712646484.\n",
      "Epoch 4638. Training loss: 0.7610344886779785. Validation loss: 2.7047781944274902.\n",
      "Epoch 4639. Training loss: 0.7610300183296204. Validation loss: 2.704777717590332.\n",
      "Epoch 4640. Training loss: 0.7610257267951965. Validation loss: 2.704777240753174.\n",
      "Epoch 4641. Training loss: 0.7610211968421936. Validation loss: 2.7047765254974365.\n",
      "Epoch 4642. Training loss: 0.7610167860984802. Validation loss: 2.7047760486602783.\n",
      "Epoch 4643. Training loss: 0.7610123157501221. Validation loss: 2.70477557182312.\n",
      "Epoch 4644. Training loss: 0.7610078454017639. Validation loss: 2.704774856567383.\n",
      "Epoch 4645. Training loss: 0.7610034942626953. Validation loss: 2.7047743797302246.\n",
      "Epoch 4646. Training loss: 0.7609990239143372. Validation loss: 2.7047736644744873.\n",
      "Epoch 4647. Training loss: 0.7609946131706238. Validation loss: 2.704773187637329.\n",
      "Epoch 4648. Training loss: 0.7609901428222656. Validation loss: 2.70477294921875.\n",
      "Epoch 4649. Training loss: 0.7609856724739075. Validation loss: 2.7047719955444336.\n",
      "Epoch 4650. Training loss: 0.7609812617301941. Validation loss: 2.7047715187072754.\n",
      "Epoch 4651. Training loss: 0.7609767317771912. Validation loss: 2.704770803451538.\n",
      "Epoch 4652. Training loss: 0.7609723210334778. Validation loss: 2.704770803451538.\n",
      "Epoch 4653. Training loss: 0.7609679102897644. Validation loss: 2.704770088195801.\n",
      "Epoch 4654. Training loss: 0.7609634399414062. Validation loss: 2.7047696113586426.\n",
      "Epoch 4655. Training loss: 0.7609589695930481. Validation loss: 2.7047688961029053.\n",
      "Epoch 4656. Training loss: 0.7609545588493347. Validation loss: 2.704768419265747.\n",
      "Epoch 4657. Training loss: 0.7609500885009766. Validation loss: 2.7047677040100098.\n",
      "Epoch 4658. Training loss: 0.7609456181526184. Validation loss: 2.7047674655914307.\n",
      "Epoch 4659. Training loss: 0.7609410881996155. Validation loss: 2.7047667503356934.\n",
      "Epoch 4660. Training loss: 0.7609367370605469. Validation loss: 2.7047667503356934.\n",
      "Epoch 4661. Training loss: 0.760932207107544. Validation loss: 2.7047653198242188.\n",
      "Epoch 4662. Training loss: 0.7609277367591858. Validation loss: 2.7047650814056396.\n",
      "Epoch 4663. Training loss: 0.7609233260154724. Validation loss: 2.7047646045684814.\n",
      "Epoch 4664. Training loss: 0.760918915271759. Validation loss: 2.7047641277313232.\n",
      "Epoch 4665. Training loss: 0.7609143853187561. Validation loss: 2.704763412475586.\n",
      "Epoch 4666. Training loss: 0.7609099745750427. Validation loss: 2.7047629356384277.\n",
      "Epoch 4667. Training loss: 0.7609055042266846. Validation loss: 2.7047624588012695.\n",
      "Epoch 4668. Training loss: 0.7609010338783264. Validation loss: 2.7047619819641113.\n",
      "Epoch 4669. Training loss: 0.760896623134613. Validation loss: 2.704761505126953.\n",
      "Epoch 4670. Training loss: 0.7608920931816101. Validation loss: 2.7047605514526367.\n",
      "Epoch 4671. Training loss: 0.760887622833252. Validation loss: 2.7047600746154785.\n",
      "Epoch 4672. Training loss: 0.7608831524848938. Validation loss: 2.7047595977783203.\n",
      "Epoch 4673. Training loss: 0.7608787417411804. Validation loss: 2.704758882522583.\n",
      "Epoch 4674. Training loss: 0.7608740925788879. Validation loss: 2.704758644104004.\n",
      "Epoch 4675. Training loss: 0.7608697414398193. Validation loss: 2.7047576904296875.\n",
      "Epoch 4676. Training loss: 0.7608652114868164. Validation loss: 2.7047576904296875.\n",
      "Epoch 4677. Training loss: 0.7608609199523926. Validation loss: 2.7047572135925293.\n",
      "Epoch 4678. Training loss: 0.7608563303947449. Validation loss: 2.704756498336792.\n",
      "Epoch 4679. Training loss: 0.7608518600463867. Validation loss: 2.704756021499634.\n",
      "Epoch 4680. Training loss: 0.7608473300933838. Validation loss: 2.7047555446624756.\n",
      "Epoch 4681. Training loss: 0.7608428597450256. Validation loss: 2.7047548294067383.\n",
      "Epoch 4682. Training loss: 0.7608384490013123. Validation loss: 2.70475435256958.\n",
      "Epoch 4683. Training loss: 0.7608339190483093. Validation loss: 2.704753875732422.\n",
      "Epoch 4684. Training loss: 0.760829508304596. Validation loss: 2.7047531604766846.\n",
      "Epoch 4685. Training loss: 0.7608250975608826. Validation loss: 2.7047524452209473.\n",
      "Epoch 4686. Training loss: 0.7608205676078796. Validation loss: 2.704751968383789.\n",
      "Epoch 4687. Training loss: 0.7608160376548767. Validation loss: 2.7047512531280518.\n",
      "Epoch 4688. Training loss: 0.7608115077018738. Validation loss: 2.7047507762908936.\n",
      "Epoch 4689. Training loss: 0.7608070373535156. Validation loss: 2.7047505378723145.\n",
      "Epoch 4690. Training loss: 0.7608025670051575. Validation loss: 2.7047500610351562.\n",
      "Epoch 4691. Training loss: 0.7607980370521545. Validation loss: 2.704749584197998.\n",
      "Epoch 4692. Training loss: 0.7607936263084412. Validation loss: 2.7047488689422607.\n",
      "Epoch 4693. Training loss: 0.760789155960083. Validation loss: 2.7047483921051025.\n",
      "Epoch 4694. Training loss: 0.7607846260070801. Validation loss: 2.704747438430786.\n",
      "Epoch 4695. Training loss: 0.7607801556587219. Validation loss: 2.704747200012207.\n",
      "Epoch 4696. Training loss: 0.7607755661010742. Validation loss: 2.704746723175049.\n",
      "Epoch 4697. Training loss: 0.7607712149620056. Validation loss: 2.7047460079193115.\n",
      "Epoch 4698. Training loss: 0.7607666850090027. Validation loss: 2.7047455310821533.\n",
      "Epoch 4699. Training loss: 0.7607622146606445. Validation loss: 2.704745054244995.\n",
      "Epoch 4700. Training loss: 0.7607576847076416. Validation loss: 2.704744577407837.\n",
      "Epoch 4701. Training loss: 0.7607531547546387. Validation loss: 2.7047438621520996.\n",
      "Epoch 4702. Training loss: 0.7607486844062805. Validation loss: 2.7047431468963623.\n",
      "Epoch 4703. Training loss: 0.7607441544532776. Validation loss: 2.7047431468963623.\n",
      "Epoch 4704. Training loss: 0.7607397437095642. Validation loss: 2.704742193222046.\n",
      "Epoch 4705. Training loss: 0.7607352137565613. Validation loss: 2.7047417163848877.\n",
      "Epoch 4706. Training loss: 0.7607307434082031. Validation loss: 2.7047410011291504.\n",
      "Epoch 4707. Training loss: 0.760726273059845. Validation loss: 2.704740524291992.\n",
      "Epoch 4708. Training loss: 0.7607216835021973. Validation loss: 2.704740047454834.\n",
      "Epoch 4709. Training loss: 0.7607172131538391. Validation loss: 2.704739570617676.\n",
      "Epoch 4710. Training loss: 0.7607126832008362. Validation loss: 2.7047390937805176.\n",
      "Epoch 4711. Training loss: 0.7607081532478333. Validation loss: 2.7047386169433594.\n",
      "Epoch 4712. Training loss: 0.7607037425041199. Validation loss: 2.704738140106201.\n",
      "Epoch 4713. Training loss: 0.7606992125511169. Validation loss: 2.704737424850464.\n",
      "Epoch 4714. Training loss: 0.760694682598114. Validation loss: 2.7047369480133057.\n",
      "Epoch 4715. Training loss: 0.7606902122497559. Validation loss: 2.7047364711761475.\n",
      "Epoch 4716. Training loss: 0.7606856822967529. Validation loss: 2.70473575592041.\n",
      "Epoch 4717. Training loss: 0.7606810927391052. Validation loss: 2.704735279083252.\n",
      "Epoch 4718. Training loss: 0.7606766819953918. Validation loss: 2.7047348022460938.\n",
      "Epoch 4719. Training loss: 0.7606721520423889. Validation loss: 2.7047343254089355.\n",
      "Epoch 4720. Training loss: 0.760667622089386. Validation loss: 2.7047338485717773.\n",
      "Epoch 4721. Training loss: 0.7606630921363831. Validation loss: 2.704733371734619.\n",
      "Epoch 4722. Training loss: 0.7606586813926697. Validation loss: 2.704732894897461.\n",
      "Epoch 4723. Training loss: 0.7606540322303772. Validation loss: 2.7047321796417236.\n",
      "Epoch 4724. Training loss: 0.7606496214866638. Validation loss: 2.7047317028045654.\n",
      "Epoch 4725. Training loss: 0.7606450915336609. Validation loss: 2.7047312259674072.\n",
      "Epoch 4726. Training loss: 0.760640561580658. Validation loss: 2.704730749130249.\n",
      "Epoch 4727. Training loss: 0.7606360912322998. Validation loss: 2.7047300338745117.\n",
      "Epoch 4728. Training loss: 0.7606315612792969. Validation loss: 2.7047293186187744.\n",
      "Epoch 4729. Training loss: 0.7606269717216492. Validation loss: 2.7047290802001953.\n",
      "Epoch 4730. Training loss: 0.760622501373291. Validation loss: 2.704728126525879.\n",
      "Epoch 4731. Training loss: 0.7606179118156433. Validation loss: 2.7047276496887207.\n",
      "Epoch 4732. Training loss: 0.7606134414672852. Validation loss: 2.7047274112701416.\n",
      "Epoch 4733. Training loss: 0.7606089115142822. Validation loss: 2.704726457595825.\n",
      "Epoch 4734. Training loss: 0.7606043815612793. Validation loss: 2.704726219177246.\n",
      "Epoch 4735. Training loss: 0.7605999112129211. Validation loss: 2.704725503921509.\n",
      "Epoch 4736. Training loss: 0.7605953216552734. Validation loss: 2.7047250270843506.\n",
      "Epoch 4737. Training loss: 0.7605908513069153. Validation loss: 2.7047245502471924.\n",
      "Epoch 4738. Training loss: 0.7605863213539124. Validation loss: 2.704724073410034.\n",
      "Epoch 4739. Training loss: 0.7605816721916199. Validation loss: 2.704723596572876.\n",
      "Epoch 4740. Training loss: 0.7605772018432617. Validation loss: 2.7047228813171387.\n",
      "Epoch 4741. Training loss: 0.7605726718902588. Validation loss: 2.7047224044799805.\n",
      "Epoch 4742. Training loss: 0.7605682015419006. Validation loss: 2.704721689224243.\n",
      "Epoch 4743. Training loss: 0.7605636119842529. Validation loss: 2.704720973968506.\n",
      "Epoch 4744. Training loss: 0.76055908203125. Validation loss: 2.7047207355499268.\n",
      "Epoch 4745. Training loss: 0.7605546116828918. Validation loss: 2.7047202587127686.\n",
      "Epoch 4746. Training loss: 0.7605500221252441. Validation loss: 2.7047197818756104.\n",
      "Epoch 4747. Training loss: 0.760545551776886. Validation loss: 2.704719066619873.\n",
      "Epoch 4748. Training loss: 0.7605409026145935. Validation loss: 2.704718589782715.\n",
      "Epoch 4749. Training loss: 0.7605364322662354. Validation loss: 2.7047181129455566.\n",
      "Epoch 4750. Training loss: 0.7605319023132324. Validation loss: 2.7047173976898193.\n",
      "Epoch 4751. Training loss: 0.7605273127555847. Validation loss: 2.704716920852661.\n",
      "Epoch 4752. Training loss: 0.7605228424072266. Validation loss: 2.704716205596924.\n",
      "Epoch 4753. Training loss: 0.7605182528495789. Validation loss: 2.7047157287597656.\n",
      "Epoch 4754. Training loss: 0.7605137825012207. Validation loss: 2.7047152519226074.\n",
      "Epoch 4755. Training loss: 0.7605092525482178. Validation loss: 2.704714775085449.\n",
      "Epoch 4756. Training loss: 0.7605047225952148. Validation loss: 2.704714298248291.\n",
      "Epoch 4757. Training loss: 0.7605001330375671. Validation loss: 2.7047133445739746.\n",
      "Epoch 4758. Training loss: 0.760495662689209. Validation loss: 2.7047133445739746.\n",
      "Epoch 4759. Training loss: 0.7604910731315613. Validation loss: 2.704712390899658.\n",
      "Epoch 4760. Training loss: 0.7604865431785583. Validation loss: 2.704712152481079.\n",
      "Epoch 4761. Training loss: 0.7604820132255554. Validation loss: 2.704711437225342.\n",
      "Epoch 4762. Training loss: 0.7604773640632629. Validation loss: 2.7047109603881836.\n",
      "Epoch 4763. Training loss: 0.76047283411026. Validation loss: 2.7047104835510254.\n",
      "Epoch 4764. Training loss: 0.7604682445526123. Validation loss: 2.704709768295288.\n",
      "Epoch 4765. Training loss: 0.7604637742042542. Validation loss: 2.70470929145813.\n",
      "Epoch 4766. Training loss: 0.7604591846466064. Validation loss: 2.7047088146209717.\n",
      "Epoch 4767. Training loss: 0.7604547142982483. Validation loss: 2.7047080993652344.\n",
      "Epoch 4768. Training loss: 0.7604501247406006. Validation loss: 2.704707145690918.\n",
      "Epoch 4769. Training loss: 0.7604455351829529. Validation loss: 2.704706907272339.\n",
      "Epoch 4770. Training loss: 0.76044100522995. Validation loss: 2.7047064304351807.\n",
      "Epoch 4771. Training loss: 0.760436475276947. Validation loss: 2.7047061920166016.\n",
      "Epoch 4772. Training loss: 0.7604319453239441. Validation loss: 2.704705238342285.\n",
      "Epoch 4773. Training loss: 0.7604272961616516. Validation loss: 2.704704761505127.\n",
      "Epoch 4774. Training loss: 0.7604227662086487. Validation loss: 2.7047042846679688.\n",
      "Epoch 4775. Training loss: 0.7604183554649353. Validation loss: 2.7047035694122314.\n",
      "Epoch 4776. Training loss: 0.760413646697998. Validation loss: 2.7047030925750732.\n",
      "Epoch 4777. Training loss: 0.7604091167449951. Validation loss: 2.704702615737915.\n",
      "Epoch 4778. Training loss: 0.7604045271873474. Validation loss: 2.704702138900757.\n",
      "Epoch 4779. Training loss: 0.7603999972343445. Validation loss: 2.7047014236450195.\n",
      "Epoch 4780. Training loss: 0.760395348072052. Validation loss: 2.7047009468078613.\n",
      "Epoch 4781. Training loss: 0.7603909373283386. Validation loss: 2.7047007083892822.\n",
      "Epoch 4782. Training loss: 0.7603862285614014. Validation loss: 2.704699993133545.\n",
      "Epoch 4783. Training loss: 0.7603816986083984. Validation loss: 2.7046995162963867.\n",
      "Epoch 4784. Training loss: 0.7603771686553955. Validation loss: 2.7046988010406494.\n",
      "Epoch 4785. Training loss: 0.7603725790977478. Validation loss: 2.704698324203491.\n",
      "Epoch 4786. Training loss: 0.7603681087493896. Validation loss: 2.704697608947754.\n",
      "Epoch 4787. Training loss: 0.7603633999824524. Validation loss: 2.704697370529175.\n",
      "Epoch 4788. Training loss: 0.7603588104248047. Validation loss: 2.7046966552734375.\n",
      "Epoch 4789. Training loss: 0.7603543400764465. Validation loss: 2.7046961784362793.\n",
      "Epoch 4790. Training loss: 0.7603497505187988. Validation loss: 2.704695463180542.\n",
      "Epoch 4791. Training loss: 0.7603452205657959. Validation loss: 2.704695224761963.\n",
      "Epoch 4792. Training loss: 0.7603406310081482. Validation loss: 2.7046947479248047.\n",
      "Epoch 4793. Training loss: 0.7603361010551453. Validation loss: 2.7046937942504883.\n",
      "Epoch 4794. Training loss: 0.7603314518928528. Validation loss: 2.704693078994751.\n",
      "Epoch 4795. Training loss: 0.7603268623352051. Validation loss: 2.704693078994751.\n",
      "Epoch 4796. Training loss: 0.7603223323822021. Validation loss: 2.7046923637390137.\n",
      "Epoch 4797. Training loss: 0.7603177428245544. Validation loss: 2.7046916484832764.\n",
      "Epoch 4798. Training loss: 0.760313093662262. Validation loss: 2.704690933227539.\n",
      "Epoch 4799. Training loss: 0.760308563709259. Validation loss: 2.704690933227539.\n",
      "Epoch 4800. Training loss: 0.7603039741516113. Validation loss: 2.7046899795532227.\n",
      "Epoch 4801. Training loss: 0.7602993845939636. Validation loss: 2.7046892642974854.\n",
      "Epoch 4802. Training loss: 0.7602947354316711. Validation loss: 2.7046890258789062.\n",
      "Epoch 4803. Training loss: 0.7602901458740234. Validation loss: 2.70468807220459.\n",
      "Epoch 4804. Training loss: 0.7602856159210205. Validation loss: 2.7046878337860107.\n",
      "Epoch 4805. Training loss: 0.7602810859680176. Validation loss: 2.7046873569488525.\n",
      "Epoch 4806. Training loss: 0.7602764964103699. Validation loss: 2.7046866416931152.\n",
      "Epoch 4807. Training loss: 0.7602719664573669. Validation loss: 2.704686403274536.\n",
      "Epoch 4808. Training loss: 0.7602673172950745. Validation loss: 2.704685688018799.\n",
      "Epoch 4809. Training loss: 0.760262668132782. Validation loss: 2.7046847343444824.\n",
      "Epoch 4810. Training loss: 0.760258138179779. Validation loss: 2.704684257507324.\n",
      "Epoch 4811. Training loss: 0.7602536082267761. Validation loss: 2.704684019088745.\n",
      "Epoch 4812. Training loss: 0.7602489590644836. Validation loss: 2.704683303833008.\n",
      "Epoch 4813. Training loss: 0.7602443695068359. Validation loss: 2.7046828269958496.\n",
      "Epoch 4814. Training loss: 0.7602397799491882. Validation loss: 2.7046823501586914.\n",
      "Epoch 4815. Training loss: 0.7602351307868958. Validation loss: 2.704681634902954.\n",
      "Epoch 4816. Training loss: 0.760230541229248. Validation loss: 2.704681396484375.\n",
      "Epoch 4817. Training loss: 0.7602260112762451. Validation loss: 2.704680919647217.\n",
      "Epoch 4818. Training loss: 0.7602214217185974. Validation loss: 2.7046799659729004.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4819. Training loss: 0.7602168917655945. Validation loss: 2.704679489135742.\n",
      "Epoch 4820. Training loss: 0.7602121829986572. Validation loss: 2.704678773880005.\n",
      "Epoch 4821. Training loss: 0.7602075934410095. Validation loss: 2.7046782970428467.\n",
      "Epoch 4822. Training loss: 0.760202944278717. Validation loss: 2.7046782970428467.\n",
      "Epoch 4823. Training loss: 0.7601983547210693. Validation loss: 2.7046773433685303.\n",
      "Epoch 4824. Training loss: 0.7601938247680664. Validation loss: 2.704677104949951.\n",
      "Epoch 4825. Training loss: 0.7601892352104187. Validation loss: 2.704676389694214.\n",
      "Epoch 4826. Training loss: 0.7601845860481262. Validation loss: 2.7046754360198975.\n",
      "Epoch 4827. Training loss: 0.7601799964904785. Validation loss: 2.7046754360198975.\n",
      "Epoch 4828. Training loss: 0.7601754069328308. Validation loss: 2.7046749591827393.\n",
      "Epoch 4829. Training loss: 0.7601707577705383. Validation loss: 2.7046737670898438.\n",
      "Epoch 4830. Training loss: 0.7601659893989563. Validation loss: 2.7046737670898438.\n",
      "Epoch 4831. Training loss: 0.7601615786552429. Validation loss: 2.7046728134155273.\n",
      "Epoch 4832. Training loss: 0.7601569294929504. Validation loss: 2.7046725749969482.\n",
      "Epoch 4833. Training loss: 0.7601523399353027. Validation loss: 2.704671859741211.\n",
      "Epoch 4834. Training loss: 0.760147750377655. Validation loss: 2.7046713829040527.\n",
      "Epoch 4835. Training loss: 0.7601430416107178. Validation loss: 2.7046704292297363.\n",
      "Epoch 4836. Training loss: 0.7601385116577148. Validation loss: 2.7046706676483154.\n",
      "Epoch 4837. Training loss: 0.7601339221000671. Validation loss: 2.70466947555542.\n",
      "Epoch 4838. Training loss: 0.7601292133331299. Validation loss: 2.7046689987182617.\n",
      "Epoch 4839. Training loss: 0.760124683380127. Validation loss: 2.7046685218811035.\n",
      "Epoch 4840. Training loss: 0.7601200938224792. Validation loss: 2.704667806625366.\n",
      "Epoch 4841. Training loss: 0.760115385055542. Validation loss: 2.704667329788208.\n",
      "Epoch 4842. Training loss: 0.7601107954978943. Validation loss: 2.704667091369629.\n",
      "Epoch 4843. Training loss: 0.7601062655448914. Validation loss: 2.7046663761138916.\n",
      "Epoch 4844. Training loss: 0.7601014971733093. Validation loss: 2.7046656608581543.\n",
      "Epoch 4845. Training loss: 0.7600969672203064. Validation loss: 2.704664945602417.\n",
      "Epoch 4846. Training loss: 0.7600923180580139. Validation loss: 2.704664468765259.\n",
      "Epoch 4847. Training loss: 0.7600877285003662. Validation loss: 2.7046642303466797.\n",
      "Epoch 4848. Training loss: 0.760083019733429. Validation loss: 2.7046632766723633.\n",
      "Epoch 4849. Training loss: 0.7600784301757812. Validation loss: 2.7046632766723633.\n",
      "Epoch 4850. Training loss: 0.7600738406181335. Validation loss: 2.704662322998047.\n",
      "Epoch 4851. Training loss: 0.7600691914558411. Validation loss: 2.7046618461608887.\n",
      "Epoch 4852. Training loss: 0.7600646018981934. Validation loss: 2.7046613693237305.\n",
      "Epoch 4853. Training loss: 0.7600600123405457. Validation loss: 2.7046608924865723.\n",
      "Epoch 4854. Training loss: 0.7600553631782532. Validation loss: 2.704660177230835.\n",
      "Epoch 4855. Training loss: 0.7600507140159607. Validation loss: 2.7046597003936768.\n",
      "Epoch 4856. Training loss: 0.7600460648536682. Validation loss: 2.7046589851379395.\n",
      "Epoch 4857. Training loss: 0.7600414156913757. Validation loss: 2.7046587467193604.\n",
      "Epoch 4858. Training loss: 0.7600367665290833. Validation loss: 2.704658031463623.\n",
      "Epoch 4859. Training loss: 0.7600321769714355. Validation loss: 2.7046570777893066.\n",
      "Epoch 4860. Training loss: 0.7600275874137878. Validation loss: 2.7046570777893066.\n",
      "Epoch 4861. Training loss: 0.7600229382514954. Validation loss: 2.7046561241149902.\n",
      "Epoch 4862. Training loss: 0.7600181698799133. Validation loss: 2.7046561241149902.\n",
      "Epoch 4863. Training loss: 0.7600136399269104. Validation loss: 2.704655408859253.\n",
      "Epoch 4864. Training loss: 0.7600089907646179. Validation loss: 2.7046546936035156.\n",
      "Epoch 4865. Training loss: 0.7600043416023254. Validation loss: 2.7046542167663574.\n",
      "Epoch 4866. Training loss: 0.759999692440033. Validation loss: 2.704653739929199.\n",
      "Epoch 4867. Training loss: 0.7599950432777405. Validation loss: 2.704653024673462.\n",
      "Epoch 4868. Training loss: 0.7599904537200928. Validation loss: 2.7046525478363037.\n",
      "Epoch 4869. Training loss: 0.7599857449531555. Validation loss: 2.7046520709991455.\n",
      "Epoch 4870. Training loss: 0.7599812150001526. Validation loss: 2.704651355743408.\n",
      "Epoch 4871. Training loss: 0.7599764466285706. Validation loss: 2.704650640487671.\n",
      "Epoch 4872. Training loss: 0.7599719166755676. Validation loss: 2.7046501636505127.\n",
      "Epoch 4873. Training loss: 0.7599671483039856. Validation loss: 2.7046496868133545.\n",
      "Epoch 4874. Training loss: 0.7599625587463379. Validation loss: 2.704648971557617.\n",
      "Epoch 4875. Training loss: 0.7599579691886902. Validation loss: 2.704648494720459.\n",
      "Epoch 4876. Training loss: 0.7599533200263977. Validation loss: 2.704648017883301.\n",
      "Epoch 4877. Training loss: 0.7599485516548157. Validation loss: 2.7046477794647217.\n",
      "Epoch 4878. Training loss: 0.7599440217018127. Validation loss: 2.7046470642089844.\n",
      "Epoch 4879. Training loss: 0.7599393725395203. Validation loss: 2.704646110534668.\n",
      "Epoch 4880. Training loss: 0.759934663772583. Validation loss: 2.7046456336975098.\n",
      "Epoch 4881. Training loss: 0.7599300742149353. Validation loss: 2.7046451568603516.\n",
      "Epoch 4882. Training loss: 0.7599254250526428. Validation loss: 2.7046446800231934.\n",
      "Epoch 4883. Training loss: 0.7599207758903503. Validation loss: 2.704644203186035.\n",
      "Epoch 4884. Training loss: 0.7599160671234131. Validation loss: 2.704643726348877.\n",
      "Epoch 4885. Training loss: 0.7599113583564758. Validation loss: 2.7046432495117188.\n",
      "Epoch 4886. Training loss: 0.7599067687988281. Validation loss: 2.7046425342559814.\n",
      "Epoch 4887. Training loss: 0.7599021792411804. Validation loss: 2.7046420574188232.\n",
      "Epoch 4888. Training loss: 0.7598974704742432. Validation loss: 2.704641342163086.\n",
      "Epoch 4889. Training loss: 0.7598927617073059. Validation loss: 2.7046408653259277.\n",
      "Epoch 4890. Training loss: 0.7598881721496582. Validation loss: 2.7046403884887695.\n",
      "Epoch 4891. Training loss: 0.7598835825920105. Validation loss: 2.7046396732330322.\n",
      "Epoch 4892. Training loss: 0.7598788142204285. Validation loss: 2.704639196395874.\n",
      "Epoch 4893. Training loss: 0.7598741054534912. Validation loss: 2.704638719558716.\n",
      "Epoch 4894. Training loss: 0.7598695158958435. Validation loss: 2.7046380043029785.\n",
      "Epoch 4895. Training loss: 0.7598648071289062. Validation loss: 2.7046375274658203.\n",
      "Epoch 4896. Training loss: 0.7598602175712585. Validation loss: 2.704636812210083.\n",
      "Epoch 4897. Training loss: 0.7598554491996765. Validation loss: 2.704636335372925.\n",
      "Epoch 4898. Training loss: 0.759850800037384. Validation loss: 2.7046358585357666.\n",
      "Epoch 4899. Training loss: 0.7598462104797363. Validation loss: 2.7046353816986084.\n",
      "Epoch 4900. Training loss: 0.7598415017127991. Validation loss: 2.704634428024292.\n",
      "Epoch 4901. Training loss: 0.7598368525505066. Validation loss: 2.704634428024292.\n",
      "Epoch 4902. Training loss: 0.7598321437835693. Validation loss: 2.7046337127685547.\n",
      "Epoch 4903. Training loss: 0.7598274350166321. Validation loss: 2.7046332359313965.\n",
      "Epoch 4904. Training loss: 0.7598227858543396. Validation loss: 2.70463228225708.\n",
      "Epoch 4905. Training loss: 0.7598181366920471. Validation loss: 2.704631805419922.\n",
      "Epoch 4906. Training loss: 0.7598134875297546. Validation loss: 2.7046315670013428.\n",
      "Epoch 4907. Training loss: 0.7598087787628174. Validation loss: 2.7046308517456055.\n",
      "Epoch 4908. Training loss: 0.7598041892051697. Validation loss: 2.7046303749084473.\n",
      "Epoch 4909. Training loss: 0.7597994804382324. Validation loss: 2.704629898071289.\n",
      "Epoch 4910. Training loss: 0.7597947716712952. Validation loss: 2.704629421234131.\n",
      "Epoch 4911. Training loss: 0.7597901225090027. Validation loss: 2.7046284675598145.\n",
      "Epoch 4912. Training loss: 0.7597854137420654. Validation loss: 2.7046284675598145.\n",
      "Epoch 4913. Training loss: 0.7597807049751282. Validation loss: 2.704627752304077.\n",
      "Epoch 4914. Training loss: 0.7597760558128357. Validation loss: 2.70462703704834.\n",
      "Epoch 4915. Training loss: 0.7597713470458984. Validation loss: 2.7046263217926025.\n",
      "Epoch 4916. Training loss: 0.7597667574882507. Validation loss: 2.7046258449554443.\n",
      "Epoch 4917. Training loss: 0.7597619891166687. Validation loss: 2.704625368118286.\n",
      "Epoch 4918. Training loss: 0.7597573399543762. Validation loss: 2.704624891281128.\n",
      "Epoch 4919. Training loss: 0.7597526907920837. Validation loss: 2.7046239376068115.\n",
      "Epoch 4920. Training loss: 0.7597479820251465. Validation loss: 2.704623222351074.\n",
      "Epoch 4921. Training loss: 0.7597432732582092. Validation loss: 2.704623222351074.\n",
      "Epoch 4922. Training loss: 0.7597386240959167. Validation loss: 2.704622268676758.\n",
      "Epoch 4923. Training loss: 0.7597339153289795. Validation loss: 2.7046220302581787.\n",
      "Epoch 4924. Training loss: 0.7597292065620422. Validation loss: 2.7046215534210205.\n",
      "Epoch 4925. Training loss: 0.7597244381904602. Validation loss: 2.704620838165283.\n",
      "Epoch 4926. Training loss: 0.7597198486328125. Validation loss: 2.704620122909546.\n",
      "Epoch 4927. Training loss: 0.7597150802612305. Validation loss: 2.704619884490967.\n",
      "Epoch 4928. Training loss: 0.7597103714942932. Validation loss: 2.7046191692352295.\n",
      "Epoch 4929. Training loss: 0.7597057223320007. Validation loss: 2.7046189308166504.\n",
      "Epoch 4930. Training loss: 0.7597010731697083. Validation loss: 2.704618453979492.\n",
      "Epoch 4931. Training loss: 0.7596963047981262. Validation loss: 2.704617500305176.\n",
      "Epoch 4932. Training loss: 0.7596916556358337. Validation loss: 2.7046167850494385.\n",
      "Epoch 4933. Training loss: 0.7596869468688965. Validation loss: 2.7046163082122803.\n",
      "Epoch 4934. Training loss: 0.7596822381019592. Validation loss: 2.704615592956543.\n",
      "Epoch 4935. Training loss: 0.7596775889396667. Validation loss: 2.704615354537964.\n",
      "Epoch 4936. Training loss: 0.7596728801727295. Validation loss: 2.7046146392822266.\n",
      "Epoch 4937. Training loss: 0.7596681118011475. Validation loss: 2.7046144008636475.\n",
      "Epoch 4938. Training loss: 0.7596634030342102. Validation loss: 2.7046139240264893.\n",
      "Epoch 4939. Training loss: 0.7596587538719177. Validation loss: 2.7046127319335938.\n",
      "Epoch 4940. Training loss: 0.7596540451049805. Validation loss: 2.7046127319335938.\n",
      "Epoch 4941. Training loss: 0.7596492767333984. Validation loss: 2.7046120166778564.\n",
      "Epoch 4942. Training loss: 0.7596446871757507. Validation loss: 2.7046115398406982.\n",
      "Epoch 4943. Training loss: 0.7596399188041687. Validation loss: 2.704610824584961.\n",
      "Epoch 4944. Training loss: 0.7596352100372314. Validation loss: 2.7046103477478027.\n",
      "Epoch 4945. Training loss: 0.7596305012702942. Validation loss: 2.7046096324920654.\n",
      "Epoch 4946. Training loss: 0.7596257328987122. Validation loss: 2.7046093940734863.\n",
      "Epoch 4947. Training loss: 0.7596211433410645. Validation loss: 2.704608678817749.\n",
      "Epoch 4948. Training loss: 0.7596163153648376. Validation loss: 2.704608201980591.\n",
      "Epoch 4949. Training loss: 0.7596116065979004. Validation loss: 2.7046077251434326.\n",
      "Epoch 4950. Training loss: 0.7596070170402527. Validation loss: 2.704606771469116.\n",
      "Epoch 4951. Training loss: 0.7596022486686707. Validation loss: 2.704606056213379.\n",
      "Epoch 4952. Training loss: 0.7595975399017334. Validation loss: 2.7046058177948.\n",
      "Epoch 4953. Training loss: 0.7595927715301514. Validation loss: 2.7046053409576416.\n",
      "Epoch 4954. Training loss: 0.7595880031585693. Validation loss: 2.7046046257019043.\n",
      "Epoch 4955. Training loss: 0.7595834136009216. Validation loss: 2.704604148864746.\n",
      "Epoch 4956. Training loss: 0.7595786452293396. Validation loss: 2.704603433609009.\n",
      "Epoch 4957. Training loss: 0.7595739364624023. Validation loss: 2.7046031951904297.\n",
      "Epoch 4958. Training loss: 0.7595691680908203. Validation loss: 2.7046029567718506.\n",
      "Epoch 4959. Training loss: 0.7595645785331726. Validation loss: 2.7046022415161133.\n",
      "Epoch 4960. Training loss: 0.7595598101615906. Validation loss: 2.704601764678955.\n",
      "Epoch 4961. Training loss: 0.7595550417900085. Validation loss: 2.7046008110046387.\n",
      "Epoch 4962. Training loss: 0.7595503330230713. Validation loss: 2.7046003341674805.\n",
      "Epoch 4963. Training loss: 0.7595455646514893. Validation loss: 2.7045998573303223.\n",
      "Epoch 4964. Training loss: 0.759540855884552. Validation loss: 2.704599380493164.\n",
      "Epoch 4965. Training loss: 0.75953608751297. Validation loss: 2.7045984268188477.\n",
      "Epoch 4966. Training loss: 0.7595314979553223. Validation loss: 2.7045979499816895.\n",
      "Epoch 4967. Training loss: 0.7595267295837402. Validation loss: 2.7045974731445312.\n",
      "Epoch 4968. Training loss: 0.7595219612121582. Validation loss: 2.704596996307373.\n",
      "Epoch 4969. Training loss: 0.759517252445221. Validation loss: 2.7045960426330566.\n",
      "Epoch 4970. Training loss: 0.7595124840736389. Validation loss: 2.7045960426330566.\n",
      "Epoch 4971. Training loss: 0.7595078349113464. Validation loss: 2.7045953273773193.\n",
      "Epoch 4972. Training loss: 0.7595031261444092. Validation loss: 2.704594850540161.\n",
      "Epoch 4973. Training loss: 0.7594982981681824. Validation loss: 2.704594135284424.\n",
      "Epoch 4974. Training loss: 0.7594935894012451. Validation loss: 2.7045938968658447.\n",
      "Epoch 4975. Training loss: 0.7594888806343079. Validation loss: 2.7045931816101074.\n",
      "Epoch 4976. Training loss: 0.7594842314720154. Validation loss: 2.70459246635437.\n",
      "Epoch 4977. Training loss: 0.7594793438911438. Validation loss: 2.704591751098633.\n",
      "Epoch 4978. Training loss: 0.7594745755195618. Validation loss: 2.7045915126800537.\n",
      "Epoch 4979. Training loss: 0.7594699263572693. Validation loss: 2.7045907974243164.\n",
      "Epoch 4980. Training loss: 0.7594651579856873. Validation loss: 2.704590320587158.\n",
      "Epoch 4981. Training loss: 0.75946044921875. Validation loss: 2.70458984375.\n",
      "Epoch 4982. Training loss: 0.759455680847168. Validation loss: 2.7045888900756836.\n",
      "Epoch 4983. Training loss: 0.7594509720802307. Validation loss: 2.7045886516571045.\n",
      "Epoch 4984. Training loss: 0.7594462037086487. Validation loss: 2.704587936401367.\n",
      "Epoch 4985. Training loss: 0.7594414353370667. Validation loss: 2.704587936401367.\n",
      "Epoch 4986. Training loss: 0.7594367861747742. Validation loss: 2.7045867443084717.\n",
      "Epoch 4987. Training loss: 0.7594320178031921. Validation loss: 2.7045862674713135.\n",
      "Epoch 4988. Training loss: 0.7594272494316101. Validation loss: 2.7045860290527344.\n",
      "Epoch 4989. Training loss: 0.7594225406646729. Validation loss: 2.704585313796997.\n",
      "Epoch 4990. Training loss: 0.7594177722930908. Validation loss: 2.704584836959839.\n",
      "Epoch 4991. Training loss: 0.759412944316864. Validation loss: 2.7045841217041016.\n",
      "Epoch 4992. Training loss: 0.7594082355499268. Validation loss: 2.7045836448669434.\n",
      "Epoch 4993. Training loss: 0.7594034075737. Validation loss: 2.704583168029785.\n",
      "Epoch 4994. Training loss: 0.7593987584114075. Validation loss: 2.7045822143554688.\n",
      "Epoch 4995. Training loss: 0.7593939900398254. Validation loss: 2.7045817375183105.\n",
      "Epoch 4996. Training loss: 0.7593891620635986. Validation loss: 2.7045812606811523.\n",
      "Epoch 4997. Training loss: 0.7593844532966614. Validation loss: 2.704580783843994.\n",
      "Epoch 4998. Training loss: 0.7593796849250793. Validation loss: 2.704580307006836.\n",
      "Epoch 4999. Training loss: 0.7593749165534973. Validation loss: 2.7045795917510986.\n"
     ]
    }
   ],
   "source": [
    "train_model = True  # TODO hyperparameter\n",
    "load_pretrained_model = True  # TODO hyperparameter\n",
    "plot_distributions = True  # TODO hyperparameter\n",
    "standardize = False  # TODO hyperparameter\n",
    "\n",
    "input_dim = 1\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "apmlp = []\n",
    "apbm = []\n",
    "\n",
    "##% TRAINING\n",
    "if train_model:\n",
    "    model = MLPsumV2(input_dim=input_dim, latent_dim=16, output_dim=4,  # latent_dim=64\n",
    "                     k=8, dropout=0, cell_layers=1,  # k=4\n",
    "                     proj_layers=2, reduction='sum')\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-6)\n",
    "    loss_func = losses.SupConLoss(distance=distances.CosineSimilarity())\n",
    "\n",
    "    epochs = 5000\n",
    "\n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        tr_loss = 0.0\n",
    "        for points, labels in train_loader:\n",
    "            feats, _ = model(points)\n",
    "\n",
    "            tr_loss_tmp = loss_func(feats, labels)\n",
    "            tr_loss += tr_loss_tmp.item()\n",
    "\n",
    "            tr_loss_tmp.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        tr_loss /= (idx + 1)\n",
    "\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        best_val = np.inf\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            points = torch.concatenate([torch.tensor(x['data'], dtype=torch.float32) for x in val_samples])\n",
    "            labels = torch.concatenate([torch.tensor(x['label'], dtype=torch.int16) for x in val_samples])\n",
    "            feats, _ = model(points)\n",
    "\n",
    "            val_loss_tmp = loss_func(feats, labels)\n",
    "            val_loss += val_loss_tmp.item()\n",
    "\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "\n",
    "        print(f\"Epoch {e}. Training loss: {tr_loss}. Validation loss: {val_loss}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93f98bc",
   "metadata": {
    "id": "c93f98bc",
    "outputId": "58014fd4-9ffa-41cd-d6a9-60e4a9f59677"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total model mAP: 0.3337187696562696 \n",
      "Total model precision at R: 0.41666666666666663\n",
      "|   compound |       AP |   precision at R |\n",
      "|-----------:|---------:|-----------------:|\n",
      "|          6 | 0.553788 |         0.583333 |\n",
      "|          7 | 0.380853 |         0.416667 |\n",
      "|          9 | 0.221032 |         0.416667 |\n",
      "|          8 | 0.179202 |         0.25     |\n",
      "Total baseline (mean) mAP: 0.2228174603174603 \n",
      "Total baseline (mean) precision at R: 0.29166666666666663\n",
      "|   compound |        AP |   precision at R |\n",
      "|-----------:|----------:|-----------------:|\n",
      "|          6 | 0.443254  |         0.5      |\n",
      "|          9 | 0.216667  |         0.5      |\n",
      "|          7 | 0.203571  |         0.166667 |\n",
      "|          8 | 0.0277778 |         0        |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rdijk/Downloads/utils.py:335: FutureWarning: DataFrame.set_axis 'inplace' keyword is deprecated and will be removed in a future version. Use `obj = obj.set_axis(..., copy=False)` instead\n",
      "  dist.set_axis(compound_names, axis=1, inplace=True)\n",
      "/Users/rdijk/Downloads/utils.py:336: FutureWarning: DataFrame.set_axis 'inplace' keyword is deprecated and will be removed in a future version. Use `obj = obj.set_axis(..., copy=False)` instead\n",
      "  dist.set_axis(compound_names, axis=0, inplace=True)\n",
      "/Users/rdijk/Downloads/utils.py:335: FutureWarning: DataFrame.set_axis 'inplace' keyword is deprecated and will be removed in a future version. Use `obj = obj.set_axis(..., copy=False)` instead\n",
      "  dist.set_axis(compound_names, axis=1, inplace=True)\n",
      "/Users/rdijk/Downloads/utils.py:336: FutureWarning: DataFrame.set_axis 'inplace' keyword is deprecated and will be removed in a future version. Use `obj = obj.set_axis(..., copy=False)` instead\n",
      "  dist.set_axis(compound_names, axis=0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "#%% Evaluate model performance\n",
    "model.eval()\n",
    "MLP_profiles = pd.DataFrame()\n",
    "BM_profiles = pd.DataFrame()\n",
    "with torch.no_grad():\n",
    "    for idx in range(len(val_samples)):\n",
    "        points = torch.tensor(val_samples[idx]['data'], dtype=torch.float32)\n",
    "        labels = val_samples[idx]['label']\n",
    "        feats, _ = model(points)\n",
    "\n",
    "        # Append everything to dataframes\n",
    "        c1 = pd.concat([pd.DataFrame(feats), pd.Series(labels)], axis=1)\n",
    "        MLP_profiles = pd.concat([MLP_profiles, c1])\n",
    "\n",
    "        c2 = pd.concat([pd.DataFrame(points.mean(dim=1)), pd.Series(labels)], axis=1)\n",
    "        BM_profiles = pd.concat([BM_profiles, c2])\n",
    "\n",
    "MLP_profiles.columns = [f\"f{x}\" for x in range(MLP_profiles.shape[1] - 1)] + ['Metadata_labels']\n",
    "BM_profiles.columns = [f\"f{x}\" for x in range(BM_profiles.shape[1] - 1)] + ['Metadata_labels']\n",
    "AP_MLP = utils.CalculateMAP(MLP_profiles, 'cosine_similarity', groupby='Metadata_labels')\n",
    "apmlp.append(AP_MLP.AP.mean())\n",
    "print('Total model mAP:', AP_MLP.AP.mean(), '\\nTotal model precision at R:', AP_MLP['precision at R'].mean())\n",
    "print(AP_MLP.groupby(by='compound').mean().sort_values(by='AP', ascending=False).to_markdown())\n",
    "\n",
    "AP_BM = utils.CalculateMAP(BM_profiles, 'cosine_similarity', groupby='Metadata_labels')\n",
    "apbm.append(AP_BM.AP.mean())\n",
    "print('Total baseline (mean) mAP:', AP_BM.AP.mean(), '\\nTotal baseline (mean) precision at R:', AP_BM['precision at R'].mean())\n",
    "print(AP_BM.groupby(by='compound').mean().sort_values(by='AP', ascending=False).to_markdown())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcad9f7",
   "metadata": {
    "id": "4fcad9f7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
